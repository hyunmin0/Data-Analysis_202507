import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
import glob
import os
import json
import logging
import matplotlib

warnings.filterwarnings('ignore')
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
matplotlib.set_loglevel("ERROR")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Malgun Gothic']
plt.rcParams['axes.unicode_minus'] = False

class KEPCOTimeSeriesAnalyzer:
    """í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, base_path='./'):
        """
        ì´ˆê¸°í™”
        Args:
            base_path: ë°ì´í„°ê°€ ì €ì¥ëœ ê¸°ë³¸ ê²½ë¡œ
        """
        self.base_path = base_path
        self.customer_data = None
        self.lp_data = None
        self.analysis_results = {}
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = os.path.join(base_path, 'analysis_results')
        os.makedirs(self.output_dir, exist_ok=True)
        
        print("=" * 80)
        print("í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸")
        print("2ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ë° ë³€ë™ì„± ì§€í‘œ ê°œë°œ")
        print("=" * 80)
        print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {self.base_path}")
        print(f"ê²°ê³¼ ì €ì¥: {self.output_dir}")
        print()

    def load_customer_data(self, filename='ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'):
        """ì‹¤ì œ ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”©"""
        print("ğŸ”„ 1ë‹¨ê³„: ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”©...")
        
        try:
            file_path = os.path.join(self.base_path, filename)
            self.customer_data = pd.read_excel(file_path, header=1)
            
            print(f"âœ… ê³ ê° ë°ì´í„° ë¡œë”© ì™„ë£Œ")
            print(f"   - ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…")
            print(f"   - ì»¬ëŸ¼: {list(self.customer_data.columns)}")
            
            # ê³ ê° ë¶„í¬ ë¶„ì„
            contract_dist = self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts()
            usage_dist = self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts()
            
            print(f"\nğŸ“Š ê³ ê° ë¶„í¬:")
            print(f"   - ê³„ì•½ì¢…ë³„: {len(contract_dist)}ê°œ ìœ í˜•")
            print(f"   - ì‚¬ìš©ìš©ë„: {len(usage_dist)}ê°œ ìœ í˜•")
            
            self.analysis_results['customer_summary'] = {
                'total_customers': len(self.customer_data),
                'contract_types': contract_dist.to_dict(),
                'usage_types': usage_dist.to_dict()
            }
            
            return True
            
        except Exception as e:
            print(f"âŒ ê³ ê° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return False

    def load_preprocessed_data(self):
        """ì‹¤ì œ LP ë°ì´í„° ë¡œë”© (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬)"""
        print("\nğŸ”„ 2ë‹¨ê³„: LP ë°ì´í„° ë¡œë”©...")
        
        try:
            analysis_results_path = './analysis_results/analysis_results.json'
            if os.path.exists(analysis_results_path):
                with open(analysis_results_path, 'r', encoding='utf-8') as f:
                    step1_rsults = json.load(f)
                print("1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ í™•ì¸")
            else:
                print("1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ")
            
            processed_hdf5 = './analysis_results/processed_lp_data.h5'
            
            start_time = datetime.now()
            
            if os.path.exists(processed_hdf5):
                print("HDF5 íŒŒì¼ ë¡œë”©")
                try:
                    self.lp_data = pd.read_hdf(processed_hdf5, key='df')
                    loading_method = "HDF5"
                    print(" ë¡œë”© ì„±ê³µ")
                except Exception as e:
                    print(f"ë¡œë”© ì‹¤íŒ¨{e}")
            
            else:
                print("ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
                return False
            
            if 'datetime' in self.lp_data.columns:
                self.lp_data['datetime'] = pd.to_datetime(self.lp_data['datetime'])
            
            return True
        except Exception as e:
            import traceback
            traceback.print_exc()
            return False
        
    def load_external_data(self):
        print("\n ì™¸ë¶€ ë°ì´í„° ë¡œë”©")
        
        try:
            weather_file = 'weather_daily_processed.csv'
            if os.path.exists(weather_file):
                self.weather_data = pd.read_csv(weather_file)
                self.weather_data['ë‚ ì§œ'] = pd.to_datetime(self.weather_data['ë‚ ì§œ'])
            else:
                print("ê¸°ìƒ ë°ì´í„° ì—†ìŒ")
                self.weather_data = None
            
            calendar_file = 'power_analysis_calendar_2022_2025.csv'
            if os.path.exists(calendar_file):
                self.calendar_data = pd.read_csv(calendar_file)
                self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date'])
                print("ë‹¬ë ¥ ë°ì´í„° ìˆìŒ")
            else:
                print("ë‹¬ë ¥ ë°ì´í„° ì—†ìŒ")
                self.calendar_data = None
            return True
        except Exception as e:
            print("ì™¸ë¶€ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            self.weather_data = None
            self.calendar_data = None
            return False


    def _validate_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        print("\nğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        # ê¸°ë³¸ í†µê³„
        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']
        available_numeric_cols = [col for col in numeric_columns if col in self.lp_data.columns]
        
        print(f"   ğŸ“ˆ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(available_numeric_cols)}ê°œ")
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        null_counts = self.lp_data[available_numeric_cols].isnull().sum()
        total_nulls = null_counts.sum()
        
        if total_nulls > 0:
            print(f"   âš ï¸ ê²°ì¸¡ì¹˜: {total_nulls:,}ê°œ ({total_nulls/len(self.lp_data)*100:.2f}%)")
            for col, count in null_counts.items():
                if count > 0:
                    print(f"      {col}: {count:,}ê°œ")
        else:
            print("   âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ")
        
        # ì‹œê°„ ê°„ê²© ì²´í¬ (ìƒ˜í”Œ ê³ ê°ìœ¼ë¡œ)
        sample_customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]
        
        print("   â° ì‹œê°„ ê°„ê²© ê²€ì¦:")
        for customer in sample_customers:
            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('LP ìˆ˜ì‹ ì¼ì')
            
            if len(customer_data) > 1:
                time_diffs = customer_data['LP ìˆ˜ì‹ ì¼ì'].diff().dt.total_seconds() / 60
                time_diffs = time_diffs.dropna()
                
                if len(time_diffs) > 0:
                    avg_interval = time_diffs.mean()
                    std_interval = time_diffs.std()
                    print(f"      {customer}: í‰ê·  {avg_interval:.1f}ë¶„ (í‘œì¤€í¸ì°¨: {std_interval:.1f})")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results['lp_data_summary'] = {
            'total_records': len(self.lp_data),
            'customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),
            'null_counts': null_counts.to_dict(),
            'date_range': {
                'start': str(self.lp_data['LP ìˆ˜ì‹ ì¼ì'].min()),
                'end': str(self.lp_data['LP ìˆ˜ì‹ ì¼ì'].max())
            }
        }
        
        return True



    def analyze_temporal_patterns(self):
        """ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„"""
        print("\nğŸ“ˆ 3ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„...")
        print("   ğŸ• ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì¤‘...")
        
        # datetime ì»¬ëŸ¼ í™•ì¸ ë° ë³€í™˜
        if 'datetime' in self.lp_data.columns:
            datetime_col = 'datetime'
        elif 'LP ìˆ˜ì‹ ì¼ì' in self.lp_data.columns:
            datetime_col = 'LP ìˆ˜ì‹ ì¼ì'
        else:
            print("âŒ ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # datetime íƒ€ì… ë³€í™˜
        if not pd.api.types.is_datetime64_any_dtype(self.lp_data[datetime_col]):
            self.lp_data[datetime_col] = pd.to_datetime(self.lp_data[datetime_col], errors='coerce')
        
        # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
        try:
            self.lp_data['ë‚ ì§œ'] = self.lp_data[datetime_col].dt.date
            self.lp_data['ì‹œê°„'] = self.lp_data[datetime_col].dt.hour
            self.lp_data['ìš”ì¼'] = self.lp_data[datetime_col].dt.weekday
            self.lp_data['ì›”'] = self.lp_data[datetime_col].dt.month
            self.lp_data['ì£¼'] = self.lp_data[datetime_col].dt.isocalendar().week
            self.lp_data['ì£¼ë§ì—¬ë¶€'] = self.lp_data['ìš”ì¼'].isin([5, 6])
            
            print("   âœ… ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
        
        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
        print("   ğŸ“Š ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„...")
        hourly_patterns = self.lp_data.groupby('ì‹œê°„')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([
            'mean', 'std', 'min', 'max', 'count'
        ]).round(2)
        
        avg_by_hour = hourly_patterns['mean']
        peak_threshold = avg_by_hour.quantile(0.75)
        off_peak_threshold = avg_by_hour.quantile(0.25)
        
        peak_hours = avg_by_hour[avg_by_hour >= peak_threshold].index.tolist()
        off_peak_hours = avg_by_hour[avg_by_hour <= off_peak_threshold].index.tolist()
        
        # ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„
        print("   ğŸ“… ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„...")
        daily_patterns = self.lp_data.groupby('ìš”ì¼')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([
            'mean', 'std', 'count'
        ]).round(2)
        
        weekday_avg = self.lp_data[~self.lp_data['ì£¼ë§ì—¬ë¶€']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
        weekend_avg = self.lp_data[self.lp_data['ì£¼ë§ì—¬ë¶€']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
        weekend_ratio = weekend_avg / weekday_avg if weekday_avg > 0 else 0
        
        # ì›”ë³„ ê³„ì ˆì„± íŒ¨í„´
        print("   ğŸ—“ï¸ ì›”ë³„ ê³„ì ˆì„± ë¶„ì„...")
        monthly_patterns = self.lp_data.groupby('ì›”')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([
            'mean', 'std', 'count'
        ]).round(2)
        
        season_map = {12: 'ê²¨ìš¸', 1: 'ê²¨ìš¸', 2: 'ê²¨ìš¸',
                     3: 'ë´„', 4: 'ë´„', 5: 'ë´„',
                     6: 'ì—¬ë¦„', 7: 'ì—¬ë¦„', 8: 'ì—¬ë¦„',
                     9: 'ê°€ì„', 10: 'ê°€ì„', 11: 'ê°€ì„'}
        
        self.lp_data['ê³„ì ˆ'] = self.lp_data['ì›”'].map(season_map)
        seasonal_patterns = self.lp_data.groupby('ê³„ì ˆ')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([
            'mean', 'std', 'count'
        ]).round(2)
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results['temporal_patterns'] = {
            'hourly_patterns': hourly_patterns.to_dict(),
            'daily_patterns': daily_patterns.to_dict(),
            'monthly_patterns': monthly_patterns.to_dict(),
            'seasonal_patterns': seasonal_patterns.to_dict(),
            'peak_hours': peak_hours,
            'off_peak_hours': off_peak_hours,
            'weekend_ratio': weekend_ratio
        }
        
        return True

    def analyze_volatility_indicators(self):
        """ë³€ë™ì„± ì§€í‘œ ë¶„ì„ (ì§‘ê³„ ì¤‘ì‹¬)"""
        print("\nğŸ“Š 4ë‹¨ê³„: ë³€ë™ì„± ì§€í‘œ ë¶„ì„...")
        
        customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        print(f"   ğŸ”„ {len(customers)}ëª… ê³ ê° ë³€ë™ì„± ë¶„ì„ ì¤‘...")
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì§‘ê³„ ë¶„ì„
        
        # 1. ì „ì²´ ë³€ë™ì„± í†µê³„
        overall_power = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
        overall_cv = overall_power.std() / overall_power.mean() if overall_power.mean() > 0 else 0
        
        print(f"   ğŸ“ˆ ì „ì²´ ë°ì´í„° ë³€ë™ì„±:")
        print(f"      ì „ì²´ ë³€ë™ê³„ìˆ˜: {overall_cv:.4f}")
        print(f"      í‰ê·  ì „ë ¥: {overall_power.mean():.2f}kW")
        print(f"      í‘œì¤€í¸ì°¨: {overall_power.std():.2f}kW")
        
        # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŒ¨í„´
        hourly_volatility = self.lp_data.groupby('ì‹œê°„')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([
            'mean', 'std', 'count'
        ])
        hourly_volatility['cv'] = hourly_volatility['std'] / hourly_volatility['mean']
        
        print(f"\n   â° ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŒ¨í„´:")
        high_volatility_hours = hourly_volatility.nlargest(3, 'cv').index.tolist()
        low_volatility_hours = hourly_volatility.nsmallest(3, 'cv').index.tolist()
        print(f"      ê³ ë³€ë™ì„± ì‹œê°„ëŒ€: {high_volatility_hours}ì‹œ (CV: {hourly_volatility.loc[high_volatility_hours, 'cv'].mean():.4f})")
        print(f"      ì €ë³€ë™ì„± ì‹œê°„ëŒ€: {low_volatility_hours}ì‹œ (CV: {hourly_volatility.loc[low_volatility_hours, 'cv'].mean():.4f})")
        
        # 3. ìš”ì¼ë³„ ë³€ë™ì„± íŒ¨í„´
        daily_volatility = self.lp_data.groupby('ìš”ì¼')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([
            'mean', 'std', 'count'
        ])
        daily_volatility['cv'] = daily_volatility['std'] / daily_volatility['mean']
        
        weekday_cv = daily_volatility.loc[0:4, 'cv'].mean()  # ì›”-ê¸ˆ
        weekend_cv = daily_volatility.loc[5:6, 'cv'].mean()  # í† -ì¼
        
        print(f"\n   ğŸ“… ìš”ì¼ë³„ ë³€ë™ì„± íŒ¨í„´:")
        print(f"      í‰ì¼ í‰ê·  ë³€ë™ê³„ìˆ˜: {weekday_cv:.4f}")
        print(f"      ì£¼ë§ í‰ê·  ë³€ë™ê³„ìˆ˜: {weekend_cv:.4f}")
        print(f"      ì£¼ë§/í‰ì¼ ë³€ë™ì„± ë¹„ìœ¨: {weekend_cv/weekday_cv:.3f}")
        
        # 4. ì›”ë³„ ë³€ë™ì„± íŒ¨í„´
        monthly_volatility = self.lp_data.groupby('ì›”')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([
            'mean', 'std', 'count'
        ])
        monthly_volatility['cv'] = monthly_volatility['std'] / monthly_volatility['mean']
        
        print(f"\n   ğŸ—“ï¸ ì›”ë³„ ë³€ë™ì„± íŒ¨í„´:")
        high_var_months = monthly_volatility.nlargest(2, 'cv').index.tolist()
        low_var_months = monthly_volatility.nsmallest(2, 'cv').index.tolist()
        print(f"      ê³ ë³€ë™ì„± ì›”: {high_var_months}ì›”")
        print(f"      ì €ë³€ë™ì„± ì›”: {low_var_months}ì›”")
        
        # 5. ê³ ê°ë³„ ë³€ë™ì„± ë¶„í¬ (ìš”ì•½ í†µê³„ë§Œ)
        print(f"\n   ğŸ‘¥ ê³ ê°ë³„ ë³€ë™ì„± ë¶„í¬ ë¶„ì„...")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        chunk_size = 100
        customer_cvs = []
        
        for i in range(0, len(customers), chunk_size):
            chunk_customers = customers[i:i+chunk_size]
            if (i // chunk_size + 1) % 5 == 0:  # 500ëª…ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                print(f"      ì§„í–‰: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)")
            
            for customer in chunk_customers:
                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]
                power_series = customer_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                
                if len(power_series) >= 96 and power_series.mean() > 0:  # ìµœì†Œ 1ì¼ ë°ì´í„°
                    cv = power_series.std() / power_series.mean()
                    customer_cvs.append(cv)
        
        # ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë¶„í¬ í†µê³„
        cv_array = np.array(customer_cvs)
        cv_percentiles = np.percentile(cv_array, [10, 25, 50, 75, 90])
        
        print(f"   ğŸ“Š ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë¶„í¬ ({len(customer_cvs)}ëª…):")
        print(f"      í‰ê· : {cv_array.mean():.4f}")
        print(f"      í‘œì¤€í¸ì°¨: {cv_array.std():.4f}")
        print(f"      10%ile: {cv_percentiles[0]:.4f}")
        print(f"      25%ile: {cv_percentiles[1]:.4f}")
        print(f"      50%ile: {cv_percentiles[2]:.4f}")
        print(f"      75%ile: {cv_percentiles[3]:.4f}")
        print(f"      90%ile: {cv_percentiles[4]:.4f}")
        
        # ë³€ë™ê³„ìˆ˜ êµ¬ê°„ë³„ ê³ ê° ìˆ˜
        cv_bins = [0, 0.1, 0.2, 0.3, 0.5, 1.0, float('inf')]
        cv_labels = ['ë§¤ìš° ì•ˆì • (<0.1)', 'ì•ˆì • (0.1-0.2)', 'ë³´í†µ (0.2-0.3)', 
                    'ë†’ìŒ (0.3-0.5)', 'ë§¤ìš° ë†’ìŒ (0.5-1.0)', 'ê·¹íˆ ë†’ìŒ (>1.0)']
        
        cv_counts = pd.cut(cv_array, bins=cv_bins, labels=cv_labels, include_lowest=True).value_counts()
        
        print(f"\n   ğŸ¯ ë³€ë™ì„± ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬:")
        for grade, count in cv_counts.items():
            percentage = count / len(customer_cvs) * 100
            print(f"      {grade}: {count}ëª… ({percentage:.1f}%)")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results['volatility_analysis'] = {
            'overall_cv': overall_cv,
            'hourly_volatility': hourly_volatility.to_dict(),
            'daily_volatility': daily_volatility.to_dict(),
            'monthly_volatility': monthly_volatility.to_dict(),
            'customer_cv_stats': {
                'count': len(customer_cvs),
                'mean': float(cv_array.mean()),
                'std': float(cv_array.std()),
                'percentiles': {
                    '10%': float(cv_percentiles[0]),
                    '25%': float(cv_percentiles[1]),
                    '50%': float(cv_percentiles[2]),
                    '75%': float(cv_percentiles[3]),
                    '90%': float(cv_percentiles[4])
                }
            },
            'volatility_distribution': cv_counts.to_dict()
        }
        
        # ìš”ì•½ ë°ì´í„°ë§Œ CSVë¡œ ì €ì¥ (ê°œë³„ ê³ ê° ë°ì´í„°ëŠ” ì œì™¸)
        summary_data = {
            'metric': ['overall_cv', 'weekday_cv', 'weekend_cv', 'customer_cv_mean', 
                      'customer_cv_std', 'customer_cv_median'],
            'value': [overall_cv, weekday_cv, weekend_cv, cv_array.mean(), 
                     cv_array.std(), cv_percentiles[2]]
        }
        
        summary_df = pd.DataFrame(summary_data)
        output_file = os.path.join(self.output_dir, 'volatility_summary.csv')
        summary_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\n   ğŸ’¾ ë³€ë™ì„± ìš”ì•½ ì €ì¥: {output_file}")
        
        return cv_array

    def detect_anomalies(self):
        """ì´ìƒ íŒ¨í„´ íƒì§€ (ì§‘ê³„ ì¤‘ì‹¬)"""
        print("\nğŸš¨ 5ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€...")
        
        customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        print(f"   ğŸ” {len(customers)}ëª… ê³ ê° ì´ìƒ íŒ¨í„´ íƒì§€ ì¤‘...")
        
        # ì „ì²´ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íŒ¨í„´ íƒì§€
        
        # 1. ì „ì²´ ë°ì´í„°ì˜ í†µê³„ì  ì´ìƒì¹˜ ì„ê³„ê°’ ì„¤ì •
        overall_power = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
        q1, q3 = overall_power.quantile([0.25, 0.75])
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        # ì „ì²´ í†µê³„ì  ì´ìƒì¹˜
        total_outliers = ((overall_power < lower_bound) | (overall_power > upper_bound)).sum()
        outlier_rate = total_outliers / len(overall_power) * 100
        
        print(f"   ğŸ“Š ì „ì²´ ë°ì´í„° ì´ìƒì¹˜ í˜„í™©:")
        print(f"      í†µê³„ì  ì´ìƒì¹˜: {total_outliers:,}ê°œ ({outlier_rate:.2f}%)")
        print(f"      ì •ìƒ ë²”ìœ„: {lower_bound:.1f} ~ {upper_bound:.1f}kW")
        
        # 2. ì‹œê°„ëŒ€ë³„ ì´ìƒ íŒ¨í„´
        night_hours = [0, 1, 2, 3, 4, 5]  # ì•¼ê°„ ì‹œê°„ëŒ€
        day_hours = [9, 10, 11, 12, 13, 14, 15, 16, 17]  # ì£¼ê°„ ì‹œê°„ëŒ€
        
        night_data = self.lp_data[self.lp_data['ì‹œê°„'].isin(night_hours)]
        day_data = self.lp_data[self.lp_data['ì‹œê°„'].isin(day_hours)]
        
        night_avg = night_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
        day_avg = day_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
        night_day_ratio = night_avg / day_avg if day_avg > 0 else 0
        
        print(f"\n   ğŸŒ™ ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´:")
        print(f"      ì•¼ê°„ í‰ê· : {night_avg:.2f}kW")
        print(f"      ì£¼ê°„ í‰ê· : {day_avg:.2f}kW")
        print(f"      ì•¼ê°„/ì£¼ê°„ ë¹„ìœ¨: {night_day_ratio:.3f}")
        
        # 3. 0ê°’ íŒ¨í„´ ë¶„ì„
        zero_count = (overall_power == 0).sum()
        zero_rate = zero_count / len(overall_power) * 100
        
        print(f"\n   âš« 0ê°’ íŒ¨í„´ ë¶„ì„:")
        print(f"      0ê°’ ì¸¡ì •: {zero_count:,}ê°œ ({zero_rate:.2f}%)")
        
        # 4. ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)
        power_changes = self.lp_data.sort_values(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LP ìˆ˜ì‹ ì¼ì'])['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].pct_change().abs()
        sudden_changes = power_changes[power_changes > 2.0]  # 200% ì´ìƒ ë³€í™”
        sudden_change_rate = len(sudden_changes) / len(power_changes.dropna()) * 100
        
        print(f"\n   âš¡ ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´:")
        print(f"      ê¸‰ê²©í•œ ë³€í™”: {len(sudden_changes):,}ê±´ ({sudden_change_rate:.2f}%)")
        
        # 5. ê³ ê°ë³„ ì´ìƒ íŒ¨í„´ ìš”ì•½ í†µê³„ (ê°œë³„ ì¶œë ¥ ì—†ì´)
        anomaly_customers = {
            'high_night_usage': 0,      # ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš©
            'excessive_zeros': 0,        # ê³¼ë„í•œ 0ê°’
            'high_volatility': 0,        # ë†’ì€ ë³€ë™ì„±
            'statistical_outliers': 0    # í†µê³„ì  ì´ìƒì¹˜ ë‹¤ìˆ˜
        }
        
        chunk_size = 100
        processed_customers = 0
        
        for i in range(0, len(customers), chunk_size):
            chunk_customers = customers[i:i+chunk_size]
            if (i // chunk_size + 1) % 5 == 0:
                print(f"      ì§„í–‰: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)")
            
            for customer in chunk_customers:
                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]
                power_series = customer_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                
                if len(power_series) < 96:  # ìµœì†Œ 1ì¼ ë°ì´í„° í•„ìš”
                    continue
                
                processed_customers += 1
                
                # ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš© ì²´í¬
                customer_night = customer_data[customer_data['ì‹œê°„'].isin(night_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
                customer_day = customer_data[customer_data['ì‹œê°„'].isin(day_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
                if customer_day > 0 and customer_night / customer_day > 0.8:
                    anomaly_customers['high_night_usage'] += 1
                
                # ê³¼ë„í•œ 0ê°’ ì²´í¬
                zero_ratio = (power_series == 0).sum() / len(power_series)
                if zero_ratio > 0.1:  # 10% ì´ìƒì´ 0ê°’
                    anomaly_customers['excessive_zeros'] += 1
                
                # ë†’ì€ ë³€ë™ì„± ì²´í¬
                if power_series.mean() > 0:
                    cv = power_series.std() / power_series.mean()
                    if cv > 1.0:  # ë³€ë™ê³„ìˆ˜ 1.0 ì´ìƒ
                        anomaly_customers['high_volatility'] += 1
                
                # í†µê³„ì  ì´ìƒì¹˜ ë‹¤ìˆ˜ ì²´í¬
                customer_outliers = ((power_series < lower_bound) | (power_series > upper_bound)).sum()
                outlier_ratio = customer_outliers / len(power_series)
                if outlier_ratio > 0.05:  # 5% ì´ìƒì´ ì´ìƒì¹˜
                    anomaly_customers['statistical_outliers'] += 1
        
        # ì¢…í•© ì´ìƒ íŒ¨í„´ ê³ ê° (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì‹¤ì œë¡œëŠ” ê·¼ì‚¬ì¹˜)
        total_anomaly_customers = max(anomaly_customers.values())  # ë‹¨ìˆœ ê·¼ì‚¬
        anomaly_rate = total_anomaly_customers / processed_customers * 100 if processed_customers > 0 else 0
        
        print(f"\n   ğŸ“Š ì´ìƒ íŒ¨í„´ ê³ ê° ìš”ì•½ ({processed_customers}ëª… ë¶„ì„):")
        print(f"      ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš©: {anomaly_customers['high_night_usage']}ëª…")
        print(f"      ê³¼ë„í•œ 0ê°’: {anomaly_customers['excessive_zeros']}ëª…")
        print(f"      ë†’ì€ ë³€ë™ì„±: {anomaly_customers['high_volatility']}ëª…")
        print(f"      í†µê³„ì  ì´ìƒì¹˜ ë‹¤ìˆ˜: {anomaly_customers['statistical_outliers']}ëª…")
        print(f"      ì „ì²´ ì´ìƒ íŒ¨í„´ ë¹„ìœ¨: ì•½ {anomaly_rate:.1f}%")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results['anomaly_analysis'] = {
            'processed_customers': processed_customers,
            'total_outliers': int(total_outliers),
            'outlier_rate': float(outlier_rate),
            'zero_count': int(zero_count),
            'zero_rate': float(zero_rate),
            'sudden_changes': len(sudden_changes),
            'sudden_change_rate': float(sudden_change_rate),
            'night_day_ratio': float(night_day_ratio),
            'anomaly_customers': anomaly_customers,
            'estimated_anomaly_rate': float(anomaly_rate)
        }
        
        return anomaly_customers


    def create_summary_visualizations(self):
        """ìš”ì•½ ì‹œê°í™” ìƒì„± (ì§‘ê³„ ë°ì´í„° ì¤‘ì‹¬)"""
        print("\nğŸ“Š 6ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì‹œê°í™”...")
        
        try:
            # 1. ì‹œê°„ëŒ€ë³„/ìš”ì¼ë³„ íŒ¨í„´ ì‹œê°í™”
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
            hourly_avg = self.lp_data.groupby('ì‹œê°„')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, color='blue')
            axes[0, 0].set_title('ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('ì‹œê°„')
            axes[0, 0].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].set_xticks(range(0, 24, 3))
            
            # ìš”ì¼ë³„ íŒ¨í„´
            daily_avg = self.lp_data.groupby('ìš”ì¼')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
            axes[0, 1].bar(range(len(daily_avg)), daily_avg.values, color='skyblue')
            axes[0, 1].set_title('ìš”ì¼ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰', fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('ìš”ì¼')
            axes[0, 1].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')
            axes[0, 1].set_xticks(range(7))
            axes[0, 1].set_xticklabels(weekday_names)
            axes[0, 1].grid(True, alpha=0.3, axis='y')
            
            # ì‹œê°„ëŒ€ë³„ ë³€ë™ì„±
            hourly_std = self.lp_data.groupby('ì‹œê°„')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].std()
            axes[1, 0].plot(hourly_std.index, hourly_std.values, marker='s', linewidth=2, color='red')
            axes[1, 0].set_title('ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë³€ë™ì„±', fontsize=14, fontweight='bold')
            axes[1, 0].set_xlabel('ì‹œê°„')
            axes[1, 0].set_ylabel('í‘œì¤€í¸ì°¨ (kW)')
            axes[1, 0].grid(True, alpha=0.3)
            axes[1, 0].set_xticks(range(0, 24, 3))
            
            # ì›”ë³„ ê³„ì ˆì„± íŒ¨í„´
            monthly_avg = self.lp_data.groupby('ì›”')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            axes[1, 1].plot(monthly_avg.index, monthly_avg.values, marker='s', linewidth=2, color='orange')
            axes[1, 1].set_title('ì›”ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (ê³„ì ˆì„±)', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel('ì›”')
            axes[1, 1].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].set_xticks(range(1, 13))
            
            plt.tight_layout()
            
            # ì´ë¯¸ì§€ ì €ì¥
            output_file = os.path.join(self.output_dir, 'temporal_patterns_summary.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"   ğŸ’¾ ì‹œê³„ì—´ íŒ¨í„´ ì‹œê°í™” ì €ì¥: {output_file}")
            
            # 2. ë³€ë™ì„± ë° ì´ìƒì¹˜ ë¶„í¬ ì‹œê°í™”
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # ì „ì²´ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë¶„í¬
            axes[0, 0].hist(self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'], bins=50, alpha=0.7, color='lightblue')
            axes[0, 0].set_title('ì „ë ¥ ì‚¬ìš©ëŸ‰ ë¶„í¬', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('ìˆœë°©í–¥ ìœ íš¨ì „ë ¥ (kW)')
            axes[0, 0].set_ylabel('ë¹ˆë„')
            axes[0, 0].grid(True, alpha=0.3, axis='y')
            
            # ì‹œê°„ëŒ€ë³„ ë³€ë™ê³„ìˆ˜
            hourly_volatility = self.analysis_results.get('volatility_analysis', {}).get('hourly_volatility', {})
            if hourly_volatility and 'cv' in hourly_volatility:
                cv_data = hourly_volatility['cv']
                hours = list(cv_data.keys())
                cv_values = list(cv_data.values())
                axes[0, 1].bar(hours, cv_values, color='lightgreen')
                axes[0, 1].set_title('ì‹œê°„ëŒ€ë³„ ë³€ë™ê³„ìˆ˜', fontsize=14, fontweight='bold')
                axes[0, 1].set_xlabel('ì‹œê°„')
                axes[0, 1].set_ylabel('ë³€ë™ê³„ìˆ˜')
                axes[0, 1].grid(True, alpha=0.3, axis='y')
            
            # ìš”ì¼ë³„ ë³€ë™ê³„ìˆ˜
            daily_volatility = self.analysis_results.get('volatility_analysis', {}).get('daily_volatility', {})
            if daily_volatility and 'cv' in daily_volatility:
                cv_data = daily_volatility['cv']
                weekdays = list(cv_data.keys())
                cv_values = list(cv_data.values())
                weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
                axes[1, 0].bar(range(len(cv_values)), cv_values, color='purple')
                axes[1, 0].set_title('ìš”ì¼ë³„ ë³€ë™ê³„ìˆ˜', fontsize=14, fontweight='bold')
                axes[1, 0].set_xlabel('ìš”ì¼')
                axes[1, 0].set_ylabel('ë³€ë™ê³„ìˆ˜')
                axes[1, 0].set_xticks(range(7))
                axes[1, 0].set_xticklabels(weekday_names)
                axes[1, 0].grid(True, alpha=0.3, axis='y')
            
            # ì›”ë³„ ë³€ë™ê³„ìˆ˜
            monthly_volatility = self.analysis_results.get('volatility_analysis', {}).get('monthly_volatility', {})
            if monthly_volatility and 'cv' in monthly_volatility:
                cv_data = monthly_volatility['cv']
                months = list(cv_data.keys())
                cv_values = list(cv_data.values())
                axes[1, 1].plot(months, cv_values, marker='o', linewidth=2, color='red')
                axes[1, 1].set_title('ì›”ë³„ ë³€ë™ê³„ìˆ˜ (ê³„ì ˆì„±)', fontsize=14, fontweight='bold')
                axes[1, 1].set_xlabel('ì›”')
                axes[1, 1].set_ylabel('ë³€ë™ê³„ìˆ˜')
                axes[1, 1].grid(True, alpha=0.3)
                axes[1, 1].set_xticks(range(1, 13))
            
            plt.tight_layout()
            
            # ì´ë¯¸ì§€ ì €ì¥
            output_file = os.path.join(self.output_dir, 'volatility_analysis_summary.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"   ğŸ’¾ ë³€ë™ì„± ë¶„ì„ ì‹œê°í™” ì €ì¥: {output_file}")
            
            # 3. ë³€ë™ì„± ë“±ê¸‰ë³„ ë¶„í¬ ì‹œê°í™” (ìˆëŠ” ê²½ìš°)
            volatility_dist = self.analysis_results.get('volatility_analysis', {}).get('volatility_distribution', {})
            if volatility_dist:
                fig, ax = plt.subplots(1, 1, figsize=(12, 8))
                
                grades = list(volatility_dist.keys())
                counts = list(volatility_dist.values())
                
                bars = ax.bar(range(len(grades)), counts, color=['green', 'lightgreen', 'yellow', 'orange', 'red', 'darkred'])
                ax.set_title('ë³€ë™ì„± ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬', fontsize=16, fontweight='bold')
                ax.set_xlabel('ë³€ë™ì„± ë“±ê¸‰', fontsize=12)
                ax.set_ylabel('ê³ ê° ìˆ˜', fontsize=12)
                ax.set_xticks(range(len(grades)))
                ax.set_xticklabels(grades, rotation=45, ha='right')
                ax.grid(True, alpha=0.3, axis='y')
                
                # ê° ë§‰ëŒ€ ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
                for i, (bar, count) in enumerate(zip(bars, counts)):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{count}ëª…', ha='center', va='bottom', fontweight='bold')
                
                plt.tight_layout()
                
                # ì´ë¯¸ì§€ ì €ì¥
                output_file = os.path.join(self.output_dir, 'volatility_distribution.png')
                plt.savefig(output_file, dpi=300, bbox_inches='tight')
                plt.close()
                print(f"   ğŸ’¾ ë³€ë™ì„± ë¶„í¬ ì‹œê°í™” ì €ì¥: {output_file}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def create_summary_visualizations(self):
        """ìš”ì•½ ì‹œê°í™” ìƒì„±"""
        print("\nğŸ“Š 6ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì‹œê°í™”...")
        
        try:
            # 1. ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš© íŒ¨í„´
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
            hourly_avg = self.lp_data.groupby('ì‹œê°„')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2)
            axes[0, 0].set_title('ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('ì‹œê°„')
            axes[0, 0].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')
            axes[0, 0].grid(True, alpha=0.3)
            axes[0, 0].set_xticks(range(0, 24, 3))
            
            # ìš”ì¼ë³„ íŒ¨í„´
            daily_avg = self.lp_data.groupby('ìš”ì¼')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
            axes[0, 1].bar(range(len(daily_avg)), daily_avg.values, color='skyblue')
            axes[0, 1].set_title('ìš”ì¼ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰', fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('ìš”ì¼')
            axes[0, 1].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')
            axes[0, 1].set_xticks(range(7))
            axes[0, 1].set_xticklabels(weekday_names)
            axes[0, 1].grid(True, alpha=0.3, axis='y')
            
            # ë³€ë™ê³„ìˆ˜ ë¶„í¬ (ë³€ë™ì„± ë¶„ì„ì´ ì™„ë£Œëœ ê²½ìš°)
            if 'volatility_analysis' in self.analysis_results:
                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')
                if os.path.exists(volatility_file):
                    volatility_df = pd.read_csv(volatility_file)
                    axes[1, 0].hist(volatility_df['cv_basic'].dropna(), bins=30, alpha=0.7, color='lightgreen')
                    axes[1, 0].set_title('ë³€ë™ê³„ìˆ˜ ë¶„í¬', fontsize=14, fontweight='bold')
                    axes[1, 0].set_xlabel('ë³€ë™ê³„ìˆ˜ (CV)')
                    axes[1, 0].set_ylabel('ê³ ê° ìˆ˜')
                    axes[1, 0].grid(True, alpha=0.3, axis='y')
            
            # ì›”ë³„ ê³„ì ˆì„± íŒ¨í„´
            monthly_avg = self.lp_data.groupby('ì›”')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            month_names = ['1ì›”', '2ì›”', '3ì›”', '4ì›”', '5ì›”', '6ì›”', 
                          '7ì›”', '8ì›”', '9ì›”', '10ì›”', '11ì›”', '12ì›”']
            axes[1, 1].plot(monthly_avg.index, monthly_avg.values, marker='s', linewidth=2, color='orange')
            axes[1, 1].set_title('ì›”ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (ê³„ì ˆì„±)', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel('ì›”')
            axes[1, 1].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')
            axes[1, 1].grid(True, alpha=0.3)
            axes[1, 1].set_xticks(range(1, 13))
            
            plt.tight_layout()
            
            # ì´ë¯¸ì§€ ì €ì¥
            output_file = os.path.join(self.output_dir, 'temporal_patterns_summary.png')
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"   ğŸ’¾ ì‹œê³„ì—´ íŒ¨í„´ ì‹œê°í™” ì €ì¥: {output_file}")
            
            # 2. ë³€ë™ì„± ê´€ë ¨ ì‹œê°í™” (ì¶”ê°€)
            if 'volatility_analysis' in self.analysis_results:
                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')
                if os.path.exists(volatility_file):
                    volatility_df = pd.read_csv(volatility_file)
                    
                    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
                    
                    # í‰ê·  ì‚¬ìš©ëŸ‰ vs ë³€ë™ê³„ìˆ˜
                    axes[0, 0].scatter(volatility_df['mean_power'], volatility_df['cv_basic'], alpha=0.6, s=20)
                    axes[0, 0].set_title('í‰ê·  ì‚¬ìš©ëŸ‰ vs ë³€ë™ê³„ìˆ˜', fontsize=14, fontweight='bold')
                    axes[0, 0].set_xlabel('í‰ê·  ì „ë ¥ (kW)')
                    axes[0, 0].set_ylabel('ë³€ë™ê³„ìˆ˜')
                    axes[0, 0].grid(True, alpha=0.3)
                    
                    # ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± vs ì¼ë³„ ë³€ë™ì„±
                    axes[0, 1].scatter(volatility_df['hourly_cv_mean'], volatility_df['daily_cv_mean'], alpha=0.6, s=20, color='red')
                    axes[0, 1].set_title('ì‹œê°„ëŒ€ë³„ vs ì¼ë³„ ë³€ë™ì„±', fontsize=14, fontweight='bold')
                    axes[0, 1].set_xlabel('ì‹œê°„ëŒ€ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜')
                    axes[0, 1].set_ylabel('ì¼ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜')
                    axes[0, 1].grid(True, alpha=0.3)
                    
                    # ì£¼ë§/í‰ì¼ ë³€ë™ê³„ìˆ˜ ë¹„êµ
                    weekend_weekday_ratio = volatility_df['weekend_weekday_cv_ratio'].dropna()
                    axes[1, 0].hist(weekend_weekday_ratio, bins=20, alpha=0.7, color='purple')
                    axes[1, 0].set_title('ì£¼ë§/í‰ì¼ ë³€ë™ê³„ìˆ˜ ë¹„ìœ¨ ë¶„í¬', fontsize=14, fontweight='bold')
                    axes[1, 0].set_xlabel('ì£¼ë§/í‰ì¼ ë³€ë™ê³„ìˆ˜ ë¹„ìœ¨')
                    axes[1, 0].set_ylabel('ê³ ê° ìˆ˜')
                    axes[1, 0].grid(True, alpha=0.3, axis='y')
                    
                    # ë³€ë™ê³„ìˆ˜ ìƒìœ„/í•˜ìœ„ ë¶„í¬
                    cv_top10 = volatility_df.nlargest(10, 'cv_basic')['cv_basic']
                    cv_bottom10 = volatility_df.nsmallest(10, 'cv_basic')['cv_basic']
                    
                    x_pos = range(10)
                    width = 0.35
                    axes[1, 1].bar([x - width/2 for x in x_pos], cv_top10.values, width, 
                                  label='ìƒìœ„ 10ëª…', alpha=0.8, color='red')
                    axes[1, 1].bar([x + width/2 for x in x_pos], cv_bottom10.values, width, 
                                  label='í•˜ìœ„ 10ëª…', alpha=0.8, color='blue')
                    axes[1, 1].set_title('ë³€ë™ê³„ìˆ˜ ìƒìœ„/í•˜ìœ„ 10ëª… ë¹„êµ', fontsize=14, fontweight='bold')
                    axes[1, 1].set_xlabel('ìˆœìœ„')
                    axes[1, 1].set_ylabel('ë³€ë™ê³„ìˆ˜')
                    axes[1, 1].legend()
                    axes[1, 1].grid(True, alpha=0.3, axis='y')
                    
                    plt.tight_layout()
                    
                    # ì´ë¯¸ì§€ ì €ì¥
                    output_file = os.path.join(self.output_dir, 'volatility_analysis_summary.png')
                    plt.savefig(output_file, dpi=300, bbox_inches='tight')
                    plt.close()
                    print(f"   ğŸ’¾ ë³€ë™ì„± ë¶„ì„ ì‹œê°í™” ì €ì¥: {output_file}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
            return False
        

    def save_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        print("\nğŸ’¾ 8ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì €ì¥...")
        
        try:
            # JSONìœ¼ë¡œ ì €ì¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            results_for_json = {}
            
            for key, value in self.analysis_results.items():
                if isinstance(value, dict):
                    results_for_json[key] = {}
                    for sub_key, sub_value in value.items():
                        if hasattr(sub_value, 'to_dict'):  # pandas ê°ì²´ì¸ ê²½ìš°
                            results_for_json[key][sub_key] = sub_value.to_dict()
                        else:
                            results_for_json[key][sub_key] = sub_value
                else:
                    results_for_json[key] = value
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            output_file = os.path.join(self.output_dir, 'analysis_results2.json')
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results_for_json, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"   ğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {output_file}")
            return True
            
        except Exception as e:
            print(f"   âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def run_complete_analysis(self):
        """ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        start_time = datetime.now()
        
        print("ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹œì‘")
        print(f"ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        try:
            # 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©
            if not self.load_preprocessed_data():
                print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ë¡œ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return False
            
            # 2. ì™¸ë¶€ ë°ì´í„° ë¡œë”©
            self.load_external_data()
            
            # 3. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„
            if not self.analyze_temporal_patterns():
                print("âŒ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨")
                return False
            
            # 4. ë³€ë™ì„± ì§€í‘œ ë¶„ì„
            cv_array = self.analyze_volatility_indicators()
            if cv_array is None or len(cv_array) == 0:
                print("âŒ ë³€ë™ì„± ì§€í‘œ ë¶„ì„ ì‹¤íŒ¨")
                return False
            
            # 5. ì´ìƒ íŒ¨í„´ íƒì§€
            anomaly_summary = self.detect_anomalies()
            if anomaly_summary is None:
                print("âŒ ì´ìƒ íŒ¨í„´ íƒì§€ ì‹¤íŒ¨")
                return False
            
            # 6. ì‹œê°í™” ìƒì„±
            self.create_summary_visualizations()
            
            
            # 7. ê²°ê³¼ ì €ì¥
            self.save_analysis_results()
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            print("\n" + "=" * 80)
            print("ğŸ‰ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!")
            print("=" * 80)
            print(f"ì†Œìš” ì‹œê°„: {duration}")
            print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return False

if __name__ == "__main__":
    analyzer = KEPCOTimeSeriesAnalyzer()
    
    success = analyzer.run_complete_analysis()
    
    if success:
        print("ì„±ê³µ")