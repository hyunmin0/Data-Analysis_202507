{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55944dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\n",
      "데이터안심구역 전용 - 실제 데이터 분석\n",
      "============================================================\n",
      "\\n[1단계] 고객 기본정보 로딩 및 분석\n",
      "=== 고객 기본정보 로딩 ===\n",
      "총 고객 수: 200명\n",
      "컬럼: ['순번', '고객번호', '계약전력', '계약종별', '사용용도', '주생산품', '산업분류']\n",
      "\\n기본 정보:\n",
      "   순번   고객번호     계약전력            계약종별    사용용도 주생산품                      산업분류\n",
      "0   1  A1001    1~199  322 산업용(갑)‖고압A  02 상업용   병원  721건축기술,엔지니어링및기타과학기술서비스업\n",
      "1   2  A1002  400~499  222 일반용(갑)‖고압A  02 상업용   교회               241제1차철강제조업\n",
      "2   3  A1003  400~499  222 일반용(갑)‖고압A  02 상업용   병원                 681부동산임대업\n",
      "3   4  A1004  500~599  322 산업용(갑)‖고압A  02 상업용   상가               241제1차철강제조업\n",
      "4   5  A1005  700~799  726 산업용(을) 고압A  02 상업용   상가            631창고및운송관련서비스업\n",
      "\\n=== 고객 분포 분석 ===\n",
      "\\n📊 계약종별 분포:\n",
      "  226 일반용(을) 고압A: 50명 (25.0%)\n",
      "  322 산업용(갑)‖고압A: 41명 (20.5%)\n",
      "  311 산업용(갑) 저압: 39명 (19.5%)\n",
      "  222 일반용(갑)‖고압A: 37명 (18.5%)\n",
      "  726 산업용(을) 고압A: 33명 (16.5%)\n",
      "\\n🏭 사용용도별 분포:\n",
      "  02 상업용: 105명 (52.5%)\n",
      "  09 광공업용: 95명 (47.5%)\n",
      "\\n⚡ 계약전력 분포:\n",
      "count       200\n",
      "unique        9\n",
      "top       1~199\n",
      "freq         27\n",
      "Name: 계약전력, dtype: object\n",
      "\\n[2단계] LP 데이터 로딩 및 품질 분석\n",
      "\\n=== LP 데이터 로딩 ===\n",
      "발견된 LP 파일 수: 4개\n",
      "   [1/4] processed_LPData_20220301_15.csv 처리 중...\n",
      "      레코드: 14,400개, 고객: 10명\n",
      "   [2/4] processed_LPData_20220316_31.csv 처리 중...\n",
      "      레코드: 14,400개, 고객: 10명\n",
      "   [3/4] processed_LPData_20220401_15.csv 처리 중...\n",
      "      레코드: 14,400개, 고객: 10명\n",
      "   [4/4] processed_LPData_20220416_29.csv 처리 중...\n",
      "      레코드: 14,400개, 고객: 10명\n",
      "\\n✅ 전체 LP 데이터 결합 완료:\n",
      "  총 레코드: 57,600\n",
      "  총 고객: 10\n",
      "   - 기간: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "\n",
      "=== LP 데이터 품질 분석 ===\n",
      "📈 기본 통계:\n",
      "           순방향 유효전력          지상무효          진상무효          피상전력\n",
      "count  57600.000000  57600.000000  57600.000000  57600.000000\n",
      "mean      53.658823      7.500729      4.015514     54.492017\n",
      "std       52.880036     12.419111      8.295770     53.722780\n",
      "min        0.200000      0.000000      0.000000      0.200000\n",
      "25%        7.575000      0.000000      0.000000      7.700000\n",
      "50%       37.500000      1.400000      0.000000     38.100000\n",
      "75%       88.300000     10.100000      3.600000     89.900000\n",
      "max      284.500000    107.100000     77.300000    291.200000\n",
      "\n",
      "⏰ 시간 간격 체크:\n",
      "  A1001: 평균 간격 7.5분, 표준편차 7.5분\n",
      "  A1002: 평균 간격 7.5분, 표준편차 7.5분\n",
      "  A1003: 평균 간격 7.5분, 표준편차 7.5분\n",
      "\n",
      "🔍 데이터 품질 체크:\n",
      "  순방향 유효전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "  지상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 18118건 (31.45%)\n",
      "  진상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 29804건 (51.74%)\n",
      "  피상전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "\n",
      "🚨 이상치 탐지:\n",
      "  순방향 유효전력: 508건 (0.88%)\n",
      "  지상무효: 5548건 (9.63%)\n",
      "  진상무효: 9002건 (15.63%)\n",
      "  피상전력: 508건 (0.88%)\n",
      "\\n[3단계] 이상치 탐지 및 데이터 정제\n",
      "  순방향 유효전력: 508건 (0.88%)\n",
      "  지상무효: 5548건 (9.63%)\n",
      "  진상무효: 9002건 (15.63%)\n",
      "  피상전력: 508건 (0.88%)\n",
      "\\n[4단계] 데이터 품질 종합 평가\n",
      "\n",
      "============================================================\n",
      "📋 데이터 품질 종합 리포트\n",
      "============================================================\n",
      "\n",
      "👥 고객 데이터:\n",
      "  총 고객 수: 200명\n",
      "  계약종별 유형: 5개\n",
      "  사용용도 유형: 2개\n",
      "\n",
      "⚡ LP 데이터:\n",
      "  총 레코드: 57,600건\n",
      "  측정 기간: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "  데이터 커버리지: 29일\n",
      "  평균 유효전력: 53.66kW\n",
      "\n",
      "💾 전처리된 LP 데이터 저장 중...\n",
      "   📊 저장 대상: 57,600개 레코드\n",
      "   💾 저장 중... (잠시만 기다려주세요)\n",
      "      📄 CSV 저장 중...\n",
      "      📦 Parquet 저장 중...\n",
      "   ✅ 전처리된 데이터 저장 완료!\n",
      "      📄 CSV: ./analysis_results/processed_lp_data.csv (0.00GB)\n",
      "      📦 Parquet: ./analysis_results/processed_lp_data.parquet (0.00GB)\n",
      "      🚀 크기 절약: 88.3%\n",
      "      ⚡ 로딩 속도 향상: 약 2-3배 빨라짐!\n",
      "   🚀 2-3단계에서 30분 → 3-5분으로 시간 단축 예상!\n",
      "\n",
      "💾 분석 결과 JSON 저장 중...\n",
      "✅ 분석 결과 JSON 저장: ./analysis_results\\analysis_results.json\n",
      "   저장된 항목: 4개\n",
      "   📁 저장된 구조:\n",
      "      - customer_summary: 고객 기본 정보\n",
      "      - lp_data_summary: LP 데이터 요약\n",
      "      - processed_lp_data: 전처리된 데이터 메타정보\n",
      "      - metadata: 시간정보 및 버전\n",
      "\n",
      "💡 다음 단계 권장사항:\n",
      "  1. 시계열 패턴 분석 (전처리된 데이터 활용)\n",
      "  2. 고객별 사용량 프로파일링\n",
      "  3. 변동성 지표 계산 및 비교\n",
      "  4. 이상 패턴 탐지 알고리즘 개발\n",
      "\n",
      "🎯 1단계 최적화 완료!\n",
      "   📁 생성 파일:\n",
      "      - analysis_results.json (2-3단계 연계용)\n",
      "      - processed_lp_data.csv (전처리된 LP 데이터)\n",
      "      - processed_lp_data.parquet (고성능 전처리된 데이터)\n",
      "\\n🎯 1단계 데이터 품질 점검 완료!\n",
      "다음: 2단계 시계열 패턴 분석 준비 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "        self.analysis_results = {}\n",
    "        \n",
    "    def load_customer_data(self, file_path='제13회 산업부 공모전 대상고객/제13회 산업부 공모전 대상고객.xlsx'):\n",
    "        \"\"\"실제 고객 기본정보 로딩 및 기본 분석\"\"\"\n",
    "        print(\"=== 고객 기본정보 로딩 ===\")\n",
    "        \n",
    "        try:\n",
    "            # 실제 Excel 파일 읽기\n",
    "            self.customer_data = pd.read_excel(file_path, header=1)\n",
    "            \n",
    "            print(f\"총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"컬럼: {list(self.customer_data.columns)}\")\n",
    "            print(\"\\\\n기본 정보:\")\n",
    "            print(self.customer_data.head())\n",
    "            \n",
    "            return self._analyze_customer_distribution()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"고객 데이터 로딩 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"고객 분포 분석\"\"\"\n",
    "        print(\"\\\\n=== 고객 분포 분석 ===\")\n",
    "        \n",
    "        # 계약종별 분포\n",
    "        contract_counts = self.customer_data['계약종별'].value_counts()\n",
    "        print(\"\\\\n📊 계약종별 분포:\")\n",
    "        for contract, count in contract_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {contract}: {count}명 ({pct:.1f}%)\")\n",
    "        \n",
    "        # 사용용도별 분포\n",
    "        usage_counts = self.customer_data['사용용도'].value_counts()\n",
    "        print(\"\\\\n🏭 사용용도별 분포:\")\n",
    "        for usage, count in usage_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {usage}: {count}명 ({pct:.1f}%)\")\n",
    "        \n",
    "        # 계약전력 분포\n",
    "        print(\"\\\\n⚡ 계약전력 분포:\")\n",
    "        power_stats = self.customer_data['계약전력'].describe()\n",
    "        print(power_stats)\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_counts,\n",
    "            'usage_distribution': usage_counts,\n",
    "            'power_stats': power_stats\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, data_directory='./제13회 산업부 공모전 대상고객 LP데이터/'):\n",
    "        \"\"\"실제 LP 데이터 로딩 (여러 CSV 파일)\"\"\"\n",
    "        print(\"\\\\n=== LP 데이터 로딩 ===\")\n",
    "        \n",
    "        try:\n",
    "            # processed_LPData_YYYYMMDD_DD.csv 패턴의 파일들 찾기\n",
    "            lp_files = glob.glob(os.path.join(data_directory, 'processed_LPData_*.csv'))\n",
    "            \n",
    "            if not lp_files:\n",
    "                print(\"LP 데이터 파일을 찾을 수 없습니다.\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"발견된 LP 파일 수: {len(lp_files)}개\")\n",
    "            \n",
    "            # 모든 LP 파일 읽기 및 결합\n",
    "            lp_dataframes = []\n",
    "            total_records = 0\n",
    "            \n",
    "            for i, file_path in enumerate(sorted(lp_files)):\n",
    "                try:\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    print(f\"   [{i+1}/{len(lp_files)}] {filename} 처리 중...\")\n",
    "\n",
    "                    #청크 단위로 읽으면서 바로 처리\n",
    "                    chunk_list = []\n",
    "\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=5000):  # 5000행씩 처리\n",
    "                        # 컬럼명 표준화\n",
    "                        if 'LP수신일자' in chunk.columns:\n",
    "                            chunk = chunk.rename(columns={'LP수신일자': 'LP 수신일자'})\n",
    "                        if '순방향유효전력' in chunk.columns:\n",
    "                            chunk = chunk.rename(columns={'순방향유효전력': '순방향 유효전력'})\n",
    "\n",
    "                        # 필수 컬럼 확인\n",
    "                        required_cols = ['대체고객번호', 'LP 수신일자', '순방향 유효전력']\n",
    "                        if all(col in chunk.columns for col in required_cols):\n",
    "\n",
    "                            # ⭐ 24:00 처리를 청크 단위로 바로 처리\n",
    "                            chunk = self._process_datetime_chunk(chunk)\n",
    "\n",
    "                            # 데이터 품질 기본 체크\n",
    "                            chunk = chunk.dropna(subset=required_cols)\n",
    "                            chunk = chunk[chunk['순방향 유효전력'] >= 0]\n",
    "\n",
    "                            chunk_list.append(chunk)\n",
    "\n",
    "                    # 파일별 청크 결합\n",
    "                    if chunk_list:\n",
    "                        file_df = pd.concat(chunk_list, ignore_index=True)\n",
    "                        lp_dataframes.append(file_df)\n",
    "                        total_records += len(file_df)\n",
    "                        print(f\"      레코드: {len(file_df):,}개, 고객: {file_df['대체고객번호'].nunique()}명\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ 파일 로딩 실패: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not lp_dataframes:\n",
    "                print(\"유효한 LP 데이터가 없습니다.\")\n",
    "                return None\n",
    "            \n",
    "            # 모든 데이터 결합\n",
    "            self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "            \n",
    "            # 시간 순서로 정렬\n",
    "            self.lp_data = self.lp_data.sort_values(['대체고객번호', 'datetime']).reset_index(drop=True)\n",
    "            \n",
    "            print(f\"\\\\n✅ 전체 LP 데이터 결합 완료:\")\n",
    "            print(f\"  총 레코드: {len(self.lp_data):,}\")\n",
    "            print(f\"  총 고객: {self.lp_data['대체고객번호'].nunique()}\")\n",
    "            print(f\"   - 기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            \n",
    "            return self._analyze_lp_quality()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LP 데이터 로딩 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP 데이터 품질 분석\"\"\"\n",
    "        print(\"\\n=== LP 데이터 품질 분석 ===\")\n",
    "\n",
    "        # 기본 통계\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        available_cols = [col for col in numeric_columns if col in self.lp_data.columns]\n",
    "\n",
    "        print(f\"📈 기본 통계:\")\n",
    "        print(self.lp_data[available_cols].describe())\n",
    "\n",
    "        # 시간 간격 체크 (샘플만)\n",
    "        sample_customers = self.lp_data['대체고객번호'].unique()[:3]\n",
    "        print(f\"\\n⏰ 시간 간격 체크:\")\n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('datetime')\n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['datetime'].diff().dt.total_seconds() / 60\n",
    "                avg_interval = time_diffs.dropna().mean()\n",
    "                std_interval = time_diffs.dropna().std()\n",
    "                print(f\"  {customer}: 평균 간격 {avg_interval:.1f}분, 표준편차 {std_interval:.1f}분\")\n",
    "\n",
    "        # 데이터 품질 체크\n",
    "        print(f\"\\n🔍 데이터 품질 체크:\")\n",
    "        for col in available_cols:\n",
    "            null_count = self.lp_data[col].isnull().sum()\n",
    "            null_pct = null_count / len(self.lp_data) * 100\n",
    "            zero_count = (self.lp_data[col] == 0).sum()\n",
    "            zero_pct = zero_count / len(self.lp_data) * 100\n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    결측치: {null_count}건 ({null_pct:.2f}%)\")\n",
    "            print(f\"    0값: {zero_count}건 ({zero_pct:.2f}%)\")\n",
    "\n",
    "        # 이상치 탐지\n",
    "        print(f\"\\n🚨 이상치 탐지:\")\n",
    "        for col in available_cols:\n",
    "            Q1 = self.lp_data[col].quantile(0.25)\n",
    "            Q3 = self.lp_data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = self.lp_data[(self.lp_data[col] < Q1 - 1.5 * IQR) | (self.lp_data[col] > Q3 + 1.5 * IQR)]\n",
    "            outlier_pct = len(outliers) / len(self.lp_data) * 100\n",
    "            print(f\"  {col}: {len(outliers)}건 ({outlier_pct:.2f}%)\")\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def _process_datetime_chunk(self, chunk):\n",
    "        \"\"\"청크 단위로 datetime 처리\"\"\"\n",
    "        try:\n",
    "            # 24:00을 00:00으로 변경하면서 다음날 표시 저장\n",
    "            original_24_mask = chunk['LP 수신일자'].str.contains(' 24:00', na=False)\n",
    "\n",
    "            # 24:00을 00:00으로 변경\n",
    "            chunk['LP 수신일자'] = chunk['LP 수신일자'].str.replace(' 24:00', ' 00:00')\n",
    "\n",
    "            # datetime 변환\n",
    "            chunk['datetime'] = pd.to_datetime(chunk['LP 수신일자'], errors='coerce')\n",
    "\n",
    "            # 원래 24:00이었던 행들은 다음날로 이동\n",
    "            if original_24_mask.any():\n",
    "                chunk.loc[original_24_mask, 'datetime'] += pd.Timedelta(days=1)\n",
    "\n",
    "            return chunk\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ datetime 처리 오류: {e}\")\n",
    "            chunk['datetime'] = pd.to_datetime(chunk['LP 수신일자'], errors='coerce')\n",
    "            return chunk\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"이상치 탐지\"\"\"\n",
    "        outlier_summary = {}\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                if method == 'iqr':\n",
    "                    Q1 = self.lp_data[col].quantile(0.25)\n",
    "                    Q3 = self.lp_data[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    \n",
    "                    outliers = self.lp_data[\n",
    "                        (self.lp_data[col] < lower_bound) | \n",
    "                        (self.lp_data[col] > upper_bound)\n",
    "                    ]\n",
    "                    \n",
    "                    outlier_count = len(outliers)\n",
    "                    outlier_pct = (outlier_count / len(self.lp_data)) * 100\n",
    "                    \n",
    "                    print(f\"  {col}: {outlier_count}건 ({outlier_pct:.2f}%)\")\n",
    "                    outlier_summary[col] = {\n",
    "                        'count': outlier_count,\n",
    "                        'percentage': outlier_pct,\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound\n",
    "                    }\n",
    "        \n",
    "        return outlier_summary\n",
    "    \n",
    "\n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"데이터 품질 종합 리포트 생성 및 전처리된 데이터 저장\"\"\"\n",
    "        import json\n",
    "        from datetime import datetime\n",
    "        import os\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 데이터 품질 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # 데이터 존재 여부 확인\n",
    "        if self.customer_data is None or self.lp_data is None:\n",
    "            print(\"❌ 데이터가 로딩되지 않았습니다.\")\n",
    "            return False\n",
    "\n",
    "        # 고객 데이터 요약\n",
    "        if self.customer_data is not None:\n",
    "            print(f\"\\n👥 고객 데이터:\")\n",
    "            print(f\"  총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"  계약종별 유형: {self.customer_data['계약종별'].nunique()}개\")\n",
    "            print(f\"  사용용도 유형: {self.customer_data['사용용도'].nunique()}개\")\n",
    "\n",
    "            # ⭐ analysis_results에 고객 정보 저장\n",
    "            self.analysis_results['customer_summary'] = {\n",
    "                'total_customers': len(self.customer_data),\n",
    "                'contract_types': self.customer_data['계약종별'].value_counts().to_dict(),\n",
    "                'usage_types': self.customer_data['사용용도'].value_counts().to_dict()\n",
    "            }\n",
    "\n",
    "        # LP 데이터 요약\n",
    "        if self.lp_data is not None:\n",
    "            print(f\"\\n⚡ LP 데이터:\")\n",
    "            print(f\"  총 레코드: {len(self.lp_data):,}건\")\n",
    "            print(f\"  측정 기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"  데이터 커버리지: {(self.lp_data['datetime'].max() - self.lp_data['datetime'].min()).days}일\")\n",
    "\n",
    "            # 평균 전력 사용량\n",
    "            avg_power = self.lp_data['순방향 유효전력'].mean()\n",
    "            print(f\"  평균 유효전력: {avg_power:.2f}kW\")\n",
    "\n",
    "            # ⭐ analysis_results에 LP 데이터 정보 저장\n",
    "            self.analysis_results['lp_data_summary'] = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['대체고객번호'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'avg_power': float(avg_power)\n",
    "            }\n",
    "\n",
    "        # ⭐⭐⭐ 핵심: 전처리된 데이터 저장\n",
    "        print(f\"\\n💾 전처리된 LP 데이터 저장 중...\")\n",
    "\n",
    "        try:\n",
    "            # 출력 디렉토리 생성\n",
    "            import os\n",
    "            os.makedirs('./analysis_results', exist_ok=True)\n",
    "\n",
    "            # 전처리된 데이터 저장\n",
    "            processed_csv = './analysis_results/processed_lp_data.csv'\n",
    "            processed_parquet = './analysis_results/processed_lp_data.parquet'\n",
    "\n",
    "            print(f\"   📊 저장 대상: {len(self.lp_data):,}개 레코드\")\n",
    "            print(f\"   💾 저장 중... (잠시만 기다려주세요)\")\n",
    "\n",
    "            # 1. CSV 저장 (호환성용)\n",
    "            print(f\"      📄 CSV 저장 중...\")\n",
    "            self.lp_data.to_csv(processed_csv, index=False, encoding='utf-8-sig')\n",
    "            csv_size_gb = os.path.getsize(processed_csv) / 1024**3\n",
    "\n",
    "            # 2. ⭐ Parquet 저장 (성능 최적화용)\n",
    "            print(f\"      📦 Parquet 저장 중...\")\n",
    "            try:\n",
    "                self.lp_data.to_parquet(processed_parquet, compression='snappy')\n",
    "                parquet_size_gb = os.path.getsize(processed_parquet) / 1024**3\n",
    "                parquet_success = True\n",
    "            except Exception as parquet_error:\n",
    "                print(f\"         ⚠️ Parquet 저장 실패: {parquet_error}\")\n",
    "                print(f\"         💡 해결방법: pip install pyarrow\")\n",
    "                parquet_success = False\n",
    "\n",
    "            print(f\"   ✅ 전처리된 데이터 저장 완료!\")\n",
    "            print(f\"      📄 CSV: {processed_csv} ({csv_size_gb:.2f}GB)\")\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"      📦 Parquet: {processed_parquet} ({parquet_size_gb:.2f}GB)\")\n",
    "                print(f\"      🚀 크기 절약: {((csv_size_gb - parquet_size_gb) / csv_size_gb * 100):.1f}%\")\n",
    "                print(f\"      ⚡ 로딩 속도 향상: 약 2-3배 빨라짐!\")\n",
    "\n",
    "            # 메타 정보 저장 (⭐ Parquet 정보 추가)\n",
    "            meta_info = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['대체고객번호'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'file_info': {\n",
    "                    'csv_file': 'processed_lp_data.csv',\n",
    "                    'csv_size_gb': csv_size_gb,\n",
    "                    'parquet_file': 'processed_lp_data.parquet' if parquet_success else None,\n",
    "                    'parquet_size_gb': parquet_size_gb if parquet_success else None,\n",
    "                    'parquet_available': parquet_success,\n",
    "                    'encoding': 'utf-8-sig'\n",
    "                },\n",
    "                'processed_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            # analysis_results에 메타 정보 추가\n",
    "            self.analysis_results['processed_lp_data'] = meta_info\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"   🚀 2-3단계에서 30분 → 3-5분으로 시간 단축 예상!\")\n",
    "            else:\n",
    "                print(f\"   📄 CSV로 저장 완료 (30분 → 8분 시간 단축)\")\n",
    "\n",
    "        except Exception as save_error:\n",
    "            print(f\"   ❌ 전처리된 데이터 저장 실패: {save_error}\")\n",
    "            print(f\"      (분석은 계속 진행됩니다)\")\n",
    "\n",
    "        # ⭐⭐⭐ 필수: JSON 결과 저장 (2-3단계 연계용)\n",
    "        print(f\"\\n💾 분석 결과 JSON 저장 중...\")\n",
    "\n",
    "        try:\n",
    "            # 타임스탬프 추가\n",
    "            self.analysis_results['metadata'] = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'analysis_stage': 'step1_preprocessing_optimized',\n",
    "                'version': '2.0',\n",
    "                'total_customers': len(self.customer_data) if self.customer_data is not None else 0,\n",
    "                'total_lp_records': len(self.lp_data) if self.lp_data is not None else 0\n",
    "            }\n",
    "\n",
    "            # JSON 파일로 저장\n",
    "            output_file = os.path.join('./analysis_results', 'analysis_results.json')\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.analysis_results, f, \n",
    "                         ensure_ascii=False, \n",
    "                         indent=2, \n",
    "                         default=str)\n",
    "\n",
    "            print(f\"✅ 분석 결과 JSON 저장: {output_file}\")\n",
    "            print(f\"   저장된 항목: {len(self.analysis_results)}개\")\n",
    "\n",
    "            # 저장된 구조 확인\n",
    "            print(f\"   📁 저장된 구조:\")\n",
    "            for key in self.analysis_results.keys():\n",
    "                if key == 'metadata':\n",
    "                    print(f\"      - metadata: 시간정보 및 버전\")\n",
    "                elif key == 'customer_summary':\n",
    "                    print(f\"      - customer_summary: 고객 기본 정보\")\n",
    "                elif key == 'lp_data_summary':\n",
    "                    print(f\"      - lp_data_summary: LP 데이터 요약\")\n",
    "                elif key == 'processed_lp_data':\n",
    "                    print(f\"      - processed_lp_data: 전처리된 데이터 메타정보\")\n",
    "                else:\n",
    "                    print(f\"      - {key}: {type(self.analysis_results[key])}\")\n",
    "\n",
    "        except Exception as json_error:\n",
    "            print(f\"❌ JSON 저장 실패: {json_error}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # 권장사항\n",
    "        print(\"\\n💡 다음 단계 권장사항:\")\n",
    "        print(\"  1. 시계열 패턴 분석 (전처리된 데이터 활용)\")\n",
    "        print(\"  2. 고객별 사용량 프로파일링\")\n",
    "        print(\"  3. 변동성 지표 계산 및 비교\")\n",
    "        print(\"  4. 이상 패턴 탐지 알고리즘 개발\")\n",
    "\n",
    "        print(f\"\\n🎯 1단계 최적화 완료!\")\n",
    "        print(f\"   📁 생성 파일:\")\n",
    "        print(f\"      - analysis_results.json (2-3단계 연계용)\")\n",
    "        print(f\"      - processed_lp_data.csv (전처리된 LP 데이터)\")\n",
    "        if 'processed_lp_data' in self.analysis_results and self.analysis_results['processed_lp_data']['file_info']['parquet_available']:\n",
    "            print(f\"      - processed_lp_data.parquet (고성능 전처리된 데이터)\")\n",
    "\n",
    "        return True\n",
    "\n",
    "# 사용 예제 (실제 데이터안심구역에서 실행)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\")\n",
    "    print(\"데이터안심구역 전용 - 실제 데이터 분석\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1단계: 고객 기본정보 분석\n",
    "    print(\"\\\\n[1단계] 고객 기본정보 로딩 및 분석\")\n",
    "    customer_analysis = analyzer.load_customer_data('제13회 산업부 공모전 대상고객/제13회 산업부 공모전 대상고객.xlsx')\n",
    "    \n",
    "    # 2단계: LP 데이터 분석\n",
    "    print(\"\\\\n[2단계] LP 데이터 로딩 및 품질 분석\")\n",
    "    lp_analysis = analyzer.load_lp_data('./제13회 산업부 공모전 대상고객 LP데이터/')  # 현재 디렉터리에서 LP 파일 찾기\n",
    "    \n",
    "    # 3단계: 이상치 탐지\n",
    "    print(\"\\\\n[3단계] 이상치 탐지 및 데이터 정제\")\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4단계: 종합 리포트\n",
    "    print(\"\\\\n[4단계] 데이터 품질 종합 평가\")\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\\\n🎯 1단계 데이터 품질 점검 완료!\")\n",
    "    print(\"다음: 2단계 시계열 패턴 분석 준비 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
