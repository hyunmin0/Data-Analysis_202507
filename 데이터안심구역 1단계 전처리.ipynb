{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55944dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\n",
      "ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ ì „ìš© - ì‹¤ì œ ë°ì´í„° ë¶„ì„\n",
      "============================================================\n",
      "\\n[1ë‹¨ê³„] ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ë¶„ì„\n",
      "=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\n",
      "ì´ ê³ ê° ìˆ˜: 200ëª…\n",
      "ì»¬ëŸ¼: ['ìˆœë²ˆ', 'ê³ ê°ë²ˆí˜¸', 'ê³„ì•½ì „ë ¥', 'ê³„ì•½ì¢…ë³„', 'ì‚¬ìš©ìš©ë„', 'ì£¼ìƒì‚°í’ˆ', 'ì‚°ì—…ë¶„ë¥˜']\n",
      "\\nê¸°ë³¸ ì •ë³´:\n",
      "   ìˆœë²ˆ   ê³ ê°ë²ˆí˜¸     ê³„ì•½ì „ë ¥            ê³„ì•½ì¢…ë³„    ì‚¬ìš©ìš©ë„ ì£¼ìƒì‚°í’ˆ                      ì‚°ì—…ë¶„ë¥˜\n",
      "0   1  A1001    1~199  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   ë³‘ì›  721ê±´ì¶•ê¸°ìˆ ,ì—”ì§€ë‹ˆì–´ë§ë°ê¸°íƒ€ê³¼í•™ê¸°ìˆ ì„œë¹„ìŠ¤ì—…\n",
      "1   2  A1002  400~499  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   êµíšŒ               241ì œ1ì°¨ì² ê°•ì œì¡°ì—…\n",
      "2   3  A1003  400~499  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   ë³‘ì›                 681ë¶€ë™ì‚°ì„ëŒ€ì—…\n",
      "3   4  A1004  500~599  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   ìƒê°€               241ì œ1ì°¨ì² ê°•ì œì¡°ì—…\n",
      "4   5  A1005  700~799  726 ì‚°ì—…ìš©(ì„) ê³ ì••A  02 ìƒì—…ìš©   ìƒê°€            631ì°½ê³ ë°ìš´ì†¡ê´€ë ¨ì„œë¹„ìŠ¤ì—…\n",
      "\\n=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\n",
      "\\nğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\n",
      "  226 ì¼ë°˜ìš©(ì„) ê³ ì••A: 50ëª… (25.0%)\n",
      "  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A: 41ëª… (20.5%)\n",
      "  311 ì‚°ì—…ìš©(ê°‘) ì €ì••: 39ëª… (19.5%)\n",
      "  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A: 37ëª… (18.5%)\n",
      "  726 ì‚°ì—…ìš©(ì„) ê³ ì••A: 33ëª… (16.5%)\n",
      "\\nğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\n",
      "  02 ìƒì—…ìš©: 105ëª… (52.5%)\n",
      "  09 ê´‘ê³µì—…ìš©: 95ëª… (47.5%)\n",
      "\\nâš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\n",
      "count       200\n",
      "unique        9\n",
      "top       1~199\n",
      "freq         27\n",
      "Name: ê³„ì•½ì „ë ¥, dtype: object\n",
      "\\n[2ë‹¨ê³„] LP ë°ì´í„° ë¡œë”© ë° í’ˆì§ˆ ë¶„ì„\n",
      "\\n=== LP ë°ì´í„° ë¡œë”© ===\n",
      "ë°œê²¬ëœ LP íŒŒì¼ ìˆ˜: 4ê°œ\n",
      "   [1/4] processed_LPData_20220301_15.csv ì²˜ë¦¬ ì¤‘...\n",
      "      ë ˆì½”ë“œ: 14,400ê°œ, ê³ ê°: 10ëª…\n",
      "   [2/4] processed_LPData_20220316_31.csv ì²˜ë¦¬ ì¤‘...\n",
      "      ë ˆì½”ë“œ: 14,400ê°œ, ê³ ê°: 10ëª…\n",
      "   [3/4] processed_LPData_20220401_15.csv ì²˜ë¦¬ ì¤‘...\n",
      "      ë ˆì½”ë“œ: 14,400ê°œ, ê³ ê°: 10ëª…\n",
      "   [4/4] processed_LPData_20220416_29.csv ì²˜ë¦¬ ì¤‘...\n",
      "      ë ˆì½”ë“œ: 14,400ê°œ, ê³ ê°: 10ëª…\n",
      "\\nâœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\n",
      "  ì´ ë ˆì½”ë“œ: 57,600\n",
      "  ì´ ê³ ê°: 10\n",
      "   - ê¸°ê°„: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "\n",
      "=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\n",
      "ğŸ“ˆ ê¸°ë³¸ í†µê³„:\n",
      "           ìˆœë°©í–¥ ìœ íš¨ì „ë ¥          ì§€ìƒë¬´íš¨          ì§„ìƒë¬´íš¨          í”¼ìƒì „ë ¥\n",
      "count  57600.000000  57600.000000  57600.000000  57600.000000\n",
      "mean      53.658823      7.500729      4.015514     54.492017\n",
      "std       52.880036     12.419111      8.295770     53.722780\n",
      "min        0.200000      0.000000      0.000000      0.200000\n",
      "25%        7.575000      0.000000      0.000000      7.700000\n",
      "50%       37.500000      1.400000      0.000000     38.100000\n",
      "75%       88.300000     10.100000      3.600000     89.900000\n",
      "max      284.500000    107.100000     77.300000    291.200000\n",
      "\n",
      "â° ì‹œê°„ ê°„ê²© ì²´í¬:\n",
      "  A1001: í‰ê·  ê°„ê²© 7.5ë¶„, í‘œì¤€í¸ì°¨ 7.5ë¶„\n",
      "  A1002: í‰ê·  ê°„ê²© 7.5ë¶„, í‘œì¤€í¸ì°¨ 7.5ë¶„\n",
      "  A1003: í‰ê·  ê°„ê²© 7.5ë¶„, í‘œì¤€í¸ì°¨ 7.5ë¶„\n",
      "\n",
      "ğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\n",
      "  ìˆœë°©í–¥ ìœ íš¨ì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "  ì§€ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 18118ê±´ (31.45%)\n",
      "  ì§„ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 29804ê±´ (51.74%)\n",
      "  í”¼ìƒì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "\n",
      "ğŸš¨ ì´ìƒì¹˜ íƒì§€:\n",
      "  ìˆœë°©í–¥ ìœ íš¨ì „ë ¥: 508ê±´ (0.88%)\n",
      "  ì§€ìƒë¬´íš¨: 5548ê±´ (9.63%)\n",
      "  ì§„ìƒë¬´íš¨: 9002ê±´ (15.63%)\n",
      "  í”¼ìƒì „ë ¥: 508ê±´ (0.88%)\n",
      "\\n[3ë‹¨ê³„] ì´ìƒì¹˜ íƒì§€ ë° ë°ì´í„° ì •ì œ\n",
      "  ìˆœë°©í–¥ ìœ íš¨ì „ë ¥: 508ê±´ (0.88%)\n",
      "  ì§€ìƒë¬´íš¨: 5548ê±´ (9.63%)\n",
      "  ì§„ìƒë¬´íš¨: 9002ê±´ (15.63%)\n",
      "  í”¼ìƒì „ë ¥: 508ê±´ (0.88%)\n",
      "\\n[4ë‹¨ê³„] ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ‘¥ ê³ ê° ë°ì´í„°:\n",
      "  ì´ ê³ ê° ìˆ˜: 200ëª…\n",
      "  ê³„ì•½ì¢…ë³„ ìœ í˜•: 5ê°œ\n",
      "  ì‚¬ìš©ìš©ë„ ìœ í˜•: 2ê°œ\n",
      "\n",
      "âš¡ LP ë°ì´í„°:\n",
      "  ì´ ë ˆì½”ë“œ: 57,600ê±´\n",
      "  ì¸¡ì • ê¸°ê°„: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "  ë°ì´í„° ì»¤ë²„ë¦¬ì§€: 29ì¼\n",
      "  í‰ê·  ìœ íš¨ì „ë ¥: 53.66kW\n",
      "\n",
      "ğŸ’¾ ì „ì²˜ë¦¬ëœ LP ë°ì´í„° ì €ì¥ ì¤‘...\n",
      "   ğŸ“Š ì €ì¥ ëŒ€ìƒ: 57,600ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ’¾ ì €ì¥ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n",
      "      ğŸ“„ CSV ì €ì¥ ì¤‘...\n",
      "      ğŸ“¦ Parquet ì €ì¥ ì¤‘...\n",
      "   âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ!\n",
      "      ğŸ“„ CSV: ./analysis_results/processed_lp_data.csv (0.00GB)\n",
      "      ğŸ“¦ Parquet: ./analysis_results/processed_lp_data.parquet (0.00GB)\n",
      "      ğŸš€ í¬ê¸° ì ˆì•½: 88.3%\n",
      "      âš¡ ë¡œë”© ì†ë„ í–¥ìƒ: ì•½ 2-3ë°° ë¹¨ë¼ì§!\n",
      "   ğŸš€ 2-3ë‹¨ê³„ì—ì„œ 30ë¶„ â†’ 3-5ë¶„ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶• ì˜ˆìƒ!\n",
      "\n",
      "ğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì¤‘...\n",
      "âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: ./analysis_results\\analysis_results.json\n",
      "   ì €ì¥ëœ í•­ëª©: 4ê°œ\n",
      "   ğŸ“ ì €ì¥ëœ êµ¬ì¡°:\n",
      "      - customer_summary: ê³ ê° ê¸°ë³¸ ì •ë³´\n",
      "      - lp_data_summary: LP ë°ì´í„° ìš”ì•½\n",
      "      - processed_lp_data: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë©”íƒ€ì •ë³´\n",
      "      - metadata: ì‹œê°„ì •ë³´ ë° ë²„ì „\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\n",
      "  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì „ì²˜ë¦¬ëœ ë°ì´í„° í™œìš©)\n",
      "  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\n",
      "  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\n",
      "  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n",
      "\n",
      "ğŸ¯ 1ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!\n",
      "   ğŸ“ ìƒì„± íŒŒì¼:\n",
      "      - analysis_results.json (2-3ë‹¨ê³„ ì—°ê³„ìš©)\n",
      "      - processed_lp_data.csv (ì „ì²˜ë¦¬ëœ LP ë°ì´í„°)\n",
      "      - processed_lp_data.parquet (ê³ ì„±ëŠ¥ ì „ì²˜ë¦¬ëœ ë°ì´í„°)\n",
      "\\nğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "        self.analysis_results = {}\n",
    "        \n",
    "    def load_customer_data(self, file_path='ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'):\n",
    "        \"\"\"ì‹¤ì œ ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ê¸°ë³¸ ë¶„ì„\"\"\"\n",
    "        print(\"=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\")\n",
    "        \n",
    "        try:\n",
    "            # ì‹¤ì œ Excel íŒŒì¼ ì½ê¸°\n",
    "            self.customer_data = pd.read_excel(file_path, header=1)\n",
    "            \n",
    "            print(f\"ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…\")\n",
    "            print(f\"ì»¬ëŸ¼: {list(self.customer_data.columns)}\")\n",
    "            print(\"\\\\nê¸°ë³¸ ì •ë³´:\")\n",
    "            print(self.customer_data.head())\n",
    "            \n",
    "            return self._analyze_customer_distribution()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ê³ ê° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"ê³ ê° ë¶„í¬ ë¶„ì„\"\"\"\n",
    "        print(\"\\\\n=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê³„ì•½ì¢…ë³„ ë¶„í¬\n",
    "        contract_counts = self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts()\n",
    "        print(\"\\\\nğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\")\n",
    "        for contract, count in contract_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {contract}: {count}ëª… ({pct:.1f}%)\")\n",
    "        \n",
    "        # ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬\n",
    "        usage_counts = self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts()\n",
    "        print(\"\\\\nğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\")\n",
    "        for usage, count in usage_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {usage}: {count}ëª… ({pct:.1f}%)\")\n",
    "        \n",
    "        # ê³„ì•½ì „ë ¥ ë¶„í¬\n",
    "        print(\"\\\\nâš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\")\n",
    "        power_stats = self.customer_data['ê³„ì•½ì „ë ¥'].describe()\n",
    "        print(power_stats)\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_counts,\n",
    "            'usage_distribution': usage_counts,\n",
    "            'power_stats': power_stats\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, data_directory='./ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/'):\n",
    "        \"\"\"ì‹¤ì œ LP ë°ì´í„° ë¡œë”© (ì—¬ëŸ¬ CSV íŒŒì¼)\"\"\"\n",
    "        print(\"\\\\n=== LP ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        try:\n",
    "            # processed_LPData_YYYYMMDD_DD.csv íŒ¨í„´ì˜ íŒŒì¼ë“¤ ì°¾ê¸°\n",
    "            lp_files = glob.glob(os.path.join(data_directory, 'processed_LPData_*.csv'))\n",
    "            \n",
    "            if not lp_files:\n",
    "                print(\"LP ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"ë°œê²¬ëœ LP íŒŒì¼ ìˆ˜: {len(lp_files)}ê°œ\")\n",
    "            \n",
    "            # ëª¨ë“  LP íŒŒì¼ ì½ê¸° ë° ê²°í•©\n",
    "            lp_dataframes = []\n",
    "            total_records = 0\n",
    "            \n",
    "            for i, file_path in enumerate(sorted(lp_files)):\n",
    "                try:\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    print(f\"   [{i+1}/{len(lp_files)}] {filename} ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "                    #ì²­í¬ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ ë°”ë¡œ ì²˜ë¦¬\n",
    "                    chunk_list = []\n",
    "\n",
    "                    for chunk in pd.read_csv(file_path, chunksize=5000):  # 5000í–‰ì”© ì²˜ë¦¬\n",
    "                        # ì»¬ëŸ¼ëª… í‘œì¤€í™”\n",
    "                        if 'LPìˆ˜ì‹ ì¼ì' in chunk.columns:\n",
    "                            chunk = chunk.rename(columns={'LPìˆ˜ì‹ ì¼ì': 'LP ìˆ˜ì‹ ì¼ì'})\n",
    "                        if 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥' in chunk.columns:\n",
    "                            chunk = chunk.rename(columns={'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'})\n",
    "\n",
    "                        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "                        required_cols = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LP ìˆ˜ì‹ ì¼ì', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']\n",
    "                        if all(col in chunk.columns for col in required_cols):\n",
    "\n",
    "                            # â­ 24:00 ì²˜ë¦¬ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë°”ë¡œ ì²˜ë¦¬\n",
    "                            chunk = self._process_datetime_chunk(chunk)\n",
    "\n",
    "                            # ë°ì´í„° í’ˆì§ˆ ê¸°ë³¸ ì²´í¬\n",
    "                            chunk = chunk.dropna(subset=required_cols)\n",
    "                            chunk = chunk[chunk['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] >= 0]\n",
    "\n",
    "                            chunk_list.append(chunk)\n",
    "\n",
    "                    # íŒŒì¼ë³„ ì²­í¬ ê²°í•©\n",
    "                    if chunk_list:\n",
    "                        file_df = pd.concat(chunk_list, ignore_index=True)\n",
    "                        lp_dataframes.append(file_df)\n",
    "                        total_records += len(file_df)\n",
    "                        print(f\"      ë ˆì½”ë“œ: {len(file_df):,}ê°œ, ê³ ê°: {file_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  âœ— íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not lp_dataframes:\n",
    "                print(\"ìœ íš¨í•œ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return None\n",
    "            \n",
    "            # ëª¨ë“  ë°ì´í„° ê²°í•©\n",
    "            self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "            \n",
    "            # ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬\n",
    "            self.lp_data = self.lp_data.sort_values(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'datetime']).reset_index(drop=True)\n",
    "            \n",
    "            print(f\"\\\\nâœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\")\n",
    "            print(f\"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}\")\n",
    "            print(f\"  ì´ ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}\")\n",
    "            print(f\"   - ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            \n",
    "            return self._analyze_lp_quality()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LP ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\")\n",
    "\n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        available_cols = [col for col in numeric_columns if col in self.lp_data.columns]\n",
    "\n",
    "        print(f\"ğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
    "        print(self.lp_data[available_cols].describe())\n",
    "\n",
    "        # ì‹œê°„ ê°„ê²© ì²´í¬ (ìƒ˜í”Œë§Œ)\n",
    "        sample_customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]\n",
    "        print(f\"\\nâ° ì‹œê°„ ê°„ê²© ì²´í¬:\")\n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('datetime')\n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['datetime'].diff().dt.total_seconds() / 60\n",
    "                avg_interval = time_diffs.dropna().mean()\n",
    "                std_interval = time_diffs.dropna().std()\n",
    "                print(f\"  {customer}: í‰ê·  ê°„ê²© {avg_interval:.1f}ë¶„, í‘œì¤€í¸ì°¨ {std_interval:.1f}ë¶„\")\n",
    "\n",
    "        # ë°ì´í„° í’ˆì§ˆ ì²´í¬\n",
    "        print(f\"\\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
    "        for col in available_cols:\n",
    "            null_count = self.lp_data[col].isnull().sum()\n",
    "            null_pct = null_count / len(self.lp_data) * 100\n",
    "            zero_count = (self.lp_data[col] == 0).sum()\n",
    "            zero_pct = zero_count / len(self.lp_data) * 100\n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    ê²°ì¸¡ì¹˜: {null_count}ê±´ ({null_pct:.2f}%)\")\n",
    "            print(f\"    0ê°’: {zero_count}ê±´ ({zero_pct:.2f}%)\")\n",
    "\n",
    "        # ì´ìƒì¹˜ íƒì§€\n",
    "        print(f\"\\nğŸš¨ ì´ìƒì¹˜ íƒì§€:\")\n",
    "        for col in available_cols:\n",
    "            Q1 = self.lp_data[col].quantile(0.25)\n",
    "            Q3 = self.lp_data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = self.lp_data[(self.lp_data[col] < Q1 - 1.5 * IQR) | (self.lp_data[col] > Q3 + 1.5 * IQR)]\n",
    "            outlier_pct = len(outliers) / len(self.lp_data) * 100\n",
    "            print(f\"  {col}: {len(outliers)}ê±´ ({outlier_pct:.2f}%)\")\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def _process_datetime_chunk(self, chunk):\n",
    "        \"\"\"ì²­í¬ ë‹¨ìœ„ë¡œ datetime ì²˜ë¦¬\"\"\"\n",
    "        try:\n",
    "            # 24:00ì„ 00:00ìœ¼ë¡œ ë³€ê²½í•˜ë©´ì„œ ë‹¤ìŒë‚  í‘œì‹œ ì €ì¥\n",
    "            original_24_mask = chunk['LP ìˆ˜ì‹ ì¼ì'].str.contains(' 24:00', na=False)\n",
    "\n",
    "            # 24:00ì„ 00:00ìœ¼ë¡œ ë³€ê²½\n",
    "            chunk['LP ìˆ˜ì‹ ì¼ì'] = chunk['LP ìˆ˜ì‹ ì¼ì'].str.replace(' 24:00', ' 00:00')\n",
    "\n",
    "            # datetime ë³€í™˜\n",
    "            chunk['datetime'] = pd.to_datetime(chunk['LP ìˆ˜ì‹ ì¼ì'], errors='coerce')\n",
    "\n",
    "            # ì›ë˜ 24:00ì´ì—ˆë˜ í–‰ë“¤ì€ ë‹¤ìŒë‚ ë¡œ ì´ë™\n",
    "            if original_24_mask.any():\n",
    "                chunk.loc[original_24_mask, 'datetime'] += pd.Timedelta(days=1)\n",
    "\n",
    "            return chunk\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ datetime ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
    "            chunk['datetime'] = pd.to_datetime(chunk['LP ìˆ˜ì‹ ì¼ì'], errors='coerce')\n",
    "            return chunk\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        outlier_summary = {}\n",
    "        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                if method == 'iqr':\n",
    "                    Q1 = self.lp_data[col].quantile(0.25)\n",
    "                    Q3 = self.lp_data[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    \n",
    "                    outliers = self.lp_data[\n",
    "                        (self.lp_data[col] < lower_bound) | \n",
    "                        (self.lp_data[col] > upper_bound)\n",
    "                    ]\n",
    "                    \n",
    "                    outlier_count = len(outliers)\n",
    "                    outlier_pct = (outlier_count / len(self.lp_data)) * 100\n",
    "                    \n",
    "                    print(f\"  {col}: {outlier_count}ê±´ ({outlier_pct:.2f}%)\")\n",
    "                    outlier_summary[col] = {\n",
    "                        'count': outlier_count,\n",
    "                        'percentage': outlier_pct,\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound\n",
    "                    }\n",
    "        \n",
    "        return outlier_summary\n",
    "    \n",
    "\n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ë° ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\"\"\"\n",
    "        import json\n",
    "        from datetime import datetime\n",
    "        import os\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "        if self.customer_data is None or self.lp_data is None:\n",
    "            print(\"âŒ ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "\n",
    "        # ê³ ê° ë°ì´í„° ìš”ì•½\n",
    "        if self.customer_data is not None:\n",
    "            print(f\"\\nğŸ‘¥ ê³ ê° ë°ì´í„°:\")\n",
    "            print(f\"  ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…\")\n",
    "            print(f\"  ê³„ì•½ì¢…ë³„ ìœ í˜•: {self.customer_data['ê³„ì•½ì¢…ë³„'].nunique()}ê°œ\")\n",
    "            print(f\"  ì‚¬ìš©ìš©ë„ ìœ í˜•: {self.customer_data['ì‚¬ìš©ìš©ë„'].nunique()}ê°œ\")\n",
    "\n",
    "            # â­ analysis_resultsì— ê³ ê° ì •ë³´ ì €ì¥\n",
    "            self.analysis_results['customer_summary'] = {\n",
    "                'total_customers': len(self.customer_data),\n",
    "                'contract_types': self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts().to_dict(),\n",
    "                'usage_types': self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts().to_dict()\n",
    "            }\n",
    "\n",
    "        # LP ë°ì´í„° ìš”ì•½\n",
    "        if self.lp_data is not None:\n",
    "            print(f\"\\nâš¡ LP ë°ì´í„°:\")\n",
    "            print(f\"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}ê±´\")\n",
    "            print(f\"  ì¸¡ì • ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"  ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {(self.lp_data['datetime'].max() - self.lp_data['datetime'].min()).days}ì¼\")\n",
    "\n",
    "            # í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰\n",
    "            avg_power = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "            print(f\"  í‰ê·  ìœ íš¨ì „ë ¥: {avg_power:.2f}kW\")\n",
    "\n",
    "            # â­ analysis_resultsì— LP ë°ì´í„° ì •ë³´ ì €ì¥\n",
    "            self.analysis_results['lp_data_summary'] = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'avg_power': float(avg_power)\n",
    "            }\n",
    "\n",
    "        # â­â­â­ í•µì‹¬: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "        print(f\"\\nğŸ’¾ ì „ì²˜ë¦¬ëœ LP ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "            import os\n",
    "            os.makedirs('./analysis_results', exist_ok=True)\n",
    "\n",
    "            # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "            processed_csv = './analysis_results/processed_lp_data.csv'\n",
    "            processed_parquet = './analysis_results/processed_lp_data.parquet'\n",
    "\n",
    "            print(f\"   ğŸ“Š ì €ì¥ ëŒ€ìƒ: {len(self.lp_data):,}ê°œ ë ˆì½”ë“œ\")\n",
    "            print(f\"   ğŸ’¾ ì €ì¥ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
    "\n",
    "            # 1. CSV ì €ì¥ (í˜¸í™˜ì„±ìš©)\n",
    "            print(f\"      ğŸ“„ CSV ì €ì¥ ì¤‘...\")\n",
    "            self.lp_data.to_csv(processed_csv, index=False, encoding='utf-8-sig')\n",
    "            csv_size_gb = os.path.getsize(processed_csv) / 1024**3\n",
    "\n",
    "            # 2. â­ Parquet ì €ì¥ (ì„±ëŠ¥ ìµœì í™”ìš©)\n",
    "            print(f\"      ğŸ“¦ Parquet ì €ì¥ ì¤‘...\")\n",
    "            try:\n",
    "                self.lp_data.to_parquet(processed_parquet, compression='snappy')\n",
    "                parquet_size_gb = os.path.getsize(processed_parquet) / 1024**3\n",
    "                parquet_success = True\n",
    "            except Exception as parquet_error:\n",
    "                print(f\"         âš ï¸ Parquet ì €ì¥ ì‹¤íŒ¨: {parquet_error}\")\n",
    "                print(f\"         ğŸ’¡ í•´ê²°ë°©ë²•: pip install pyarrow\")\n",
    "                parquet_success = False\n",
    "\n",
    "            print(f\"   âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")\n",
    "            print(f\"      ğŸ“„ CSV: {processed_csv} ({csv_size_gb:.2f}GB)\")\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"      ğŸ“¦ Parquet: {processed_parquet} ({parquet_size_gb:.2f}GB)\")\n",
    "                print(f\"      ğŸš€ í¬ê¸° ì ˆì•½: {((csv_size_gb - parquet_size_gb) / csv_size_gb * 100):.1f}%\")\n",
    "                print(f\"      âš¡ ë¡œë”© ì†ë„ í–¥ìƒ: ì•½ 2-3ë°° ë¹¨ë¼ì§!\")\n",
    "\n",
    "            # ë©”íƒ€ ì •ë³´ ì €ì¥ (â­ Parquet ì •ë³´ ì¶”ê°€)\n",
    "            meta_info = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'file_info': {\n",
    "                    'csv_file': 'processed_lp_data.csv',\n",
    "                    'csv_size_gb': csv_size_gb,\n",
    "                    'parquet_file': 'processed_lp_data.parquet' if parquet_success else None,\n",
    "                    'parquet_size_gb': parquet_size_gb if parquet_success else None,\n",
    "                    'parquet_available': parquet_success,\n",
    "                    'encoding': 'utf-8-sig'\n",
    "                },\n",
    "                'processed_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            # analysis_resultsì— ë©”íƒ€ ì •ë³´ ì¶”ê°€\n",
    "            self.analysis_results['processed_lp_data'] = meta_info\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"   ğŸš€ 2-3ë‹¨ê³„ì—ì„œ 30ë¶„ â†’ 3-5ë¶„ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶• ì˜ˆìƒ!\")\n",
    "            else:\n",
    "                print(f\"   ğŸ“„ CSVë¡œ ì €ì¥ ì™„ë£Œ (30ë¶„ â†’ 8ë¶„ ì‹œê°„ ë‹¨ì¶•)\")\n",
    "\n",
    "        except Exception as save_error:\n",
    "            print(f\"   âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {save_error}\")\n",
    "            print(f\"      (ë¶„ì„ì€ ê³„ì† ì§„í–‰ë©ë‹ˆë‹¤)\")\n",
    "\n",
    "        # â­â­â­ í•„ìˆ˜: JSON ê²°ê³¼ ì €ì¥ (2-3ë‹¨ê³„ ì—°ê³„ìš©)\n",
    "        print(f\"\\nğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€\n",
    "            self.analysis_results['metadata'] = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'analysis_stage': 'step1_preprocessing_optimized',\n",
    "                'version': '2.0',\n",
    "                'total_customers': len(self.customer_data) if self.customer_data is not None else 0,\n",
    "                'total_lp_records': len(self.lp_data) if self.lp_data is not None else 0\n",
    "            }\n",
    "\n",
    "            # JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "            output_file = os.path.join('./analysis_results', 'analysis_results.json')\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.analysis_results, f, \n",
    "                         ensure_ascii=False, \n",
    "                         indent=2, \n",
    "                         default=str)\n",
    "\n",
    "            print(f\"âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {output_file}\")\n",
    "            print(f\"   ì €ì¥ëœ í•­ëª©: {len(self.analysis_results)}ê°œ\")\n",
    "\n",
    "            # ì €ì¥ëœ êµ¬ì¡° í™•ì¸\n",
    "            print(f\"   ğŸ“ ì €ì¥ëœ êµ¬ì¡°:\")\n",
    "            for key in self.analysis_results.keys():\n",
    "                if key == 'metadata':\n",
    "                    print(f\"      - metadata: ì‹œê°„ì •ë³´ ë° ë²„ì „\")\n",
    "                elif key == 'customer_summary':\n",
    "                    print(f\"      - customer_summary: ê³ ê° ê¸°ë³¸ ì •ë³´\")\n",
    "                elif key == 'lp_data_summary':\n",
    "                    print(f\"      - lp_data_summary: LP ë°ì´í„° ìš”ì•½\")\n",
    "                elif key == 'processed_lp_data':\n",
    "                    print(f\"      - processed_lp_data: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë©”íƒ€ì •ë³´\")\n",
    "                else:\n",
    "                    print(f\"      - {key}: {type(self.analysis_results[key])}\")\n",
    "\n",
    "        except Exception as json_error:\n",
    "            print(f\"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {json_error}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # ê¶Œì¥ì‚¬í•­\n",
    "        print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\")\n",
    "        print(\"  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì „ì²˜ë¦¬ëœ ë°ì´í„° í™œìš©)\")\n",
    "        print(\"  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\")\n",
    "        print(\"  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\")\n",
    "        print(\"  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\")\n",
    "\n",
    "        print(f\"\\nğŸ¯ 1ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!\")\n",
    "        print(f\"   ğŸ“ ìƒì„± íŒŒì¼:\")\n",
    "        print(f\"      - analysis_results.json (2-3ë‹¨ê³„ ì—°ê³„ìš©)\")\n",
    "        print(f\"      - processed_lp_data.csv (ì „ì²˜ë¦¬ëœ LP ë°ì´í„°)\")\n",
    "        if 'processed_lp_data' in self.analysis_results and self.analysis_results['processed_lp_data']['file_info']['parquet_available']:\n",
    "            print(f\"      - processed_lp_data.parquet (ê³ ì„±ëŠ¥ ì „ì²˜ë¦¬ëœ ë°ì´í„°)\")\n",
    "\n",
    "        return True\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ (ì‹¤ì œ ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ì—ì„œ ì‹¤í–‰)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\")\n",
    "    print(\"ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ ì „ìš© - ì‹¤ì œ ë°ì´í„° ë¶„ì„\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1ë‹¨ê³„: ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\n",
    "    print(\"\\\\n[1ë‹¨ê³„] ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ë¶„ì„\")\n",
    "    customer_analysis = analyzer.load_customer_data('ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx')\n",
    "    \n",
    "    # 2ë‹¨ê³„: LP ë°ì´í„° ë¶„ì„\n",
    "    print(\"\\\\n[2ë‹¨ê³„] LP ë°ì´í„° ë¡œë”© ë° í’ˆì§ˆ ë¶„ì„\")\n",
    "    lp_analysis = analyzer.load_lp_data('./ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/')  # í˜„ì¬ ë””ë ‰í„°ë¦¬ì—ì„œ LP íŒŒì¼ ì°¾ê¸°\n",
    "    \n",
    "    # 3ë‹¨ê³„: ì´ìƒì¹˜ íƒì§€\n",
    "    print(\"\\\\n[3ë‹¨ê³„] ì´ìƒì¹˜ íƒì§€ ë° ë°ì´í„° ì •ì œ\")\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4ë‹¨ê³„: ì¢…í•© ë¦¬í¬íŠ¸\n",
    "    print(\"\\\\n[4ë‹¨ê³„] ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€\")\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\\\nğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\")\n",
    "    print(\"ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
