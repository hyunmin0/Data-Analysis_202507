"""
í•œêµ­ì „ë ¥ê³µì‚¬ ê³µëª¨ì „: ì°½ì˜ì  ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ 2ë‹¨ê³„ ì•Œê³ ë¦¬ì¦˜
'ê¸°ì—… ê²½ì˜í™œë™ ë””ì§€í„¸ ë°”ì´ì˜¤ë§ˆì»¤ ì‹œìŠ¤í…œ'

ğŸ¯ í•µì‹¬ ì°½ì˜ í¬ì¸íŠ¸:
1. "ì „ë ¥ DNA ë¶„ì„" - ê¸°ì—… ê³ ìœ ì˜ ì „ë ¥ ì‚¬ìš© ì§€ë¬¸ ì‹ë³„
2. "ê²½ì˜ ê±´ê°•ë„ ì§„ë‹¨" - ì˜í•™ì  ì ‘ê·¼ìœ¼ë¡œ ê¸°ì—… ìƒíƒœ í‰ê°€  
3. "ì‹œê°„ì—¬í–‰ ì˜ˆì¸¡" - ê³¼ê±°/í˜„ì¬/ë¯¸ë˜ë¥¼ ì—°ê²°í•˜ëŠ” ë³€ë™ì„± ì˜ˆì¸¡
4. "ì—…ì¢…ë³„ AI ì „ë¬¸ê°€" - ê° ì—…ì¢…ì— íŠ¹í™”ëœ ë¶„ì„ ì—”ì§„
5. "ë””ì§€í„¸ ì „í™˜ ê°ì§€" - ê¸°ì—…ì˜ ë””ì§€í„¸í™” ìˆ˜ì¤€ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy import stats
from scipy.fft import fft, fftfreq
import warnings
warnings.filterwarnings('ignore')

class CreativePowerDNAAnalyzer:
    """
    ğŸ§¬ ì°½ì˜ì  ì „ë ¥ DNA ë¶„ì„ê¸°
    ê¸°ì—…ì˜ ì „ë ¥ ì‚¬ìš©ì„ ìƒì²´ ì‹ í˜¸ì²˜ëŸ¼ ë¶„ì„í•˜ì—¬ ê²½ì˜ ìƒíƒœ ì§„ë‹¨
    """
    
    def __init__(self, analysis_results_path='./analysis_result/analysis_results.json'):
        """
        ì´ˆê¸°í™” - 1ë‹¨ê³„ ì „ì²˜ë¦¬ ê²°ê³¼ í™œìš©
        """
        self.analysis_results_path = analysis_results_path
        self.preprocessing_results = self._load_preprocessing_results()
        
        # í•µì‹¬ ë¶„ì„ ì—”ì§„ë“¤
        self.dna_profiles = {}           # ì „ë ¥ DNA í”„ë¡œí•„
        self.health_diagnostics = {}     # ê²½ì˜ ê±´ê°•ë„ ì§„ë‹¨
        self.time_travel_predictions = {} # ì‹œê°„ì—¬í–‰ ì˜ˆì¸¡
        self.industry_experts = {}       # ì—…ì¢…ë³„ ì „ë¬¸ê°€
        self.digital_transformation = {} # ë””ì§€í„¸ ì „í™˜ ì§€ìˆ˜
        
        print("ğŸ§¬ ì°½ì˜ì  ì „ë ¥ DNA ë¶„ì„ê¸° ì´ˆê¸°í™”")
        print("=" * 60)
        print("ğŸ“‹ ë¶„ì„ ì—”ì§„:")
        print("  ğŸ”¬ ì „ë ¥ DNA ì‹œí€€ì‹±")
        print("  ğŸ¥ ê²½ì˜ ê±´ê°•ë„ ì§„ë‹¨")
        print("  ğŸ•°ï¸ ì‹œê°„ì—¬í–‰ ë³€ë™ì„± ì˜ˆì¸¡")
        print("  ğŸ‘¨â€ğŸ’¼ ì—…ì¢…ë³„ AI ì „ë¬¸ê°€")
        print("  ğŸš€ ë””ì§€í„¸ ì „í™˜ ê°ì§€ê¸°")
        print()
    
    def _load_preprocessing_results(self):
        """1ë‹¨ê³„ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©"""
        try:
            if os.path.exists(self.analysis_results_path):
                with open(self.analysis_results_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"âœ… ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”© ì„±ê³µ: {self.analysis_results_path}")
                return results
            else:
                print(f"âš ï¸ ì „ì²˜ë¦¬ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {self.analysis_results_path}")
                return {}
        except Exception as e:
            print(f"âŒ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    # ============ 1. ì „ë ¥ DNA ì‹œí€€ì‹± ============
    
    def extract_power_dna(self, lp_data):
        """
        ğŸ§¬ ì „ë ¥ DNA ì¶”ì¶œ - ê¸°ì—…ì˜ ê³ ìœ í•œ ì „ë ¥ ì‚¬ìš© ì§€ë¬¸ ë¶„ì„
        """
        print("ğŸ§¬ ì „ë ¥ DNA ì‹œí€€ì‹± ì¤‘...")
        
        dna_profiles = {}
        customers = lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        for customer in customers:
            customer_data = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()
            customer_data['datetime'] = pd.to_datetime(customer_data['LPìˆ˜ì‹ ì¼ì'])
            customer_data = customer_data.sort_values('datetime')
            
            # DNA ì—¼ê¸°ì„œì—´ êµ¬ì„± ìš”ì†Œ
            dna_sequence = {
                'A_gene': self._extract_activity_gene(customer_data),      # í™œë™ì„± ìœ ì „ì
                'T_gene': self._extract_timing_gene(customer_data),        # ì‹œê°„ì„± ìœ ì „ì  
                'G_gene': self._extract_growth_gene(customer_data),        # ì„±ì¥ì„± ìœ ì „ì
                'C_gene': self._extract_consistency_gene(customer_data)     # ì¼ê´€ì„± ìœ ì „ì
            }
            
            # DNA ì§€ë¬¸ ìƒì„±
            dna_fingerprint = self._create_dna_fingerprint(dna_sequence)
            
            # ê¸°ì—… DNA íƒ€ì… ë¶„ë¥˜
            dna_type = self._classify_dna_type(dna_sequence)
            
            dna_profiles[customer] = {
                'dna_sequence': dna_sequence,
                'dna_fingerprint': dna_fingerprint,
                'dna_type': dna_type,
                'uniqueness_score': self._calculate_uniqueness_score(dna_sequence)
            }
        
        self.dna_profiles = dna_profiles
        print(f"âœ… {len(customers)}ê°œ ê¸°ì—… DNA ë¶„ì„ ì™„ë£Œ")
        return dna_profiles
    
    def _extract_activity_gene(self, data):
        """í™œë™ì„± ìœ ì „ì - ì „ë ¥ ì‚¬ìš©ì˜ í™œë°œí•¨ ì •ë„"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        # í™œë™ ê°•ë„ (í‰ê·  ëŒ€ë¹„ í”¼í¬ ì‚¬ìš©ëŸ‰)
        activity_intensity = np.max(power_values) / np.mean(power_values) if np.mean(power_values) > 0 else 0
        
        # í™œë™ ë¹ˆë„ (ì„ê³„ê°’ ì´ìƒ ì‚¬ìš© íšŸìˆ˜)
        threshold = np.percentile(power_values, 75)
        activity_frequency = np.sum(power_values > threshold) / len(power_values)
        
        # í™œë™ ì§€ì†ì„± (ì—°ì†ì ì¸ ê³ ì‚¬ìš©ëŸ‰ êµ¬ê°„)
        high_usage_periods = self._find_continuous_periods(power_values > threshold)
        activity_persistence = np.mean([len(period) for period in high_usage_periods]) if high_usage_periods else 0
        
        return {
            'intensity': round(activity_intensity, 4),
            'frequency': round(activity_frequency, 4),
            'persistence': round(activity_persistence, 4)
        }
    
    def _extract_timing_gene(self, data):
        """ì‹œê°„ì„± ìœ ì „ì - ì‹œê°„ íŒ¨í„´ì˜ ê·œì¹™ì„±"""
        data['hour'] = data['datetime'].dt.hour
        data['weekday'] = data['datetime'].dt.weekday
        
        # ì‹œê°„ ê·œì¹™ì„± (ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´ì˜ ì¼ê´€ì„±)
        hourly_pattern = data.groupby('hour')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        timing_regularity = 1 / (hourly_pattern.std() / hourly_pattern.mean() + 1) if hourly_pattern.mean() > 0 else 0
        
        # ì£¼ê¸°ì„± ê°•ë„ (FFTë¥¼ í†µí•œ ì£¼íŒŒìˆ˜ ë¶„ì„)
        power_series = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        fft_values = fft(power_series)
        frequencies = fftfreq(len(power_series))
        periodicity_strength = np.max(np.abs(fft_values[1:len(fft_values)//2]))
        
        # ì—…ë¬´ì‹œê°„ ì§‘ì¤‘ë„ (9-18ì‹œ ì‚¬ìš©ëŸ‰ ë¹„ì¤‘)
        business_hours = data[(data['hour'] >= 9) & (data['hour'] <= 18)]
        if len(data) > 0:
            business_concentration = business_hours['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].sum() / data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].sum()
        else:
            business_concentration = 0
        
        return {
            'regularity': round(timing_regularity, 4),
            'periodicity': round(float(periodicity_strength), 4),
            'business_focus': round(business_concentration, 4)
        }
    
    def _extract_growth_gene(self, data):
        """ì„±ì¥ì„± ìœ ì „ì - ì‚¬ìš©ëŸ‰ ë³€í™” íŠ¸ë Œë“œ"""
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        data = data.sort_values('datetime')
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        # ì„±ì¥ íŠ¸ë Œë“œ (ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°)
        x = np.arange(len(power_values))
        if len(power_values) > 1:
            slope, intercept, r_value, p_value, std_err = stats.linregress(x, power_values)
            growth_trend = slope
            trend_confidence = abs(r_value)
        else:
            growth_trend = 0
            trend_confidence = 0
        
        # ì„±ì¥ ê°€ì†ë„ (2ì°¨ ë¯¸ë¶„)
        if len(power_values) > 2:
            growth_acceleration = np.mean(np.diff(power_values, 2))
        else:
            growth_acceleration = 0
        
        # ì„±ì¥ ì•ˆì •ì„± (íŠ¸ë Œë“œì˜ ì¼ê´€ì„±)
        window_size = min(96, len(power_values) // 4)  # 1ì¼ ë˜ëŠ” ì „ì²´ì˜ 1/4
        if window_size > 1:
            rolling_trends = []
            for i in range(0, len(power_values) - window_size, window_size):
                window_data = power_values[i:i+window_size]
                window_x = np.arange(len(window_data))
                window_slope, _, _, _, _ = stats.linregress(window_x, window_data)
                rolling_trends.append(window_slope)
            
            growth_stability = 1 / (np.std(rolling_trends) + 1) if rolling_trends else 0
        else:
            growth_stability = 0
        
        return {
            'trend': round(growth_trend, 4),
            'acceleration': round(growth_acceleration, 4),
            'stability': round(growth_stability, 4),
            'confidence': round(trend_confidence, 4)
        }
    
    def _extract_consistency_gene(self, data):
        """ì¼ê´€ì„± ìœ ì „ì - ì‚¬ìš© íŒ¨í„´ì˜ ì˜ˆì¸¡ê°€ëŠ¥ì„±"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        # ê¸°ë³¸ ì¼ê´€ì„± (ë³€ë™ê³„ìˆ˜ì˜ ì—­ìˆ˜)
        if np.mean(power_values) > 0:
            basic_consistency = 1 / (np.std(power_values) / np.mean(power_values) + 1)
        else:
            basic_consistency = 0
        
        # íŒ¨í„´ ì¼ê´€ì„± (ìê¸°ìƒê´€)
        if len(power_values) > 1:
            autocorr = np.corrcoef(power_values[:-1], power_values[1:])[0, 1]
            pattern_consistency = abs(autocorr) if not np.isnan(autocorr) else 0
        else:
            pattern_consistency = 0
        
        # ì˜ˆì¸¡ê°€ëŠ¥ì„± (ë‹¨ìˆœ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì •í™•ë„)
        if len(power_values) > 10:
            # ì´ë™í‰ê·  ì˜ˆì¸¡
            window = min(5, len(power_values) // 2)
            predictions = []
            actuals = []
            
            for i in range(window, len(power_values)):
                prediction = np.mean(power_values[i-window:i])
                actual = power_values[i]
                predictions.append(prediction)
                actuals.append(actual)
            
            if predictions:
                mape = np.mean(np.abs((np.array(actuals) - np.array(predictions)) / (np.array(actuals) + 1)))
                predictability = 1 / (mape + 1)
            else:
                predictability = 0
        else:
            predictability = 0
        
        return {
            'basic': round(basic_consistency, 4),
            'pattern': round(pattern_consistency, 4),
            'predictability': round(predictability, 4)
        }
    
    def _find_continuous_periods(self, boolean_array):
        """ì—°ì†ëœ True êµ¬ê°„ ì°¾ê¸°"""
        periods = []
        start = None
        
        for i, value in enumerate(boolean_array):
            if value and start is None:
                start = i
            elif not value and start is not None:
                periods.append(list(range(start, i)))
                start = None
        
        # ë§ˆì§€ë§‰ êµ¬ê°„ ì²˜ë¦¬
        if start is not None:
            periods.append(list(range(start, len(boolean_array))))
        
        return periods
    
    def _create_dna_fingerprint(self, dna_sequence):
        """DNA ì§€ë¬¸ ìƒì„± - ê³ ìœ  ì‹ë³„ì"""
        # ê° ìœ ì „ìì˜ ì£¼ìš” íŠ¹ì„±ì„ ê²°í•©í•˜ì—¬ ê³ ìœ  ì§€ë¬¸ ìƒì„±
        fingerprint_components = [
            dna_sequence['A_gene']['intensity'],
            dna_sequence['T_gene']['regularity'],
            dna_sequence['G_gene']['trend'],
            dna_sequence['C_gene']['basic']
        ]
        
        # ì •ê·œí™” í›„ í•´ì‹œê°’ ìƒì„±
        normalized = MinMaxScaler().fit_transform(np.array(fingerprint_components).reshape(-1, 1)).flatten()
        fingerprint = ''.join([f"{x:.2f}" for x in normalized])
        
        return fingerprint
    
    def _classify_dna_type(self, dna_sequence):
        """DNA íƒ€ì… ë¶„ë¥˜"""
        activity_score = np.mean(list(dna_sequence['A_gene'].values()))
        timing_score = np.mean(list(dna_sequence['T_gene'].values()))
        growth_score = dna_sequence['G_gene']['trend']
        consistency_score = np.mean(list(dna_sequence['C_gene'].values()))
        
        # ì°½ì˜ì  DNA íƒ€ì… ë¶„ë¥˜
        if activity_score > 0.7 and growth_score > 0:
            return "í˜ì‹  ì„±ì¥í˜• (Innovation Growth)"
        elif consistency_score > 0.8 and timing_score > 0.7:
            return "ì•ˆì • ìš´ì˜í˜• (Stable Operation)"
        elif activity_score > 0.6 and timing_score < 0.4:
            return "ìœ ì—° ì ì‘í˜• (Flexible Adaptation)"
        elif growth_score < -0.1:
            return "êµ¬ì¡° ì¡°ì •í˜• (Restructuring)"
        else:
            return "ê· í˜• ë°œì „í˜• (Balanced Development)"
    
    def _calculate_uniqueness_score(self, dna_sequence):
        """DNA ê³ ìœ ì„± ì ìˆ˜"""
        # ê° ìœ ì „ìì˜ í¸ì°¨ë¥¼ í†µí•´ ê³ ìœ ì„± ì¸¡ì •
        all_values = []
        for gene in dna_sequence.values():
            all_values.extend(gene.values())
        
        uniqueness = np.std(all_values) * np.mean(all_values) if all_values else 0
        return round(uniqueness, 4)
    
    # ============ 2. ê²½ì˜ ê±´ê°•ë„ ì§„ë‹¨ ============
    
    def diagnose_business_health(self, lp_data):
        """
        ğŸ¥ ê²½ì˜ ê±´ê°•ë„ ì§„ë‹¨ - ì˜í•™ì  ì ‘ê·¼ìœ¼ë¡œ ê¸°ì—… ìƒíƒœ í‰ê°€
        """
        print("ğŸ¥ ê²½ì˜ ê±´ê°•ë„ ì§„ë‹¨ ì¤‘...")
        
        health_diagnostics = {}
        customers = lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        for customer in customers:
            customer_data = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()
            
            # ì¢…í•© ê±´ê°• ê²€ì§„
            vital_signs = self._check_vital_signs(customer_data)           # ìƒì²´ ì‹ í˜¸
            risk_factors = self._assess_risk_factors(customer_data)        # ìœ„í—˜ ìš”ì†Œ
            wellness_index = self._calculate_wellness_index(customer_data) # ì›°ë‹ˆìŠ¤ ì§€ìˆ˜
            health_grade = self._assign_health_grade(vital_signs, risk_factors, wellness_index)
            
            health_diagnostics[customer] = {
                'vital_signs': vital_signs,
                'risk_factors': risk_factors,
                'wellness_index': wellness_index,
                'health_grade': health_grade,
                'diagnosis_date': datetime.now().isoformat()
            }
        
        self.health_diagnostics = health_diagnostics
        print(f"âœ… {len(customers)}ê°œ ê¸°ì—… ê±´ê°• ì§„ë‹¨ ì™„ë£Œ")
        return health_diagnostics
    
    def _check_vital_signs(self, data):
        """ê²½ì˜ ìƒì²´ ì‹ í˜¸ ì¸¡ì •"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        # ì „ë ¥ ë§¥ë°• (ì‚¬ìš©ëŸ‰ì˜ ì£¼ê¸°ì  ë³€í™”)
        if len(power_values) > 1:
            power_pulse = np.mean(np.abs(np.diff(power_values)))
        else:
            power_pulse = 0
        
        # ì „ë ¥ í˜ˆì•• (ìµœëŒ€/ìµœì†Œ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨)
        if np.min(power_values) > 0:
            power_pressure = np.max(power_values) / np.min(power_values)
        else:
            power_pressure = float('inf') if np.max(power_values) > 0 else 1
        
        # ì „ë ¥ ì²´ì˜¨ (í‰ê·  ì‚¬ìš© ê°•ë„)
        power_temperature = np.mean(power_values)
        
        # ì „ë ¥ í˜¸í¡ (ì‚¬ìš©ëŸ‰ ë³€ë™ì˜ ê·œì¹™ì„±)
        if len(power_values) > 2:
            breath_intervals = np.diff(power_values)
            power_breathing = np.std(breath_intervals) / (np.mean(np.abs(breath_intervals)) + 1)
        else:
            power_breathing = 0
        
        return {
            'pulse': round(power_pulse, 2),
            'pressure': round(min(power_pressure, 999.99), 2),  # Cap at reasonable value
            'temperature': round(power_temperature, 2),
            'breathing': round(power_breathing, 4)
        }
    
    def _assess_risk_factors(self, data):
        """ê²½ì˜ ìœ„í—˜ ìš”ì†Œ í‰ê°€"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        # ê¸‰ì„± ë¦¬ìŠ¤í¬ (ê¸‰ê²©í•œ ë³€í™”)
        if len(power_values) > 1:
            sudden_changes = np.sum(np.abs(np.diff(power_values)) > 3 * np.std(power_values))
            acute_risk = sudden_changes / len(power_values)
        else:
            acute_risk = 0
        
        # ë§Œì„± ë¦¬ìŠ¤í¬ (ì§€ì†ì ì¸ ë¶ˆì•ˆì •ì„±)
        chronic_risk = np.std(power_values) / (np.mean(power_values) + 1)
        
        # êµ¬ì¡°ì  ë¦¬ìŠ¤í¬ (ë¹„ì •ìƒì  íŒ¨í„´)
        # ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš©
        if 'datetime' not in data.columns:
            data['datetime'] = pd.to_datetime(data['LPìˆ˜ì‹ ì¼ì'])
        data['hour'] = data['datetime'].dt.hour
        
        night_usage = data[(data['hour'] >= 22) | (data['hour'] <= 6)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        day_usage = data[(data['hour'] >= 9) & (data['hour'] <= 18)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        
        if day_usage > 0:
            structural_risk = night_usage / day_usage
        else:
            structural_risk = 0
        
        return {
            'acute': round(acute_risk, 4),
            'chronic': round(chronic_risk, 4),
            'structural': round(min(structural_risk, 5.0), 4)  # Cap at 5.0
        }
    
    def _calculate_wellness_index(self, data):
        """ì›°ë‹ˆìŠ¤ ì§€ìˆ˜ ê³„ì‚°"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        # íš¨ìœ¨ì„± ì§€ìˆ˜ (ì‚¬ìš©ëŸ‰ ëŒ€ë¹„ ì•ˆì •ì„±)
        if np.mean(power_values) > 0:
            efficiency_index = 1 / (np.std(power_values) / np.mean(power_values) + 1)
        else:
            efficiency_index = 0
        
        # ì ì‘ì„± ì§€ìˆ˜ (í™˜ê²½ ë³€í™” ëŒ€ì‘ë ¥)
        if len(power_values) > 10:
            # ì‹œê°„ì— ë”°ë¥¸ ì ì‘ ëŠ¥ë ¥
            first_half = power_values[:len(power_values)//2]
            second_half = power_values[len(power_values)//2:]
            
            adaptation_ability = 1 - abs(np.mean(first_half) - np.mean(second_half)) / (np.mean(power_values) + 1)
        else:
            adaptation_ability = 0.5
        
        # ì§€ì†ì„± ì§€ìˆ˜ (ì¥ê¸°ê°„ ìš´ì˜ ëŠ¥ë ¥)
        sustainability_index = 1 - (np.sum(power_values == 0) / len(power_values))
        
        # ì¢…í•© ì›°ë‹ˆìŠ¤ ì§€ìˆ˜
        wellness_score = (efficiency_index * 0.4 + 
                         adaptation_ability * 0.3 + 
                         sustainability_index * 0.3)
        
        return {
            'efficiency': round(efficiency_index, 4),
            'adaptation': round(adaptation_ability, 4),
            'sustainability': round(sustainability_index, 4),
            'overall': round(wellness_score, 4)
        }
    
    def _assign_health_grade(self, vital_signs, risk_factors, wellness_index):
        """ì¢…í•© ê±´ê°• ë“±ê¸‰ ì‚°ì •"""
        # ìƒì²´ ì‹ í˜¸ ì •ìƒì„± í‰ê°€
        vital_score = 1.0
        if vital_signs['pressure'] > 50:  # ê³¼ë„í•œ ë³€ë™ì„±
            vital_score -= 0.3
        if vital_signs['breathing'] > 1.0:  # ë¶ˆê·œì¹™í•œ íŒ¨í„´
            vital_score -= 0.2
        
        # ìœ„í—˜ ìš”ì†Œ ì°¨ê°
        risk_penalty = (risk_factors['acute'] * 0.4 + 
                       risk_factors['chronic'] * 0.4 + 
                       min(risk_factors['structural'], 1.0) * 0.2)
        
        # ìµœì¢… ê±´ê°• ì ìˆ˜
        final_score = (vital_score * 0.3 + 
                      (1 - risk_penalty) * 0.3 + 
                      wellness_index['overall'] * 0.4)
        
        # ë“±ê¸‰ ì‚°ì •
        if final_score >= 0.9:
            grade = "A+ (ë§¤ìš° ê±´ê°•)"
            status = "excellent"
        elif final_score >= 0.8:
            grade = "A (ê±´ê°•)"
            status = "good"
        elif final_score >= 0.7:
            grade = "B+ (ì–‘í˜¸)"
            status = "fair"
        elif final_score >= 0.6:
            grade = "B (ë³´í†µ)"
            status = "average"
        elif final_score >= 0.5:
            grade = "C+ (ì£¼ì˜)"
            status = "caution"
        elif final_score >= 0.4:
            grade = "C (ìœ„í—˜)"
            status = "risk"
        else:
            grade = "D (ë§¤ìš° ìœ„í—˜)"
            status = "critical"
        
        return {
            'score': round(final_score, 4),
            'grade': grade,
            'status': status
        }
    
    # ============ 3. ì‹œê°„ì—¬í–‰ ë³€ë™ì„± ì˜ˆì¸¡ ============
    
    def build_time_travel_predictor(self, lp_data):
        """
        ğŸ•°ï¸ ì‹œê°„ì—¬í–‰ ë³€ë™ì„± ì˜ˆì¸¡ê¸° - ê³¼ê±°/í˜„ì¬/ë¯¸ë˜ ì—°ê²°
        """
        print("ğŸ•°ï¸ ì‹œê°„ì—¬í–‰ ì˜ˆì¸¡ê¸° êµ¬ì¶• ì¤‘...")
        
        predictions = {}
        customers = lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        for customer in customers[:5]:  # ìƒ˜í”Œë¡œ 5ê°œ ê³ ê°ë§Œ
            customer_data = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()
            
            # ì‹œê°„ì—¬í–‰ ë¶„ì„
            past_patterns = self._analyze_past_patterns(customer_data)
            present_state = self._assess_present_state(customer_data)
            future_forecast = self._predict_future_volatility(customer_data)
            
            predictions[customer] = {
                'past_patterns': past_patterns,
                'present_state': present_state,
                'future_forecast': future_forecast,
                'time_continuity_score': self._calculate_time_continuity(past_patterns, present_state, future_forecast)
            }
        
        self.time_travel_predictions = predictions
        print(f"âœ… {len(predictions)}ê°œ ê¸°ì—… ì‹œê°„ì—¬í–‰ ì˜ˆì¸¡ ì™„ë£Œ")
        return predictions
    
    def _analyze_past_patterns(self, data):
        """ê³¼ê±° íŒ¨í„´ ë¶„ì„"""
        # ê³¼ê±° ë°ì´í„°ë¥¼ 3ë¶„í• í•˜ì—¬ íŠ¸ë Œë“œ ë¶„ì„
        data_sorted = data.sort_values('LPìˆ˜ì‹ ì¼ì')
        power_values = data_sorted['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        if len(power_values) < 3:
            return {'trend': 0, 'stability': 0, 'cycles': 0}
        
        # 3ê°œ êµ¬ê°„ìœ¼ë¡œ ë¶„í• 
        segment_size = len(power_values) // 3
        past_segment = power_values[:segment_size]
        middle_segment = power_values[segment_size:2*segment_size]
        recent_segment = power_values[2*segment_size:]
        
        # ê³¼ê±° íŠ¸ë Œë“œ
        past_trend = (np.mean(middle_segment) - np.mean(past_segment)) / (np.mean(past_segment) + 1)
        
        # ê³¼ê±° ì•ˆì •ì„±
        past_stability = 1 / (np.std(past_segment) / (np.mean(past_segment) + 1) + 1)
        
        # ì£¼ê¸°ì„± ë°œê²¬
        cycles_detected = len(self._detect_cycles(power_values))
        
        return {
            'trend': round(past_trend, 4),
            'stability': round(past_stability, 4),
            'cycles': cycles_detected
        }
    
    def _assess_present_state(self, data):
        """í˜„ì¬ ìƒíƒœ í‰ê°€"""
        # ìµœê·¼ ë°ì´í„°ë¡œ í˜„ì¬ ìƒíƒœ ë¶„ì„
        recent_data = data.tail(min(96, len(data)))  # ìµœê·¼ 1ì¼ ë˜ëŠ” ì „ì²´
        power_values = recent_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        if len(power_values) == 0:
            return {'volatility': 0, 'trend': 0, 'anomaly_score': 0}
        
        # í˜„ì¬ ë³€ë™ì„±
        current_volatility = np.std(power_values) / (np.mean(power_values) + 1)
        
        # í˜„ì¬ íŠ¸ë Œë“œ
        if len(power_values) > 1:
            x = np.arange(len(power_values))
            slope, _, _, _, _ = stats.linregress(x, power_values)
            current_trend = slope
        else:
            current_trend = 0
        
        # ì´ìƒì¹˜ ì ìˆ˜
        if len(power_values) > 1:
            isolation_forest = IsolationForest(contamination=0.1, random_state=42)
            anomaly_scores = isolation_forest.fit_predict(power_values.reshape(-1, 1))
            anomaly_score = np.mean(anomaly_scores == -1)
        else:
            anomaly_score = 0
        
        return {
            'volatility': round(current_volatility, 4),
            'trend': round(current_trend, 4),
            'anomaly_score': round(anomaly_score, 4)
        }
    
    def _predict_future_volatility(self, data):
        """ë¯¸ë˜ ë³€ë™ì„± ì˜ˆì¸¡"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        if len(power_values) < 10:
            return {'volatility_forecast': 0, 'trend_forecast': 0, 'confidence': 0}
        
        # ê°„ë‹¨í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ (ì´ë™í‰ê·  + íŠ¸ë Œë“œ)
        window_size = min(10, len(power_values) // 2)
        
        # ìµœê·¼ ë°ì´í„°ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
        recent_values = power_values[-window_size:]
        x = np.arange(len(recent_values))
        slope, intercept, r_value, _, _ = stats.linregress(x, recent_values)
        
        # ë¯¸ë˜ ë³€ë™ì„± ì˜ˆì¸¡ (ìµœê·¼ ë³€ë™ì„±ì— íŠ¸ë Œë“œ ì ìš©)
        recent_volatility = np.std(recent_values) / (np.mean(recent_values) + 1)
        volatility_trend = slope / (np.mean(recent_values) + 1)
        future_volatility = recent_volatility + volatility_trend
        
        # ì‹ ë¢°ë„ (ìƒê´€ê³„ìˆ˜ ê¸°ë°˜)
        confidence = abs(r_value)
        
        return {
            'volatility_forecast': round(max(0, future_volatility), 4),
            'trend_forecast': round(slope, 4),
            'confidence': round(confidence, 4)
        }
    
    def _detect_cycles(self, power_values):
        """ì£¼ê¸°ì„± íƒì§€"""
        if len(power_values) < 20:
            return []
        
        # FFTë¥¼ ì‚¬ìš©í•œ ì£¼ê¸° íƒì§€
        fft_values = fft(power_values)
        frequencies = fftfreq(len(power_values))
        
        # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„ ì°¾ê¸°
        magnitude = np.abs(fft_values)
        peak_indices = np.where(magnitude > np.mean(magnitude) + 2 * np.std(magnitude))[0]
        
        cycles = []
        for idx in peak_indices[:5]:  # ìƒìœ„ 5ê°œë§Œ
            if frequencies[idx] > 0:
                period = 1 / frequencies[idx]
                cycles.append(period)
        
        return cycles
    
    def _calculate_time_continuity(self, past, present, future):
        """ì‹œê°„ ì—°ì†ì„± ì ìˆ˜"""
        # ê³¼ê±°-í˜„ì¬-ë¯¸ë˜ì˜ ì—°ê²°ì„± í‰ê°€
        past_present_continuity = 1 - abs(past['trend'] - present['trend'])
        present_future_continuity = 1 - abs(present['trend'] - future['trend_forecast'])
        
        overall_continuity = (past_present_continuity + present_future_continuity) / 2
        return round(max(0, overall_continuity), 4)
    
    # ============ 4. ì—…ì¢…ë³„ AI ì „ë¬¸ê°€ ì‹œìŠ¤í…œ ============
    
    def create_industry_experts(self, customer_data, lp_data):
        """
        ğŸ‘¨â€ğŸ’¼ ì—…ì¢…ë³„ AI ì „ë¬¸ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•
        """
        print("ğŸ‘¨â€ğŸ’¼ ì—…ì¢…ë³„ AI ì „ë¬¸ê°€ ì‹œìŠ¤í…œ êµ¬ì¶• ì¤‘...")
        
        # ì—…ì¢…ë³„ ì „ë¬¸ê°€ ì •ì˜
        experts = {
            'manufacturing_expert': self._create_manufacturing_expert(),
            'commercial_expert': self._create_commercial_expert(),
            'service_expert': self._create_service_expert(),
            'general_expert': self._create_general_expert()
        }
        
        # ê° ì „ë¬¸ê°€ì˜ ë¶„ì„ ê²°ê³¼
        expert_analyses = {}
        
        for expert_name, expert_config in experts.items():
            expert_analyses[expert_name] = self._run_expert_analysis(
                expert_config, customer_data, lp_data
            )
        
        self.industry_experts = expert_analyses
        print(f"âœ… {len(experts)}ê°œ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ")
        return expert_analyses
    
    def _create_manufacturing_expert(self):
        """ì œì¡°ì—… ì „ë¬¸ê°€"""
        return {
            'name': 'ì œì¡°ì—… ì „ë¬¸ê°€',
            'specialty': 'ìƒì‚°ë¼ì¸ íš¨ìœ¨ì„± ë¶„ì„',
            'key_indicators': [
                'shift_pattern_consistency',    # êµëŒ€ íŒ¨í„´ ì¼ê´€ì„±
                'production_efficiency',        # ìƒì‚° íš¨ìœ¨ì„±
                'equipment_utilization',        # ì„¤ë¹„ ê°€ë™ë¥ 
                'energy_intensity'              # ì—ë„ˆì§€ ì§‘ì•½ë„
            ],
            'risk_factors': [
                'production_halt_signals',      # ìƒì‚° ì¤‘ë‹¨ ì‹ í˜¸
                'equipment_degradation',        # ì„¤ë¹„ ë…¸í›„í™”
                'shift_irregularities'          # êµëŒ€ ë¶ˆê·œì¹™ì„±
            ],
            'thresholds': {
                'high_risk_cv': 0.8,
                'low_efficiency': 0.3,
                'abnormal_night_ratio': 0.1
            }
        }
    
    def _create_commercial_expert(self):
        """ìƒì—…ì‹œì„¤ ì „ë¬¸ê°€"""
        return {
            'name': 'ìƒì—…ì‹œì„¤ ì „ë¬¸ê°€',
            'specialty': 'ê³ ê° ìœ ì… íŒ¨í„´ ë¶„ì„',
            'key_indicators': [
                'business_hour_efficiency',     # ì˜ì—…ì‹œê°„ íš¨ìœ¨ì„±
                'customer_flow_pattern',        # ê³ ê° ìœ ì… íŒ¨í„´
                'seasonal_adaptation',          # ê³„ì ˆì  ì ì‘ì„±
                'peak_management'               # í”¼í¬ì‹œê°„ ê´€ë¦¬
            ],
            'risk_factors': [
                'declining_customer_flow',      # ê³ ê° ìœ ì… ê°ì†Œ
                'inefficient_operations',       # ë¹„íš¨ìœ¨ì  ìš´ì˜
                'seasonal_vulnerability'        # ê³„ì ˆì  ì·¨ì•½ì„±
            ],
            'thresholds': {
                'high_risk_cv': 0.6,
                'low_efficiency': 0.4,
                'weekend_dependency': 0.7
            }
        }
    
    def _create_service_expert(self):
        """ì„œë¹„ìŠ¤ì—… ì „ë¬¸ê°€"""
        return {
            'name': 'ì„œë¹„ìŠ¤ì—… ì „ë¬¸ê°€',
            'specialty': 'ì„œë¹„ìŠ¤ ìš´ì˜ ìµœì í™”',
            'key_indicators': [
                'service_consistency',          # ì„œë¹„ìŠ¤ ì¼ê´€ì„±
                'operational_flexibility',      # ìš´ì˜ ìœ ì—°ì„±
                'resource_optimization',        # ìì› ìµœì í™”
                'digital_readiness'             # ë””ì§€í„¸ ì¤€ë¹„ë„
            ],
            'risk_factors': [
                'service_disruption',           # ì„œë¹„ìŠ¤ ì¤‘ë‹¨
                'resource_waste',               # ìì› ë‚­ë¹„
                'digital_lag'                   # ë””ì§€í„¸ ì§€ì—°
            ],
            'thresholds': {
                'high_risk_cv': 0.7,
                'low_efficiency': 0.35,
                'digital_threshold': 0.2
            }
        }
    
    def _create_general_expert(self):
        """ì¼ë°˜ ì—…ì¢… ì „ë¬¸ê°€"""
        return {
            'name': 'ì¼ë°˜ ì—…ì¢… ì „ë¬¸ê°€',
            'specialty': 'ì¢…í•©ì  ê²½ì˜ ë¶„ì„',
            'key_indicators': [
                'overall_stability',            # ì „ë°˜ì  ì•ˆì •ì„±
                'growth_sustainability',        # ì„±ì¥ ì§€ì†ê°€ëŠ¥ì„±
                'risk_management',              # ìœ„í—˜ ê´€ë¦¬
                'operational_excellence'        # ìš´ì˜ ìš°ìˆ˜ì„±
            ],
            'risk_factors': [
                'general_instability',          # ì¼ë°˜ì  ë¶ˆì•ˆì •ì„±
                'growth_stagnation',            # ì„±ì¥ ì •ì²´
                'operational_inefficiency'      # ìš´ì˜ ë¹„íš¨ìœ¨ì„±
            ],
            'thresholds': {
                'high_risk_cv': 0.75,
                'low_efficiency': 0.3,
                'stagnation_threshold': 0.05
            }
        }
    
    def _run_expert_analysis(self, expert_config, customer_data, lp_data):
        """ì „ë¬¸ê°€ ë¶„ì„ ì‹¤í–‰"""
        analysis_results = {
            'expert_name': expert_config['name'],
            'specialty': expert_config['specialty'],
            'analyzed_customers': 0,
            'recommendations': [],
            'risk_alerts': [],
            'insights': []
        }
        
        # ìƒ˜í”Œ ê³ ê°ë“¤ì— ëŒ€í•´ ì „ë¬¸ê°€ ë¶„ì„ ìˆ˜í–‰
        sample_customers = lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]
        
        for customer in sample_customers:
            customer_lp = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]
            
            # ì „ë¬¸ê°€ë³„ ì§€í‘œ ê³„ì‚°
            indicators = self._calculate_expert_indicators(
                expert_config, customer_lp
            )
            
            # ìœ„í—˜ ìš”ì†Œ í‰ê°€
            risks = self._assess_expert_risks(
                expert_config, customer_lp, indicators
            )
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            recommendations = self._generate_expert_recommendations(
                expert_config, indicators, risks
            )
            
            if risks:
                analysis_results['risk_alerts'].extend(risks)
            if recommendations:
                analysis_results['recommendations'].extend(recommendations)
        
        analysis_results['analyzed_customers'] = len(sample_customers)
        
        # ì „ë¬¸ê°€ë³„ ì¸ì‚¬ì´íŠ¸ ìƒì„±
        analysis_results['insights'] = self._generate_expert_insights(expert_config)
        
        return analysis_results
    
    def _calculate_expert_indicators(self, expert_config, customer_data):
        """ì „ë¬¸ê°€ë³„ ì§€í‘œ ê³„ì‚°"""
        power_values = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        indicators = {}
        
        # ê¸°ë³¸ í†µê³„
        if len(power_values) > 0:
            mean_power = np.mean(power_values)
            std_power = np.std(power_values)
            cv = std_power / mean_power if mean_power > 0 else 0
        else:
            mean_power = std_power = cv = 0
        
        # ì—…ì¢…ë³„ íŠ¹í™” ì§€í‘œ ê³„ì‚°
        if 'shift_pattern_consistency' in expert_config['key_indicators']:
            # ì œì¡°ì—…: êµëŒ€ íŒ¨í„´ ì¼ê´€ì„±
            indicators['shift_pattern_consistency'] = self._calculate_shift_consistency(customer_data)
        
        if 'business_hour_efficiency' in expert_config['key_indicators']:
            # ìƒì—…: ì˜ì—…ì‹œê°„ íš¨ìœ¨ì„±
            indicators['business_hour_efficiency'] = self._calculate_business_hour_efficiency(customer_data)
        
        if 'service_consistency' in expert_config['key_indicators']:
            # ì„œë¹„ìŠ¤: ì„œë¹„ìŠ¤ ì¼ê´€ì„±
            indicators['service_consistency'] = 1 / (cv + 1)
        
        if 'overall_stability' in expert_config['key_indicators']:
            # ì¼ë°˜: ì „ë°˜ì  ì•ˆì •ì„±
            indicators['overall_stability'] = 1 / (cv + 1)
        
        return indicators
    
    def _calculate_shift_consistency(self, data):
        """êµëŒ€ íŒ¨í„´ ì¼ê´€ì„± ê³„ì‚°"""
        if 'datetime' not in data.columns:
            data['datetime'] = pd.to_datetime(data['LPìˆ˜ì‹ ì¼ì'])
        data['hour'] = data['datetime'].dt.hour
        
        # 3êµëŒ€ íŒ¨í„´ ê°€ì • (0-8, 8-16, 16-24)
        shift_1 = data[(data['hour'] >= 0) & (data['hour'] < 8)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        shift_2 = data[(data['hour'] >= 8) & (data['hour'] < 16)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        shift_3 = data[(data['hour'] >= 16) & (data['hour'] < 24)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        
        shifts = [shift_1, shift_2, shift_3]
        shifts = [s for s in shifts if not np.isnan(s)]
        
        if len(shifts) > 1:
            shift_consistency = 1 / (np.std(shifts) / np.mean(shifts) + 1)
        else:
            shift_consistency = 0
        
        return round(shift_consistency, 4)
    
    def _calculate_business_hour_efficiency(self, data):
        """ì˜ì—…ì‹œê°„ íš¨ìœ¨ì„± ê³„ì‚°"""
        if 'datetime' not in data.columns:
            data['datetime'] = pd.to_datetime(data['LPìˆ˜ì‹ ì¼ì'])
        data['hour'] = data['datetime'].dt.hour
        
        # ì˜ì—…ì‹œê°„ (9-21ì‹œ) vs ë¹„ì˜ì—…ì‹œê°„
        business_hours = data[(data['hour'] >= 9) & (data['hour'] <= 21)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        non_business_hours = data[(data['hour'] < 9) | (data['hour'] > 21)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        
        if non_business_hours > 0:
            efficiency = business_hours / (business_hours + non_business_hours)
        else:
            efficiency = 1.0
        
        return round(efficiency, 4)
    
    def _assess_expert_risks(self, expert_config, customer_data, indicators):
        """ì „ë¬¸ê°€ë³„ ìœ„í—˜ ìš”ì†Œ í‰ê°€"""
        risks = []
        thresholds = expert_config['thresholds']
        
        # ê¸°ë³¸ ë³€ë™ê³„ìˆ˜ ì²´í¬
        power_values = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        if len(power_values) > 0:
            cv = np.std(power_values) / (np.mean(power_values) + 1)
            if cv > thresholds.get('high_risk_cv', 0.7):
                risks.append(f"ë†’ì€ ë³€ë™ì„± ê°ì§€ (CV: {cv:.3f})")
        
        # ì „ë¬¸ê°€ë³„ íŠ¹í™” ìœ„í—˜ í‰ê°€
        for indicator, value in indicators.items():
            if 'efficiency' in indicator and value < thresholds.get('low_efficiency', 0.3):
                risks.append(f"ë‚®ì€ {indicator}: {value:.3f}")
        
        return risks
    
    def _generate_expert_recommendations(self, expert_config, indicators, risks):
        """ì „ë¬¸ê°€ë³„ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if risks:
            if expert_config['name'] == 'ì œì¡°ì—… ì „ë¬¸ê°€':
                recommendations.extend([
                    "ìƒì‚° ìŠ¤ì¼€ì¤„ ìµœì í™” ê²€í† ",
                    "ì„¤ë¹„ íš¨ìœ¨ì„± ê°œì„  ë°©ì•ˆ ìˆ˜ë¦½",
                    "ì˜ˆì¸¡ ì •ë¹„ ì‹œìŠ¤í…œ ë„ì… ê³ ë ¤"
                ])
            elif expert_config['name'] == 'ìƒì—…ì‹œì„¤ ì „ë¬¸ê°€':
                recommendations.extend([
                    "ê³ ê° ìœ ì… íŒ¨í„´ ë¶„ì„ ê°•í™”",
                    "í”¼í¬ì‹œê°„ ìš´ì˜ ìµœì í™”",
                    "ê³„ì ˆë³„ ìš´ì˜ ì „ëµ ìˆ˜ë¦½"
                ])
            elif expert_config['name'] == 'ì„œë¹„ìŠ¤ì—… ì „ë¬¸ê°€':
                recommendations.extend([
                    "ì„œë¹„ìŠ¤ í‘œì¤€í™” í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•",
                    "ë””ì§€í„¸ ì „í™˜ ë¡œë“œë§µ ìˆ˜ë¦½",
                    "ê³ ê° ê²½í—˜ ê°œì„  ë°©ì•ˆ ê²€í† "
                ])
            else:  # ì¼ë°˜ ì—…ì¢…
                recommendations.extend([
                    "ì¢…í•©ì  ìš´ì˜ íš¨ìœ¨ì„± ì§„ë‹¨",
                    "ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì²´ê³„ êµ¬ì¶•",
                    "ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì‹œìŠ¤í…œ ë„ì…"
                ])
        
        return recommendations
    
    def _generate_expert_insights(self, expert_config):
        """ì „ë¬¸ê°€ë³„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = [
            f"{expert_config['name']}ì˜ {expert_config['specialty']} ê´€ì ì—ì„œ ë¶„ì„ ì™„ë£Œ",
            f"ì£¼ìš” ì§€í‘œ: {', '.join(expert_config['key_indicators'][:3])}",
            f"ìœ„í—˜ ìš”ì†Œ: {', '.join(expert_config['risk_factors'][:2])}"
        ]
        return insights
    
    # ============ 5. ë””ì§€í„¸ ì „í™˜ ê°ì§€ê¸° ============
    
    def detect_digital_transformation(self, lp_data):
        """
        ğŸš€ ë””ì§€í„¸ ì „í™˜ ê°ì§€ê¸° - ê¸°ì—…ì˜ ë””ì§€í„¸í™” ìˆ˜ì¤€ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
        """
        print("ğŸš€ ë””ì§€í„¸ ì „í™˜ ê°ì§€ ì¤‘...")
        
        digital_scores = {}
        customers = lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        for customer in customers[:5]:  # ìƒ˜í”Œ 5ê°œ ê³ ê°
            customer_data = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()
            
            # ë””ì§€í„¸ ì „í™˜ ì§€í‘œë“¤
            automation_level = self._measure_automation_level(customer_data)
            remote_work_readiness = self._assess_remote_work_readiness(customer_data)
            smart_energy_usage = self._evaluate_smart_energy_usage(customer_data)
            digital_resilience = self._calculate_digital_resilience(customer_data)
            
            # ì¢…í•© ë””ì§€í„¸ ì „í™˜ ì ìˆ˜
            digital_transformation_score = self._calculate_digital_transformation_score(
                automation_level, remote_work_readiness, smart_energy_usage, digital_resilience
            )
            
            digital_scores[customer] = {
                'automation_level': automation_level,
                'remote_work_readiness': remote_work_readiness,
                'smart_energy_usage': smart_energy_usage,
                'digital_resilience': digital_resilience,
                'digital_transformation_score': digital_transformation_score,
                'digital_maturity_level': self._classify_digital_maturity(digital_transformation_score)
            }
        
        self.digital_transformation = digital_scores
        print(f"âœ… {len(digital_scores)}ê°œ ê¸°ì—… ë””ì§€í„¸ ì „í™˜ ë¶„ì„ ì™„ë£Œ")
        return digital_scores
    
    def _measure_automation_level(self, data):
        """ìë™í™” ìˆ˜ì¤€ ì¸¡ì •"""
        if 'datetime' not in data.columns:
            data['datetime'] = pd.to_datetime(data['LPìˆ˜ì‹ ì¼ì'])
        data['hour'] = data['datetime'].dt.hour
        
        # ì•¼ê°„ ìë™í™” ìš´ì˜ (ë¬´ì¸ ìš´ì˜ ìˆ˜ì¤€)
        night_hours = data[(data['hour'] >= 22) | (data['hour'] <= 6)]
        day_hours = data[(data['hour'] >= 9) & (data['hour'] <= 18)]
        
        if len(day_hours) > 0 and day_hours['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean() > 0:
            night_automation = night_hours['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean() / day_hours['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        else:
            night_automation = 0
        
        # ìš´ì˜ ì¼ê´€ì„± (ìë™í™”ëœ ì‹œìŠ¤í…œì˜ íŠ¹ì§•)
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        if len(power_values) > 1:
            operation_consistency = 1 / (np.std(power_values) / (np.mean(power_values) + 1) + 1)
        else:
            operation_consistency = 0
        
        automation_score = (night_automation * 0.6 + operation_consistency * 0.4)
        return round(min(automation_score, 1.0), 4)
    
    def _assess_remote_work_readiness(self, data):
        """ì›ê²©ê·¼ë¬´ ì¤€ë¹„ë„ í‰ê°€"""
        if 'datetime' not in data.columns:
            data['datetime'] = pd.to_datetime(data['LPìˆ˜ì‹ ì¼ì'])
        data['weekday'] = data['datetime'].dt.weekday
        
        # ì£¼ë§/íœ´ì¼ í™œë™ (ì›ê²© ì ‘ê·¼ì„±)
        weekend_data = data[data['weekday'].isin([5, 6])]  # í† , ì¼
        weekday_data = data[~data['weekday'].isin([5, 6])]
        
        if len(weekday_data) > 0 and weekday_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean() > 0:
            remote_accessibility = weekend_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean() / weekday_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        else:
            remote_accessibility = 0
        
        # ì‹œê°„ ìœ ì—°ì„± (ë‹¤ì–‘í•œ ì‹œê°„ëŒ€ í™œìš©)
        hourly_usage = data.groupby(data['datetime'].dt.hour)['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        time_flexibility = 1 - (hourly_usage.std() / (hourly_usage.mean() + 1))
        
        remote_readiness = (remote_accessibility * 0.4 + time_flexibility * 0.6)
        return round(min(remote_readiness, 1.0), 4)
    
    def _evaluate_smart_energy_usage(self, data):
        """ìŠ¤ë§ˆíŠ¸ ì—ë„ˆì§€ ì‚¬ìš© í‰ê°€"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        if len(power_values) < 10:
            return 0
        
        # ì—ë„ˆì§€ íš¨ìœ¨ì„± (ìµœì í™”ëœ ì‚¬ìš© íŒ¨í„´)
        # í”¼í¬ ì‹œê°„ íšŒí”¼ ì •ë„
        if 'datetime' not in data.columns:
            data['datetime'] = pd.to_datetime(data['LPìˆ˜ì‹ ì¼ì'])
        data['hour'] = data['datetime'].dt.hour
        
        peak_hours = [14, 15, 16, 17, 18, 19]  # ì¼ë°˜ì  í”¼í¬ ì‹œê°„
        off_peak_hours = [1, 2, 3, 4, 5, 22, 23]  # ì €ë¶€í•˜ ì‹œê°„
        
        peak_usage = data[data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        off_peak_usage = data[data['hour'].isin(off_peak_hours)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()
        
        if peak_usage > 0:
            peak_avoidance = 1 - (peak_usage / (peak_usage + off_peak_usage))
        else:
            peak_avoidance = 0
        
        # ì—ë„ˆì§€ ì‚¬ìš© ìµœì í™” (ë³€ë™ì„± ìµœì†Œí™”)
        energy_optimization = 1 / (np.std(power_values) / (np.mean(power_values) + 1) + 1)
        
        smart_usage = (peak_avoidance * 0.5 + energy_optimization * 0.5)
        return round(smart_usage, 4)
    
    def _calculate_digital_resilience(self, data):
        """ë””ì§€í„¸ ë³µì›ë ¥ ê³„ì‚°"""
        power_values = data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values
        
        if len(power_values) < 5:
            return 0
        
        # ì‹œìŠ¤í…œ ì•ˆì •ì„± (ê¸‰ê²©í•œ ë³€í™”ì— ëŒ€í•œ ë³µì›ë ¥)
        stability_score = 1 - (np.sum(np.abs(np.diff(power_values)) > 2 * np.std(power_values)) / len(power_values))
        
        # ì—°ì†ì„± (0ê°’ ìµœì†Œí™”)
        continuity_score = 1 - (np.sum(power_values == 0) / len(power_values))
        
        # ì ì‘ì„± (ì ì§„ì  ë³€í™” ìˆ˜ìš© ëŠ¥ë ¥)
        if len(power_values) > 10:
            first_half = power_values[:len(power_values)//2]
            second_half = power_values[len(power_values)//2:]
            adaptability = 1 - abs(np.mean(second_half) - np.mean(first_half)) / (np.mean(power_values) + 1)
        else:
            adaptability = 0.5
        
        resilience = (stability_score * 0.4 + continuity_score * 0.3 + adaptability * 0.3)
        return round(resilience, 4)
    
    def _calculate_digital_transformation_score(self, automation, remote, smart, resilience):
        """ì¢…í•© ë””ì§€í„¸ ì „í™˜ ì ìˆ˜"""
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        weights = {
            'automation': 0.3,
            'remote': 0.2,
            'smart': 0.25,
            'resilience': 0.25
        }
        
        total_score = (automation * weights['automation'] + 
                      remote * weights['remote'] + 
                      smart * weights['smart'] + 
                      resilience * weights['resilience'])
        
        return round(total_score, 4)
    
    def _classify_digital_maturity(self, score):
        """ë””ì§€í„¸ ì„±ìˆ™ë„ ë¶„ë¥˜"""
        if score >= 0.8:
            return "Level 5: ë””ì§€í„¸ ë„¤ì´í‹°ë¸Œ (Digital Native)"
        elif score >= 0.7:
            return "Level 4: ë””ì§€í„¸ ì„ ë„ (Digital Leader)"
        elif score >= 0.6:
            return "Level 3: ë””ì§€í„¸ ì ì‘ (Digital Adaptor)"
        elif score >= 0.4:
            return "Level 2: ë””ì§€í„¸ í•™ìŠµ (Digital Learner)"
        elif score >= 0.2:
            return "Level 1: ë””ì§€í„¸ ì‹œì‘ (Digital Beginner)"
        else:
            return "Level 0: ë””ì§€í„¸ ì¤€ë¹„ (Digital Ready)"
    
    # ============ 6. ì°½ì˜ì  ì¢…í•© ë³€ë™ê³„ìˆ˜ ê³„ì‚° ============
    
    def calculate_creative_volatility_coefficient(self, lp_data):
        """
        ğŸ¯ ì°½ì˜ì  ì¢…í•© ë³€ë™ê³„ìˆ˜ ê³„ì‚° - ëª¨ë“  ë¶„ì„ ê²°ê³¼ í†µí•©
        """
        print("ğŸ¯ ì°½ì˜ì  ì¢…í•© ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì¤‘...")
        
        # ëª¨ë“  ë¶„ì„ ìˆ˜í–‰
        dna_profiles = self.extract_power_dna(lp_data)
        health_diagnostics = self.diagnose_business_health(lp_data)
        time_predictions = self.build_time_travel_predictor(lp_data)
        digital_scores = self.detect_digital_transformation(lp_data)
        
        # ì¢…í•© ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        creative_coefficients = {}
        
        for customer in dna_profiles.keys():
            if customer in health_diagnostics and customer in digital_scores:
                
                # ê° ì°¨ì›ë³„ ì ìˆ˜ ì¶”ì¶œ
                dna_score = dna_profiles[customer]['uniqueness_score']
                health_score = health_diagnostics[customer]['health_grade']['score']
                digital_score = digital_scores[customer]['digital_transformation_score']
                
                time_score = 0.5  # ê¸°ë³¸ê°’
                if customer in time_predictions:
                    time_score = time_predictions[customer]['time_continuity_score']
                
                # ì°½ì˜ì  ë³€ë™ê³„ìˆ˜ ê³µì‹
                creative_vc = self._compute_creative_volatility_formula(
                    dna_score, health_score, digital_score, time_score
                )
                
                # ê²½ì˜ í™œë™ ë³€í™” ì˜ˆì¸¡
                business_change_prediction = self._predict_business_activity_change(
                    dna_profiles[customer], health_diagnostics[customer], digital_scores[customer]
                )
                
                creative_coefficients[customer] = {
                    'creative_volatility_coefficient': creative_vc,
                    'dna_contribution': dna_score,
                    'health_contribution': health_score,
                    'digital_contribution': digital_score,
                    'time_contribution': time_score,
                    'business_change_prediction': business_change_prediction,
                    'risk_level': self._assess_integrated_risk_level(creative_vc),
                    'recommendations': self._generate_integrated_recommendations(creative_vc, business_change_prediction)
                }
        
        print(f"âœ… {len(creative_coefficients)}ê°œ ê¸°ì—… ì°½ì˜ì  ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì™„ë£Œ")
        return creative_coefficients
    
    def _compute_creative_volatility_formula(self, dna, health, digital, time):
        """ì°½ì˜ì  ë³€ë™ê³„ìˆ˜ ê³µì‹"""
        # í˜ì‹ ì  ê°€ì¤‘ì¹˜ ì„¤ê³„
        weights = {
            'business_stability': 0.35,    # ê²½ì˜ ì•ˆì •ì„± (health ê¸°ë°˜)
            'innovation_capacity': 0.25,   # í˜ì‹  ì—­ëŸ‰ (digital ê¸°ë°˜)
            'unique_identity': 0.25,       # ê³ ìœ ì„± (dna ê¸°ë°˜)
            'predictability': 0.15         # ì˜ˆì¸¡ê°€ëŠ¥ì„± (time ê¸°ë°˜)
        }
        
        # ì •ê·œí™” ë° ê°€ì¤‘ í•©ì‚°
        stability_component = health * weights['business_stability']
        innovation_component = digital * weights['innovation_capacity']
        identity_component = min(dna, 1.0) * weights['unique_identity']
        predictability_component = time * weights['predictability']
        
        # ì°½ì˜ì  ë³€ë™ê³„ìˆ˜ = 1 - ì¢…í•©ì•ˆì •ì„±ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë¶ˆì•ˆì •)
        creative_vc = 1 - (stability_component + innovation_component + 
                          identity_component + predictability_component)
        
        return round(max(0, creative_vc), 4)
    
    def _predict_business_activity_change(self, dna_profile, health_diagnostic, digital_score):
        """ê²½ì˜ í™œë™ ë³€í™” ì˜ˆì¸¡"""
        # DNA íƒ€ì…ë³„ ë³€í™” íŒ¨í„´
        dna_type = dna_profile['dna_type']
        health_grade = health_diagnostic['health_grade']['status']
        digital_level = digital_score['digital_maturity_level']
        
        # ë³€í™” ì˜ˆì¸¡ ë¡œì§
        if "í˜ì‹  ì„±ì¥í˜•" in dna_type and "excellent" in health_grade:
            prediction = "ê¸‰ì†í•œ ë””ì§€í„¸ í™•ì¥ ì˜ˆìƒ"
        elif "ì•ˆì • ìš´ì˜í˜•" in dna_type and "good" in health_grade:
            prediction = "ì ì§„ì  íš¨ìœ¨ì„± ê°œì„  ì˜ˆìƒ"
        elif "ìœ ì—° ì ì‘í˜•" in dna_type:
            prediction = "ì‹œì¥ ë³€í™”ì— ë”°ë¥¸ ìš´ì˜ ì¡°ì • ì˜ˆìƒ"
        elif "êµ¬ì¡° ì¡°ì •í˜•" in dna_type:
            prediction = "ëŒ€ê·œëª¨ ì‚¬ì—… ì¬í¸ ê°€ëŠ¥ì„±"
        elif "critical" in health_grade or "risk" in health_grade:
            prediction = "ê¸´ê¸‰ ê²½ì˜ ê°œì„  ì¡°ì¹˜ í•„ìš”"
        else:
            prediction = "í˜„ ìƒíƒœ ìœ ì§€ ë˜ëŠ” ì†Œí­ ë³€í™” ì˜ˆìƒ"
        
        return prediction
    
    def _assess_integrated_risk_level(self, creative_vc):
        """í†µí•© ìœ„í—˜ ìˆ˜ì¤€ í‰ê°€"""
        if creative_vc >= 0.8:
            return "ë§¤ìš° ë†’ìŒ (ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”)"
        elif creative_vc >= 0.6:
            return "ë†’ìŒ (ì£¼ì˜ ê¹Šì€ ëª¨ë‹ˆí„°ë§)"
        elif creative_vc >= 0.4:
            return "ë³´í†µ (ì •ê¸°ì  ì ê²€)"
        elif creative_vc >= 0.2:
            return "ë‚®ìŒ (ì•ˆì •ì  ìš´ì˜)"
        else:
            return "ë§¤ìš° ë‚®ìŒ (ìµœì  ìƒíƒœ)"
    
    def _generate_integrated_recommendations(self, creative_vc, business_prediction):
        """í†µí•© ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if creative_vc >= 0.7:
            recommendations.extend([
                "ê¸´ê¸‰ ê²½ì˜ì§„ íšŒì˜ ì†Œì§‘",
                "ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ìƒì„¸ ë¶„ì„",
                "ë¹„ìƒ ìš´ì˜ ê³„íš í™œì„±í™”",
                "ì™¸ë¶€ ì „ë¬¸ê°€ ì»¨ì„¤íŒ… ê²€í† "
            ])
        elif creative_vc >= 0.5:
            recommendations.extend([
                "ì›”ê°„ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•",
                "ìš´ì˜ íš¨ìœ¨ì„± ê°œì„  ë°©ì•ˆ ìˆ˜ë¦½",
                "ì˜ˆì¸¡ ë¶„ì„ ì‹œìŠ¤í…œ ë„ì…",
                "ë¦¬ìŠ¤í¬ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ ê°•í™”"
            ])
        elif creative_vc >= 0.3:
            recommendations.extend([
                "ë¶„ê¸°ë³„ ì„±ê³¼ ë¦¬ë·°",
                "ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • í™•ëŒ€",
                "ë””ì§€í„¸ ì „í™˜ ë¡œë“œë§µ ì ê²€",
                "ì—ë„ˆì§€ íš¨ìœ¨ì„± ìµœì í™”"
            ])
        else:
            recommendations.extend([
                "í˜„ì¬ ìš´ì˜ ë°©ì‹ ìœ ì§€",
                "ëª¨ë²” ì‚¬ë¡€ ë²¤ì¹˜ë§ˆí‚¹",
                "í˜ì‹  ê¸°íšŒ íƒìƒ‰",
                "ì§€ì†ê°€ëŠ¥ì„± ê°•í™”"
            ])
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë³€í™” ì˜ˆì¸¡ì— ë”°ë¥¸ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        if "ë””ì§€í„¸ í™•ì¥" in business_prediction:
            recommendations.append("ë””ì§€í„¸ ì¸í”„ë¼ íˆ¬ì ê³„íš ìˆ˜ë¦½")
        elif "êµ¬ì¡° ì¡°ì •" in business_prediction:
            recommendations.append("ë³€í™” ê´€ë¦¬ ì „ëµ ìˆ˜ë¦½")
        elif "ê¸´ê¸‰ ê°œì„ " in business_prediction:
            recommendations.append("ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ê³¼ì œ ë„ì¶œ")
        
        return recommendations
    
    # ============ 7. ì°½ì˜ì  ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ============
    
    def generate_creative_report(self, lp_data, output_path='./creative_volatility_report.json'):
        """
        ğŸ“‹ ì°½ì˜ì  ë¶„ì„ ê²°ê³¼ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        """
        print("ğŸ“‹ ì°½ì˜ì  ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ì „ì²´ ë¶„ì„ ì‹¤í–‰
        creative_results = self.calculate_creative_volatility_coefficient(lp_data)
        
        # ì¢…í•© ë¦¬í¬íŠ¸ êµ¬ì„±
        comprehensive_report = {
            'report_metadata': {
                'title': 'í•œêµ­ì „ë ¥ê³µì‚¬ ì°½ì˜ì  ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ë¦¬í¬íŠ¸',
                'subtitle': 'ê¸°ì—… ê²½ì˜í™œë™ ë””ì§€í„¸ ë°”ì´ì˜¤ë§ˆì»¤ ì‹œìŠ¤í…œ',
                'generated_at': datetime.now().isoformat(),
                'analysis_version': '2.0_creative',
                'total_customers_analyzed': len(creative_results)
            },
            
            'executive_summary': {
                'key_findings': self._generate_key_findings(creative_results),
                'overall_risk_distribution': self._analyze_risk_distribution(creative_results),
                'digital_maturity_overview': self._summarize_digital_maturity(),
                'business_health_summary': self._summarize_business_health(),
                'strategic_recommendations': self._generate_strategic_recommendations(creative_results)
            },
            
            'detailed_analysis': {
                'power_dna_insights': self._summarize_dna_insights(),
                'health_diagnostic_results': self._summarize_health_results(),
                'time_travel_predictions': self._summarize_time_predictions(),
                'industry_expert_findings': self._summarize_expert_findings(),
                'digital_transformation_analysis': self._summarize_digital_analysis()
            },
            
            'customer_profiles': creative_results,
            
            'methodology': {
                'innovative_approach': [
                    "ì „ë ¥ DNA ì‹œí€€ì‹±: ê¸°ì—… ê³ ìœ ì˜ ì „ë ¥ ì‚¬ìš© ì§€ë¬¸ ë¶„ì„",
                    "ê²½ì˜ ê±´ê°•ë„ ì§„ë‹¨: ì˜í•™ì  ì ‘ê·¼ìœ¼ë¡œ ê¸°ì—… ìƒíƒœ í‰ê°€",
                    "ì‹œê°„ì—¬í–‰ ì˜ˆì¸¡: ê³¼ê±°-í˜„ì¬-ë¯¸ë˜ ì—°ê²° ë³€ë™ì„± ì˜ˆì¸¡",
                    "ì—…ì¢…ë³„ AI ì „ë¬¸ê°€: íŠ¹í™”ëœ ë¶„ì„ ì—”ì§„ í™œìš©",
                    "ë””ì§€í„¸ ì „í™˜ ê°ì§€: ì‹¤ì‹œê°„ ë””ì§€í„¸í™” ìˆ˜ì¤€ ëª¨ë‹ˆí„°ë§"
                ],
                'creative_volatility_formula': {
                    'components': {
                        'business_stability': 0.35,
                        'innovation_capacity': 0.25,
                        'unique_identity': 0.25,
                        'predictability': 0.15
                    },
                    'interpretation': "1 - ì¢…í•©ì•ˆì •ì„±ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë³€ë™ì„± í¼)"
                }
            },
            
            'actionable_insights': {
                'immediate_actions': self._extract_immediate_actions(creative_results),
                'medium_term_strategy': self._extract_medium_term_strategy(creative_results),
                'long_term_vision': self._extract_long_term_vision(creative_results)
            }
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_report, f, ensure_ascii=False, indent=2, default=str)
            print(f"âœ… ì°½ì˜ì  ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")
        except Exception as e:
            print(f"âŒ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ìš”ì•½ ì¶œë ¥
        self._print_executive_summary(comprehensive_report['executive_summary'])
        
        return comprehensive_report
    
    def _generate_key_findings(self, results):
        """í•µì‹¬ ë°œê²¬ì‚¬í•­ ìƒì„±"""
        if not results:
            return []
        
        # ë³€ë™ê³„ìˆ˜ ë¶„í¬ ë¶„ì„
        vc_values = [r['creative_volatility_coefficient'] for r in results.values()]
        avg_vc = np.mean(vc_values)
        max_vc = max(vc_values)
        min_vc = min(vc_values)
        
        findings = [
            f"í‰ê·  ì°½ì˜ì  ë³€ë™ê³„ìˆ˜: {avg_vc:.3f}",
            f"ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {min_vc:.3f} ~ {max_vc:.3f}",
            f"ê³ ìœ„í—˜ ê¸°ì—… ë¹„ìœ¨: {sum(1 for vc in vc_values if vc >= 0.6) / len(vc_values) * 100:.1f}%"
        ]
        
        # DNA íƒ€ì… ë¶„í¬
        if hasattr(self, 'dna_profiles') and self.dna_profiles:
            dna_types = [profile['dna_type'] for profile in self.dna_profiles.values()]
            most_common_dna = max(set(dna_types), key=dna_types.count)
            findings.append(f"ê°€ì¥ í”í•œ DNA íƒ€ì…: {most_common_dna}")
        
        # ë””ì§€í„¸ ì„±ìˆ™ë„ ë¶„í¬
        if hasattr(self, 'digital_transformation') and self.digital_transformation:
            digital_levels = [score['digital_maturity_level'] for score in self.digital_transformation.values()]
            advanced_digital = sum(1 for level in digital_levels if "Level 4" in level or "Level 5" in level)
            findings.append(f"ë””ì§€í„¸ ì„ ë„ ê¸°ì—…: {advanced_digital}ê°œ ({advanced_digital/len(digital_levels)*100:.1f}%)")
        
        return findings
    
    def _analyze_risk_distribution(self, results):
        """ìœ„í—˜ ë¶„í¬ ë¶„ì„"""
        if not results:
            return {}
        
        risk_levels = [r['risk_level'] for r in results.values()]
        risk_distribution = {}
        
        for risk in risk_levels:
            if risk in risk_distribution:
                risk_distribution[risk] += 1
            else:
                risk_distribution[risk] = 1
        
        # ë¹„ìœ¨ë¡œ ë³€í™˜
        total = len(risk_levels)
        for risk in risk_distribution:
            risk_distribution[risk] = {
                'count': risk_distribution[risk],
                'percentage': round(risk_distribution[risk] / total * 100, 1)
            }
        
        return risk_distribution
    
    def _summarize_digital_maturity(self):
        """ë””ì§€í„¸ ì„±ìˆ™ë„ ìš”ì•½"""
        if not hasattr(self, 'digital_transformation') or not self.digital_transformation:
            return "ë””ì§€í„¸ ì„±ìˆ™ë„ ë¶„ì„ ë°ì´í„° ì—†ìŒ"
        
        levels = list(self.digital_transformation.values())
        avg_score = np.mean([level['digital_transformation_score'] for level in levels])
        
        return f"í‰ê·  ë””ì§€í„¸ ì „í™˜ ì ìˆ˜: {avg_score:.3f}"
    
    def _summarize_business_health(self):
        """ê²½ì˜ ê±´ê°•ë„ ìš”ì•½"""
        if not hasattr(self, 'health_diagnostics') or not self.health_diagnostics:
            return "ê²½ì˜ ê±´ê°•ë„ ë¶„ì„ ë°ì´í„° ì—†ìŒ"
        
        health_scores = [diag['health_grade']['score'] for diag in self.health_diagnostics.values()]
        avg_health = np.mean(health_scores)
        
        return f"í‰ê·  ê²½ì˜ ê±´ê°•ë„: {avg_health:.3f}"
    
    def _generate_strategic_recommendations(self, results):
        """ì „ëµì  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        if not results:
            return []
        
        high_risk_count = sum(1 for r in results.values() if r['creative_volatility_coefficient'] >= 0.6)
        total_count = len(results)
        
        recommendations = []
        
        if high_risk_count / total_count > 0.3:
            recommendations.extend([
                "ì „ì‚¬ì  ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì²´ê³„ êµ¬ì¶• í•„ìš”",
                "ê³ ìœ„í—˜ ê³ ê° ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë„ì…",
                "ì˜ˆì¸¡ ë¶„ì„ ê¸°ë°˜ ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ êµ¬ì¶•"
            ])
        
        recommendations.extend([
            "ì—…ì¢…ë³„ ë§ì¶¤í˜• ë³€ë™ê³„ìˆ˜ ê¸°ì¤€ ê°œë°œ",
            "ë””ì§€í„¸ ì „í™˜ ì§€ì› í”„ë¡œê·¸ë¨ í™•ëŒ€",
            "ì—ë„ˆì§€ íš¨ìœ¨ì„± ì»¨ì„¤íŒ… ì„œë¹„ìŠ¤ ê°•í™”",
            "ë°ì´í„° ê¸°ë°˜ ê³ ê° ì„¸ë¶„í™” ì „ëµ ìˆ˜ë¦½"
        ])
        
        return recommendations
    
    def _summarize_dna_insights(self):
        """DNA ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìš”ì•½"""
        if not hasattr(self, 'dna_profiles') or not self.dna_profiles:
            return {}
        
        return {
            'total_analyzed': len(self.dna_profiles),
            'unique_patterns_found': len(set(profile['dna_type'] for profile in self.dna_profiles.values())),
            'avg_uniqueness_score': round(np.mean([profile['uniqueness_score'] for profile in self.dna_profiles.values()]), 3)
        }
    
    def _summarize_health_results(self):
        """ê±´ê°• ì§„ë‹¨ ê²°ê³¼ ìš”ì•½"""
        if not hasattr(self, 'health_diagnostics') or not self.health_diagnostics:
            return {}
        
        health_grades = [diag['health_grade']['status'] for diag in self.health_diagnostics.values()]
        grade_distribution = {}
        for grade in health_grades:
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
        
        return {
            'total_diagnosed': len(self.health_diagnostics),
            'grade_distribution': grade_distribution,
            'critical_patients': grade_distribution.get('critical', 0)
        }
    
    def _summarize_time_predictions(self):
        """ì‹œê°„ì—¬í–‰ ì˜ˆì¸¡ ìš”ì•½"""
        if not hasattr(self, 'time_travel_predictions') or not self.time_travel_predictions:
            return {}
        
        return {
            'predictions_made': len(self.time_travel_predictions),
            'avg_continuity_score': round(np.mean([pred['time_continuity_score'] for pred in self.time_travel_predictions.values()]), 3)
        }
    
    def _summarize_expert_findings(self):
        """ì „ë¬¸ê°€ ë¶„ì„ ìš”ì•½"""
        if not hasattr(self, 'industry_experts') or not self.industry_experts:
            return {}
        
        total_recommendations = sum(len(expert['recommendations']) for expert in self.industry_experts.values())
        total_risks = sum(len(expert['risk_alerts']) for expert in self.industry_experts.values())
        
        return {
            'experts_deployed': len(self.industry_experts),
            'total_recommendations': total_recommendations,
            'total_risk_alerts': total_risks
        }
    
    def _summarize_digital_analysis(self):
        """ë””ì§€í„¸ ë¶„ì„ ìš”ì•½"""
        if not hasattr(self, 'digital_transformation') or not self.digital_transformation:
            return {}
        
        scores = list(self.digital_transformation.values())
        automation_avg = np.mean([s['automation_level'] for s in scores])
        smart_usage_avg = np.mean([s['smart_energy_usage'] for s in scores])
        
        return {
            'companies_analyzed': len(scores),
            'avg_automation_level': round(automation_avg, 3),
            'avg_smart_usage': round(smart_usage_avg, 3)
        }
    
    def _extract_immediate_actions(self, results):
        """ì¦‰ì‹œ ì‹¤í–‰ ê³¼ì œ ì¶”ì¶œ"""
        immediate_actions = []
        
        for customer, result in results.items():
            if result['creative_volatility_coefficient'] >= 0.7:
                immediate_actions.append(f"{customer}: ê¸´ê¸‰ ëª¨ë‹ˆí„°ë§ ë° ê°œì… í•„ìš”")
        
        if not immediate_actions:
            immediate_actions = ["í˜„ì¬ ê¸´ê¸‰ ëŒ€ì‘ì´ í•„ìš”í•œ ê³ ê° ì—†ìŒ"]
        
        return immediate_actions
    
    def _extract_medium_term_strategy(self, results):
        """ì¤‘ê¸° ì „ëµ ì¶”ì¶œ"""
        return [
            "ì—…ì¢…ë³„ ë³€ë™ê³„ìˆ˜ ë²¤ì¹˜ë§ˆí¬ êµ¬ì¶•",
            "ê³ ê° ì„¸ë¶„í™” ê¸°ë°˜ ë§ì¶¤í˜• ì„œë¹„ìŠ¤ ê°œë°œ",
            "ì˜ˆì¸¡ ë¶„ì„ ëª¨ë¸ ê³ ë„í™”",
            "ë””ì§€í„¸ ì „í™˜ ì§€ì› í”„ë¡œê·¸ë¨ í™•ëŒ€"
        ]
    
    def _extract_long_term_vision(self, results):
        """ì¥ê¸° ë¹„ì „ ì¶”ì¶œ"""
        return [
            "AI ê¸°ë°˜ ì‹¤ì‹œê°„ ì—ë„ˆì§€ ìµœì í™” í”Œë«í¼ êµ¬ì¶•",
            "ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜ ì‹ ê·œ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ê°œë°œ",
            "ì‚°ì—…ë³„ ì—ë„ˆì§€ íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ êµ¬ì¶•",
            "ì§€ì†ê°€ëŠ¥í•œ ì—ë„ˆì§€ ìƒíƒœê³„ ì¡°ì„±"
        ]
    
    def _print_executive_summary(self, summary):
        """ê²½ì˜ì§„ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ† ì°½ì˜ì  ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        print("\nğŸ“Š í•µì‹¬ ë°œê²¬ì‚¬í•­:")
        for finding in summary['key_findings']:
            print(f"  â€¢ {finding}")
        
        print(f"\nğŸ¥ ê²½ì˜ ê±´ê°•ë„: {summary['business_health_summary']}")
        print(f"ğŸš€ ë””ì§€í„¸ ì„±ìˆ™ë„: {summary['digital_maturity_overview']}")
        
        print("\nğŸ“‹ ì „ëµì  ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(summary['strategic_recommendations'][:3], 1):
            print(f"  {i}. {rec}")
        
        print("\nğŸ¯ ë¶„ì„ ì™„ë£Œ: í•œêµ­ì „ë ¥ê³µì‚¬ ì°½ì˜ì  ë³€ë™ê³„ìˆ˜ ì‹œìŠ¤í…œ ê°€ë™!")

# ============ ì‹¤í–‰ í•¨ìˆ˜ ============

def main_creative_analysis():
    """ì°½ì˜ì  ë¶„ì„ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¨ í•œêµ­ì „ë ¥ê³µì‚¬ ì°½ì˜ì  ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ì‹œìŠ¤í…œ")
    print("ğŸ§¬ ê¸°ì—… ê²½ì˜í™œë™ ë””ì§€í„¸ ë°”ì´ì˜¤ë§ˆì»¤ ë¶„ì„ ì‹œì‘")
    print("=" * 80)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = CreativePowerDNAAnalyzer()
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” LP ë°ì´í„° ë¡œë”©)
    sample_lp_data = create_enhanced_sample_data()
    
    # ì°½ì˜ì  ë¶„ì„ ì‹¤í–‰
    report = analyzer.generate_creative_report(sample_lp_data)
    
    print("\nğŸ‰ ì°½ì˜ì  ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“ ê²°ê³¼ íŒŒì¼: creative_volatility_report.json")
    
    return report

def create_enhanced_sample_data():
    """í–¥ìƒëœ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    
    data = []
    customers = [f'KEPCO_{i:04d}' for i in range(1, 11)]  # 10ê°œ ê³ ê°
    
    base_date = datetime(2024, 1, 1)
    
    for customer in customers:
        # ê³ ê°ë³„ íŠ¹ì„± ë¶€ì—¬
        customer_idx = int(customer.split('_')[1])
        
        # ê¸°ì—… íƒ€ì…ë³„ íŒ¨í„´ ì„¤ì •
        if customer_idx <= 3:
            # ì œì¡°ì—… íŒ¨í„´: ë†’ì€ ì‚¬ìš©ëŸ‰, ê·œì¹™ì 
            base_power = np.random.uniform(200, 500)
            volatility = np.random.uniform(0.1, 0.3)
        elif customer_idx <= 6:
            # ìƒì—…ì‹œì„¤ íŒ¨í„´: ì¤‘ê°„ ì‚¬ìš©ëŸ‰, ì‹œê°„ëŒ€ë³„ ë³€ë™
            base_power = np.random.uniform(100, 300)
            volatility = np.random.uniform(0.3, 0.6)
        else:
            # ì„œë¹„ìŠ¤ì—… íŒ¨í„´: ë‚®ì€ ì‚¬ìš©ëŸ‰, ë†’ì€ ë³€ë™ì„±
            base_power = np.random.uniform(50, 200)
            volatility = np.random.uniform(0.5, 0.8)
        
        # 30ì¼ê°„ 15ë¶„ ê°„ê²© ë°ì´í„° ìƒì„±
        for day in range(30):
            for hour in range(24):
                for minute in [0, 15, 30, 45]:
                    timestamp = base_date + timedelta(days=day, hours=hour, minutes=minute)
                    
                    # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë°˜ì˜
                    time_factor = 1.0
                    if 9 <= hour <= 18:  # ì—…ë¬´ì‹œê°„
                        time_factor = 1.2
                    elif 22 <= hour or hour <= 6:  # ì•¼ê°„
                        time_factor = 0.3
                    
                    # ìš”ì¼ë³„ íŒ¨í„´
                    weekday = timestamp.weekday()
                    if weekday >= 5:  # ì£¼ë§
                        time_factor *= 0.6
                    
                    # ë…¸ì´ì¦ˆ ì¶”ê°€
                    noise = np.random.normal(0, base_power * volatility)
                    power = max(0, base_power * time_factor + noise)
                    
                    data.append({
                        'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,
                        'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),
                        'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 2),
                        'ì§€ìƒë¬´íš¨': round(power * 0.1, 2),
                        'ì§„ìƒë¬´íš¨': round(power * 0.05, 2),
                        'í”¼ìƒì „ë ¥': round(power * 1.1, 2)
                    })
    
    df = pd.DataFrame(data)
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df):,}ë ˆì½”ë“œ")
    return df

if __name__ == "__main__":
    # ì°½ì˜ì  ë¶„ì„ ì‹¤í–‰
    main_creative_analysis()