"""
í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ (ì •í™•ë„ ìš°ì„  ë²„ì „)
- ê³ ì†ëª¨ë“œ ì œê±°, ì •í™•ë„ ìµœìš°ì„ 
- ì¶©ë¶„í•œ êµì°¨ê²€ì¦ê³¼ ëª¨ë¸ ì„±ëŠ¥ í™•ë³´
- ê³¼ì í•© ë°©ì§€ ê°•í™”
"""

import pandas as pd
import numpy as np
import json
import os
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, ElasticNet
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from math import pi
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'

class KEPCOVolatilityAnalyzer:
    """í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ê¸° (ì •í™•ë„ ìš°ì„  ë²„ì „)"""
    
    def __init__(self, results_dir='./analysis_results', sampling_config=None):
        self.results_dir = results_dir
        self.scaler = StandardScaler()
        self.robust_scaler = RobustScaler()
        self.level0_models = {}
        self.meta_model = None
        
        # ìƒ˜í”Œë§ ì„¤ì • (ì •í™•ë„ ìš°ì„ )
        self.sampling_config = sampling_config or {
            'customer_sample_ratio': 0.7,      # ê³ ê°ì˜ 70%ë§Œ ìƒ˜í”Œë§ (ì •í™•ë„ í™•ë³´)
            'time_sample_ratio': 0.5,          # ì‹œê°„ ë°ì´í„°ì˜ 50%ë§Œ ìƒ˜í”Œë§  
            'min_customers': 50,               # ìµœì†Œ 50ëª…
            'min_records_per_customer': 200,   # ê³ ê°ë‹¹ ìµœì†Œ 200ê°œ ë ˆì½”ë“œ
            'stratified_sampling': True,       # ê³„ì¸µ ìƒ˜í”Œë§ ì‚¬ìš©
            'validation_folds': 5              # 5-fold êµì°¨ê²€ì¦
        }
        
        # ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©
        self.step1_results = self._load_step1_results()
        self.step2_results = self._load_step2_results()
        self.lp_data = None
        self.kepco_data = None
        
        print("ğŸ”§ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ê¸° ì´ˆê¸°í™” (ì •í™•ë„ ìš°ì„ )")
        print(f"   ğŸ“Š ìƒ˜í”Œë§ ì„¤ì •: ê³ ê° {self.sampling_config['customer_sample_ratio']*100:.0f}%, ì‹œê°„ {self.sampling_config['time_sample_ratio']*100:.0f}%")
        print(f"   ğŸ¯ ì •í™•ë„ ìš°ì„  ëª¨ë“œ: ì¶©ë¶„í•œ ê²€ì¦ê³¼ ê³¼ì í•© ë°©ì§€")
        
    def _load_step1_results(self):
        """1ë‹¨ê³„ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   âŒ 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_step2_results(self):
        """2ë‹¨ê³„ ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results2.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   âŒ 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def load_preprocessed_data_with_sampling(self):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© + ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§"""
        print("\nğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ë° ìƒ˜í”Œë§ ì¤‘...")
        
        # 1. LP ë°ì´í„° ë¡œë”©
        hdf5_path = os.path.join(self.results_dir, 'processed_lp_data.h5')
        csv_path = os.path.join(self.results_dir, 'processed_lp_data.csv')
        
        if os.path.exists(hdf5_path):
            try:
                self.lp_data = pd.read_hdf(hdf5_path, key='df')
                loading_method = "HDF5"
            except Exception as e:
                if os.path.exists(csv_path):
                    self.lp_data = pd.read_csv(csv_path)
                    loading_method = "CSV"
                else:
                    print(f"   âŒ LP ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
        elif os.path.exists(csv_path):
            self.lp_data = pd.read_csv(csv_path)
            loading_method = "CSV"
        else:
            print(f"   âŒ ì „ì²˜ë¦¬ëœ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        original_size = len(self.lp_data)
        print(f"   ğŸ“ ì›ë³¸ ë°ì´í„°: {original_size:,}ê±´ ({loading_method})")
        
        # 2. ì»¬ëŸ¼ ì •ë¦¬ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
        self._prepare_columns()
        
        # 3. ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì ìš© (ì •í™•ë„ ìš°ì„ )
        self._apply_smart_sampling()
        
        sampled_size = len(self.lp_data)
        reduction_ratio = (1 - sampled_size/original_size) * 100
        
        print(f"   âœ‚ï¸ ìƒ˜í”Œë§ í›„: {sampled_size:,}ê±´")
        print(f"   ğŸ“‰ ë°ì´í„° ê°ì†Œ: {reduction_ratio:.1f}%")
        print(f"   ğŸ“… ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}")
        print(f"   ğŸ‘¥ ê³ ê°ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}")
        
        return True
    
    def _prepare_columns(self):
        """ì»¬ëŸ¼ ì •ë¦¬ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
        # datetime ì»¬ëŸ¼ ì²˜ë¦¬
        datetime_col = None
        for col in ['datetime', 'LP ìˆ˜ì‹ ì¼ì', 'LPìˆ˜ì‹ ì¼ì', 'timestamp']:
            if col in self.lp_data.columns:
                datetime_col = col
                break
        
        if datetime_col:
            self.lp_data['datetime'] = pd.to_datetime(self.lp_data[datetime_col], errors='coerce')
            self.lp_data = self.lp_data.dropna(subset=['datetime'])
        else:
            raise ValueError("ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì „ë ¥ ì»¬ëŸ¼ ì²˜ë¦¬
        power_col = None
        for col in ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'power', 'ì „ë ¥ëŸ‰']:
            if col in self.lp_data.columns:
                if col != 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥':
                    self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] = self.lp_data[col]
                power_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
                break
        
        if not power_col:
            raise ValueError("ìˆœë°©í–¥ ìœ íš¨ì „ë ¥ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° í’ˆì§ˆ ì •ë¦¬
        self.lp_data = self.lp_data.dropna(subset=['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'])
        self.lp_data.loc[self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] < 0, 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] = 0
        
        # ê·¹ë‹¨ ì´ìƒì¹˜ ì²˜ë¦¬ (99.9% ë¶„ìœ„ìˆ˜ë¡œ ìº¡í•‘)
        q999 = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].quantile(0.999)
        self.lp_data.loc[self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] > q999, 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] = q999
    
    def _apply_smart_sampling(self):
        """ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì ìš© (ì •í™•ë„ ìš°ì„ )"""
        print("   ğŸ¯ ì •í™•ë„ ìš°ì„  ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì ìš© ì¤‘...")
        
        # 1. ê³ ê°ë³„ ë°ì´í„° ì¶©ë¶„ì„± í™•ì¸
        customer_counts = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].value_counts()
        sufficient_customers = customer_counts[
            customer_counts >= self.sampling_config['min_records_per_customer']
        ].index.tolist()
        
        print(f"      ì¶©ë¶„í•œ ë°ì´í„° ë³´ìœ  ê³ ê°: {len(sufficient_customers)}ëª…")
        
        # 2. ê³„ì¸µ ìƒ˜í”Œë§ (ì—…ì¢…ë³„, ê·œëª¨ë³„)
        if self.sampling_config['stratified_sampling']:
            sampled_customers = self._stratified_customer_sampling(sufficient_customers)
        else:
            # ë‹¨ìˆœ ëœë¤ ìƒ˜í”Œë§
            n_customers = max(
                self.sampling_config['min_customers'],
                int(len(sufficient_customers) * self.sampling_config['customer_sample_ratio'])
            )
            sampled_customers = np.random.choice(
                sufficient_customers, 
                size=min(n_customers, len(sufficient_customers)), 
                replace=False
            ).tolist()
        
        print(f"      ìƒ˜í”Œë§ëœ ê³ ê°: {len(sampled_customers)}ëª…")
        
        # 3. ê³ ê° í•„í„°ë§
        self.lp_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].isin(sampled_customers)]
        
        # 4. ì‹œê°„ ìƒ˜í”Œë§ (ê° ê³ ê°ë³„ë¡œ) - ì •í™•ë„ ë³´ì¥ì„ ìœ„í•´ ë” ë§ì€ ë°ì´í„° ìœ ì§€
        if self.sampling_config['time_sample_ratio'] < 1.0:
            sampled_data = []
            
            for customer_id in sampled_customers:
                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id]
                
                # ì‹œê°„ ê¸°ë°˜ ê³„ì¸µ ìƒ˜í”Œë§ (í”¼í¬/ë¹„í”¼í¬, ì£¼ì¤‘/ì£¼ë§ ê· ë“±í•˜ê²Œ)
                n_samples = max(
                    self.sampling_config['min_records_per_customer'],
                    int(len(customer_data) * self.sampling_config['time_sample_ratio'])
                )
                
                if len(customer_data) <= n_samples:
                    sampled_data.append(customer_data)
                else:
                    # ì‹œê°„ ê· ë“± ìƒ˜í”Œë§ (ì •í™•ë„ í™•ë³´ë¥¼ ìœ„í•´ ëŒ€í‘œì ì¸ ì‹œê°„ëŒ€ í¬í•¨)
                    sampled_data.append(self._balanced_time_sampling(customer_data, n_samples))
            
            self.lp_data = pd.concat(sampled_data, ignore_index=True)
            print(f"      ì‹œê°„ ìƒ˜í”Œë§ ì™„ë£Œ (ê· í˜• ì¡íŒ ì‹œê°„ëŒ€ í¬í•¨)")
    
    def _balanced_time_sampling(self, customer_data, n_samples):
        """ê· í˜• ì¡íŒ ì‹œê°„ ìƒ˜í”Œë§ (í”¼í¬/ë¹„í”¼í¬, ì£¼ì¤‘/ì£¼ë§ ê³ ë ¤)"""
        customer_data = customer_data.copy()
        customer_data['hour'] = customer_data['datetime'].dt.hour
        customer_data['weekday'] = customer_data['datetime'].dt.weekday
        customer_data['is_weekend'] = customer_data['weekday'].isin([5, 6])
        
        # ì‹œê°„ëŒ€ë³„, ì£¼ì¤‘/ì£¼ë§ë³„ ê·¸ë£¹ ìƒì„±
        groups = []
        
        # í”¼í¬ ì‹œê°„ëŒ€ (9-11, 14-15, 18-19) - ì£¼ì¤‘
        peak_weekday = customer_data[
            (customer_data['hour'].isin([9, 10, 11, 14, 15, 18, 19])) & 
            (~customer_data['is_weekend'])
        ]
        
        # ë¹„í”¼í¬ ì‹œê°„ëŒ€ (0-5, 22-23) - ì£¼ì¤‘
        off_peak_weekday = customer_data[
            (customer_data['hour'].isin([0, 1, 2, 3, 4, 5, 22, 23])) & 
            (~customer_data['is_weekend'])
        ]
        
        # ì¼ë°˜ ì‹œê°„ëŒ€ - ì£¼ì¤‘
        normal_weekday = customer_data[
            (~customer_data['hour'].isin([0, 1, 2, 3, 4, 5, 9, 10, 11, 14, 15, 18, 19, 22, 23])) & 
            (~customer_data['is_weekend'])
        ]
        
        # ì£¼ë§ ë°ì´í„°
        weekend_data = customer_data[customer_data['is_weekend']]
        
        # ê° ê·¸ë£¹ì—ì„œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œë§
        total_samples = n_samples
        samples_per_group = total_samples // 4
        
        sampled_groups = []
        for group in [peak_weekday, off_peak_weekday, normal_weekday, weekend_data]:
            if len(group) > 0:
                group_samples = min(samples_per_group, len(group))
                if group_samples > 0:
                    sampled_groups.append(group.sample(n=group_samples, random_state=42))
        
        # ë‚¨ì€ ìƒ˜í”Œ ìˆ˜ë¥¼ ê°€ì¥ í° ê·¸ë£¹ì—ì„œ ì¶”ê°€ ìƒ˜í”Œë§
        current_total = sum(len(g) for g in sampled_groups)
        remaining = total_samples - current_total
        
        if remaining > 0 and len(customer_data) > current_total:
            used_indices = set()
            for g in sampled_groups:
                used_indices.update(g.index)
            
            remaining_data = customer_data[~customer_data.index.isin(used_indices)]
            if len(remaining_data) > 0:
                additional_samples = min(remaining, len(remaining_data))
                sampled_groups.append(remaining_data.sample(n=additional_samples, random_state=42))
        
        if sampled_groups:
            return pd.concat(sampled_groups)
        else:
            # í´ë°±: ë‹¨ìˆœ ëœë¤ ìƒ˜í”Œë§
            return customer_data.sample(n=min(n_samples, len(customer_data)), random_state=42)
    
    def _stratified_customer_sampling(self, customers):
        """ê³„ì¸µë³„ ê³ ê° ìƒ˜í”Œë§ (ì •í™•ë„ ìš°ì„ )"""
        # ê³ ê°ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ê³„ì¸µ êµ¬ë¶„
        customer_power_avg = self.lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
        
        # 4ê°œ ê³„ì¸µìœ¼ë¡œ êµ¬ë¶„ (ì†Œí˜•, ì¤‘ì†Œí˜•, ì¤‘í˜•, ëŒ€í˜•) - ë” ì„¸ë°€í•œ êµ¬ë¶„
        q25, q50, q75 = customer_power_avg.quantile([0.25, 0.50, 0.75])
        
        small_customers = customer_power_avg[customer_power_avg <= q25].index.tolist()
        medium_small_customers = customer_power_avg[(customer_power_avg > q25) & (customer_power_avg <= q50)].index.tolist()
        medium_customers = customer_power_avg[(customer_power_avg > q50) & (customer_power_avg <= q75)].index.tolist()
        large_customers = customer_power_avg[customer_power_avg > q75].index.tolist()
        
        # ê° ê³„ì¸µì—ì„œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œë§
        total_target = max(
            self.sampling_config['min_customers'],
            int(len(customers) * self.sampling_config['customer_sample_ratio'])
        )
        
        small_n = min(len(small_customers), max(1, total_target // 4))
        medium_small_n = min(len(medium_small_customers), max(1, total_target // 4))
        medium_n = min(len(medium_customers), max(1, total_target // 4))
        large_n = min(len(large_customers), max(1, total_target - small_n - medium_small_n - medium_n))
        
        sampled = []
        if small_customers:
            sampled.extend(np.random.choice(small_customers, size=small_n, replace=False))
        if medium_small_customers:
            sampled.extend(np.random.choice(medium_small_customers, size=medium_small_n, replace=False))
        if medium_customers:
            sampled.extend(np.random.choice(medium_customers, size=medium_n, replace=False))
        if large_customers:
            sampled.extend(np.random.choice(large_customers, size=large_n, replace=False))
        
        print(f"      ê³„ì¸µë³„ ìƒ˜í”Œë§: ì†Œí˜•{small_n}ëª…, ì¤‘ì†Œí˜•{medium_small_n}ëª…, ì¤‘í˜•{medium_n}ëª…, ëŒ€í˜•{large_n}ëª…")
        return sampled
    
    def calculate_enhanced_volatility_coefficient(self):
        """í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° (ì •í™•ë„ ìš°ì„ )"""
        print("\nğŸ“ í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì¤‘ (ì •í™•ë„ ìš°ì„ )...")
        
        if self.lp_data is None:
            print("   âŒ LP ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2ë‹¨ê³„ ê²°ê³¼ì—ì„œ ì‹œê°„ íŒ¨í„´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        temporal_patterns = self.step2_results.get('temporal_patterns', {})
        peak_hours = temporal_patterns.get('peak_hours', [9, 10, 11, 14, 15, 18, 19])
        off_peak_hours = temporal_patterns.get('off_peak_hours', [0, 1, 2, 3, 4, 5])
        weekend_ratio = temporal_patterns.get('weekend_ratio', 1.0)
        
        print(f"   ğŸ• í”¼í¬ ì‹œê°„: {peak_hours}")
        print(f"   ğŸŒ™ ë¹„í”¼í¬ ì‹œê°„: {off_peak_hours}")
        
        # ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour
        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday
        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])
        self.lp_data['month'] = self.lp_data['datetime'].dt.month
        self.lp_data['date'] = self.lp_data['datetime'].dt.date
        
        customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        volatility_results = {}
        volatility_components = []
        processed_count = 0
        
        print(f"   ğŸ‘¥ ë¶„ì„ ëŒ€ìƒ: {len(customers)}ëª…")
        
        for customer_id in customers:
            try:
                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id].copy()
                
                if len(customer_data) < self.sampling_config['min_records_per_customer']:
                    continue
                
                power_values = customer_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].values
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (ë” ì—„ê²©í•œ ê¸°ì¤€)
                if np.std(power_values) == 0 or np.mean(power_values) <= 0:
                    continue
                
                # ë³€ë™ì„± ì§€í‘œ ê³„ì‚° (ë” ì •ë°€í•œ ê³„ì‚°)
                volatility_metrics = self._calculate_volatility_metrics_accurate(
                    customer_data, power_values, peak_hours, off_peak_hours, weekend_ratio
                )
                
                if volatility_metrics:
                    volatility_components.append({
                        'customer_id': customer_id,
                        **volatility_metrics
                    })
                    processed_count += 1
                
                if processed_count % 10 == 0:
                    print(f"      ì§„í–‰ë¥ : {processed_count}/{len(customers)} ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âš ï¸ ê³ ê° {customer_id} ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        print(f"   âœ… {processed_count}ëª… ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        
        # ê°€ì¤‘ì¹˜ ìµœì í™” (ì •í™•ë„ ìš°ì„  - ì¶©ë¶„í•œ ìµœì í™”)
        if len(volatility_components) >= 20:
            optimal_weights = self.optimize_volatility_weights_accurate(volatility_components)
        else:
            optimal_weights = [0.35, 0.25, 0.20, 0.10, 0.10]  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            print(f"   âš ï¸ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
        
        print(f"   ğŸ¯ ìµœì¢… ê°€ì¤‘ì¹˜: {[round(w, 3) for w in optimal_weights]}")
        
        # ìµœì¢… ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        for component in volatility_components:
            customer_id = component['customer_id']
            
            enhanced_volatility_coefficient = (
                optimal_weights[0] * component['basic_cv'] +
                optimal_weights[1] * component['hourly_cv'] +
                optimal_weights[2] * component['peak_cv'] +
                optimal_weights[3] * component['weekend_diff'] +
                optimal_weights[4] * component['seasonal_cv']
            )
            
            volatility_results[customer_id] = {
                'enhanced_volatility_coefficient': round(enhanced_volatility_coefficient, 4),
                'basic_cv': round(component['basic_cv'], 4),
                'hourly_cv': round(component['hourly_cv'], 4),
                'peak_cv': round(component['peak_cv'], 4),
                'off_peak_cv': round(component['off_peak_cv'], 4),
                'weekday_cv': round(component['weekday_cv'], 4),
                'weekend_cv': round(component['weekend_cv'], 4),
                'weekend_diff': round(component['weekend_diff'], 4),
                'seasonal_cv': round(component['seasonal_cv'], 4),
                'load_factor': round(component['load_factor'], 4),
                'peak_load_ratio': round(component['peak_load_ratio'], 4),
                'mean_power': round(component['mean_power'], 4),
                'zero_ratio': round(component['zero_ratio'], 4),
                'extreme_changes': int(component['extreme_changes']),
                'data_points': component['data_points'],
                'optimized_weights': [round(w, 3) for w in optimal_weights],
                'stability_score': round(component.get('stability_score', 0), 4),
                'predictability_score': round(component.get('predictability_score', 0), 4)
            }
        
        if len(volatility_results) > 0:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   ğŸ“ˆ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
            print(f"   ğŸ“Š ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        return volatility_results
    
    def _calculate_volatility_metrics_accurate(self, customer_data, power_values, peak_hours, off_peak_hours, weekend_ratio):
        """ê°œë³„ ê³ ê°ì˜ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° (ì •í™•ë„ ìš°ì„ )"""
        try:
            mean_power = np.mean(power_values)
            
            # 1. ê¸°ë³¸ ë³€ë™ê³„ìˆ˜ (ë” ì •ë°€í•œ ê³„ì‚°)
            basic_cv = np.std(power_values, ddof=1) / mean_power
            
            # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ê³„ìˆ˜ (24ì‹œê°„ ì„¸ë¶„í™”)
            hourly_avg = customer_data.groupby('hour')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg(['mean', 'std', 'count'])
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ì‹œê°„ëŒ€ë§Œ ê³ ë ¤
            valid_hours = hourly_avg[hourly_avg['count'] >= 5]
            hourly_cv = (np.std(valid_hours['mean']) / np.mean(valid_hours['mean'])) if len(valid_hours) > 3 and np.mean(valid_hours['mean']) > 0 else basic_cv
            
            # 3. í”¼í¬/ë¹„í”¼í¬ ë³€ë™ì„± (ë” ì •ë°€í•œ ë¶„ì„)
            peak_data = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            off_peak_data = customer_data[customer_data['hour'].isin(off_peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            
            peak_cv = (np.std(peak_data, ddof=1) / np.mean(peak_data)) if len(peak_data) > 10 and np.mean(peak_data) > 0 else basic_cv
            off_peak_cv = (np.std(off_peak_data, ddof=1) / np.mean(off_peak_data)) if len(off_peak_data) > 10 and np.mean(off_peak_data) > 0 else basic_cv
            
            # 4. ì£¼ë§/í‰ì¼ ë³€ë™ì„± (ë” ì„¸ë°€í•œ ë¶„ì„)
            weekday_data = customer_data[~customer_data['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            weekend_data = customer_data[customer_data['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            
            weekday_cv = (np.std(weekday_data, ddof=1) / np.mean(weekday_data)) if len(weekday_data) > 20 and np.mean(weekday_data) > 0 else basic_cv
            weekend_cv = (np.std(weekend_data, ddof=1) / np.mean(weekend_data)) if len(weekend_data) > 10 and np.mean(weekend_data) > 0 else basic_cv
            weekend_diff = abs(weekday_cv - weekend_cv) * weekend_ratio
            
            # 5. ê³„ì ˆë³„ ë³€ë™ì„± (ì¼ë³„/ì£¼ë³„ ì§‘ê³„ë¡œ ë” ì •ë°€í•˜ê²Œ)
            daily_avg = customer_data.groupby('date')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            seasonal_cv = (np.std(daily_avg, ddof=1) / np.mean(daily_avg)) if len(daily_avg) > 7 and np.mean(daily_avg) > 0 else basic_cv
            
            # 6. ì¶”ê°€ ì•ˆì •ì„± ì§€í‘œë“¤
            max_power = np.max(power_values)
            min_power = np.min(power_values[power_values > 0]) if np.sum(power_values > 0) > 0 else 0
            load_factor = mean_power / max_power if max_power > 0 else 0
            zero_ratio = (power_values == 0).sum() / len(power_values)
            
            # ê¸‰ê²©í•œ ë³€í™” ê°ì§€ (ë” ì •ë°€í•œ ì„ê³„ê°’)
            power_series = pd.Series(power_values)
            pct_changes = power_series.pct_change().dropna()
            extreme_changes = (np.abs(pct_changes) > 2.0).sum()  # 200% ë³€í™”
            
            # í”¼í¬/ë¹„í”¼í¬ ë¶€í•˜ ë¹„ìœ¨
            peak_avg = np.mean(peak_data) if len(peak_data) > 0 else mean_power
            off_peak_avg = np.mean(off_peak_data) if len(off_peak_data) > 0 else mean_power
            peak_load_ratio = peak_avg / off_peak_avg if off_peak_avg > 0 else 1.0
            
            # 7. ì•ˆì •ì„± ì ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
            # ë‚®ì€ ë³€ë™ì„±, ë†’ì€ ë¶€í•˜ìœ¨, ì ì€ ì œë¡œê°’, ì ì€ ê·¹í•œ ë³€í™”
            stability_score = (
                (1 - min(basic_cv, 1.0)) * 0.4 +  # ê¸°ë³¸ ë³€ë™ì„± ì—­ìˆ˜
                load_factor * 0.3 +  # ë¶€í•˜ìœ¨
                (1 - zero_ratio) * 0.2 +  # ì œë¡œê°’ ì—­ìˆ˜
                (1 - min(extreme_changes / len(power_values), 1.0)) * 0.1  # ê·¹í•œ ë³€í™” ì—­ìˆ˜
            )
            
            # 8. ì˜ˆì¸¡ê°€ëŠ¥ì„± ì ìˆ˜ (íŒ¨í„´ì˜ ê·œì¹™ì„±)
            # ì‹œê°„ëŒ€ë³„ ì¼ê´€ì„±, ì£¼ì¤‘/ì£¼ë§ ì¼ê´€ì„±
            time_consistency = 1 - (hourly_cv / (basic_cv + 1e-6))
            day_consistency = 1 - abs(weekday_cv - weekend_cv) / (basic_cv + 1e-6)
            predictability_score = (time_consistency * 0.6 + day_consistency * 0.4)
            predictability_score = max(0, min(1, predictability_score))
            
            return {
                'basic_cv': basic_cv,
                'hourly_cv': hourly_cv,
                'peak_cv': peak_cv,
                'off_peak_cv': off_peak_cv,
                'weekday_cv': weekday_cv,
                'weekend_cv': weekend_cv,
                'weekend_diff': weekend_diff,
                'seasonal_cv': seasonal_cv,
                'load_factor': load_factor,
                'zero_ratio': zero_ratio,
                'extreme_changes': extreme_changes,
                'peak_load_ratio': peak_load_ratio,
                'mean_power': mean_power,
                'max_power': max_power,
                'min_power': min_power,
                'data_points': len(power_values),
                'stability_score': stability_score,
                'predictability_score': predictability_score
            }
            
        except Exception as e:
            return None
    
    def optimize_volatility_weights_accurate(self, volatility_components):
        """ê°€ì¤‘ì¹˜ ìµœì í™” (ì •í™•ë„ ìš°ì„ )"""
        print("\nâš™ï¸ ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘ (ì •í™•ë„ ìš°ì„ )...")
        
        try:
            from scipy.optimize import minimize, differential_evolution
        except ImportError:
            print("   âš ï¸ scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            return [0.35, 0.25, 0.20, 0.10, 0.10]
        
        components_df = pd.DataFrame(volatility_components)
        
        # ë³µí•© ëª©í‘œ í•¨ìˆ˜: ì˜ì—…í™œë™ ë¶ˆì•ˆì •ì„±ê³¼ ì˜ˆì¸¡ ì–´ë ¤ì›€
        target_instability = (
            components_df['basic_cv'] * 3.0 +  # ê¸°ë³¸ ë³€ë™ì„±
            components_df['zero_ratio'] * 2.0 +  # ì‚¬ìš© ì¤‘ë‹¨ ë¹ˆë„
            (1 - components_df['load_factor']) * 1.5 +  # ë¹„íš¨ìœ¨ì  ì‚¬ìš©
            (1 - components_df['stability_score']) * 2.0 +  # ë¶ˆì•ˆì •ì„±
            (1 - components_df['predictability_score']) * 1.0  # ì˜ˆì¸¡ ì–´ë ¤ì›€
        ).values
        
        X = components_df[['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']].values
        y = target_instability
        
        # í‘œì¤€í™”
        from sklearn.preprocessing import StandardScaler
        scaler_x = StandardScaler()
        scaler_y = StandardScaler()
        X_scaled = scaler_x.fit_transform(X)
        y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()
        
        def objective(weights):
            predicted = X_scaled @ weights
            mse = np.mean((predicted - y_scaled) ** 2)
            # ê°€ì¤‘ì¹˜ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ì—¬ ê·¹ë‹¨ì ì¸ ê°€ì¤‘ì¹˜ ë°©ì§€ (ì •ê·œí™”)
            weight_penalty = np.std(weights) * 0.1
            return mse + weight_penalty
        
        # ì œì•½ ì¡°ê±´
        constraints = [
            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0},  # ê°€ì¤‘ì¹˜ í•© = 1
            {'type': 'ineq', 'fun': lambda w: w[0] - 0.1},  # ê¸°ë³¸ CVëŠ” ìµœì†Œ 10%
        ]
        bounds = [(0.05, 0.6) for _ in range(5)]  # ê° ê°€ì¤‘ì¹˜ëŠ” 5%-60% ë²”ìœ„
        
        # ì—¬ëŸ¬ ë²ˆ ìµœì í™” ì‹œë„í•˜ì—¬ ìµœì í•´ ì°¾ê¸°
        best_result = None
        best_score = float('inf')
        
        for seed in range(5):
            initial_weights = np.random.dirichlet([1, 1, 1, 1, 1])  # í•©ì´ 1ì¸ ëœë¤ ê°€ì¤‘ì¹˜
            
            try:
                # SLSQP ë°©ë²•
                result = minimize(objective, initial_weights, method='SLSQP', 
                                bounds=bounds, constraints=constraints, 
                                options={'maxiter': 200})
                
                if result.success and result.fun < best_score:
                    best_result = result
                    best_score = result.fun
                    
            except:
                continue
        
        # Differential Evolution ì‹œë„ (ê¸€ë¡œë²Œ ìµœì í™”)
        try:
            def objective_de(weights):
                if abs(np.sum(weights) - 1.0) > 0.01:  # ê°€ì¤‘ì¹˜ í•© ì œì•½
                    return 1e6
                if np.any(weights < 0.05) or np.any(weights > 0.6):  # ë²”ìœ„ ì œì•½
                    return 1e6
                return objective(weights)
            
            bounds_de = [(0.05, 0.6) for _ in range(5)]
            result_de = differential_evolution(objective_de, bounds_de, 
                                             maxiter=100, seed=42)
            
            if result_de.success and result_de.fun < best_score:
                best_result = result_de
                best_score = result_de.fun
                
        except:
            pass
        
        if best_result and best_result.success:
            optimal_weights = best_result.x
            # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ ì •í™•íˆ 1ì´ ë˜ë„ë¡)
            optimal_weights = optimal_weights / np.sum(optimal_weights)
            print(f"   âœ… ê°€ì¤‘ì¹˜ ìµœì í™” ì™„ë£Œ (ëª©ì í•¨ìˆ˜ê°’: {best_score:.4f})")
            
            # ìµœì í™” í’ˆì§ˆ ê²€ì¦
            r2_score_weights = self._validate_weight_optimization(X_scaled, y_scaled, optimal_weights)
            print(f"   ğŸ“Š ê°€ì¤‘ì¹˜ ìµœì í™” RÂ²: {r2_score_weights:.4f}")
            
            return optimal_weights.tolist()
        else:
            print("   âš ï¸ ìµœì í™” ì‹¤íŒ¨, ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            return [0.35, 0.25, 0.20, 0.10, 0.10]
    
    def _validate_weight_optimization(self, X_scaled, y_scaled, weights):
        """ê°€ì¤‘ì¹˜ ìµœì í™” ê²€ì¦"""
        predicted = X_scaled @ weights
        ss_res = np.sum((y_scaled - predicted) ** 2)
        ss_tot = np.sum((y_scaled - np.mean(y_scaled)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        return r2
    
    def train_stacking_ensemble_model_accurate(self, volatility_results):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ (ì •í™•ë„ ìš°ì„ )"""
        print("\nğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘ (ì •í™•ë„ ìš°ì„ )...")
        
        if len(volatility_results) < 20:
            print("   âŒ í›ˆë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 20ê°œ í•„ìš”)")
            return None
        
        # íŠ¹ì„± ì¶”ì¶œ (ë” ë§ì€ íŠ¹ì„± í¬í•¨)
        features = []
        targets = []
        
        for customer_id, data in volatility_results.items():
            try:
                feature_vector = [
                    data['basic_cv'], data['hourly_cv'], data['peak_cv'],
                    data['off_peak_cv'], data['weekday_cv'], data['weekend_cv'],
                    data['seasonal_cv'], data['load_factor'], data['peak_load_ratio'],
                    data['mean_power'], data['zero_ratio'],
                    data['extreme_changes'] / data['data_points'],
                    data['stability_score'], data['predictability_score'],
                    np.log1p(data['mean_power']),  # ë¡œê·¸ ë³€í™˜ëœ í‰ê·  ì „ë ¥
                    data['max_power'] / data['mean_power'] if data['mean_power'] > 0 else 0,  # ìµœëŒ€/í‰ê·  ë¹„ìœ¨
                ]
                
                if any(np.isnan(x) or np.isinf(x) for x in feature_vector):
                    continue
                    
                features.append(feature_vector)
                targets.append(data['enhanced_volatility_coefficient'])
                
            except KeyError as e:
                print(f"   âš ï¸ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                continue
        
        X = np.array(features)
        y = np.array(targets)
        
        print(f"   ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±")
        
        # ë°ì´í„° ë¶„í•  (ê³„ì¸µí™” ë¶„í• )
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.25, random_state=42, stratify=pd.cut(y, bins=5, labels=False)
        )
        
        # ì •ê·œí™” (RobustScaler - ì´ìƒì¹˜ì— ê°•í•¨)
        X_train_scaled = self.robust_scaler.fit_transform(X_train)
        X_test_scaled = self.robust_scaler.transform(X_test)
        
        # Level-0 ëª¨ë¸ë“¤ (ì •í™•ë„ ìš°ì„  - ê³¼ì í•© ë°©ì§€)
        self.level0_models = {
            'rf': RandomForestRegressor(
                n_estimators=100, max_depth=8, min_samples_split=5, 
                min_samples_leaf=3, random_state=42, n_jobs=-1
            ),
            'gbm': GradientBoostingRegressor(
                n_estimators=100, max_depth=6, learning_rate=0.1,
                min_samples_split=5, min_samples_leaf=3, random_state=42
            ),
            'ridge': Ridge(alpha=1.0, random_state=42),
            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=2000)
        }
        
        # 5-fold êµì°¨ê²€ì¦
        kf = KFold(n_splits=self.sampling_config['validation_folds'], shuffle=True, random_state=42)
        
        meta_features_train = np.zeros((len(X_train_scaled), len(self.level0_models)))
        meta_features_test = np.zeros((len(X_test_scaled), len(self.level0_models)))
        
        level0_performance = {}
        
        print(f"   ğŸ”„ Level-0 ëª¨ë¸ í›ˆë ¨ ({self.sampling_config['validation_folds']}-Fold CV):")
        
        for i, (name, model) in enumerate(self.level0_models.items()):
            fold_predictions = np.zeros(len(X_train_scaled))
            fold_scores = []
            
            for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):
                try:
                    fold_model = type(model)(**model.get_params())
                    fold_model.fit(X_train_scaled[train_idx], y_train[train_idx])
                    
                    val_pred = fold_model.predict(X_train_scaled[val_idx])
                    fold_predictions[val_idx] = val_pred
                    
                    # Fold ì„±ëŠ¥ í‰ê°€
                    fold_mae = mean_absolute_error(y_train[val_idx], val_pred)
                    fold_r2 = r2_score(y_train[val_idx], val_pred)
                    fold_scores.append({'mae': fold_mae, 'r2': fold_r2})
                    
                except Exception as e:
                    print(f"      âš ï¸ {name} Fold {fold+1} ì‹¤íŒ¨: {e}")
                    fold_predictions[val_idx] = np.mean(y_train[train_idx])
            
            meta_features_train[:, i] = fold_predictions
            
            # ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¡œ ì¬í›ˆë ¨
            try:
                model.fit(X_train_scaled, y_train)
                meta_features_test[:, i] = model.predict(X_test_scaled)
                
                # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€
                test_pred = model.predict(X_test_scaled)
                test_mae = mean_absolute_error(y_test, test_pred)
                test_r2 = r2_score(y_test, test_pred) if len(set(y_test)) > 1 else 0.0
                test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
                
                # CV ì„±ëŠ¥ í‰ê· 
                cv_mae = np.mean([score['mae'] for score in fold_scores])
                cv_r2 = np.mean([score['r2'] for score in fold_scores])
                
                level0_performance[name] = {
                    'cv_mae': cv_mae, 'cv_r2': cv_r2,
                    'test_mae': test_mae, 'test_r2': test_r2, 'test_rmse': test_rmse
                }
                
                print(f"      {name}: CV MAE={cv_mae:.4f}, Test MAE={test_mae:.4f}, Test RÂ²={test_r2:.4f}")
                
            except Exception as e:
                print(f"      âŒ {name} í›ˆë ¨ ì‹¤íŒ¨: {e}")
                meta_features_test[:, i] = np.mean(y_train)
                level0_performance[name] = {'cv_mae': 999, 'cv_r2': 0, 'test_mae': 999, 'test_r2': 0, 'test_rmse': 999}
        
        # Level-1 ë©”íƒ€ ëª¨ë¸ (Ridge Regression with CV)
        meta_models = {
            'ridge': Ridge(alpha=1.0),
            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=2000),
            'linear': LinearRegression()
        }
        
        best_meta_model = None
        best_meta_score = -999
        final_performance = {}
        
        print(f"   ğŸ”„ Level-1 ë©”íƒ€ëª¨ë¸ ì„ íƒ:")
        
        for meta_name, meta_model in meta_models.items():
            try:
                # ë©”íƒ€ëª¨ë¸ êµì°¨ê²€ì¦
                cv_scores = cross_val_score(meta_model, meta_features_train, y_train, 
                                          cv=3, scoring='r2')
                avg_cv_score = np.mean(cv_scores)
                
                # ë©”íƒ€ëª¨ë¸ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸
                meta_model.fit(meta_features_train, y_train)
                final_pred = meta_model.predict(meta_features_test)
                
                final_mae = mean_absolute_error(y_test, final_pred)
                final_r2 = r2_score(y_test, final_pred) if len(set(y_test)) > 1 else 0.0
                final_rmse = np.sqrt(mean_squared_error(y_test, final_pred))
                
                print(f"      {meta_name}: CV RÂ²={avg_cv_score:.4f}, Test RÂ²={final_r2:.4f}")
                
                if final_r2 > best_meta_score:
                    best_meta_model = meta_model
                    best_meta_score = final_r2
                    final_performance = {
                        'final_mae': final_mae,
                        'final_r2': final_r2,
                        'final_rmse': final_rmse,
                        'cv_r2': avg_cv_score,
                        'meta_model_name': meta_name
                    }
                
            except Exception as e:
                print(f"      âŒ {meta_name} ë©”íƒ€ëª¨ë¸ ì‹¤íŒ¨: {e}")
        
        if best_meta_model is not None:
            self.meta_model = best_meta_model
            
            print(f"   âœ… ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ")
            print(f"      ìµœê³  ë©”íƒ€ëª¨ë¸: {final_performance['meta_model_name']}")
            print(f"      ìµœì¢… MAE: {final_performance['final_mae']:.4f}")
            print(f"      ìµœì¢… RÂ²: {final_performance['final_r2']:.4f}")
            print(f"      ìµœì¢… RMSE: {final_performance['final_rmse']:.4f}")
            
            # ê³¼ì í•© ê²€ì‚¬
            train_pred = self.meta_model.predict(meta_features_train)
            train_r2 = r2_score(y_train, train_pred)
            overfitting_gap = train_r2 - final_performance['final_r2']
            
            print(f"      ê³¼ì í•© ì ê²€: í›ˆë ¨ RÂ²={train_r2:.4f}, ì°¨ì´={overfitting_gap:.4f}")
            if overfitting_gap > 0.1:
                print(f"      âš ï¸ ê³¼ì í•© ì˜ì‹¬ (ì°¨ì´ > 0.1)")
            else:
                print(f"      âœ… ê³¼ì í•© ì—†ìŒ")
            
            return {
                **final_performance,
                'level0_performance': level0_performance,
                'level0_models': list(self.level0_models.keys()),
                'n_samples': len(X),
                'n_features': X.shape[1],
                'overfitting_gap': overfitting_gap,
                'accuracy_optimized': True
            }
        else:
            print("   âŒ ëª¨ë“  ë©”íƒ€ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
            return None

    def analyze_business_stability_accurate(self, volatility_results):
        """ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„ (ì •í™•ë„ ìš°ì„ )"""
        print("\nğŸ” ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„ ì¤‘ (ì •í™•ë„ ìš°ì„ )...")
        
        if not volatility_results:
            return {}
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
        stability_scores = [v['stability_score'] for v in volatility_results.values()]
        predictability_scores = [v['predictability_score'] for v in volatility_results.values()]
        
        # ë‹¤ì°¨ì› ë¶„ì„ì„ ìœ„í•œ ë¶„ìœ„ìˆ˜ ê³„ì‚°
        cv_p33, cv_p67 = np.percentile(coefficients, [33, 67])
        stability_p33, stability_p67 = np.percentile(stability_scores, [33, 67])
        
        stability_analysis = {}
        grade_counts = {'ì•ˆì •': 0, 'ë³´í†µ': 0, 'ì£¼ì˜': 0, 'ìœ„í—˜': 0}
        
        for customer_id, data in volatility_results.items():
            coeff = data['enhanced_volatility_coefficient']
            stability = data['stability_score']
            predictability = data['predictability_score']
            
            # 4ë‹¨ê³„ ë“±ê¸‰ ë¶„ë¥˜ (ë” ì„¸ë°€í•œ ë¶„ë¥˜)
            if coeff <= cv_p33 and stability >= stability_p67:
                grade = 'ì•ˆì •'
                risk_level = 'low'
            elif coeff <= cv_p67 and stability >= stability_p33:
                grade = 'ë³´í†µ'
                risk_level = 'medium'
            elif coeff > cv_p67 or stability < stability_p33:
                grade = 'ì£¼ì˜'
                risk_level = 'high'
            else:
                grade = 'ìœ„í—˜'
                risk_level = 'very_high'
            
            grade_counts[grade] += 1
            
            # ì •ë°€í•œ ìœ„í—˜ ìš”ì¸ ë¶„ì„
            risk_factors = []
            if data.get('zero_ratio', 0) > 0.15:
                risk_factors.append('ë¹ˆë²ˆí•œ_ì‚¬ìš©ì¤‘ë‹¨')
            if data.get('load_factor', 1) < 0.2:
                risk_factors.append('ë§¤ìš°_ë‚®ì€_ë¶€í•˜ìœ¨')
            elif data.get('load_factor', 1) < 0.4:
                risk_factors.append('ë‚®ì€_ë¶€í•˜ìœ¨')
            if data.get('peak_cv', 0) > data.get('basic_cv', 0) * 2.5:
                risk_factors.append('í”¼í¬ì‹œê°„_ê·¹ë„_ë¶ˆì•ˆì •')
            elif data.get('peak_cv', 0) > data.get('basic_cv', 0) * 1.5:
                risk_factors.append('í”¼í¬ì‹œê°„_ë¶ˆì•ˆì •')
            if data.get('extreme_changes', 0) / data.get('data_points', 1) > 0.05:
                risk_factors.append('ê¸‰ê²©í•œ_ë³€í™”_ë¹ˆë°œ')
            if predictability < 0.3:
                risk_factors.append('ì˜ˆì¸¡_ë¶ˆê°€ëŠ¥í•œ_íŒ¨í„´')
            if data.get('weekend_diff', 0) > 0.3:
                risk_factors.append('ì£¼ì¤‘ì£¼ë§_íŒ¨í„´_ìƒì´')
            
            # ì˜ì—…í™œë™ ë³€í™” ì˜ˆì¸¡
            business_trend = self._predict_business_trend(data)
            
            stability_analysis[customer_id] = {
                'enhanced_volatility_coefficient': round(coeff, 4),
                'stability_grade': grade,
                'risk_level': risk_level,
                'risk_factors': risk_factors,
                'stability_score': round(stability, 4),
                'predictability_score': round(predictability, 4),
                'load_factor': data.get('load_factor', 0.0),
                'peak_load_ratio': data.get('peak_load_ratio', 1.0),
                'business_trend_prediction': business_trend,
                'monitoring_priority': self._calculate_monitoring_priority(coeff, stability, risk_factors)
            }
        
        print(f"   ğŸ“‹ ì•ˆì •ì„± ë“±ê¸‰ ë¶„í¬:")
        total = len(stability_analysis)
        for grade, count in grade_counts.items():
            percentage = count / total * 100 if total > 0 else 0
            print(f"      {grade}: {count}ëª… ({percentage:.1f}%)")
        
        # ìœ„í—˜ ìš”ì¸ ìƒìœ„ ë¶„ì„
        all_risk_factors = []
        for analysis in stability_analysis.values():
            all_risk_factors.extend(analysis['risk_factors'])
        
        from collections import Counter
        risk_factor_counts = Counter(all_risk_factors)
        print(f"   ğŸš¨ ì£¼ìš” ìœ„í—˜ ìš”ì¸:")
        for factor, count in risk_factor_counts.most_common(5):
            print(f"      {factor}: {count}ê±´")
        
        return stability_analysis
    
    def _predict_business_trend(self, customer_data):
        """ì˜ì—…í™œë™ ë³€í™” ì˜ˆì¸¡"""
        coeff = customer_data['enhanced_volatility_coefficient']
        stability = customer_data['stability_score']
        predictability = customer_data['predictability_score']
        zero_ratio = customer_data.get('zero_ratio', 0)
        load_factor = customer_data.get('load_factor', 1)
        
        # ë³µí•© ì§€í‘œ ê¸°ë°˜ íŠ¸ë Œë“œ ì˜ˆì¸¡
        if coeff < 0.2 and stability > 0.7 and zero_ratio < 0.05:
            return 'ì•ˆì •ì _ì„±ì¥'
        elif coeff < 0.3 and stability > 0.5:
            return 'ì ì§„ì _ê°œì„ '
        elif coeff > 0.5 or stability < 0.3 or zero_ratio > 0.2:
            return 'ì˜ì—…í™œë™_ìœ„ì¶•'
        elif load_factor < 0.3 and predictability < 0.4:
            return 'ë¶ˆê·œì¹™ì _ìš´ì˜'
        else:
            return 'í˜„ìƒ_ìœ ì§€'
    
    def _calculate_monitoring_priority(self, coeff, stability, risk_factors):
        """ëª¨ë‹ˆí„°ë§ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        priority_score = 0
        
        # ë³€ë™ê³„ìˆ˜ ì ìˆ˜
        if coeff > 0.6:
            priority_score += 3
        elif coeff > 0.4:
            priority_score += 2
        elif coeff > 0.3:
            priority_score += 1
        
        # ì•ˆì •ì„± ì ìˆ˜
        if stability < 0.3:
            priority_score += 3
        elif stability < 0.5:
            priority_score += 2
        elif stability < 0.7:
            priority_score += 1
        
        # ìœ„í—˜ ìš”ì¸ ì ìˆ˜
        high_risk_factors = ['ë¹ˆë²ˆí•œ_ì‚¬ìš©ì¤‘ë‹¨', 'ë§¤ìš°_ë‚®ì€_ë¶€í•˜ìœ¨', 'í”¼í¬ì‹œê°„_ê·¹ë„_ë¶ˆì•ˆì •']
        for factor in risk_factors:
            if factor in high_risk_factors:
                priority_score += 2
            else:
                priority_score += 1
        
        # ìš°ì„ ìˆœìœ„ ë“±ê¸‰
        if priority_score >= 7:
            return 'ìµœìš°ì„ '
        elif priority_score >= 5:
            return 'ë†’ìŒ'
        elif priority_score >= 3:
            return 'ë³´í†µ'
        else:
            return 'ë‚®ìŒ'

    def generate_comprehensive_report(self, volatility_results, model_performance, stability_analysis):
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (ì •í™•ë„ ìš°ì„ )"""
        print("\nğŸ“‹ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()] if volatility_results else []
        
        # ê³ ìœ„í—˜ ê³ ê° ì‹ë³„
        high_risk_customers = [
            customer_id for customer_id, analysis in stability_analysis.items()
            if analysis['risk_level'] in ['high', 'very_high']
        ] if stability_analysis else []
        
        # ëª¨ë‹ˆí„°ë§ ìš°ì„ ìˆœìœ„ë³„ ê³ ê° ë¶„ë¥˜
        priority_groups = {'ìµœìš°ì„ ': [], 'ë†’ìŒ': [], 'ë³´í†µ': [], 'ë‚®ìŒ': []}
        for customer_id, analysis in stability_analysis.items():
            priority = analysis.get('monitoring_priority', 'ë‚®ìŒ')
            priority_groups[priority].append(customer_id)
        
        # ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì§‘ê³„
        all_risk_factors = []
        for analysis in stability_analysis.values():
            all_risk_factors.extend(analysis['risk_factors'])
        
        from collections import Counter
        risk_factor_counts = Counter(all_risk_factors)
        
        # ì˜ì—…í™œë™ íŠ¸ë Œë“œ ë¶„ì„
        trend_counts = Counter([
            analysis['business_trend_prediction'] 
            for analysis in stability_analysis.values()
        ])
        
        report = {
            'analysis_metadata': {
                'timestamp': pd.Timestamp.now().isoformat(),
                'algorithm_version': 'accuracy_optimized_v1',
                'sampling_config': self.sampling_config,
                'total_customers_analyzed': len(volatility_results),
                'execution_mode': 'accuracy_first',
                'validation_method': f"{self.sampling_config['validation_folds']}-fold_cross_validation"
            },
            
            'data_quality_summary': {
                'customer_sample_ratio': self.sampling_config['customer_sample_ratio'],
                'time_sample_ratio': self.sampling_config['time_sample_ratio'],
                'min_records_per_customer': self.sampling_config['min_records_per_customer'],
                'stratified_sampling_used': self.sampling_config['stratified_sampling']
            },
            
            'volatility_coefficient_analysis': {
                'total_customers': len(volatility_results),
                'mean_coefficient': round(np.mean(coefficients), 4) if coefficients else 0,
                'std_coefficient': round(np.std(coefficients), 4) if coefficients else 0,
                'percentiles': {
                    '10%': round(np.percentile(coefficients, 10), 4) if coefficients else 0,
                    '25%': round(np.percentile(coefficients, 25), 4) if coefficients else 0,
                    '50%': round(np.percentile(coefficients, 50), 4) if coefficients else 0,
                    '75%': round(np.percentile(coefficients, 75), 4) if coefficients else 0,
                    '90%': round(np.percentile(coefficients, 90), 4) if coefficients else 0
                },
                'distribution_analysis': {
                    'ë§¤ìš°_ì•ˆì • (<0.2)': len([c for c in coefficients if c < 0.2]),
                    'ì•ˆì • (0.2-0.3)': len([c for c in coefficients if 0.2 <= c < 0.3]),
                    'ë³´í†µ (0.3-0.4)': len([c for c in coefficients if 0.3 <= c < 0.4]),
                    'ì£¼ì˜ (0.4-0.6)': len([c for c in coefficients if 0.4 <= c < 0.6]),
                    'ìœ„í—˜ (0.6-0.8)': len([c for c in coefficients if 0.6 <= c < 0.8]),
                    'ê³ ìœ„í—˜ (>=0.8)': len([c for c in coefficients if c >= 0.8])
                }
            },
            
            'model_performance_analysis': model_performance or {},
            
            'business_stability_assessment': {
                'grade_distribution': {
                    grade: sum(1 for a in stability_analysis.values() if a['stability_grade'] == grade)
                    for grade in ['ì•ˆì •', 'ë³´í†µ', 'ì£¼ì˜', 'ìœ„í—˜']
                } if stability_analysis else {},
                
                'trend_prediction': dict(trend_counts),
                
                'monitoring_priority': {
                    priority: len(customers) for priority, customers in priority_groups.items()
                }
            },
            
            'risk_analysis': {
                'high_risk_customers': len(high_risk_customers),
                'high_risk_percentage': round(len(high_risk_customers) / len(stability_analysis) * 100, 1) if stability_analysis else 0,
                'top_risk_factors': dict(risk_factor_counts.most_common(5)),
                'critical_alerts': self._generate_critical_alerts(stability_analysis)
            },
            
            'algorithmic_insights': {
                'weight_optimization_quality': model_performance.get('final_r2', 0) if model_performance else 0,
                'overfitting_assessment': model_performance.get('overfitting_gap', 0) if model_performance else 0,
                'feature_importance_analysis': self._analyze_feature_importance(volatility_results),
                'prediction_confidence': self._calculate_prediction_confidence(model_performance, stability_analysis)
            },
            
            'business_actionability': {
                'immediate_attention_required': len(priority_groups.get('ìµœìš°ì„ ', [])),
                'monitoring_recommended': len(priority_groups.get('ë†’ìŒ', [])),
                'stable_customers': len([
                    c for c in stability_analysis.values() 
                    if c['stability_grade'] == 'ì•ˆì •'
                ]) if stability_analysis else 0,
                'efficiency_improvement_opportunities': self._identify_efficiency_opportunities(stability_analysis)
            },
            
            'technical_validation': {
                'data_sufficiency': self._assess_data_sufficiency(volatility_results),
                'statistical_significance': self._check_statistical_significance(coefficients),
                'algorithm_robustness': self._evaluate_algorithm_robustness(model_performance),
                'generalization_capability': model_performance.get('final_r2', 0) > 0.7 if model_performance else False
            },
            
            'recommendations': {
                'operational': [
                    f"ìµœìš°ì„  ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ {len(priority_groups.get('ìµœìš°ì„ ', []))}ê°œ ê³ ê° ì¦‰ì‹œ ì ê²€",
                    f"ê³ ìœ„í—˜ ê³ ê° {len(high_risk_customers)}ëª…ì— ëŒ€í•œ ì˜ì—…í™œë™ ë³€í™” ë¶„ì„",
                    "ë¹ˆë²ˆí•œ ì‚¬ìš©ì¤‘ë‹¨ ê³ ê°ì˜ ìš´ì˜ ìƒíƒœ í™•ì¸",
                    "ë‚®ì€ ë¶€í•˜ìœ¨ ê³ ê°ì˜ ì „ë ¥ ì‚¬ìš© íš¨ìœ¨ì„± ê°œì„  ì§€ì›"
                ],
                'technical': [
                    f"ëª¨ë¸ ì˜ˆì¸¡ ì •í™•ë„ RÂ²={model_performance.get('final_r2', 0):.3f} ë‹¬ì„±" if model_performance else "ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • í•„ìš”",
                    f"ê³¼ì í•© ì ê²€: ì°¨ì´={model_performance.get('overfitting_gap', 0):.3f}" if model_performance else "ê³¼ì í•© ì ê²€ í•„ìš”",
                    "ê³„ì¸µë³„ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€í‘œì„± í™•ë³´",
                    "5-fold êµì°¨ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì‹ ë¢°ì„± í™•ë³´"
                ],
                'business': [
                    "ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ë¥˜ ì²´ê³„ êµ¬ì¶•",
                    "ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ íŒ¨í„´ ê³ ê°ì— ëŒ€í•œ ë§ì¶¤í˜• ì„œë¹„ìŠ¤ ê°œë°œ",
                    "ì•ˆì •ì  ì„±ì¥ ê³ ê° ìš°ëŒ€ ì •ì±… ìˆ˜ë¦½",
                    "ì˜ì—…í™œë™ ìœ„ì¶• ì˜ˆìƒ ê³ ê° ì‚¬ì „ ê´€ë¦¬ ê°•í™”"
                ]
            },
            
            'quality_assurance': {
                'accuracy_optimization_applied': True,
                'overfitting_prevention': model_performance.get('overfitting_gap', 1) < 0.1 if model_performance else False,
                'robust_validation': self.sampling_config['validation_folds'] >= 5,
                'sufficient_data_coverage': len(volatility_results) >= 50,
                'statistical_confidence': self._calculate_overall_confidence(volatility_results, model_performance)
            }
        }
        
        return report
    
    def _generate_critical_alerts(self, stability_analysis):
        """ì¤‘ìš” ì•Œë¦¼ ìƒì„±"""
        alerts = []
        
        if not stability_analysis:
            return alerts
        
        # ìœ„í—˜ ê³ ê° ìˆ˜ í™•ì¸
        very_high_risk = [c for c in stability_analysis.values() if c['risk_level'] == 'very_high']
        if len(very_high_risk) > 0:
            alerts.append(f"ê·¹ê³ ìœ„í—˜ ê³ ê° {len(very_high_risk)}ëª… ë°œê²¬ - ì¦‰ì‹œ ì ê²€ í•„ìš”")
        
        # ì˜ì—…í™œë™ ìœ„ì¶• ê³ ê°
        declining_customers = [
            c for c in stability_analysis.values() 
            if c['business_trend_prediction'] == 'ì˜ì—…í™œë™_ìœ„ì¶•'
        ]
        if len(declining_customers) > len(stability_analysis) * 0.1:
            alerts.append(f"ì˜ì—…í™œë™ ìœ„ì¶• ì˜ˆìƒ ê³ ê° {len(declining_customers)}ëª… - ì „ì²´ì˜ {len(declining_customers)/len(stability_analysis)*100:.1f}%")
        
        # ì‚¬ìš©ì¤‘ë‹¨ ë¹ˆë°œ ê³ ê°
        frequent_outages = [
            c for c in stability_analysis.values()
            if 'ë¹ˆë²ˆí•œ_ì‚¬ìš©ì¤‘ë‹¨' in c['risk_factors']
        ]
        if len(frequent_outages) > 0:
            alerts.append(f"ì‚¬ìš©ì¤‘ë‹¨ ë¹ˆë°œ ê³ ê° {len(frequent_outages)}ëª… - ìš´ì˜ìƒíƒœ ì ê²€ í•„ìš”")
        
        return alerts
    
    def _analyze_feature_importance(self, volatility_results):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        if not volatility_results:
            return {}
        
        # ê° êµ¬ì„±ìš”ì†Œì˜ ê¸°ì—¬ë„ ë¶„ì„
        components = ['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']
        correlations = {}
        
        enhanced_cvs = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
        
        for component in components:
            component_values = [v.get(component, 0) for v in volatility_results.values()]
            correlation = np.corrcoef(enhanced_cvs, component_values)[0, 1]
            correlations[component] = round(correlation, 4) if not np.isnan(correlation) else 0
        
        return correlations
    
    def _calculate_prediction_confidence(self, model_performance, stability_analysis):
        """ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not model_performance or not stability_analysis:
            return 0.0
        
        r2_score = model_performance.get('final_r2', 0)
        overfitting_gap = model_performance.get('overfitting_gap', 1)
        
        # ë†’ì€ RÂ², ë‚®ì€ ê³¼ì í•©ì¼ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
        confidence = r2_score * (1 - min(overfitting_gap, 0.5))
        return round(max(0, min(1, confidence)), 4)
    
    def _identify_efficiency_opportunities(self, stability_analysis):
        """íš¨ìœ¨ì„± ê°œì„  ê¸°íšŒ ì‹ë³„"""
        if not stability_analysis:
            return []
        
        opportunities = []
        
        # ë‚®ì€ ë¶€í•˜ìœ¨ ê³ ê°
        low_load_factor = [
            c for c in stability_analysis.values()
            if c['load_factor'] < 0.3
        ]
        if len(low_load_factor) > 0:
            opportunities.append(f"ë¶€í•˜ìœ¨ ê°œì„  ëŒ€ìƒ {len(low_load_factor)}ëª…")
        
        # ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒ¨í„´ì´ì§€ë§Œ ë†’ì€ ë³€ë™ì„±
        predictable_but_volatile = [
            c for c in stability_analysis.values()
            if c['predictability_score'] > 0.7 and c['enhanced_volatility_coefficient'] > 0.4
        ]
        if len(predictable_but_volatile) > 0:
            opportunities.append(f"íŒ¨í„´ ìµœì í™” ê°€ëŠ¥ {len(predictable_but_volatile)}ëª…")
        
        return opportunities
    
    def _assess_data_sufficiency(self, volatility_results):
        """ë°ì´í„° ì¶©ë¶„ì„± í‰ê°€"""
        if not volatility_results:
            return False
        
        total_customers = len(volatility_results)
        avg_data_points = np.mean([v['data_points'] for v in volatility_results.values()])
        
        return (total_customers >= 50 and 
                avg_data_points >= self.sampling_config['min_records_per_customer'])
    
    def _check_statistical_significance(self, coefficients):
        """í†µê³„ì  ìœ ì˜ì„± í™•ì¸"""
        if len(coefficients) < 30:
            return False
        
        # ì •ê·œì„± ê²€ì • (ê°„ë‹¨í•œ ë°©ë²•)
        mean_cv = np.mean(coefficients)
        std_cv = np.std(coefficients)
        
        # ë³€ë™ê³„ìˆ˜ê°€ ì˜ë¯¸ìˆëŠ” ë¶„í¬ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
        return std_cv > 0.01 and len(set(np.round(coefficients, 3))) > len(coefficients) * 0.5
    
    def _evaluate_algorithm_robustness(self, model_performance):
        """ì•Œê³ ë¦¬ì¦˜ ê²¬ê³ ì„± í‰ê°€"""
        if not model_performance:
            return False
        
        r2_score = model_performance.get('final_r2', 0)
        overfitting_gap = model_performance.get('overfitting_gap', 1)
        
        return r2_score > 0.6 and overfitting_gap < 0.15
    
    def _calculate_overall_confidence(self, volatility_results, model_performance):
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        factors = []
        
        # ë°ì´í„° í’ˆì§ˆ
        if len(volatility_results) >= 100:
            factors.append(0.25)
        elif len(volatility_results) >= 50:
            factors.append(0.15)
        else:
            factors.append(0.05)
        
        # ëª¨ë¸ ì„±ëŠ¥
        if model_performance:
            r2 = model_performance.get('final_r2', 0)
            factors.append(min(r2 * 0.3, 0.3))
        
        # ê³¼ì í•© ë°©ì§€
        if model_performance and model_performance.get('overfitting_gap', 1) < 0.1:
            factors.append(0.2)
        
        # ê²€ì¦ ë°©ë²•
        if self.sampling_config['validation_folds'] >= 5:
            factors.append(0.15)
        
        # ìƒ˜í”Œë§ í’ˆì§ˆ
        if (self.sampling_config['customer_sample_ratio'] >= 0.5 and 
            self.sampling_config['stratified_sampling']):
            factors.append(0.1)
        
        return min(sum(factors), 1.0)

def main_accurate():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì •í™•ë„ ìš°ì„  ë²„ì „)"""
    print("ğŸ† í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ (ì •í™•ë„ ìš°ì„ )")
    print("=" * 70)
    
    start_time = datetime.now()
    
    # ì •í™•ë„ ìš°ì„  ì„¤ì •
    sampling_config = {
        'customer_sample_ratio': 0.8,      # 80% ê³ ê° ìƒ˜í”Œë§ (ë†’ì€ ëŒ€í‘œì„±)
        'time_sample_ratio': 0.6,          # 60% ì‹œê°„ ë°ì´í„° ìƒ˜í”Œë§
        'min_customers': 100,              # ìµœì†Œ 100ëª…
        'min_records_per_customer': 300,   # ê³ ê°ë‹¹ ìµœì†Œ 300ê°œ ë ˆì½”ë“œ
        'stratified_sampling': True,       # ê³„ì¸µ ìƒ˜í”Œë§ ì‚¬ìš©
        'validation_folds': 5              # 5-fold êµì°¨ê²€ì¦
    }
    
    print(f"ğŸ¯ ì •í™•ë„ ìš°ì„  ì„¤ì •:")
    print(f"   ğŸ“Š ê³ ê° ìƒ˜í”Œë§: {sampling_config['customer_sample_ratio']*100:.0f}%")
    print(f"   â° ì‹œê°„ ìƒ˜í”Œë§: {sampling_config['time_sample_ratio']*100:.0f}%")
    print(f"   ğŸ” êµì°¨ê²€ì¦: {sampling_config['validation_folds']}-fold")
    print(f"   ğŸ“ˆ ê³¼ì í•© ë°©ì§€ ê°•í™”")
    print()
    
    try:
        # 1. ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = KEPCOVolatilityAnalyzer('./analysis_results', sampling_config)
        
        # 2. ë°ì´í„° ë¡œë”© + ì •ë°€ ìƒ˜í”Œë§
        if not analyzer.load_preprocessed_data_with_sampling():
            print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            return None
        
        # 3. ë³€ë™ê³„ìˆ˜ ê³„ì‚° (ì •í™•ë„ ìš°ì„ )
        volatility_results = analyzer.calculate_enhanced_volatility_coefficient()
        if not volatility_results:
            print("âŒ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
            return None
        
        # 4. ëª¨ë¸ í›ˆë ¨ (ì •í™•ë„ ìš°ì„ )
        model_performance = analyzer.train_stacking_ensemble_model_accurate(volatility_results)
        
        # 5. ì•ˆì •ì„± ë¶„ì„ (ì •í™•ë„ ìš°ì„ )
        stability_analysis = analyzer.analyze_business_stability_accurate(volatility_results)
        
        # 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        report = analyzer.generate_comprehensive_report(volatility_results, model_performance, stability_analysis)
        
        # 7. ê²°ê³¼ ì €ì¥
        save_accurate_results(volatility_results, stability_analysis, report, model_performance)
        
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"\nğŸ‰ ì •í™•ë„ ìš°ì„  ë¶„ì„ ì™„ë£Œ!")
        print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ")
        print(f"   ğŸ‘¥ ë¶„ì„ ê³ ê°: {len(volatility_results)}ëª…")
        
        if volatility_results:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   ğŸ“ˆ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
            print(f"   ğŸ“Š ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        if model_performance:
            print(f"   ğŸ¯ ëª¨ë¸ ì„±ëŠ¥: RÂ²={model_performance['final_r2']:.4f}")
            print(f"   âœ… ê³¼ì í•© ì ê²€: ì°¨ì´={model_performance.get('overfitting_gap', 0):.4f}")
        
        if stability_analysis:
            high_risk = len([a for a in stability_analysis.values() if a['risk_level'] in ['high', 'very_high']])
            print(f"   ğŸš¨ ê³ ìœ„í—˜ ê³ ê°: {high_risk}ëª…")
        
        # í’ˆì§ˆ ë³´ì¦ ì ê²€
        quality_check = report.get('quality_assurance', {})
        print(f"\nğŸ” í’ˆì§ˆ ë³´ì¦ ì ê²€:")
        print(f"   ì •í™•ë„ ìµœì í™”: {'âœ…' if quality_check.get('accuracy_optimization_applied') else 'âŒ'}")
        print(f"   ê³¼ì í•© ë°©ì§€: {'âœ…' if quality_check.get('overfitting_prevention') else 'âŒ'}")
        print(f"   ê²¬ê³ í•œ ê²€ì¦: {'âœ…' if quality_check.get('robust_validation') else 'âŒ'}")
        print(f"   ì¶©ë¶„í•œ ë°ì´í„°: {'âœ…' if quality_check.get('sufficient_data_coverage') else 'âŒ'}")
        print(f"   ì „ì²´ ì‹ ë¢°ë„: {quality_check.get('statistical_confidence', 0):.3f}")
        
        return {
            'volatility_results': volatility_results,
            'model_performance': model_performance,
            'stability_analysis': stability_analysis,
            'report': report,
            'execution_time': execution_time,
            'sampling_config': sampling_config,
            'quality_assurance': quality_check
        }
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_accurate_results(volatility_results, stability_analysis, report, model_performance):
    """ì •í™•ë„ ìš°ì„  ê²°ê³¼ ì €ì¥"""
    try:
        os.makedirs('./analysis_results', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ë³€ë™ê³„ìˆ˜ ê²°ê³¼ (ìƒì„¸ ì •ë³´ í¬í•¨)
        if volatility_results:
            df = pd.DataFrame.from_dict(volatility_results, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
            
            # ê°€ì¤‘ì¹˜ ì •ë³´ë¥¼ ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ë¦¬
            if 'optimized_weights' in df.columns and len(df) > 0:
                weights = df.loc[0, 'optimized_weights']
                if isinstance(weights, list) and len(weights) == 5:
                    df['weight_basic_cv'] = weights[0]
                    df['weight_hourly_cv'] = weights[1]
                    df['weight_peak_cv'] = weights[2]
                    df['weight_weekend_diff'] = weights[3]
                    df['weight_seasonal_cv'] = weights[4]
            
            csv_path = f'./analysis_results/volatility_accurate_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ ë³€ë™ê³„ìˆ˜ ê²°ê³¼: {csv_path}")
        
        # 2. ì•ˆì •ì„± ë¶„ì„ (ìœ„í—˜ ìš”ì¸ ë¬¸ìì—´í™”)
        if stability_analysis:
            df = pd.DataFrame.from_dict(stability_analysis, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
            
            if 'risk_factors' in df.columns:
                df['risk_factors_list'] = df['risk_factors'].apply(
                    lambda x: ', '.join(x) if isinstance(x, list) else str(x)
                )
            
            csv_path = f'./analysis_results/stability_accurate_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ ì•ˆì •ì„± ë¶„ì„: {csv_path}")
        
        # 3. ì¢…í•© ë¦¬í¬íŠ¸
        if report:
            json_path = f'./analysis_results/comprehensive_report_{timestamp}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"   ğŸ’¾ ì¢…í•© ë¦¬í¬íŠ¸: {json_path}")
        
        # 4. ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ ì •ë³´
        if model_performance:
            json_path = f'./analysis_results/model_performance_{timestamp}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(model_performance, f, ensure_ascii=False, indent=2, default=str)
            print(f"   ğŸ’¾ ëª¨ë¸ ì„±ëŠ¥: {json_path}")
        
        # 5. ìš”ì•½ ë³´ê³ ì„œ (í…ìŠ¤íŠ¸)
        summary_path = f'./analysis_results/executive_summary_{timestamp}.txt'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ìš”ì•½ ë³´ê³ ì„œ\n")
            f.write("=" * 60 + "\n\n")
            
            if volatility_results:
                cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
                f.write(f"ë¶„ì„ ê³ ê° ìˆ˜: {len(volatility_results):,}ëª…\n")
                f.write(f"í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}\n")
                f.write(f"ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}\n\n")
            
            if model_performance:
                f.write("ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ:\n")
                f.write(f"  - ì˜ˆì¸¡ ì •í™•ë„ (RÂ²): {model_performance.get('final_r2', 0):.4f}\n")
                f.write(f"  - í‰ê·  ì ˆëŒ€ ì˜¤ì°¨: {model_performance.get('final_mae', 0):.4f}\n")
                f.write(f"  - ê³¼ì í•© ì ê²€: {model_performance.get('overfitting_gap', 0):.4f}\n\n")
            
            if stability_analysis:
                grade_counts = {}
                for analysis in stability_analysis.values():
                    grade = analysis['stability_grade']
                    grade_counts[grade] = grade_counts.get(grade, 0) + 1
                
                f.write("ì•ˆì •ì„± ë“±ê¸‰ ë¶„í¬:\n")
                for grade, count in grade_counts.items():
                    percentage = count / len(stability_analysis) * 100
                    f.write(f"  - {grade}: {count}ëª… ({percentage:.1f}%)\n")
                
                high_risk = len([a for a in stability_analysis.values() if a['risk_level'] in ['high', 'very_high']])
                f.write(f"\nê³ ìœ„í—˜ ê³ ê°: {high_risk}ëª…\n")
        
        print(f"   ğŸ“‹ ìš”ì•½ ë³´ê³ ì„œ: {summary_path}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def create_test_environment_accurate():
    """ì •í™•ë„ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±"""
    print("ğŸ§ª ì •í™•ë„ ìš°ì„  í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„± ì¤‘...")
    
    import json
    os.makedirs('./analysis_results', exist_ok=True)
    
    # 1ë‹¨ê³„, 2ë‹¨ê³„ ê²°ê³¼ ìƒì„±
    step1_results = {
        'metadata': {'timestamp': datetime.now().isoformat(), 'total_customers': 500}
    }
    with open('./analysis_results/analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(step1_results, f, ensure_ascii=False, indent=2, default=str)
    
    step2_results = {
        'temporal_patterns': {
            'peak_hours': [9, 10, 11, 14, 15, 18, 19],
            'off_peak_hours': [0, 1, 2, 3, 4, 5, 22, 23],
            'weekend_ratio': 0.75
        }
    }
    with open('./analysis_results/analysis_results2.json', 'w', encoding='utf-8') as f:
        json.dump(step2_results, f, ensure_ascii=False, indent=2, default=str)
    
    # ê³ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (500ëª…, 30ì¼)
    print("   ğŸ“Š ê³ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ LP ë°ì´í„° ìƒì„± ì¤‘...")
    
    np.random.seed(42)
    data = []
    
    for customer in range(1, 501):  # 500ëª…
        # ê³ ê°ë³„ ë‹¤ì–‘í•œ íŠ¹ì„± ë¶€ì—¬
        base_power = 20 + customer * 0.5 + np.random.normal(0, 10)
        base_power = max(10, base_power)  # ìµœì†Œ 10kW
        
        # ì—…ì¢…ë³„ ë³€ë™ì„± (ë” í˜„ì‹¤ì ì¸ íŒ¨í„´)
        if customer % 5 == 0:  # ì œì¡°ì—… (20%)
            cv_base = 0.15 + np.random.uniform(0, 0.2)
            night_operation = True
        elif customer % 5 == 1:  # ìƒì—… (20%)
            cv_base = 0.25 + np.random.uniform(0, 0.3)
            night_operation = False
        elif customer % 5 == 2:  # ì‚¬ë¬´ (20%)
            cv_base = 0.20 + np.random.uniform(0, 0.15)
            night_operation = False
        elif customer % 5 == 3:  # ì„œë¹„ìŠ¤ì—… (20%)
            cv_base = 0.30 + np.random.uniform(0, 0.4)
            night_operation = False
        else:  # ê¸°íƒ€ (20%)
            cv_base = 0.10 + np.random.uniform(0, 0.5)
            night_operation = np.random.choice([True, False])
        
        for day in range(30):  # 30ì¼
            for hour in range(24):
                for minute in [0, 15, 30, 45]:  # 15ë¶„ ê°„ê²©
                    timestamp = datetime(2024, 3, 1) + timedelta(days=day, hours=hour, minutes=minute)
                    
                    # ì‹œê°„ëŒ€ë³„ ë¶€í•˜ íŒ¨í„´
                    hour_factor = 1.0
                    
                    # í”¼í¬ ì‹œê°„
                    if hour in [9, 10, 11, 14, 15, 18, 19]:
                        hour_factor = 1.2 + np.random.normal(0, 0.1)
                    # ë¹„í”¼í¬ ì‹œê°„
                    elif hour in [0, 1, 2, 3, 4, 5, 22, 23]:
                        if night_operation:
                            hour_factor = 1.1 + np.random.normal(0, 0.1)
                        else:
                            hour_factor = 0.3 + np.random.normal(0, 0.1)
                    # ì¼ë°˜ ì‹œê°„
                    else:
                        hour_factor = 0.9 + np.random.normal(0, 0.1)
                    
                    # ìš”ì¼ íš¨ê³¼
                    weekday = timestamp.weekday()
                    if weekday >= 5:  # ì£¼ë§
                        if customer % 3 == 0:  # ì¼ë¶€ ê³ ê°ì€ ì£¼ë§ì—ë„ ìš´ì˜
                            hour_factor *= 0.9
                        else:
                            hour_factor *= 0.4
                    
                    # ì›”ë³„ ê³„ì ˆ íš¨ê³¼
                    month = timestamp.month
                    if month in [6, 7, 8]:  # ì—¬ë¦„ (ëƒ‰ë°©)
                        hour_factor *= 1.3
                    elif month in [12, 1, 2]:  # ê²¨ìš¸ (ë‚œë°©)
                        hour_factor *= 1.2
                    
                    # ìµœì¢… ì „ë ¥ëŸ‰ ê³„ì‚°
                    power = base_power * hour_factor * (1 + np.random.normal(0, cv_base))
                    
                    # íŠ¹ìˆ˜ ìƒí™© (ì •ì „, íœ´ì—… ë“±)
                    if np.random.random() < 0.02:  # 2% í™•ë¥ 
                        power = 0
                    else:
                        power = max(1, power)  # ìµœì†Œ 1kW
                    
                    data.append({
                        'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': f'ACC_{customer:04d}',
                        'datetime': timestamp,
                        'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥': round(power, 2),
                        'ì§€ìƒë¬´íš¨': round(power * 0.3 * np.random.uniform(0.8, 1.2), 2),
                        'ì§„ìƒë¬´íš¨': round(power * 0.1 * np.random.uniform(0.8, 1.2), 2),
                        'í”¼ìƒì „ë ¥': round(power * 1.1 * np.random.uniform(0.95, 1.05), 2)
                    })
    
    df = pd.DataFrame(data)
    
    # CSVë¡œ ì €ì¥
    df.to_csv('./analysis_results/processed_lp_data.csv', index=False)
    print(f"   âœ… ê³ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {len(df):,}ê±´, {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…")
    print(f"   ğŸ“… ê¸°ê°„: {df['datetime'].min()} ~ {df['datetime'].max()}")

if __name__ == "__main__":
    print("ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì‹œì‘ (ì •í™•ë„ ìš°ì„  ë²„ì „)!")
    print("=" * 80)
    print("ğŸ¯ ì •í™•ë„ ìµœìš°ì„  | ê³¼ì í•© ë°©ì§€ ê°•í™” | ì¶©ë¶„í•œ ê²€ì¦")
    print("ğŸ“Š ë” ë§ì€ ë°ì´í„° | ë” ì •ë°€í•œ ë¶„ì„ | ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼")
    print()
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    required_files = ['./analysis_results/processed_lp_data.csv']
    missing_files = [f for f in required_files if not os.path.exists(f)]
    
    if missing_files:
        print("âš ï¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê³ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        create_test_environment_accurate()
        print()
    
    # ë©”ì¸ ì‹¤í–‰
    results = main_accurate()
    
    if results:
        print(f"\nğŸŠ ì •í™•ë„ ìš°ì„  ë¶„ì„ ì„±ê³µ!")
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼: ./analysis_results/ ë””ë ‰í† ë¦¬")
        print(f"   ğŸ¯ ì •í™•ë„: RÂ²={results.get('model_performance', {}).get('final_r2', 0):.4f}")
        print(f"   âœ… ê³¼ì í•© ë°©ì§€: ì°¨ì´={results.get('model_performance', {}).get('overfitting_gap', 0):.4f}")
        print(f"   ğŸ” ê²€ì¦ ë°©ë²•: {results.get('sampling_config', {}).get('validation_folds', 5)}-fold CV")
        
        quality = results.get('quality_assurance', {})
        print(f"\nğŸ’¯ í’ˆì§ˆ ì§€í‘œ:")
        print(f"   ì •í™•ë„ ìµœì í™”: {'âœ…' if quality.get('accuracy_optimization_applied') else 'âŒ'}")
        print(f"   ê³¼ì í•© ë°©ì§€: {'âœ…' if quality.get('overfitting_prevention') else 'âŒ'}")
        print(f"   ê²¬ê³ í•œ ê²€ì¦: {'âœ…' if quality.get('robust_validation') else 'âŒ'}")
        print(f"   ì¶©ë¶„í•œ ë°ì´í„°: {'âœ…' if quality.get('sufficient_data_coverage') else 'âŒ'}")
        print(f"   ì „ì²´ ì‹ ë¢°ë„: {quality.get('statistical_confidence', 0):.3f}")
        
        print(f"\nğŸ“ˆ ì£¼ìš” ì„±ê³¼:")
        if results.get('volatility_results'):
            cv_values = [v['enhanced_volatility_coefficient'] for v in results['volatility_results'].values()]
            print(f"   â€¢ ë¶„ì„ ì™„ë£Œ: {len(results['volatility_results'])}ëª… ê³ ê°")
            print(f"   â€¢ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
            print(f"   â€¢ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        if results.get('stability_analysis'):
            high_risk = len([a for a in results['stability_analysis'].values() 
                           if a['risk_level'] in ['high', 'very_high']])
            print(f"   â€¢ ê³ ìœ„í—˜ ê³ ê° ì‹ë³„: {high_risk}ëª…")
        
        print(f"\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:")
        print(f"   â€¢ ì „ë ¥ ì‚¬ìš© ì•ˆì •ì„± ì •ëŸ‰í™”")
        print(f"   â€¢ ì˜ì—…í™œë™ ë³€í™” ì˜ˆì¸¡ ê°€ëŠ¥")
        print(f"   â€¢ ìœ„í—˜ ê³ ê° ì¡°ê¸° ë°œê²¬")
        print(f"   â€¢ ë§ì¶¤í˜• ê´€ë¦¬ ì „ëµ ìˆ˜ë¦½")
        
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨")

print("\n" + "=" * 80)
print("ğŸ† í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ (ì •í™•ë„ ìš°ì„ )")
print("ğŸ¯ ê³¼ì í•© ë°©ì§€ | ğŸ“Š ì¶©ë¶„í•œ ê²€ì¦ | âœ… ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼")
print("=" * 80)