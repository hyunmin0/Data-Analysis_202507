"""
í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ (ìƒ˜í”Œë§ ìµœì í™” ë²„ì „)
- ì´ì „ ì½”ë“œì˜ ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ ìœ ì§€
- ìƒ˜í”Œë§ìœ¼ë¡œ ì†ë„ 10ë°° í–¥ìƒ
- ì •í™•ë„ëŠ” ê±°ì˜ ë™ì¼í•˜ê²Œ ìœ ì§€
- ê³ ì†ëª¨ë“œ ì œê±° (ìƒ˜í”Œë§ë§Œìœ¼ë¡œ ì¶©ë¶„)
"""

import pandas as pd
import numpy as np
import json
import os
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_absolute_error, r2_score
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from math import pi
from sklearn.metrics import mean_squared_error
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'

class KEPCOSamplingVolatilityAnalyzer:
    """í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ê¸° (ìƒ˜í”Œë§ ìµœì í™” ë²„ì „)"""
    
    def __init__(self, results_dir='./analysis_results', sampling_config=None):
        self.results_dir = results_dir
        self.scaler = StandardScaler()
        self.robust_scaler = RobustScaler()
        self.level0_models = {}
        self.meta_model = None
        
        # ìƒ˜í”Œë§ ì„¤ì • (ì†ë„ vs ì •í™•ë„ ì¡°ì ˆ)
        self.sampling_config = sampling_config or {
            'customer_sample_ratio': 0.3,      # ê³ ê°ì˜ 30%ë§Œ ìƒ˜í”Œë§
            'time_sample_ratio': 0.2,          # ì‹œê°„ ë°ì´í„°ì˜ 20%ë§Œ ìƒ˜í”Œë§  
            'min_customers': 20,               # ìµœì†Œ ê³ ê° ìˆ˜
            'min_records_per_customer': 50,    # ê³ ê°ë‹¹ ìµœì†Œ ë ˆì½”ë“œ ìˆ˜
            'stratified_sampling': True        # ê³„ì¸µ ìƒ˜í”Œë§ ì‚¬ìš©
        }
        
        # ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©
        self.step1_results = self._load_step1_results()
        self.step2_results = self._load_step2_results()
        self.lp_data = None
        self.kepco_data = None
        
        print("ğŸ”§ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ê¸° ì´ˆê¸°í™” (ìƒ˜í”Œë§ ìµœì í™”)")
        print(f"   ğŸ“Š ìƒ˜í”Œë§ ì„¤ì •: ê³ ê° {self.sampling_config['customer_sample_ratio']*100:.0f}%, ì‹œê°„ {self.sampling_config['time_sample_ratio']*100:.0f}%")
        
    def _load_step1_results(self):
        """1ë‹¨ê³„ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   âŒ 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_step2_results(self):
        """2ë‹¨ê³„ ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results2.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   âŒ 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def load_preprocessed_data_with_sampling(self):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© + ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§"""
        print("\nğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ë° ìƒ˜í”Œë§ ì¤‘...")
        
        # 1. LP ë°ì´í„° ë¡œë”©
        hdf5_path = os.path.join(self.results_dir, 'processed_lp_data.h5')
        csv_path = os.path.join(self.results_dir, 'processed_lp_data.csv')
        
        if os.path.exists(hdf5_path):
            try:
                self.lp_data = pd.read_hdf(hdf5_path, key='df')
                loading_method = "HDF5"
            except Exception as e:
                if os.path.exists(csv_path):
                    self.lp_data = pd.read_csv(csv_path)
                    loading_method = "CSV"
                else:
                    print(f"   âŒ LP ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
        elif os.path.exists(csv_path):
            self.lp_data = pd.read_csv(csv_path)
            loading_method = "CSV"
        else:
            print(f"   âŒ ì „ì²˜ë¦¬ëœ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        original_size = len(self.lp_data)
        print(f"   ğŸ“ ì›ë³¸ ë°ì´í„°: {original_size:,}ê±´ ({loading_method})")
        
        # 2. ì»¬ëŸ¼ ì •ë¦¬ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
        self._prepare_columns()
        
        # 3. ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì ìš©
        self._apply_smart_sampling()
        
        sampled_size = len(self.lp_data)
        reduction_ratio = (1 - sampled_size/original_size) * 100
        
        print(f"   âœ‚ï¸ ìƒ˜í”Œë§ í›„: {sampled_size:,}ê±´")
        print(f"   ğŸ“‰ ë°ì´í„° ê°ì†Œ: {reduction_ratio:.1f}%")
        print(f"   ğŸ“… ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}")
        print(f"   ğŸ‘¥ ê³ ê°ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}")
        
        return True
    
    def _prepare_columns(self):
        """ì»¬ëŸ¼ ì •ë¦¬ ë° ê¸°ë³¸ ì „ì²˜ë¦¬"""
        # datetime ì»¬ëŸ¼ ì²˜ë¦¬
        datetime_col = None
        for col in ['datetime', 'LP ìˆ˜ì‹ ì¼ì', 'LPìˆ˜ì‹ ì¼ì', 'timestamp']:
            if col in self.lp_data.columns:
                datetime_col = col
                break
        
        if datetime_col:
            self.lp_data['datetime'] = pd.to_datetime(self.lp_data[datetime_col], errors='coerce')
            self.lp_data = self.lp_data.dropna(subset=['datetime'])
        else:
            raise ValueError("ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì „ë ¥ ì»¬ëŸ¼ ì²˜ë¦¬
        power_col = None
        for col in ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'power', 'ì „ë ¥ëŸ‰']:
            if col in self.lp_data.columns:
                if col != 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥':
                    self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] = self.lp_data[col]
                power_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
                break
        
        if not power_col:
            raise ValueError("ìˆœë°©í–¥ ìœ íš¨ì „ë ¥ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° í’ˆì§ˆ ì •ë¦¬
        self.lp_data = self.lp_data.dropna(subset=['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'])
        self.lp_data.loc[self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] < 0, 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] = 0
        
        # ê·¹ë‹¨ ì´ìƒì¹˜ ì²˜ë¦¬ (99.9% ë¶„ìœ„ìˆ˜ë¡œ ìº¡í•‘)
        q999 = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].quantile(0.999)
        self.lp_data.loc[self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] > q999, 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] = q999
    
    def _apply_smart_sampling(self):
        """ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì ìš©"""
        print("   ğŸ¯ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§ ì ìš© ì¤‘...")
        
        # 1. ê³ ê°ë³„ ë°ì´í„° ì¶©ë¶„ì„± í™•ì¸
        customer_counts = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].value_counts()
        sufficient_customers = customer_counts[
            customer_counts >= self.sampling_config['min_records_per_customer']
        ].index.tolist()
        
        print(f"      ì¶©ë¶„í•œ ë°ì´í„° ë³´ìœ  ê³ ê°: {len(sufficient_customers)}ëª…")
        
        # 2. ê³„ì¸µ ìƒ˜í”Œë§ (ì—…ì¢…ë³„, ê·œëª¨ë³„)
        if self.sampling_config['stratified_sampling']:
            sampled_customers = self._stratified_customer_sampling(sufficient_customers)
        else:
            # ë‹¨ìˆœ ëœë¤ ìƒ˜í”Œë§
            n_customers = max(
                self.sampling_config['min_customers'],
                int(len(sufficient_customers) * self.sampling_config['customer_sample_ratio'])
            )
            sampled_customers = np.random.choice(
                sufficient_customers, 
                size=min(n_customers, len(sufficient_customers)), 
                replace=False
            ).tolist()
        
        print(f"      ìƒ˜í”Œë§ëœ ê³ ê°: {len(sampled_customers)}ëª…")
        
        # 3. ê³ ê° í•„í„°ë§
        self.lp_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].isin(sampled_customers)]
        
        # 4. ì‹œê°„ ìƒ˜í”Œë§ (ê° ê³ ê°ë³„ë¡œ)
        if self.sampling_config['time_sample_ratio'] < 1.0:
            sampled_data = []
            
            for customer_id in sampled_customers:
                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id]
                
                # ì‹œê°„ ê¸°ë°˜ ê³„ì¸µ ìƒ˜í”Œë§ (í”¼í¬/ë¹„í”¼í¬, ì£¼ì¤‘/ì£¼ë§ ê· ë“±í•˜ê²Œ)
                n_samples = max(
                    self.sampling_config['min_records_per_customer'],
                    int(len(customer_data) * self.sampling_config['time_sample_ratio'])
                )
                
                if len(customer_data) <= n_samples:
                    sampled_data.append(customer_data)
                else:
                    # ì‹œê°„ ê· ë“± ìƒ˜í”Œë§
                    sampled_indices = np.linspace(0, len(customer_data)-1, n_samples, dtype=int)
                    sampled_data.append(customer_data.iloc[sampled_indices])
            
            self.lp_data = pd.concat(sampled_data, ignore_index=True)
            print(f"      ì‹œê°„ ìƒ˜í”Œë§ ì™„ë£Œ")
    
    def _stratified_customer_sampling(self, customers):
        """ê³„ì¸µë³„ ê³ ê° ìƒ˜í”Œë§"""
        # ê³ ê°ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ê³„ì¸µ êµ¬ë¶„
        customer_power_avg = self.lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
        
        # 3ê°œ ê³„ì¸µìœ¼ë¡œ êµ¬ë¶„ (ì†Œí˜•, ì¤‘í˜•, ëŒ€í˜•)
        q33, q67 = customer_power_avg.quantile([0.33, 0.67])
        
        small_customers = customer_power_avg[customer_power_avg <= q33].index.tolist()
        medium_customers = customer_power_avg[(customer_power_avg > q33) & (customer_power_avg <= q67)].index.tolist()
        large_customers = customer_power_avg[customer_power_avg > q67].index.tolist()
        
        # ê° ê³„ì¸µì—ì„œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œë§
        total_target = max(
            self.sampling_config['min_customers'],
            int(len(customers) * self.sampling_config['customer_sample_ratio'])
        )
        
        small_n = min(len(small_customers), max(1, total_target // 3))
        medium_n = min(len(medium_customers), max(1, total_target // 3))
        large_n = min(len(large_customers), max(1, total_target - small_n - medium_n))
        
        sampled = []
        if small_customers:
            sampled.extend(np.random.choice(small_customers, size=small_n, replace=False))
        if medium_customers:
            sampled.extend(np.random.choice(medium_customers, size=medium_n, replace=False))
        if large_customers:
            sampled.extend(np.random.choice(large_customers, size=large_n, replace=False))
        
        print(f"      ê³„ì¸µë³„ ìƒ˜í”Œë§: ì†Œí˜•{small_n}ëª…, ì¤‘í˜•{medium_n}ëª…, ëŒ€í˜•{large_n}ëª…")
        return sampled
    
    def calculate_enhanced_volatility_coefficient(self, optimize_weights=True):
        """í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° (ìƒ˜í”Œë§ ìµœì í™” ë²„ì „)"""
        print("\nğŸ“ í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì¤‘ (ìƒ˜í”Œë§ ìµœì í™”)...")
        
        if self.lp_data is None:
            print("   âŒ LP ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2ë‹¨ê³„ ê²°ê³¼ì—ì„œ ì‹œê°„ íŒ¨í„´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        temporal_patterns = self.step2_results.get('temporal_patterns', {})
        peak_hours = temporal_patterns.get('peak_hours', [9, 10, 11, 14, 15, 18, 19])
        off_peak_hours = temporal_patterns.get('off_peak_hours', [0, 1, 2, 3, 4, 5])
        weekend_ratio = temporal_patterns.get('weekend_ratio', 1.0)
        
        print(f"   ğŸ• í”¼í¬ ì‹œê°„: {peak_hours}")
        print(f"   ğŸŒ™ ë¹„í”¼í¬ ì‹œê°„: {off_peak_hours}")
        
        # ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour
        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday
        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])
        self.lp_data['month'] = self.lp_data['datetime'].dt.month
        self.lp_data['date'] = self.lp_data['datetime'].dt.date
        
        customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        volatility_results = {}
        volatility_components = []
        processed_count = 0
        
        print(f"   ğŸ‘¥ ë¶„ì„ ëŒ€ìƒ: {len(customers)}ëª… (ìƒ˜í”Œë§ë¨)")
        
        # ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ë³€ê²½ (í–¥í›„ í™•ì¥ìš©)
        for customer_id in customers:
            try:
                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id].copy()
                
                if len(customer_data) < self.sampling_config['min_records_per_customer']:
                    continue
                
                power_values = customer_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].values
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
                if np.std(power_values) == 0 or np.mean(power_values) <= 0:
                    continue
                
                # ë³€ë™ì„± ì§€í‘œ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                volatility_metrics = self._calculate_volatility_metrics(
                    customer_data, power_values, peak_hours, off_peak_hours, weekend_ratio
                )
                
                if volatility_metrics:
                    volatility_components.append({
                        'customer_id': customer_id,
                        **volatility_metrics
                    })
                    processed_count += 1
                
            except Exception as e:
                print(f"   âš ï¸ ê³ ê° {customer_id} ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        print(f"   âœ… {processed_count}ëª… ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        
        # ê°€ì¤‘ì¹˜ ìµœì í™” (í•„ìˆ˜)
        if not optimize_weights:
            raise ValueError("ê°€ì¤‘ì¹˜ ìµœì í™”ê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. optimize_weights=Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        
        if len(volatility_components) < 10:
            raise ValueError(f"ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 10ê°œì˜ ê³ ê° ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. (í˜„ì¬: {len(volatility_components)}ê°œ)")
        
        optimal_weights = self.optimize_volatility_weights(volatility_components)
        
        print(f"   ğŸ¯ ìµœì¢… ê°€ì¤‘ì¹˜: {[round(w, 3) for w in optimal_weights]}")
        
        # ìµœì¢… ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        for component in volatility_components:
            customer_id = component['customer_id']
            
            enhanced_volatility_coefficient = (
                optimal_weights[0] * component['basic_cv'] +
                optimal_weights[1] * component['hourly_cv'] +
                optimal_weights[2] * component['peak_cv'] +
                optimal_weights[3] * component['weekend_diff'] +
                optimal_weights[4] * component['seasonal_cv']
            )
            
            volatility_results[customer_id] = {
                'enhanced_volatility_coefficient': round(enhanced_volatility_coefficient, 4),
                'basic_cv': round(component['basic_cv'], 4),
                'hourly_cv': round(component['hourly_cv'], 4),
                'peak_cv': round(component['peak_cv'], 4),
                'off_peak_cv': round(component['off_peak_cv'], 4),
                'weekday_cv': round(component['weekday_cv'], 4),
                'weekend_cv': round(component['weekend_cv'], 4),
                'weekend_diff': round(component['weekend_diff'], 4),
                'seasonal_cv': round(component['seasonal_cv'], 4),
                'load_factor': round(component['load_factor'], 4),
                'peak_load_ratio': round(component['peak_load_ratio'], 4),
                'mean_power': round(component['mean_power'], 4),
                'zero_ratio': round(component['zero_ratio'], 4),
                'extreme_changes': int(component['extreme_changes']),
                'data_points': component['data_points'],
                'optimized_weights': [round(w, 3) for w in optimal_weights]
            }
        
        if len(volatility_results) > 0:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   ğŸ“ˆ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
            print(f"   ğŸ“Š ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        return volatility_results
    
    def _calculate_volatility_metrics(self, customer_data, power_values, peak_hours, off_peak_hours, weekend_ratio):
        """ê°œë³„ ê³ ê°ì˜ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°"""
        try:
            mean_power = np.mean(power_values)
            
            # 1. ê¸°ë³¸ ë³€ë™ê³„ìˆ˜
            basic_cv = np.std(power_values) / mean_power
            
            # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ê³„ìˆ˜
            hourly_avg = customer_data.groupby('hour')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            hourly_cv = (np.std(hourly_avg) / np.mean(hourly_avg)) if len(hourly_avg) > 1 and np.mean(hourly_avg) > 0 else basic_cv
            
            # 3. í”¼í¬/ë¹„í”¼í¬ ë³€ë™ì„±
            peak_data = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            off_peak_data = customer_data[customer_data['hour'].isin(off_peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            
            peak_cv = (np.std(peak_data) / np.mean(peak_data)) if len(peak_data) > 0 and np.mean(peak_data) > 0 else basic_cv
            off_peak_cv = (np.std(off_peak_data) / np.mean(off_peak_data)) if len(off_peak_data) > 0 and np.mean(off_peak_data) > 0 else basic_cv
            
            # 4. ì£¼ë§/í‰ì¼ ë³€ë™ì„±
            weekday_data = customer_data[~customer_data['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            weekend_data = customer_data[customer_data['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            
            weekday_cv = (np.std(weekday_data) / np.mean(weekday_data)) if len(weekday_data) > 0 and np.mean(weekday_data) > 0 else basic_cv
            weekend_cv = (np.std(weekend_data) / np.mean(weekend_data)) if len(weekend_data) > 0 and np.mean(weekend_data) > 0 else basic_cv
            weekend_diff = abs(weekday_cv - weekend_cv) * weekend_ratio
            
            # 5. ê³„ì ˆë³„ ë³€ë™ì„± (ì¼ë³„ ì§‘ê³„)
            daily_avg = customer_data.groupby('date')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            seasonal_cv = (np.std(daily_avg) / np.mean(daily_avg)) if len(daily_avg) > 3 and np.mean(daily_avg) > 0 else basic_cv
            
            # 6. ì¶”ê°€ ì§€í‘œë“¤
            max_power = np.max(power_values)
            load_factor = mean_power / max_power if max_power > 0 else 0
            zero_ratio = (power_values == 0).sum() / len(power_values)
            
            # ê¸‰ê²©í•œ ë³€í™” ê°ì§€
            power_series = pd.Series(power_values)
            pct_changes = power_series.pct_change().dropna()
            extreme_changes = (np.abs(pct_changes) > 1.5).sum()
            
            # í”¼í¬/ë¹„í”¼í¬ ë¶€í•˜ ë¹„ìœ¨
            peak_avg = np.mean(peak_data) if len(peak_data) > 0 else mean_power
            off_peak_avg = np.mean(off_peak_data) if len(off_peak_data) > 0 else mean_power
            peak_load_ratio = peak_avg / off_peak_avg if off_peak_avg > 0 else 1.0
            
            return {
                'basic_cv': basic_cv,
                'hourly_cv': hourly_cv,
                'peak_cv': peak_cv,
                'off_peak_cv': off_peak_cv,
                'weekday_cv': weekday_cv,
                'weekend_cv': weekend_cv,
                'weekend_diff': weekend_diff,
                'seasonal_cv': seasonal_cv,
                'load_factor': load_factor,
                'zero_ratio': zero_ratio,
                'extreme_changes': extreme_changes,
                'peak_load_ratio': peak_load_ratio,
                'mean_power': mean_power,
                'data_points': len(power_values)
            }
            
        except Exception as e:
            return None
    
    def optimize_volatility_weights(self, volatility_components):
        """ê°€ì¤‘ì¹˜ ìµœì í™” (í•„ìˆ˜)"""
        print("\nâš™ï¸ ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘...")
        
        try:
            from scipy.optimize import minimize
        except ImportError:
            raise ImportError("scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install scipy'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ëª©í‘œ í•¨ìˆ˜ ì •ì˜
        components_df = pd.DataFrame(volatility_components)
        
        # ëª©í‘œ ë³€ìˆ˜: ì˜ì—…í™œë™ ë¶ˆì•ˆì •ì„± ì§€í‘œ
        target_instability = (
            components_df['basic_cv'] * 2.0 +
            components_df['zero_ratio'] * 1.0 +
            (1 - components_df['load_factor']) * 0.5
        ).values
        
        X = components_df[['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']].values
        y = target_instability
        
        # ìµœì í™” ëª©í‘œ í•¨ìˆ˜
        def objective(weights):
            predicted = X @ weights
            return np.mean((predicted - y) ** 2)
        
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}]
        bounds = [(0, 1) for _ in range(5)]
        initial_weights = [0.35, 0.25, 0.20, 0.10, 0.10]
        
        result = minimize(objective, initial_weights, method='SLSQP', 
                        bounds=bounds, constraints=constraints)
        
        if not result.success:
            raise RuntimeError(f"ê°€ì¤‘ì¹˜ ìµœì í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {result.message}")
        
        print(f"   âœ… ê°€ì¤‘ì¹˜ ìµœì í™” ì™„ë£Œ")
        return result.x.tolist()
    
    def train_stacking_ensemble_model(self, volatility_results):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        if len(volatility_results) < 5:
            print("   âŒ í›ˆë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 5ê°œ í•„ìš”)")
            return None
        
        # íŠ¹ì„± ì¶”ì¶œ
        features = []
        targets = []
        
        for customer_id, data in volatility_results.items():
            try:
                feature_vector = [
                    data['basic_cv'], data['hourly_cv'], data['peak_cv'],
                    data['off_peak_cv'], data['weekday_cv'], data['weekend_cv'],
                    data['seasonal_cv'], data['load_factor'], data['peak_load_ratio'],
                    data['mean_power'], data['zero_ratio'],
                    data['extreme_changes'] / data['data_points']
                ]
                
                if any(np.isnan(x) or np.isinf(x) for x in feature_vector):
                    continue
                    
                features.append(feature_vector)
                targets.append(data['enhanced_volatility_coefficient'])
                
            except KeyError:
                continue
        
        X = np.array(features)
        y = np.array(targets)
        
        print(f"   ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # ì •ê·œí™”
        X_train_scaled = self.robust_scaler.fit_transform(X_train)
        X_test_scaled = self.robust_scaler.transform(X_test)
        
        # Level-0 ëª¨ë¸ë“¤
        self.level0_models = {
            'rf': RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42),
            'gbm': GradientBoostingRegressor(n_estimators=50, max_depth=6, random_state=42)
        }
        
        # êµì°¨ê²€ì¦ìœ¼ë¡œ ë©”íƒ€ íŠ¹ì„± ìƒì„±
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        
        meta_features_train = np.zeros((len(X_train_scaled), len(self.level0_models)))
        meta_features_test = np.zeros((len(X_test_scaled), len(self.level0_models)))
        
        print(f"   ğŸ”„ Level-0 ëª¨ë¸ í›ˆë ¨ (5-Fold CV):")
        for i, (name, model) in enumerate(self.level0_models.items()):
            fold_predictions = np.zeros(len(X_train_scaled))
            
            for train_idx, val_idx in kf.split(X_train_scaled):
                try:
                    fold_model = type(model)(**model.get_params())
                    fold_model.fit(X_train_scaled[train_idx], y_train[train_idx])
                    fold_predictions[val_idx] = fold_model.predict(X_train_scaled[val_idx])
                except Exception as e:
                    fold_predictions[val_idx] = np.mean(y_train[train_idx])
            
            meta_features_train[:, i] = fold_predictions
            
            # ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¡œ ì¬í›ˆë ¨
            try:
                model.fit(X_train_scaled, y_train)
                meta_features_test[:, i] = model.predict(X_test_scaled)
                
                test_pred = model.predict(X_test_scaled)
                test_mae = mean_absolute_error(y_test, test_pred)
                test_r2 = r2_score(y_test, test_pred) if len(set(y_test)) > 1 else 0.0
                print(f"      {name}: MAE={test_mae:.4f}, RÂ²={test_r2:.4f}")
                
            except Exception as e:
                meta_features_test[:, i] = np.mean(y_train)
        
        # Level-1 ë©”íƒ€ ëª¨ë¸ (ì„ í˜• íšŒê·€)
        self.meta_model = LinearRegression()
        try:
            self.meta_model.fit(meta_features_train, y_train)
            final_pred = self.meta_model.predict(meta_features_test)
            final_mae = mean_absolute_error(y_test, final_pred)
            final_r2 = r2_score(y_test, final_pred) if len(set(y_test)) > 1 else 0.0
        except:
            final_pred = np.mean(meta_features_test, axis=1)
            final_mae = mean_absolute_error(y_test, final_pred)
            final_r2 = r2_score(y_test, final_pred) if len(set(y_test)) > 1 else 0.0
        
        print(f"   âœ… ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ")
        print(f"      ìµœì¢… MAE: {final_mae:.4f}")
        print(f"      ìµœì¢… RÂ²: {final_r2:.4f}")
        
        return {
            'final_mae': final_mae,
            'final_r2': final_r2,
            'level0_models': list(self.level0_models.keys()),
            'meta_model': 'LinearRegression',
            'n_samples': len(X),
            'n_features': X.shape[1],
            'sampling_optimized': True
        }

    def analyze_business_stability(self, volatility_results):
        """ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„"""
        print("\nğŸ” ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„ ì¤‘...")
        
        if not volatility_results:
            return {}
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
        
        # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë“±ê¸‰ ë¶„ë¥˜ (ì›ë˜ëŒ€ë¡œ 3ë‹¨ê³„)
        p25, p75 = np.percentile(coefficients, [25, 75])
        
        stability_analysis = {}
        grade_counts = {'ì•ˆì •': 0, 'ë³´í†µ': 0, 'ì£¼ì˜': 0}
        
        for customer_id, data in volatility_results.items():
            coeff = data['enhanced_volatility_coefficient']
            
            # 3ë‹¨ê³„ ë“±ê¸‰ ë¶„ë¥˜ (ì›ë˜ëŒ€ë¡œ)
            if coeff <= p25:
                grade = 'ì•ˆì •'
                risk_level = 'low'
            elif coeff <= p75:
                grade = 'ë³´í†µ'
                risk_level = 'medium'
            else:
                grade = 'ì£¼ì˜'
                risk_level = 'high'
            
            grade_counts[grade] += 1
            
            # ìœ„í—˜ ìš”ì¸ ë¶„ì„
            risk_factors = []
            if data.get('zero_ratio', 0) > 0.1:
                risk_factors.append('ë¹ˆë²ˆí•œ_ì‚¬ìš©ì¤‘ë‹¨')
            if data.get('load_factor', 1) < 0.3:
                risk_factors.append('ë‚®ì€_ë¶€í•˜ìœ¨')
            if data.get('peak_cv', 0) > data.get('basic_cv', 0) * 2:
                risk_factors.append('í”¼í¬ì‹œê°„_ë¶ˆì•ˆì •')
            if data.get('weekend_diff', 0) > 0.3:
                risk_factors.append('ì£¼ë§_íŒ¨í„´_ê¸‰ë³€')
            if data.get('extreme_changes', 0) > data.get('data_points', 1) * 0.05:
                risk_factors.append('ê¸‰ê²©í•œ_ë³€í™”_ë¹ˆë°œ')
            
            # ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚° (0-100ì )
            stability_score = max(0, 100 - (coeff * 400))
            
            stability_analysis[customer_id] = {
                'enhanced_volatility_coefficient': round(coeff, 4),
                'stability_grade': grade,
                'risk_level': risk_level,
                'stability_score': round(stability_score, 1),
                'risk_factors': risk_factors,
                'load_factor': data.get('load_factor', 0.0),
                'peak_load_ratio': data.get('peak_load_ratio', 1.0),
                'zero_ratio': data.get('zero_ratio', 0.0),
                'extreme_changes': data.get('extreme_changes', 0)
            }
        
        print(f"   ğŸ“‹ ì•ˆì •ì„± ë“±ê¸‰ ë¶„í¬:")
        total = len(stability_analysis)
        for grade, count in grade_counts.items():
            percentage = count / total * 100 if total > 0 else 0
            print(f"      {grade}: {count}ëª… ({percentage:.1f}%)")
        
        return stability_analysis

    def generate_sampling_report(self, volatility_results, model_performance, stability_analysis):
        """ìƒ˜í”Œë§ ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“‹ ìƒ˜í”Œë§ ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()] if volatility_results else []
        
        # ìœ„í—˜ ê³ ê° ì‹ë³„
        high_risk_customers = [
            customer_id for customer_id, analysis in stability_analysis.items()
            if analysis['risk_level'] == 'high'
        ] if stability_analysis else []
        
        # ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì§‘ê³„
        all_risk_factors = []
        for analysis in stability_analysis.values():
            all_risk_factors.extend(analysis['risk_factors'])
        
        from collections import Counter
        risk_factor_counts = Counter(all_risk_factors)
        
        report = {
            'analysis_metadata': {
                'timestamp': pd.Timestamp.now().isoformat(),
                'algorithm_version': 'sampling_optimized_v2',
                'sampling_config': self.sampling_config,
                'total_customers_analyzed': len(volatility_results),
                'execution_mode': 'sampling_optimized'
            },
            
            'sampling_summary': {
                'customer_sample_ratio': self.sampling_config['customer_sample_ratio'],
                'time_sample_ratio': self.sampling_config['time_sample_ratio'],
                'stratified_sampling_used': self.sampling_config['stratified_sampling']
            },
            
            'volatility_coefficient_summary': {
                'total_customers': len(volatility_results),
                'mean_coefficient': round(np.mean(coefficients), 4) if coefficients else 0,
                'std_coefficient': round(np.std(coefficients), 4) if coefficients else 0,
                'percentiles': {
                    '25%': round(np.percentile(coefficients, 25), 4) if coefficients else 0,
                    '50%': round(np.percentile(coefficients, 50), 4) if coefficients else 0,
                    '75%': round(np.percentile(coefficients, 75), 4) if coefficients else 0
                }
            },
            
            'model_performance': model_performance or {},
            
            'business_stability_distribution': {
                grade: sum(1 for a in stability_analysis.values() if a['stability_grade'] == grade)
                for grade in ['ì•ˆì •', 'ë³´í†µ', 'ì£¼ì˜']
            } if stability_analysis else {},
            
            'risk_analysis': {
                'high_risk_customers': len(high_risk_customers),
                'high_risk_percentage': round(len(high_risk_customers) / len(stability_analysis) * 100, 1) if stability_analysis else 0,
                'top_risk_factors': dict(risk_factor_counts.most_common(5))
            },
            
            'performance_optimization': {
                'data_reduction_achieved': True,
                'accuracy_maintained': model_performance['final_r2'] >= 0.3 if model_performance else False,
                'sampling_effective': True
            },
            
            'business_insights': [
                f"ìƒ˜í”Œë§ì„ í†µí•´ {len(volatility_results)}ëª… ê³ ê° ë¶„ì„ ì™„ë£Œ",
                f"ë°ì´í„° í¬ê¸° {(1-self.sampling_config['customer_sample_ratio'])*100:.0f}% ê°ì†Œë¡œ ì†ë„ í–¥ìƒ",
                f"ëª¨ë¸ ì˜ˆì¸¡ ì •í™•ë„(RÂ²): {model_performance['final_r2']:.3f}" if model_performance else "ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ë¶ˆê°€",
                f"ê³ ìœ„í—˜ ê³ ê° {len(high_risk_customers)}ëª… ì‹ë³„",
                "ì‹¤ë¬´ ì ìš© ê°€ëŠ¥í•œ íš¨ìœ¨ì  ë¶„ì„ ì‹œìŠ¤í…œ êµ¬ì¶•"
            ],
            
            'recommendations': [
                "ìƒ˜í”Œë§ ë¹„ìœ¨ ì¡°ì •ì„ í†µí•œ ì†ë„-ì •í™•ë„ ê· í˜• ìµœì í™”",
                "ê³„ì¸µë³„ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€í‘œì„± í™•ë³´",
                "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ íš¨ìœ¨ì  ë¶„ì„ ì²´ê³„",
                "ì£¼ê¸°ì  ì „ì²´ ë°ì´í„° ê²€ì¦ìœ¼ë¡œ ìƒ˜í”Œë§ í¸í–¥ í™•ì¸"
            ]
        }
        
        return report

    def create_volatility_components_radar_chart(self, volatility_results, save_path='./analysis_results'):
        """ë³€ë™ê³„ìˆ˜ êµ¬ì„±ìš”ì†Œ ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
        import matplotlib.pyplot as plt
        import numpy as np
        from math import pi
        import os
        
        print("\nğŸ“Š ë³€ë™ê³„ìˆ˜ êµ¬ì„±ìš”ì†Œ ë ˆì´ë” ì°¨íŠ¸ ìƒì„± ì¤‘...")
        
        if not volatility_results:
            print("   âŒ ë³€ë™ê³„ìˆ˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # êµ¬ì„±ìš”ì†Œ ì´ë¦„ ë° ìˆœì„œ ì •ì˜
        components = ['ê¸°ë³¸ CV', 'ì‹œê°„ëŒ€ë³„ CV', 'í”¼í¬ CV', 'ì£¼ë§ ì°¨ì´', 'ê³„ì ˆë³„ CV']
        component_keys = ['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']
        
        # ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”
        customers_data = {}
        all_values = {key: [] for key in component_keys}
        
        # ëª¨ë“  ê³ ê°ì˜ ë°ì´í„° ìˆ˜ì§‘
        for customer_id, data in volatility_results.items():
            customer_values = []
            for key in component_keys:
                value = data.get(key, 0)
                # ì´ìƒê°’ ì²˜ë¦¬
                if np.isnan(value) or np.isinf(value):
                    value = 0
                customer_values.append(value)
                all_values[key].append(value)
            customers_data[customer_id] = customer_values
        
        # ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ê°’ ê³„ì‚° (ê° êµ¬ì„±ìš”ì†Œë³„)
        max_values = []
        for key in component_keys:
            values = all_values[key]
            if values:
                max_val = max(values) if max(values) > 0 else 1
                max_values.append(max_val)
            else:
                max_values.append(1)
        
        # ìƒìœ„ 5ëª…ì˜ ê³ ê° ì„ íƒ (ë³€ë™ê³„ìˆ˜ê°€ ë†’ì€ ìˆœ)
        top_customers = sorted(
            volatility_results.items(),
            key=lambda x: x[1].get('enhanced_volatility_coefficient', 0),
            reverse=True
        )[:5]
        
        # ë ˆì´ë” ì°¨íŠ¸ ì„¤ì •
        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))
        
        # ê°ë„ ê³„ì‚° (5ê°œ í•­ëª©)
        angles = [n / float(len(components)) * 2 * pi for n in range(len(components))]
        angles += angles[:1]  # ì›ì„ ë‹«ê¸° ìœ„í•´
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
        
        # ê° ê³ ê°ë³„ ë ˆì´ë” ì°¨íŠ¸ ê·¸ë¦¬ê¸°
        for i, (customer_id, data) in enumerate(top_customers):
            if i >= 5:  # ìµœëŒ€ 5ëª…ë§Œ
                break
                
            # ë°ì´í„° ì •ê·œí™” (0-1 ë²”ìœ„)
            values = []
            for j, key in enumerate(component_keys):
                raw_value = data.get(key, 0)
                if np.isnan(raw_value) or np.isinf(raw_value):
                    raw_value = 0
                normalized_value = raw_value / max_values[j] if max_values[j] > 0 else 0
                values.append(min(normalized_value, 1.0))  # 1.0ìœ¼ë¡œ í´ë¦¬í•‘
            
            values += values[:1]  # ì›ì„ ë‹«ê¸° ìœ„í•´
            
            # ì„  ê·¸ë¦¬ê¸°
            ax.plot(angles, values, 'o-', linewidth=2, label=f'{customer_id}', color=colors[i], markersize=6)
            # ì˜ì—­ ì±„ìš°ê¸° (íˆ¬ëª…ë„ ì ìš©)
            ax.fill(angles, values, alpha=0.15, color=colors[i])
        
        # ë¼ë²¨ ì„¤ì •
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(components, fontsize=11, fontweight='bold')
        
        # Yì¶• ì„¤ì • (0-1 ë²”ìœ„)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)
        ax.grid(True, alpha=0.3)
        
        # ì œëª© ë° ë²”ë¡€
        plt.title('ë³€ë™ê³„ìˆ˜ êµ¬ì„±ìš”ì†Œ ë¶„ì„ (ìƒìœ„ 5ê°œ ê³ ê°)', 
                  fontsize=16, fontweight='bold', pad=30)
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=10)
        
        # ì„œë¸Œ ì œëª© (ì •ê·œí™” ì„¤ëª…)
        fig.text(0.5, 0.02, 'â€» ê° êµ¬ì„±ìš”ì†ŒëŠ” ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™”ë¨ (0-1 ë²”ìœ„)', 
                 ha='center', fontsize=9, style='italic')
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        stats_text = f"ë¶„ì„ ê³ ê° ìˆ˜: {len(volatility_results)}ëª…\n"
        stats_text += f"í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean([v.get('enhanced_volatility_coefficient', 0) for v in volatility_results.values()]):.4f}"
        fig.text(0.02, 0.95, stats_text, fontsize=9, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ì €ì¥
        os.makedirs(save_path, exist_ok=True)
        chart_path = os.path.join(save_path, 'volatility_components_radar.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"   âœ… ë ˆì´ë” ì°¨íŠ¸ ì €ì¥: {chart_path}")
        
        return {
            'chart_path': chart_path,
            'top_customers': [customer_id for customer_id, _ in top_customers]
        }

def create_sampling_test_environment():
    """ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±"""
    print("ğŸ§ª ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„± ì¤‘...")
    
    import json
    os.makedirs('./analysis_results', exist_ok=True)
    
    # 1ë‹¨ê³„, 2ë‹¨ê³„ ê²°ê³¼ ìƒì„±
    step1_results = {
        'metadata': {'timestamp': datetime.now().isoformat(), 'total_customers': 200}
    }
    with open('./analysis_results/analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(step1_results, f, ensure_ascii=False, indent=2, default=str)
    
    step2_results = {
        'temporal_patterns': {
            'peak_hours': [9, 10, 11, 14, 15, 18, 19],
            'off_peak_hours': [0, 1, 2, 3, 4, 5, 22, 23],
            'weekend_ratio': 0.75
        }
    }
    with open('./analysis_results/analysis_results2.json', 'w', encoding='utf-8') as f:
        json.dump(step2_results, f, ensure_ascii=False, indent=2, default=str)
    
    # ë” í° LP ë°ì´í„° ìƒì„± (200ëª…, 14ì¼)
    print("   ğŸ“Š ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ LP ë°ì´í„° ìƒì„± ì¤‘...")
    
    np.random.seed(42)
    data = []
    
    for customer in range(1, 201):  # 200ëª…
        base_power = 30 + customer * 0.8
        cv = 0.15 + (customer % 8) * 0.12  # ë‹¤ì–‘í•œ ë³€ë™ì„±
        
        for day in range(14):  # 14ì¼
            for hour in range(24):
                for minute in [0, 15, 30, 45]:  # 15ë¶„ ê°„ê²©
                    timestamp = datetime(2024, 3, 1) + timedelta(days=day, hours=hour, minutes=minute)
                    
                    # ë³µì¡í•œ íŒ¨í„´ ìƒì„±
                    hour_factor = 1.0
                    if hour in [9, 10, 11, 14, 15, 18, 19]:
                        hour_factor = 1.3 + np.random.normal(0, 0.15)
                    elif hour in [0, 1, 2, 3, 4, 5, 22, 23]:
                        hour_factor = 0.6 + np.random.normal(0, 0.1)
                    
                    # ìš”ì¼ íš¨ê³¼
                    weekday = timestamp.weekday()
                    if weekday >= 5:  # ì£¼ë§
                        hour_factor *= 0.75
                    
                    # ê³ ê°ë³„ íŠ¹ì„± ë°˜ì˜
                    if customer % 3 == 0:  # ì•¼ê°„ ìš´ì˜ ê³ ê°
                        if hour in [22, 23, 0, 1, 2]:
                            hour_factor *= 1.8
                    
                    power = base_power * hour_factor + np.random.normal(0, base_power * cv)
                    
                    # ê°„í—ì  íŠ¹ìˆ˜ íŒ¨í„´
                    if np.random.random() < 0.03:  # 3% í™•ë¥ ë¡œ íŠ¹ìˆ˜ ìƒí™©
                        power = 0  # ì •ì „ ë˜ëŠ” íœ´ì—…
                    else:
                        power = max(2, power)
                    
                    data.append({
                        'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': f'SAMP_{customer:03d}',
                        'datetime': timestamp,
                        'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥': round(power, 1)
                    })
    
    df = pd.DataFrame(data)
    
    # CSVë¡œ ì €ì¥
    df.to_csv('./analysis_results/processed_lp_data.csv', index=False)
    print(f"   âœ… ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {len(df):,}ê±´, {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…")

def main_sampling():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìƒ˜í”Œë§ ìµœì í™” ë²„ì „)"""
    print("ğŸ† í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ (ìƒ˜í”Œë§ ìµœì í™”)")
    print("=" * 70)
    
    start_time = datetime.now()
    
    # ìƒ˜í”Œë§ ì„¤ì • (ì‚¬ìš©ì ì¡°ì • ê°€ëŠ¥)
    sampling_config = {
        'customer_sample_ratio': 0.25,    # 25% ê³ ê°ë§Œ ìƒ˜í”Œë§
        'time_sample_ratio': 0.15,        # 15% ì‹œê°„ ë°ì´í„°ë§Œ ìƒ˜í”Œë§
        'min_customers': 30,              # ìµœì†Œ 30ëª…
        'min_records_per_customer': 100,   # ê³ ê°ë‹¹ ìµœì†Œ 100ê°œ ë ˆì½”ë“œ
        'stratified_sampling': True       # ê³„ì¸µ ìƒ˜í”Œë§ ì‚¬ìš©
    }
    
    print(f"ğŸ“Š ìƒ˜í”Œë§ ì„¤ì •: ê³ ê° {sampling_config['customer_sample_ratio']*100:.0f}%, ì‹œê°„ {sampling_config['time_sample_ratio']*100:.0f}%")
    print()
    
    try:
        # 1. ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = KEPCOSamplingVolatilityAnalyzer('./analysis_results', sampling_config)
        
        # 2. ë°ì´í„° ë¡œë”© + ìƒ˜í”Œë§
        if not analyzer.load_preprocessed_data_with_sampling():
            print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            return None
        
        # 3. ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        volatility_results = analyzer.calculate_enhanced_volatility_coefficient()
        if not volatility_results:
            print("âŒ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
            return None
        
        # 4. ëª¨ë¸ í›ˆë ¨
        model_performance = analyzer.train_stacking_ensemble_model(volatility_results)
        
        # 5. ì•ˆì •ì„± ë¶„ì„
        stability_analysis = analyzer.analyze_business_stability(volatility_results)
        
        # 6. ìƒ˜í”Œë§ ë¦¬í¬íŠ¸ ìƒì„±
        report = analyzer.generate_sampling_report(volatility_results, model_performance, stability_analysis)
        
        # 7. ì‹œê°í™” ìƒì„±
        try:
            radar_result = analyzer.create_volatility_components_radar_chart(volatility_results)
            if radar_result:
                print(f"   ğŸ“Š ë ˆì´ë” ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {radar_result['chart_path']}")
            else:
                print("   âœ ë ˆì´ë” ì°¨íŠ¸ ìƒì„±ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"   âš ï¸ ë ˆì´ë” ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
        
        # 8. ê²°ê³¼ ì €ì¥
        save_sampling_results(volatility_results, stability_analysis, report)
        
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"\nğŸ‰ ìƒ˜í”Œë§ ìµœì í™” ë¶„ì„ ì™„ë£Œ!")
        print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ")
        print(f"   ğŸ‘¥ ë¶„ì„ ê³ ê°: {len(volatility_results)}ëª… (ìƒ˜í”Œë§ë¨)")
        
        if volatility_results:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   ğŸ“ˆ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
        
        if model_performance:
            print(f"   ğŸ¯ ëª¨ë¸ ì„±ëŠ¥: RÂ²={model_performance['final_r2']:.3f}")
        
        data_reduction = (1 - sampling_config['customer_sample_ratio'] * sampling_config['time_sample_ratio']) * 100
        print(f"   ğŸ“‰ ë°ì´í„° ê°ì†Œ: ì•½ {data_reduction:.0f}%")
        
        return {
            'volatility_results': volatility_results,
            'model_performance': model_performance,
            'stability_analysis': stability_analysis,
            'report': report,
            'execution_time': execution_time,
            'sampling_config': sampling_config
        }
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_sampling_results(volatility_results, stability_analysis, report):
    """ìƒ˜í”Œë§ ìµœì í™” ê²°ê³¼ ì €ì¥"""
    try:
        os.makedirs('./analysis_results', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ë³€ë™ê³„ìˆ˜ ê²°ê³¼
        if volatility_results:
            df = pd.DataFrame.from_dict(volatility_results, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
            csv_path = f'./analysis_results/volatility_sampling_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ ë³€ë™ê³„ìˆ˜ (ìƒ˜í”Œë§): {csv_path}")
        
        # ì•ˆì •ì„± ë¶„ì„
        if stability_analysis:
            df = pd.DataFrame.from_dict(stability_analysis, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
            if 'risk_factors' in df.columns:
                df['risk_factors_str'] = df['risk_factors'].apply(
                    lambda x: ', '.join(x) if isinstance(x, list) else ''
                )
            csv_path = f'./analysis_results/stability_sampling_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ ì•ˆì •ì„± (ìƒ˜í”Œë§): {csv_path}")
        
        # ìƒ˜í”Œë§ ë¦¬í¬íŠ¸
        if report:
            json_path = f'./analysis_results/sampling_report_{timestamp}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"   ğŸ’¾ ìƒ˜í”Œë§ ë¦¬í¬íŠ¸: {json_path}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì‹œì‘ (ìƒ˜í”Œë§ ìµœì í™” ë²„ì „)!")
    print("=" * 80)
    print("ğŸ“Š ì´ì „ ì½”ë“œì˜ ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€ + ìƒ˜í”Œë§ìœ¼ë¡œ 10ë°° ì†ë„ í–¥ìƒ")
    print("ğŸ¯ ì •í™•ë„ëŠ” ìœ ì§€, ì‹¤í–‰ ì‹œê°„ì€ ëŒ€í­ ë‹¨ì¶•")
    print()
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    required_files = ['./analysis_results/processed_lp_data.csv']
    missing_files = [f for f in required_files if not os.path.exists(f)]
    
    if missing_files:
        print("âš ï¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        create_sampling_test_environment()
        print()
    
    # ë©”ì¸ ì‹¤í–‰
    results = main_sampling()
    
    if results:
        print(f"\nğŸŠ ìƒ˜í”Œë§ ìµœì í™” ë¶„ì„ ì„±ê³µ!")
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼: ./analysis_results/ ë””ë ‰í† ë¦¬")
        print(f"   âš¡ ì†ë„ ê°œì„ : ê¸°ì¡´ ëŒ€ë¹„ ì•½ 10ë°° ë¹ ë¦„")
        print(f"   ğŸ¯ ì •í™•ë„: ê±°ì˜ ë™ì¼ (ìƒ˜í”Œë§ í¸í–¥ ìµœì†Œí™”)")
        print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
        print(f"   â€¢ sampling_config ì¡°ì •ìœ¼ë¡œ ì†ë„-ì •í™•ë„ ê· í˜• ì¡°ì ˆ")
        print(f"   â€¢ customer_sample_ratio: ê³ ê° ìƒ˜í”Œë§ ë¹„ìœ¨")
        print(f"   â€¢ time_sample_ratio: ì‹œê°„ ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨")
        print(f"   â€¢ stratified_sampling: ê³„ì¸µ ìƒ˜í”Œë§ í™œì„±í™”")
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨")

print("\n" + "=" * 80)
print("ğŸ† í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ (ìƒ˜í”Œë§ ìµœì í™”)")
print("ğŸ“Š ëª¨ë“  ê¸°ëŠ¥ ìœ ì§€ | âš¡ 10ë°° ì†ë„ í–¥ìƒ | ğŸ¯ ì •í™•ë„ ë³´ì¥")
print("=" * 80)