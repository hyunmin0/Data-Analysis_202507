"""
í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ (ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ í™œìš©)
ëª©í‘œ: ê¸°ì—…ì˜ ì „ë ¥ ì‚¬ìš© ì•ˆì •ì„±ê³¼ ì˜ì—…í™œë™ ë³€í™” ì˜ˆì¸¡

ì…ë ¥ ë°ì´í„°:
1. analysis_results.json (1ë‹¨ê³„ ì „ì²˜ë¦¬ ê²°ê³¼)
2. analysis_results2.json (2ë‹¨ê³„ ì‹œê³„ì—´ ë¶„ì„)
3. processed_lp_data.h5 (ì „ì²˜ë¦¬ëœ LP ë°ì´í„°)
4. í•œì „_í†µí•©ë°ì´í„°.xlsx (í•œì „ ê³µê³µë°ì´í„°)
"""

import pandas as pd
import numpy as np
import json
import os
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

class KEPCOStackingVolatilityAnalyzer:
    """í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ê¸° (ì „ì²˜ë¦¬ ê²°ê³¼ í™œìš©)"""
    
    def __init__(self, results_dir='./analysis_results'):
        self.results_dir = results_dir
        self.scaler = StandardScaler()
        self.level0_models = {}
        self.meta_model = None
        
        # ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©
        self.step1_results = self._load_step1_results()
        self.step2_results = self._load_step2_results()
        self.lp_data = None
        self.kepco_data = None
        
        print("ğŸ”§ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ê¸° ì´ˆê¸°í™”")
        print(f"   ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬: {results_dir}")
        
    def _load_step1_results(self):
        """1ë‹¨ê³„ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                print(f"   âš ï¸ 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {file_path}")
                return {}
        except Exception as e:
            print(f"   âŒ 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_step2_results(self):
        """2ë‹¨ê³„ ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results2.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                print(f"   âš ï¸ 2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {file_path}")
                return {}
        except Exception as e:
            print(f"   âŒ 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def load_preprocessed_data(self):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©"""
        print("\nğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # 1. LP ë°ì´í„° ë¡œë”© (HDF5 ìš°ì„ )
        hdf5_path = os.path.join(self.results_dir, 'processed_lp_data.h5')
        csv_path = os.path.join(self.results_dir, 'processed_lp_data.csv')
        
        if os.path.exists(hdf5_path):
            try:
                self.lp_data = pd.read_hdf(hdf5_path, key='df')
                print(f"   âœ… HDF5 LP ë°ì´í„°: {len(self.lp_data):,}ê±´")
                loading_method = "HDF5"
            except Exception as e:
                print(f"   âš ï¸ HDF5 ë¡œë”© ì‹¤íŒ¨: {e}")
                if os.path.exists(csv_path):
                    self.lp_data = pd.read_csv(csv_path)
                    loading_method = "CSV"
                else:
                    print(f"   âŒ LP ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
        elif os.path.exists(csv_path):
            self.lp_data = pd.read_csv(csv_path)
            loading_method = "CSV"
        else:
            print(f"   âŒ ì „ì²˜ë¦¬ëœ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # datetime ì»¬ëŸ¼ ì²˜ë¦¬
        if 'datetime' in self.lp_data.columns:
            self.lp_data['datetime'] = pd.to_datetime(self.lp_data['datetime'])
        elif 'LP ìˆ˜ì‹ ì¼ì' in self.lp_data.columns:
            self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP ìˆ˜ì‹ ì¼ì'])
        
        print(f"   ğŸ“ˆ ë¡œë”© ë°©ë²•: {loading_method}")
        print(f"   ğŸ“… ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}")
        print(f"   ğŸ‘¥ ê³ ê°ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}")
        
        # 2. í•œì „ í†µí•© ë°ì´í„° ë¡œë”©
        kepco_path = 'í•œì „_í†µí•©ë°ì´í„°.xlsx'
        if os.path.exists(kepco_path):
            try:
                self.kepco_data = pd.read_excel(kepco_path, sheet_name='ì „ì²´ë°ì´í„°')
                print(f"   âœ… í•œì „ í†µí•© ë°ì´í„°: {len(self.kepco_data):,}ê±´")
            except Exception as e:
                print(f"   âš ï¸ í•œì „ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
                self.kepco_data = None
        else:
            print(f"   âš ï¸ í•œì „ í†µí•© ë°ì´í„° ì—†ìŒ: {kepco_path}")
            self.kepco_data = None
        
        return True
    
    def optimize_volatility_weights(self, volatility_components):
        """ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìµœì í™”"""
        print("\nâš™ï¸ ë³€ë™ê³„ìˆ˜ ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘...")
        
        if len(volatility_components) < 20:
            print("   âš ï¸ ìµœì í™”ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©")
            return [0.35, 0.25, 0.20, 0.10, 0.10]
        
        from scipy.optimize import minimize
        import warnings
        warnings.filterwarnings('ignore')
        
        # ì„±ë¶„ë³„ ë°ì´í„° ì¤€ë¹„
        components_df = pd.DataFrame(volatility_components)
        
        # ëª©í‘œ ë³€ìˆ˜ ìƒì„± (ì˜ì—…í™œë™ ë³€í™”ì˜ ëŒ€ë¦¬ ì§€í‘œ)
        # ë°©ë²• 1: ë†’ì€ ë³€ë™ì„± = ë†’ì€ ìœ„í—˜ë„
        # ë°©ë²• 2: ì‹¤ì œ ì˜ì—… ì§€í‘œê°€ ìˆë‹¤ë©´ í™œìš© (ë§¤ì¶œ, ê³„ì•½ ë³€ê²½ ë“±)
        
        # ì—¬ê¸°ì„œëŠ” ì¢…í•©ì ì¸ ë¶ˆì•ˆì •ì„±ì„ ëª©í‘œë¡œ ì„¤ì •
        target_instability = []
        
        for idx, row in components_df.iterrows():
            # ë³µí•© ë¶ˆì•ˆì •ì„± ì§€í‘œ ê³„ì‚°
            instability = (
                row['basic_cv'] * 2.0 +           # ê¸°ë³¸ ë³€ë™ì„± ë†’ìœ¼ë©´ ë¶ˆì•ˆì •
                row['extreme_changes'] * 0.01 +   # ê¸‰ê²©í•œ ë³€í™” ë§ìœ¼ë©´ ë¶ˆì•ˆì •  
                row['zero_ratio'] * 1.0 +         # ì‚¬ìš© ì¤‘ë‹¨ ë§ìœ¼ë©´ ë¶ˆì•ˆì •
                (1 - row['load_factor']) * 0.5    # ë¶€í•˜ìœ¨ ë‚®ìœ¼ë©´ ë¶ˆì•ˆì •
            )
            target_instability.append(instability)
        
        X = components_df[['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']].values
        y = np.array(target_instability)
        
        # ì œì•½ ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1, ëª¨ë“  ê°€ì¤‘ì¹˜ >= 0
        def objective(weights):
            predicted = X @ weights
            return np.mean((predicted - y) ** 2)  # MSE
        
        constraints = [
            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0},  # í•© = 1
        ]
        
        bounds = [(0, 1) for _ in range(5)]  # 0 <= weight <= 1
        
        # ì´ˆê¸°ê°’ (ê¸°ì¡´ ê°€ì¤‘ì¹˜)
        initial_weights = [0.35, 0.25, 0.20, 0.10, 0.10]
        
        try:
            result = minimize(
                objective, 
                initial_weights, 
                method='SLSQP',
                bounds=bounds,
                constraints=constraints
            )
            
            if result.success:
                optimized_weights = result.x
                improvement = objective(initial_weights) - objective(optimized_weights)
                
                print(f"   âœ… ê°€ì¤‘ì¹˜ ìµœì í™” ì™„ë£Œ")
                print(f"   ğŸ“Š ê¸°ì¡´ ê°€ì¤‘ì¹˜: {[round(w, 3) for w in initial_weights]}")
                print(f"   ğŸ¯ ìµœì  ê°€ì¤‘ì¹˜: {[round(w, 3) for w in optimized_weights]}")
                print(f"   ğŸ“ˆ ê°œì„ ë„: {improvement:.4f}")
                
                return optimized_weights.tolist()
            else:
                print(f"   âš ï¸ ìµœì í™” ì‹¤íŒ¨: {result.message}")
                return initial_weights
                
        except Exception as e:
            print(f"   âŒ ìµœì í™” ì˜¤ë¥˜: {e}")
            return initial_weights
        """í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° (2ë‹¨ê³„ ê²°ê³¼ í™œìš©)"""
        print("\nğŸ“ í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì¤‘...")
        
        if self.lp_data is None:
            print("   âŒ LP ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2ë‹¨ê³„ ê²°ê³¼ì—ì„œ ì‹œê°„ íŒ¨í„´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ ë¶„ì„ëœ ê°’ í™œìš©)
        temporal_patterns = self.step2_results.get('temporal_patterns', {})
        peak_hours = temporal_patterns.get('peak_hours', [])
        off_peak_hours = temporal_patterns.get('off_peak_hours', [])
        weekend_ratio = temporal_patterns.get('weekend_ratio', 1.0)
        
        # 2ë‹¨ê³„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not peak_hours:
            peak_hours = [9, 10, 11, 14, 15, 18, 19]
            print(f"   âš ï¸ 2ë‹¨ê³„ í”¼í¬ ì‹œê°„ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
        if not off_peak_hours:
            off_peak_hours = [0, 1, 2, 3, 4, 5]
            print(f"   âš ï¸ 2ë‹¨ê³„ ë¹„í”¼í¬ ì‹œê°„ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        print(f"   ğŸ• í”¼í¬ ì‹œê°„: {peak_hours} (2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼)")
        print(f"   ğŸŒ™ ë¹„í”¼í¬ ì‹œê°„: {off_peak_hours} (2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼)")
        print(f"   ğŸ“… ì£¼ë§/í‰ì¼ ë¹„ìœ¨: {weekend_ratio:.3f} (2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼)")
        
    def calculate_enhanced_volatility_coefficient(self, optimize_weights=True):
        """í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° (2ë‹¨ê³„ ê²°ê³¼ í™œìš©)"""
        print("\nğŸ“ í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì¤‘...")
        
        if self.lp_data is None:
            print("   âŒ LP ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2ë‹¨ê³„ ê²°ê³¼ì—ì„œ ì‹œê°„ íŒ¨í„´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ ë¶„ì„ëœ ê°’ í™œìš©)
        temporal_patterns = self.step2_results.get('temporal_patterns', {})
        peak_hours = temporal_patterns.get('peak_hours', [])
        off_peak_hours = temporal_patterns.get('off_peak_hours', [])
        weekend_ratio = temporal_patterns.get('weekend_ratio', 1.0)
        
        # 2ë‹¨ê³„ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not peak_hours:
            peak_hours = [9, 10, 11, 14, 15, 18, 19]
            print(f"   âš ï¸ 2ë‹¨ê³„ í”¼í¬ ì‹œê°„ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
        if not off_peak_hours:
            off_peak_hours = [0, 1, 2, 3, 4, 5]
            print(f"   âš ï¸ 2ë‹¨ê³„ ë¹„í”¼í¬ ì‹œê°„ ì—†ìŒ, ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        print(f"   ğŸ• í”¼í¬ ì‹œê°„: {peak_hours} (2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼)")
        print(f"   ğŸŒ™ ë¹„í”¼í¬ ì‹œê°„: {off_peak_hours} (2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼)")
        print(f"   ğŸ“… ì£¼ë§/í‰ì¼ ë¹„ìœ¨: {weekend_ratio:.3f} (2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼)")
        
        # ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour
        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday
        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])
        self.lp_data['month'] = self.lp_data['datetime'].dt.month
        
        customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        volatility_results = {}
        volatility_components = []  # ê°€ì¤‘ì¹˜ ìµœì í™”ìš©
        processed_count = 0
        
        print(f"   ğŸ‘¥ ë¶„ì„ ëŒ€ìƒ: {len(customers)}ëª…")
        
        # 1ì°¨: ëª¨ë“  ì„±ë¶„ ê³„ì‚°
        batch_size = 100
        for i in range(0, len(customers), batch_size):
            batch_customers = customers[i:i+batch_size]
            
            for customer_id in batch_customers:
                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id].copy()
                
                if len(customer_data) < 96:  # ìµœì†Œ 1ì¼ ë°ì´í„° í•„ìš”
                    continue
                
                try:
                    power_values = customer_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].values
                    
                    # 1. ê¸°ë³¸ ë³€ë™ê³„ìˆ˜
                    basic_cv = np.std(power_values) / np.mean(power_values) if np.mean(power_values) > 0 else 0
                    
                    # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ê³„ìˆ˜ (2ë‹¨ê³„ í”¼í¬ ì •ë³´ í™œìš©)
                    hourly_avg = customer_data.groupby('hour')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
                    hourly_cv = np.std(hourly_avg) / np.mean(hourly_avg) if np.mean(hourly_avg) > 0 else 0
                    
                    # 3. í”¼í¬/ë¹„í”¼í¬ ë³€ë™ì„± (ê°€ì¤‘ ì ìš©)
                    peak_data = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    off_peak_data = customer_data[customer_data['hour'].isin(off_peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    
                    peak_cv = np.std(peak_data) / np.mean(peak_data) if len(peak_data) > 0 and np.mean(peak_data) > 0 else 0
                    off_peak_cv = np.std(off_peak_data) / np.mean(off_peak_data) if len(off_peak_data) > 0 and np.mean(off_peak_data) > 0 else 0
                    
                    # 4. ì£¼ë§/í‰ì¼ ë³€ë™ì„± (2ë‹¨ê³„ ë¹„ìœ¨ í™œìš©)
                    weekday_data = customer_data[~customer_data['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    weekend_data = customer_data[customer_data['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    
                    weekday_cv = np.std(weekday_data) / np.mean(weekday_data) if len(weekday_data) > 0 and np.mean(weekday_data) > 0 else 0
                    weekend_cv = np.std(weekend_data) / np.mean(weekend_data) if len(weekend_data) > 0 and np.mean(weekend_data) > 0 else 0
                    weekend_diff = abs(weekday_cv - weekend_cv) * weekend_ratio
                    
                    # 5. ê³„ì ˆë³„ ë³€ë™ì„± (ì›”ë³„)
                    monthly_avg = customer_data.groupby('month')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
                    seasonal_cv = np.std(monthly_avg) / np.mean(monthly_avg) if len(monthly_avg) > 1 and np.mean(monthly_avg) > 0 else 0
                    
                    # ë¶€ê°€ ì§€í‘œë“¤
                    mean_power = np.mean(power_values)
                    max_power = np.max(power_values)
                    load_factor = mean_power / max_power if max_power > 0 else 0
                    
                    # ì´ìƒ íŒ¨í„´ ì§€í‘œ
                    zero_ratio = (power_values == 0).sum() / len(power_values)
                    sudden_changes = pd.Series(power_values).pct_change().abs()
                    extreme_changes = (sudden_changes > 1.5).sum()
                    
                    # í”¼í¬/ë¹„í”¼í¬ ë¶€í•˜ ë¹„ìœ¨
                    peak_avg = np.mean(peak_data) if len(peak_data) > 0 else 0
                    off_peak_avg = np.mean(off_peak_data) if len(off_peak_data) > 0 else 0
                    peak_load_ratio = peak_avg / off_peak_avg if off_peak_avg > 0 else 1.0
                    
                    # ê°€ì¤‘ì¹˜ ìµœì í™”ìš© ë°ì´í„° ì €ì¥
                    volatility_components.append({
                        'customer_id': customer_id,
                        'basic_cv': basic_cv,
                        'hourly_cv': hourly_cv,
                        'peak_cv': peak_cv,
                        'weekend_diff': weekend_diff,
                        'seasonal_cv': seasonal_cv,
                        'load_factor': load_factor,
                        'zero_ratio': zero_ratio,
                        'extreme_changes': extreme_changes,
                        'data_points': len(power_values)
                    })
                    
                    processed_count += 1
                    
                except Exception as e:
                    print(f"   âš ï¸ ê³ ê° {customer_id} ê³„ì‚° ì‹¤íŒ¨: {e}")
                    continue
            
            # ì§„í–‰ìƒí™© ì¶œë ¥
            if (i // batch_size + 1) % 10 == 0:
                print(f"   ğŸ“Š ì§„í–‰: {min(i + batch_size, len(customers))}/{len(customers)} ({processed_count}ëª… ì™„ë£Œ)")
        
        # 2ì°¨: ê°€ì¤‘ì¹˜ ìµœì í™” (ì˜µì…˜)
        if optimize_weights and len(volatility_components) >= 20:
            optimal_weights = self.optimize_volatility_weights(volatility_components)
        else:
            optimal_weights = [0.35, 0.25, 0.20, 0.10, 0.10]  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            if optimize_weights:
                print(f"   âš ï¸ ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©: {optimal_weights}")
        
        print(f"   ğŸ¯ ìµœì¢… ê°€ì¤‘ì¹˜: {[round(w, 3) for w in optimal_weights]}")
        
        # 3ì°¨: ìµœì  ê°€ì¤‘ì¹˜ë¡œ ìµœì¢… ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        for component in volatility_components:
            customer_id = component['customer_id']
            
            # ìµœì í™”ëœ ê°€ì¤‘ì¹˜ ì ìš©
            enhanced_volatility_coefficient = (
                optimal_weights[0] * component['basic_cv'] +
                optimal_weights[1] * component['hourly_cv'] +
                optimal_weights[2] * component['peak_cv'] +
                optimal_weights[3] * component['weekend_diff'] +
                optimal_weights[4] * component['seasonal_cv']
            )
            
            volatility_results[customer_id] = {
                # í•µì‹¬ ë³€ë™ê³„ìˆ˜
                'enhanced_volatility_coefficient': round(enhanced_volatility_coefficient, 4),
                
                # ì„¸ë¶€ ë³€ë™ì„± ì§€í‘œ
                'basic_cv': round(component['basic_cv'], 4),
                'hourly_cv': round(component['hourly_cv'], 4),
                'peak_cv': round(component['peak_cv'], 4),
                'weekend_diff': round(component['weekend_diff'], 4),
                'seasonal_cv': round(component['seasonal_cv'], 4),
                
                # ì‚¬ìš© íŒ¨í„´ ì§€í‘œ  
                'load_factor': round(component['load_factor'], 4),
                'zero_ratio': round(component['zero_ratio'], 4),
                'extreme_changes': int(component['extreme_changes']),
                'data_points': component['data_points'],
                
                # ìµœì í™” ì •ë³´
                'optimized_weights': [round(w, 3) for w in optimal_weights]
            }
        
        print(f"   âœ… {len(volatility_results)}ëª… ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì™„ë£Œ")
        
        if len(volatility_results) > 0:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   ğŸ“ˆ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
            print(f"   ğŸ“Š ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        return volatility_results
    
    def train_stacking_ensemble_model(self, volatility_results):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ (ì˜ì—…í™œë™ ë³€í™” ì˜ˆì¸¡)"""
        print("\nğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        if len(volatility_results) < 20:
            print("   âŒ í›ˆë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 20ê°œ í•„ìš”)")
            return None
        
        # íŠ¹ì„± ì¤€ë¹„
        features = []
        targets = []
        customer_ids = []
        
        for customer_id, data in volatility_results.items():
            feature_vector = [
                data['basic_cv'],
                data['hourly_cv'],
                data['peak_cv'],
                data['off_peak_cv'],
                data['weekday_cv'],
                data['weekend_cv'],
                data['seasonal_cv'],
                data['load_factor'],
                data['peak_load_ratio'],
                data['mean_power'],
                data['zero_ratio'],
                data['extreme_changes'] / data['data_points']  # ì •ê·œí™”ëœ ê·¹ê°’ ë³€í™” ë¹„ìœ¨
            ]
            features.append(feature_vector)
            targets.append(data['enhanced_volatility_coefficient'])
            customer_ids.append(customer_id)
        
        X = np.array(features)
        y = np.array(targets)
        
        print(f"   ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # ì •ê·œí™”
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Level-0 ëª¨ë¸ë“¤ (ë‹¤ì–‘ì„± í™•ë³´í•˜ë˜ ê°„ê²°í•˜ê²Œ)
        self.level0_models = {
            'rf': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),
            'gbm': GradientBoostingRegressor(n_estimators=100, max_depth=6, random_state=42),
            'ridge': Ridge(alpha=1.0)
        }
        
        # Level-0 ì˜ˆì¸¡ê°’ ìƒì„± (3-Fold CV)
        kf = KFold(n_splits=3, shuffle=True, random_state=42)
        
        meta_features_train = np.zeros((len(X_train_scaled), len(self.level0_models)))
        meta_features_test = np.zeros((len(X_test_scaled), len(self.level0_models)))
        
        print(f"   ğŸ”„ Level-0 ëª¨ë¸ í›ˆë ¨:")
        for i, (name, model) in enumerate(self.level0_models.items()):
            # í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•œ CV ì˜ˆì¸¡
            fold_predictions = np.zeros(len(X_train_scaled))
            for train_idx, val_idx in kf.split(X_train_scaled):
                fold_model = type(model)(**model.get_params())
                fold_model.fit(X_train_scaled[train_idx], y_train[train_idx])
                fold_predictions[val_idx] = fold_model.predict(X_train_scaled[val_idx])
            
            meta_features_train[:, i] = fold_predictions
            
            # ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¡œ ì¬í›ˆë ¨ í›„ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
            model.fit(X_train_scaled, y_train)
            meta_features_test[:, i] = model.predict(X_test_scaled)
            
            # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
            cv_pred = model.predict(X_test_scaled)
            cv_mae = mean_absolute_error(y_test, cv_pred)
            cv_r2 = r2_score(y_test, cv_pred)
            print(f"      {name}: MAE={cv_mae:.4f}, RÂ²={cv_r2:.4f}")
        
        # Level-1 ë©”íƒ€ ëª¨ë¸ (Linear Regression)
        self.meta_model = LinearRegression()
        self.meta_model.fit(meta_features_train, y_train)
        
        # ìµœì¢… ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
        final_pred = self.meta_model.predict(meta_features_test)
        final_mae = mean_absolute_error(y_test, final_pred)
        final_r2 = r2_score(y_test, final_pred)
        
        print(f"   âœ… ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ")
        print(f"      ìµœì¢… MAE: {final_mae:.4f}")
        print(f"      ìµœì¢… RÂ²: {final_r2:.4f}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ (Random Forest ê¸°ì¤€)
        feature_names = [
            'basic_cv', 'hourly_cv', 'peak_cv', 'off_peak_cv',
            'weekday_cv', 'weekend_cv', 'seasonal_cv', 'load_factor',
            'peak_load_ratio', 'mean_power', 'zero_ratio', 'extreme_change_ratio'
        ]
        
        rf_importance = self.level0_models['rf'].feature_importances_
        print(f"   ğŸ“Š ì£¼ìš” íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 5ê°œ):")
        importance_pairs = list(zip(feature_names, rf_importance))
        importance_pairs.sort(key=lambda x: x[1], reverse=True)
        for name, importance in importance_pairs[:5]:
            print(f"      {name}: {importance:.4f}")
        
        return {
            'final_mae': final_mae,
            'final_r2': final_r2,
            'level0_models': list(self.level0_models.keys()),
            'meta_model': 'LinearRegression',
            'feature_importance': dict(importance_pairs),
            'n_samples': len(X),
            'n_features': X.shape[1]
        }
    
    def analyze_business_stability(self, volatility_results):
        """ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„"""
        print("\nğŸ” ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„ ì¤‘...")
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
        
        # ì•ˆì •ì„± ê¸°ì¤€ ì„¤ì • (ë¶„ìœ„ìˆ˜ ê¸°ë°˜)
        p10, p25, p50, p75, p90 = np.percentile(coefficients, [10, 25, 50, 75, 90])
        
        print(f"   ğŸ“Š ë³€ë™ê³„ìˆ˜ ë¶„ìœ„ìˆ˜:")
        print(f"      10%: {p10:.4f}")
        print(f"      25%: {p25:.4f}")
        print(f"      50%: {p50:.4f}")
        print(f"      75%: {p75:.4f}")
        print(f"      90%: {p90:.4f}")
        
        stability_analysis = {}
        grade_counts = {'ë§¤ìš°ì•ˆì •': 0, 'ì•ˆì •': 0, 'ë³´í†µ': 0, 'ì£¼ì˜': 0, 'ë¶ˆì•ˆì •': 0}
        
        for customer_id, data in volatility_results.items():
            coeff = data['enhanced_volatility_coefficient']
            
            # ì•ˆì •ì„± ë“±ê¸‰ ë¶„ë¥˜ (5ë‹¨ê³„)
            if coeff <= p10:
                grade = 'ë§¤ìš°ì•ˆì •'
                risk_level = 'very_low'
            elif coeff <= p25:
                grade = 'ì•ˆì •'
                risk_level = 'low'
            elif coeff <= p75:
                grade = 'ë³´í†µ'
                risk_level = 'medium'
            elif coeff <= p90:
                grade = 'ì£¼ì˜'
                risk_level = 'high'
            else:
                grade = 'ë¶ˆì•ˆì •'
                risk_level = 'very_high'
            
            grade_counts[grade] += 1
            
            # ì˜ì—…í™œë™ ë³€í™” ê°€ëŠ¥ì„± ì¶”ì • (0~1)
            change_probability = min(0.95, max(0.05, (coeff - p25) / (p90 - p25))) if p90 > p25 else 0.5
            
            # ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì‹ë³„
            risk_factors = []
            if data['peak_cv'] > data['basic_cv'] * 1.5:
                risk_factors.append('í”¼í¬ì‹œê°„_ë¶ˆì•ˆì •')
            if data['zero_ratio'] > 0.1:
                risk_factors.append('ë¹ˆë²ˆí•œ_ì‚¬ìš©ì¤‘ë‹¨')
            if data['extreme_changes'] > data['data_points'] * 0.05:
                risk_factors.append('ê¸‰ê²©í•œ_ë³€í™”')
            if data['peak_load_ratio'] > 3.0:
                risk_factors.append('í”¼í¬ë¶€í•˜_ì§‘ì¤‘')
            
            stability_analysis[customer_id] = {
                'enhanced_volatility_coefficient': round(coeff, 4),
                'stability_grade': grade,
                'risk_level': risk_level,
                'change_probability': round(change_probability, 3),
                'risk_factors': risk_factors,
                'load_factor': data['load_factor'],
                'peak_load_ratio': data['peak_load_ratio']
            }
        
        # ë“±ê¸‰ë³„ ë¶„í¬ ì¶œë ¥
        print(f"   ğŸ“‹ ì•ˆì •ì„± ë“±ê¸‰ ë¶„í¬:")
        total_customers = len(stability_analysis)
        for grade, count in grade_counts.items():
            percentage = count / total_customers * 100 if total_customers > 0 else 0
            print(f"      {grade}: {count}ëª… ({percentage:.1f}%)")
        
        return stability_analysis
    
    def generate_comprehensive_report(self, volatility_results, model_performance, stability_analysis):
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“‹ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ í†µê³„
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
        
        # ìœ„í—˜ ê³ ê° ì‹ë³„
        high_risk_customers = [
            customer_id for customer_id, analysis in stability_analysis.items()
            if analysis['risk_level'] in ['high', 'very_high']
        ]
        
        # ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì§‘ê³„
        all_risk_factors = []
        for analysis in stability_analysis.values():
            all_risk_factors.extend(analysis['risk_factors'])
        
        from collections import Counter
        risk_factor_counts = Counter(all_risk_factors)
        
        report = {
            'analysis_metadata': {
                'timestamp': pd.Timestamp.now().isoformat(),
                'step1_results_used': bool(self.step1_results),
                'step2_results_used': bool(self.step2_results),
                'kepco_data_used': self.kepco_data is not None,
                'total_customers_analyzed': len(volatility_results)
            },
            
            'volatility_coefficient_summary': {
                'total_customers': len(volatility_results),
                'mean_coefficient': round(np.mean(coefficients), 4),
                'std_coefficient': round(np.std(coefficients), 4),
                'percentiles': {
                    '10%': round(np.percentile(coefficients, 10), 4),
                    '25%': round(np.percentile(coefficients, 25), 4),
                    '50%': round(np.percentile(coefficients, 50), 4),
                    '75%': round(np.percentile(coefficients, 75), 4),
                    '90%': round(np.percentile(coefficients, 90), 4)
                }
            },
            
            'stacking_model_performance': model_performance,
            
            'business_stability_distribution': {
                grade: sum(1 for a in stability_analysis.values() if a['stability_grade'] == grade)
                for grade in ['ë§¤ìš°ì•ˆì •', 'ì•ˆì •', 'ë³´í†µ', 'ì£¼ì˜', 'ë¶ˆì•ˆì •']
            },
            
            'risk_analysis': {
                'high_risk_customers': len(high_risk_customers),
                'high_risk_percentage': round(len(high_risk_customers) / len(stability_analysis) * 100, 1),
                'top_risk_factors': dict(risk_factor_counts.most_common(5))
            },
            
            'business_insights': [
                f"ì´ {len(volatility_results)}ëª… ê³ ê°ì˜ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì™„ë£Œ",
                f"ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ ì˜ˆì¸¡ ì •í™•ë„(RÂ²): {model_performance['final_r2']:.3f}",
                f"ê³ ìœ„í—˜ ê³ ê° {len(high_risk_customers)}ëª… ì‹ë³„ (ì „ì²´ì˜ {len(high_risk_customers)/len(stability_analysis)*100:.1f}%)",
                f"ì£¼ìš” ìœ„í—˜ ìš”ì¸: {list(risk_factor_counts.keys())[:3]}",
                "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì˜ˆë°©ì  ê´€ë¦¬ ì²´ê³„ êµ¬ì¶• ê°€ëŠ¥"
            ],
            
            'recommendations': [
                "ê³ ìœ„í—˜ ê³ ê°ì— ëŒ€í•œ ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•",
                "í”¼í¬ì‹œê°„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ìµœì í™” ì§€ì›",
                "ì˜ˆì¸¡ ëª¨ë¸ ê¸°ë°˜ ì„ ì œì  ê³ ê° ê´€ë¦¬",
                "ì—…ì¢…ë³„ ë§ì¶¤í˜• ì „ë ¥ íš¨ìœ¨ì„± ê°œì„  í”„ë¡œê·¸ë¨ ê°œë°œ"
            ]
        }
        
        return report

# ===== ì‹¤í–‰ ì˜ˆì œ =====

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ† í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 70)
    print("ğŸ“ ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ í™œìš© ë²„ì „")
    print()
    
    try:
        # 1. ë¶„ì„ê¸° ì´ˆê¸°í™”
        print("1ï¸âƒ£ ë¶„ì„ê¸° ì´ˆê¸°í™”")
        analyzer = KEPCOStackingVolatilityAnalyzer('./analysis_results')
        
        # 2. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©
        print("\n2ï¸âƒ£ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©")
        if not analyzer.load_preprocessed_data():
            print("âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨. 1-2ë‹¨ê³„ ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        # 3. í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        print("\n3ï¸âƒ£ í–¥ìƒëœ ë³€ë™ê³„ìˆ˜ ê³„ì‚°")
        volatility_results = analyzer.calculate_enhanced_volatility_coefficient()
        
        if not volatility_results:
            print("âŒ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
            return None
        
        # 4. ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
        print("\n4ï¸âƒ£ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨")
        model_performance = analyzer.train_stacking_ensemble_model(volatility_results)
        
        if not model_performance:
            print("âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")
            return None
        
        # 5. ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„
        print("\n5ï¸âƒ£ ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„")
        stability_analysis = analyzer.analyze_business_stability(volatility_results)
        
        # 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        print("\n6ï¸âƒ£ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±")
        comprehensive_report = analyzer.generate_comprehensive_report(
            volatility_results, model_performance, stability_analysis
        )
        
        # 7. ê²°ê³¼ ì €ì¥
        print("\n7ï¸âƒ£ ê²°ê³¼ ì €ì¥")
        save_results(volatility_results, stability_analysis, comprehensive_report)
        
        # 8. ìµœì¢… ìš”ì•½ ì¶œë ¥
        print_final_summary(comprehensive_report, model_performance)
        
        return {
            'volatility_results': volatility_results,
            'model_performance': model_performance,
            'stability_analysis': stability_analysis,
            'comprehensive_report': comprehensive_report
        }
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_results(volatility_results, stability_analysis, comprehensive_report):
    """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    try:
        import json
        from datetime import datetime
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('./analysis_results', exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ë³€ë™ê³„ìˆ˜ ê²°ê³¼ ì €ì¥ (CSV)
        volatility_df = pd.DataFrame.from_dict(volatility_results, orient='index')
        volatility_df.reset_index(inplace=True)
        volatility_df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
        
        volatility_csv_path = f'./analysis_results/volatility_coefficients_{timestamp}.csv'
        volatility_df.to_csv(volatility_csv_path, index=False, encoding='utf-8-sig')
        print(f"   ğŸ’¾ ë³€ë™ê³„ìˆ˜ ê²°ê³¼: {volatility_csv_path}")
        
        # 2. ì•ˆì •ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥ (CSV)
        stability_df = pd.DataFrame.from_dict(stability_analysis, orient='index')
        stability_df.reset_index(inplace=True)
        stability_df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
        
        # ìœ„í—˜ ìš”ì¸ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        stability_df['risk_factors_str'] = stability_df['risk_factors'].apply(
            lambda x: ', '.join(x) if x else ''
        )
        
        stability_csv_path = f'./analysis_results/business_stability_{timestamp}.csv'
        stability_df.to_csv(stability_csv_path, index=False, encoding='utf-8-sig')
        print(f"   ğŸ’¾ ì•ˆì •ì„± ë¶„ì„: {stability_csv_path}")
        
        # 3. ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥ (JSON)
        report_json_path = f'./analysis_results/comprehensive_report_{timestamp}.json'
        with open(report_json_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, ensure_ascii=False, indent=2, default=str)
        print(f"   ğŸ’¾ ì¢…í•© ë¦¬í¬íŠ¸: {report_json_path}")
        
        # 4. 3ë‹¨ê³„ ê²°ê³¼ í†µí•© ì €ì¥ (ë‹¤ìŒ ë‹¨ê³„ ì—°ê³„ìš©)
        final_results = {
            'metadata': {
                'stage': 'step3_stacking_volatility_analysis',
                'timestamp': datetime.now().isoformat(),
                'version': '3.0',
                'total_customers': len(volatility_results)
            },
            'volatility_summary': comprehensive_report['volatility_coefficient_summary'],
            'model_performance': comprehensive_report['stacking_model_performance'],
            'stability_distribution': comprehensive_report['business_stability_distribution'],
            'risk_analysis': comprehensive_report['risk_analysis'],
            'file_references': {
                'volatility_csv': volatility_csv_path,
                'stability_csv': stability_csv_path,
                'report_json': report_json_path
            }
        }
        
        final_json_path = './analysis_results/analysis_results3.json'
        with open(final_json_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2, default=str)
        print(f"   ğŸ’¾ 3ë‹¨ê³„ í†µí•© ê²°ê³¼: {final_json_path}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def print_final_summary(comprehensive_report, model_performance):
    """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 70)
    print("ğŸ‰ í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 70)
    
    # í•µì‹¬ ì„±ê³¼
    print("ğŸ“Š í•µì‹¬ ì„±ê³¼:")
    print(f"   âœ… ë¶„ì„ ê³ ê°: {comprehensive_report['volatility_coefficient_summary']['total_customers']:,}ëª…")
    print(f"   âœ… í‰ê·  ë³€ë™ê³„ìˆ˜: {comprehensive_report['volatility_coefficient_summary']['mean_coefficient']}")
    print(f"   âœ… ëª¨ë¸ ì˜ˆì¸¡ ì •í™•ë„(RÂ²): {model_performance['final_r2']:.3f}")
    print(f"   âœ… ëª¨ë¸ ì˜¤ì°¨(MAE): {model_performance['final_mae']:.4f}")
    
    # ì•ˆì •ì„± ë¶„í¬
    print("\nğŸ” ê³ ê° ì•ˆì •ì„± ë¶„í¬:")
    stability_dist = comprehensive_report['business_stability_distribution']
    total = sum(stability_dist.values())
    for grade, count in stability_dist.items():
        percentage = count / total * 100 if total > 0 else 0
        print(f"   {grade}: {count}ëª… ({percentage:.1f}%)")
    
    # ìœ„í—˜ ë¶„ì„
    risk_info = comprehensive_report['risk_analysis']
    print(f"\nâš ï¸ ìœ„í—˜ ë¶„ì„:")
    print(f"   ê³ ìœ„í—˜ ê³ ê°: {risk_info['high_risk_customers']}ëª… ({risk_info['high_risk_percentage']}%)")
    print(f"   ì£¼ìš” ìœ„í—˜ ìš”ì¸:")
    for factor, count in list(risk_info['top_risk_factors'].items())[:3]:
        print(f"      - {factor}: {count}ê±´")
    
    # ê¸°ìˆ ì  ì„±ê³¼
    print(f"\nğŸ¯ ê¸°ìˆ ì  ì„±ê³¼:")
    print(f"   ìŠ¤íƒœí‚¹ ì•™ìƒë¸” êµ¬ì„±: {len(model_performance['level0_models'])}ê°œ Level-0 ëª¨ë¸")
    print(f"   íŠ¹ì„± ê°œìˆ˜: {model_performance['n_features']}ê°œ")
    print(f"   í›ˆë ¨ ìƒ˜í”Œ: {model_performance['n_samples']}ê°œ")
    
    # ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
    print(f"\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜:")
    for insight in comprehensive_report['business_insights']:
        print(f"   â€¢ {insight}")
    
    # ê¶Œì¥ì‚¬í•­
    print(f"\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
    for recommendation in comprehensive_report['recommendations']:
        print(f"   â€¢ {recommendation}")
    
    print(f"\nğŸ† ê³µëª¨ì „ ì œì¶œ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼: ./analysis_results/ ë””ë ‰í† ë¦¬")
    print(f"   ğŸ“Š í•µì‹¬ ì•Œê³ ë¦¬ì¦˜: ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ê¸°ë°˜ ë³€ë™ê³„ìˆ˜")
    print(f"   ğŸ¯ ì‹¤ë¬´ í™œìš©: ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê³ ê° ë¦¬ìŠ¤í¬ ê´€ë¦¬ ì‹œìŠ¤í…œ")

# ë‹¨ë… ì‹¤í–‰ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± í•¨ìˆ˜
def create_test_environment():
    """í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„± (ì‹¤ì œ ì „ì²˜ë¦¬ íŒŒì¼ì´ ì—†ì„ ë•Œ)"""
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„± ì¤‘...")
    
    import json
    from datetime import datetime, timedelta
    
    # í…ŒìŠ¤íŠ¸ìš© ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('./analysis_results', exist_ok=True)
    
    # 1. 1ë‹¨ê³„ ê²°ê³¼ ìƒì„±
    step1_results = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'analysis_stage': 'step1_preprocessing',
            'total_customers': 50,
            'total_lp_records': 48000
        },
        'customer_summary': {
            'total_customers': 50,
            'contract_types': {'222': 15, '226': 10, '322': 15, '726': 10},
            'usage_types': {'02': 20, '09': 30}
        },
        'lp_data_summary': {
            'total_records': 48000,
            'total_customers': 50,
            'avg_power': 75.5
        }
    }
    
    with open('./analysis_results/analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(step1_results, f, ensure_ascii=False, indent=2, default=str)
    
    # 2. 2ë‹¨ê³„ ê²°ê³¼ ìƒì„± (ì‹¤ì œ 2ë‹¨ê³„ ê²°ê³¼ í˜•íƒœë¡œ)
    step2_results = {
        'temporal_patterns': {
            'peak_hours': [10, 11, 14, 15, 18, 19],  # ì‹¤ì œ ë¶„ì„ëœ ê²ƒì²˜ëŸ¼
            'off_peak_hours': [0, 1, 2, 3, 4, 5, 22, 23],
            'weekend_ratio': 0.72,
            'hourly_patterns': {
                'mean': {str(h): 50 + h*2 for h in range(24)}
            },
            'seasonal_patterns': {
                'ë´„': {'mean': 65.5}, 'ì—¬ë¦„': {'mean': 85.2}, 
                'ê°€ì„': {'mean': 70.1}, 'ê²¨ìš¸': {'mean': 90.3}
            }
        },
        'volatility_analysis': {
            'overall_cv': 0.35,
            'customer_cv_stats': {
                'mean': 0.32,
                'std': 0.15,
                'percentiles': {
                    '10%': 0.15, '25%': 0.22, '50%': 0.31, 
                    '75%': 0.41, '90%': 0.55
                }
            },
            'volatility_distribution': {
                'ë§¤ìš° ì•ˆì • (<0.1)': 3, 'ì•ˆì • (0.1-0.2)': 8, 
                'ë³´í†µ (0.2-0.3)': 15, 'ë†’ìŒ (0.3-0.5)': 18, 
                'ë§¤ìš° ë†’ìŒ (0.5-1.0)': 5, 'ê·¹íˆ ë†’ìŒ (>1.0)': 1
            }
        },
        'anomaly_analysis': {
            'processed_customers': 50,
            'anomaly_customers': {
                'high_night_usage': 3,
                'excessive_zeros': 2,
                'high_volatility': 6,
                'statistical_outliers': 4
            }
        }
    }
    
    with open('./analysis_results/analysis_results2.json', 'w', encoding='utf-8') as f:
        json.dump(step2_results, f, ensure_ascii=False, indent=2, default=str)
    
    # 3. í…ŒìŠ¤íŠ¸ìš© LP ë°ì´í„° ìƒì„±
    print("   ğŸ“Š í…ŒìŠ¤íŠ¸ LP ë°ì´í„° ìƒì„± ì¤‘...")
    
    np.random.seed(42)
    test_data = []
    
    # 50ëª… ê³ ê°, 20ì¼ê°„, 15ë¶„ ê°„ê²©
    for customer in range(1, 51):
        for day in range(20):
            for hour in range(24):
                for minute in [0, 15, 30, 45]:
                    timestamp = datetime(2024, 3, 1) + timedelta(days=day, hours=hour, minutes=minute)
                    
                    # ê³ ê°ë³„ ë‹¤ë¥¸ ë³€ë™ì„± íŒ¨í„´
                    base_power = 50 + customer * 2
                    noise_level = 0.1 + (customer % 5) * 0.1
                    
                    # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ì ìš© (2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ í™œìš©)
                    if hour in [10, 11, 14, 15, 18, 19]:  # 2ë‹¨ê³„ì—ì„œ ë¶„ì„ëœ í”¼í¬ì‹œê°„
                        base_power *= 1.4
                    elif hour in [0, 1, 2, 3, 4, 5, 22, 23]:  # 2ë‹¨ê³„ì—ì„œ ë¶„ì„ëœ ë¹„í”¼í¬ì‹œê°„
                        base_power *= 0.5
                    
                    power = base_power + np.random.normal(0, base_power * noise_level)
                    power = max(0, power)
                    
                    test_data.append({
                        'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': f'TEST_{customer:03d}',
                        'datetime': timestamp,
                        'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥': round(power, 1),
                        'ì§€ìƒë¬´íš¨': round(power * 0.1, 1),
                        'ì§„ìƒë¬´íš¨': round(power * 0.05, 1),
                        'í”¼ìƒì „ë ¥': round(power * 1.1, 1)
                    })
    
    test_df = pd.DataFrame(test_data)
    
    # HDF5ë¡œ ì €ì¥ ì‹œë„
    try:
        test_df.to_hdf('./analysis_results/processed_lp_data.h5', key='df', mode='w')
        print("   âœ… HDF5 í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    except Exception as e:
        # CSVë¡œ ëŒ€ì²´
        test_df.to_csv('./analysis_results/processed_lp_data.csv', index=False)
        print(f"   âœ… CSV í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ (HDF5 ì‹¤íŒ¨: {e})")
    
    print(f"   ğŸ“Š ìƒì„±ëœ ë°ì´í„°: {len(test_df):,}ê±´")
    print(f"   ğŸ‘¥ í…ŒìŠ¤íŠ¸ ê³ ê°: {test_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…")
    print("   ğŸ¯ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!")

if __name__ == "__main__":
    print("ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ë¶„ì„ ì‹œì‘!")
    
    # ì‹¤ì œ ì „ì²˜ë¦¬ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    required_files = [
        './analysis_results/analysis_results.json',
        './analysis_results/analysis_results2.json'
    ]
    
    data_files = [
        './analysis_results/processed_lp_data.h5',
        './analysis_results/processed_lp_data.csv'
    ]
    
    missing_required = [f for f in required_files if not os.path.exists(f)]
    missing_data = not any(os.path.exists(f) for f in data_files)
    
    if missing_required or missing_data:
        print("\nâš ï¸ í•„ìˆ˜ ì „ì²˜ë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("   ëˆ„ë½ëœ íŒŒì¼:")
        for f in missing_required:
            print(f"      - {f}")
        if missing_data:
            print(f"      - LP ë°ì´í„° íŒŒì¼ (HDF5 ë˜ëŠ” CSV)")
        
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ í™˜ê²½ì„ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ", end="")
        user_input = input().strip().lower()
        
        if user_input == 'y':
            create_test_environment()
            print("\nâœ… í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„± ì™„ë£Œ. ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤...\n")
        else:
            print("\nâŒ 1-2ë‹¨ê³„ ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            exit(1)
    
    # ë©”ì¸ ë¶„ì„ ì‹¤í–‰
    results = main()
    
    if results:
        print(f"\nğŸŠ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ë¶„ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"   ğŸ“ ê²°ê³¼ í™•ì¸: ./analysis_results/ ë””ë ‰í† ë¦¬")
        print(f"   ğŸ† ê³µëª¨ì „ ì œì¶œ ì¤€ë¹„ ì™„ë£Œ!")
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨")

# ìŠ¤íƒœí‚¹ ì˜ˆì¸¡ í•¨ìˆ˜ (ì¶”ê°€ ìœ í‹¸ë¦¬í‹°)
def predict_new_customer(analyzer, customer_features):
    """ìƒˆë¡œìš´ ê³ ê°ì˜ ë³€ë™ì„± ì˜ˆì¸¡"""
    if analyzer.level0_models and analyzer.meta_model:
        # Level-0 ì˜ˆì¸¡
        level0_preds = []
        scaled_features = analyzer.scaler.transform([customer_features])
        
        for model in analyzer.level0_models.values():
            pred = model.predict(scaled_features)[0]
            level0_preds.append(pred)
        
        # Level-1 ì˜ˆì¸¡
        meta_features = np.array([level0_preds])
        final_prediction = analyzer.meta_model.predict(meta_features)[0]
        
        return final_prediction
    else:
        raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ìš© í•¨ìˆ˜ (ì¶”ê°€ ìœ í‹¸ë¦¬í‹°)
def monitor_customer_volatility(analyzer, customer_id, new_lp_data):
    """ì‹¤ì‹œê°„ ê³ ê° ë³€ë™ì„± ëª¨ë‹ˆí„°ë§"""
    # ìƒˆë¡œìš´ LP ë°ì´í„°ë¡œ ë³€ë™ê³„ìˆ˜ ê³„ì‚°
    # ... (ì‹¤ì œ êµ¬í˜„ì€ calculate_enhanced_volatility_coefficient ë¡œì§ í™œìš©)
    pass