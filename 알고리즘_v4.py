"""
한국전력공사 전력 사용패턴 변동계수 개발
"""

import pandas as pd
import numpy as np
import json
import os
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_absolute_error, r2_score
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from math import pi
from sklearn.metrics import mean_squared_error
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'

class KEPCOSamplingVolatilityAnalyzer:
    """KEPCO 변동계수 분석기"""
    
    def __init__(self, results_dir='./analysis_results', sampling_config=None):
        self.results_dir = results_dir
        self.scaler = StandardScaler()
        self.robust_scaler = RobustScaler()
        self.level0_models = {}
        self.meta_model = None
        
        # 샘플링 설정
        self.sampling_config = sampling_config or {
            'customer_sample_ratio': 0.3,      # 고객의 30%만 샘플링
            'time_sample_ratio': 0.2,          # 시간 데이터의 20%만 샘플링  
            'min_customers': 20,               # 최소 고객 수
            'min_records_per_customer': 50,    # 고객당 최소 레코드 수
            'stratified_sampling': True        # 계층 샘플링 사용
        }
        
        # 기존 전처리 결과 로딩
        self.step1_results = self._load_step1_results()
        self.step2_results = self._load_step2_results()
        self.lp_data = None
        self.kepco_data = None
        
        print("한국전력공사 변동계수 스태킹 분석기 초기화 (샘플링 최적화)")
        print(f"   샘플링 설정: 고객 {self.sampling_config['customer_sample_ratio']*100:.0f}%, 시간 {self.sampling_config['time_sample_ratio']*100:.0f}%")
        
    def _load_step1_results(self):
        """1단계 전처리 결과 로딩"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   1단계 결과 로딩: {len(results)}개 항목")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   1단계 결과 로딩 실패: {e}")
            return {}
    
    def _load_step2_results(self):
        """2단계 시계열 분석 결과 로딩"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results2.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   2단계 결과 로딩: {len(results)}개 항목")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   2단계 결과 로딩 실패: {e}")
            return {}
    
    def load_data(self):
        """전처리된 데이터 로딩 + 스마트 샘플링"""
        print("\n전처리된 데이터 로딩 및 샘플링 중...")
        
        # 1. LP 데이터 로딩
        hdf5_path = os.path.join(self.results_dir, 'processed_lp_data.h5')
        csv_path = os.path.join(self.results_dir, 'processed_lp_data.csv')
        
        if os.path.exists(hdf5_path):
            try:
                self.lp_data = pd.read_hdf(hdf5_path, key='df')
                loading_method = "HDF5"
            except Exception as e:
                if os.path.exists(csv_path):
                    self.lp_data = pd.read_csv(csv_path)
                    loading_method = "CSV"
                else:
                    print(f"   LP 데이터를 찾을 수 없습니다.")
                    return False
        elif os.path.exists(csv_path):
            self.lp_data = pd.read_csv(csv_path)
            loading_method = "CSV"
        else:
            print(f"   전처리된 LP 데이터가 없습니다.")
            return False
        
        original_size = len(self.lp_data)
        print(f"   원본 데이터: {original_size:,}건 ({loading_method})")
        
        # 2. 컬럼 정리 및 기본 전처리
        self._prepare_columns()
        
        # 3. 스마트 샘플링 적용
        self._apply_smart_sampling()
        
        sampled_size = len(self.lp_data)
        reduction_ratio = (1 - sampled_size/original_size) * 100
        
        print(f"   샘플링 후: {sampled_size:,}건")
        print(f"   데이터 감소: {reduction_ratio:.1f}%")
        print(f"   기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}")
        print(f"   고객수: {self.lp_data['대체고객번호'].nunique()}")
        
        return True
    
    def _prepare_columns(self):
        """컬럼 정리 및 기본 전처리"""
        # datetime 컬럼 처리
        datetime_col = None
        for col in ['datetime', 'LP 수신일자', 'LP수신일자', 'timestamp']:
            if col in self.lp_data.columns:
                datetime_col = col
                break
        
        if datetime_col:
            self.lp_data['datetime'] = pd.to_datetime(self.lp_data[datetime_col], errors='coerce')
            self.lp_data = self.lp_data.dropna(subset=['datetime'])
        else:
            raise ValueError("날짜/시간 컬럼을 찾을 수 없습니다.")
        
        # 전력 컬럼 처리
        power_col = None
        for col in ['순방향 유효전력', '순방향유효전력', 'power', '전력량']:
            if col in self.lp_data.columns:
                if col != '순방향 유효전력':
                    self.lp_data['순방향 유효전력'] = self.lp_data[col]
                power_col = '순방향 유효전력'
                break
        
        if not power_col:
            raise ValueError("순방향 유효전력 컬럼을 찾을 수 없습니다.")
        
        # 데이터 품질 정리
        self.lp_data = self.lp_data.dropna(subset=['순방향 유효전력'])
        self.lp_data.loc[self.lp_data['순방향 유효전력'] < 0, '순방향 유효전력'] = 0
        
        # 극단 이상치 처리 (99.9% 분위수로 캡핑)
        q999 = self.lp_data['순방향 유효전력'].quantile(0.999)
        self.lp_data.loc[self.lp_data['순방향 유효전력'] > q999, '순방향 유효전력'] = q999
    
    def _apply_smart_sampling(self):
        """스마트 샘플링 적용"""
        print("   스마트 샘플링 적용 중...")
        
        # 1. 고객별 데이터 충분성 확인
        customer_counts = self.lp_data['대체고객번호'].value_counts()
        sufficient_customers = customer_counts[
            customer_counts >= self.sampling_config['min_records_per_customer']
        ].index.tolist()
        
        
        # 2. 계층 샘플링 (업종별, 규모별)
        if self.sampling_config['stratified_sampling']:
            sampled_customers = self._stratified_customer_sampling(sufficient_customers)
        else:
            # 단순 랜덤 샘플링
            n_customers = max(
                self.sampling_config['min_customers'],
                int(len(sufficient_customers) * self.sampling_config['customer_sample_ratio'])
            )
            sampled_customers = np.random.choice(
                sufficient_customers, 
                size=min(n_customers, len(sufficient_customers)), 
                replace=False
            ).tolist()
        
        # 3. 고객 필터링
        self.lp_data = self.lp_data[self.lp_data['대체고객번호'].isin(sampled_customers)]
        
        # 4. 시간 샘플링 (각 고객별로)
        if self.sampling_config['time_sample_ratio'] < 1.0:
            sampled_data = []
            
            for customer_id in sampled_customers:
                customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer_id]
                
                # 시간 기반 계층 샘플링 (피크/비피크, 주중/주말 균등하게)
                n_samples = max(
                    self.sampling_config['min_records_per_customer'],
                    int(len(customer_data) * self.sampling_config['time_sample_ratio'])
                )
                
                if len(customer_data) <= n_samples:
                    sampled_data.append(customer_data)
                else:
                    # 시간 균등 샘플링
                    sampled_indices = np.linspace(0, len(customer_data)-1, n_samples, dtype=int)
                    sampled_data.append(customer_data.iloc[sampled_indices])
            
            self.lp_data = pd.concat(sampled_data, ignore_index=True)
            print(f"      시간 샘플링 완료")
    
    def _stratified_customer_sampling(self, customers):
        """계층별 고객 샘플링"""
        # 고객별 평균 전력 사용량으로 계층 구분
        customer_power_avg = self.lp_data.groupby('대체고객번호')['순방향 유효전력'].mean()
        
        # 3개 계층으로 구분 (소형, 중형, 대형)
        q33, q67 = customer_power_avg.quantile([0.33, 0.67])
        
        small_customers = customer_power_avg[customer_power_avg <= q33].index.tolist()
        medium_customers = customer_power_avg[(customer_power_avg > q33) & (customer_power_avg <= q67)].index.tolist()
        large_customers = customer_power_avg[customer_power_avg > q67].index.tolist()
        
        # 각 계층에서 비례적으로 샘플링
        total_target = max(
            self.sampling_config['min_customers'],
            int(len(customers) * self.sampling_config['customer_sample_ratio'])
        )
        
        small_n = min(len(small_customers), max(1, total_target // 3))
        medium_n = min(len(medium_customers), max(1, total_target // 3))
        large_n = min(len(large_customers), max(1, total_target - small_n - medium_n))
        
        sampled = []
        if small_customers:
            sampled.extend(np.random.choice(small_customers, size=small_n, replace=False))
        if medium_customers:
            sampled.extend(np.random.choice(medium_customers, size=medium_n, replace=False))
        if large_customers:
            sampled.extend(np.random.choice(large_customers, size=large_n, replace=False))
            
        return sampled
    
    def calculate_volatility(self):
        """향상된 변동계수 계산 (샘플링 최적화 버전)"""
        print("\n향상된 변동계수 계산 중 (샘플링 최적화)...")
        
        if self.lp_data is None:
            print("   LP 데이터가 로딩되지 않았습니다.")
            return {}
        
        # 2단계 결과에서 시간 패턴 정보 가져오기
        temporal_patterns = self.step2_results.get('temporal_patterns', {})
        peak_hours = temporal_patterns.get('peak_hours', [9, 10, 11, 14, 15, 18, 19])
        off_peak_hours = temporal_patterns.get('off_peak_hours', [0, 1, 2, 3, 4, 5])
        weekend_ratio = temporal_patterns.get('weekend_ratio', 1.0)
        
        
        # 시간 파생 변수 생성
        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour
        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday
        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])
        self.lp_data['month'] = self.lp_data['datetime'].dt.month
        self.lp_data['date'] = self.lp_data['datetime'].dt.date
        
        customers = self.lp_data['대체고객번호'].unique()
        volatility_results = {}
        volatility_components = []
        processed_count = 0
        
        print(f"   분석 대상: {len(customers)}명 (샘플링됨)")
        
        for customer_id in customers:
            try:
                customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer_id].copy()
                
                if len(customer_data) < self.sampling_config['min_records_per_customer']:
                    continue
                
                power_values = customer_data['순방향 유효전력'].values
                
                # 데이터 품질 검증
                if np.std(power_values) == 0 or np.mean(power_values) <= 0:
                    continue
                
                # 변동성 지표 계산 (기존 로직 유지)
                volatility_metrics = self._calculate_volatility_metrics(
                    customer_data, power_values, peak_hours, off_peak_hours, weekend_ratio
                )
                
                if volatility_metrics:
                    volatility_components.append({
                        'customer_id': customer_id,
                        **volatility_metrics
                    })
                    processed_count += 1
                
            except Exception as e:
                print(f"   고객 {customer_id} 계산 실패: {e}")
                continue
        
        print(f"   {processed_count}명 변동성 지표 계산 완료")
        
        
        if len(volatility_components) < 10:
            raise ValueError(f"가중치 최적화를 위해서는 최소 10개의 고객 데이터가 필요합니다. (현재: {len(volatility_components)}개)")
        
        optimal_weights = self.optimize_volatility_weights(volatility_components)
        
        print(f"   최종 가중치: {[round(w, 3) for w in optimal_weights]}")
        
        # 최종 변동계수 계산
        for component in volatility_components:
            customer_id = component['customer_id']
            
            enhanced_volatility_coefficient = (
                optimal_weights[0] * component['basic_cv'] +
                optimal_weights[1] * component['hourly_cv'] +
                optimal_weights[2] * component['peak_cv'] +
                optimal_weights[3] * component['weekend_diff'] +
                optimal_weights[4] * component['seasonal_cv']
            )
            
            volatility_results[customer_id] = {
                'enhanced_volatility_coefficient': round(enhanced_volatility_coefficient, 4),
                'basic_cv': round(component['basic_cv'], 4),
                'hourly_cv': round(component['hourly_cv'], 4),
                'peak_cv': round(component['peak_cv'], 4),
                'off_peak_cv': round(component['off_peak_cv'], 4),
                'weekday_cv': round(component['weekday_cv'], 4),
                'weekend_cv': round(component['weekend_cv'], 4),
                'weekend_diff': round(component['weekend_diff'], 4),
                'seasonal_cv': round(component['seasonal_cv'], 4),
                'load_factor': round(component['load_factor'], 4),
                'peak_load_ratio': round(component['peak_load_ratio'], 4),
                'mean_power': round(component['mean_power'], 4),
                'zero_ratio': round(component['zero_ratio'], 4),
                'extreme_changes': int(component['extreme_changes']),
                'data_points': component['data_points'],
                'optimized_weights': [round(w, 3) for w in optimal_weights]
            }
        
        if len(volatility_results) > 0:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   평균 변동계수: {np.mean(cv_values):.4f}")
            print(f"   변동계수 범위: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        return volatility_results
    
    def _calculate_volatility_metrics(self, customer_data, power_values, peak_hours, off_peak_hours, weekend_ratio):
        """개별 고객의 변동성 지표 계산"""
        try:
            mean_power = np.mean(power_values)
            
            # 1. 기본 변동계수
            basic_cv = np.std(power_values) / mean_power
            
            # 2. 시간대별 변동계수
            hourly_avg = customer_data.groupby('hour')['순방향 유효전력'].mean()
            hourly_cv = (np.std(hourly_avg) / np.mean(hourly_avg)) if len(hourly_avg) > 1 and np.mean(hourly_avg) > 0 else basic_cv
            
            # 3. 피크/비피크 변동성
            peak_data = customer_data[customer_data['hour'].isin(peak_hours)]['순방향 유효전력']
            off_peak_data = customer_data[customer_data['hour'].isin(off_peak_hours)]['순방향 유효전력']
            
            peak_cv = (np.std(peak_data) / np.mean(peak_data)) if len(peak_data) > 0 and np.mean(peak_data) > 0 else basic_cv
            off_peak_cv = (np.std(off_peak_data) / np.mean(off_peak_data)) if len(off_peak_data) > 0 and np.mean(off_peak_data) > 0 else basic_cv
            
            # 4. 주말/평일 변동성
            weekday_data = customer_data[~customer_data['is_weekend']]['순방향 유효전력']
            weekend_data = customer_data[customer_data['is_weekend']]['순방향 유효전력']
            
            weekday_cv = (np.std(weekday_data) / np.mean(weekday_data)) if len(weekday_data) > 0 and np.mean(weekday_data) > 0 else basic_cv
            weekend_cv = (np.std(weekend_data) / np.mean(weekend_data)) if len(weekend_data) > 0 and np.mean(weekend_data) > 0 else basic_cv
            weekend_diff = abs(weekday_cv - weekend_cv) * weekend_ratio
            
            # 5. 계절별 변동성 (일별 집계)
            daily_avg = customer_data.groupby('date')['순방향 유효전력'].mean()
            seasonal_cv = (np.std(daily_avg) / np.mean(daily_avg)) if len(daily_avg) > 3 and np.mean(daily_avg) > 0 else basic_cv
            
            # 6. 추가 지표들
            max_power = np.max(power_values)
            load_factor = mean_power / max_power if max_power > 0 else 0
            zero_ratio = (power_values == 0).sum() / len(power_values)
            
            # 급격한 변화 감지
            power_series = pd.Series(power_values)
            pct_changes = power_series.pct_change().dropna()
            extreme_changes = (np.abs(pct_changes) > 1.5).sum()
            
            # 피크/비피크 부하 비율
            peak_avg = np.mean(peak_data) if len(peak_data) > 0 else mean_power
            off_peak_avg = np.mean(off_peak_data) if len(off_peak_data) > 0 else mean_power
            peak_load_ratio = peak_avg / off_peak_avg if off_peak_avg > 0 else 1.0
            
            return {
                'basic_cv': basic_cv,
                'hourly_cv': hourly_cv,
                'peak_cv': peak_cv,
                'off_peak_cv': off_peak_cv,
                'weekday_cv': weekday_cv,
                'weekend_cv': weekend_cv,
                'weekend_diff': weekend_diff,
                'seasonal_cv': seasonal_cv,
                'load_factor': load_factor,
                'zero_ratio': zero_ratio,
                'extreme_changes': extreme_changes,
                'peak_load_ratio': peak_load_ratio,
                'mean_power': mean_power,
                'data_points': len(power_values)
            }
            
        except Exception as e:
            return None
    
    def optimize_volatility_weights(self, volatility_components):
        """가중치 최적화 (필수)"""
        print("\n가중치 최적화 중...")
        
        try:
            from scipy.optimize import minimize
        except ImportError:
            raise ImportError("scipy가 설치되지 않았습니다. 'pip install scipy'로 설치해주세요.")
        
        # 목표 함수 정의
        components_df = pd.DataFrame(volatility_components)
        
        # 목표 변수: 영업활동 불안정성 지표
        target_instability = (
            components_df['basic_cv'] * 2.0 +
            components_df['zero_ratio'] * 1.0 +
            (1 - components_df['load_factor']) * 0.5
        ).values
        
        X = components_df[['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']].values
        y = target_instability
        
        # 최적화 목표 함수
        def objective(weights):
            predicted = X @ weights
            return np.mean((predicted - y) ** 2)
        
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}]
        bounds = [(0, 1) for _ in range(5)]
        initial_weights = [0.35, 0.25, 0.20, 0.10, 0.10]
        
        result = minimize(objective, initial_weights, method='SLSQP', 
                        bounds=bounds, constraints=constraints)
        
        if not result.success:
            raise RuntimeError(f"가중치 최적화에 실패했습니다: {result.message}")
        
        print(f"   가중치 최적화 완료")
        return result.x.tolist()
    
    def train_stacking_ensemble_model(self, volatility_results):
        """스태킹 앙상블 모델 훈련"""
        print("\n스태킹 앙상블 모델 훈련 중...")
        
        if len(volatility_results) < 5:
            print("   훈련 데이터가 부족합니다 (최소 5개 필요)")
            return None
        
        # 특성 추출
        features = []
        targets = []
        
        for customer_id, data in volatility_results.items():
            try:
                feature_vector = [
                    data['basic_cv'], data['hourly_cv'], data['peak_cv'],
                    data['off_peak_cv'], data['weekday_cv'], data['weekend_cv'],
                    data['seasonal_cv'], data['load_factor'], data['peak_load_ratio'],
                    data['mean_power'], data['zero_ratio'],
                    data['extreme_changes'] / data['data_points']
                ]
                
                if any(np.isnan(x) or np.isinf(x) for x in feature_vector):
                    continue
                    
                features.append(feature_vector)
                targets.append(data['enhanced_volatility_coefficient'])
                
            except KeyError:
                continue
        
        X = np.array(features)
        y = np.array(targets)
        
        print(f"   훈련 데이터: {len(X)}개 샘플, {X.shape[1]}개 특성")
        
        # 데이터 분할
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # 정규화
        X_train_scaled = self.robust_scaler.fit_transform(X_train)
        X_test_scaled = self.robust_scaler.transform(X_test)
        
        # Level-0 모델들
        self.level0_models = {
            'rf': RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42),
            'gbm': GradientBoostingRegressor(n_estimators=50, max_depth=6, random_state=42),
            'ridge': Ridge(alpha=1.0),
            'linear': LinearRegression()
        }
        
        # 교차검증으로 메타 특성 생성
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        
        meta_features_train = np.zeros((len(X_train_scaled), len(self.level0_models)))
        meta_features_test = np.zeros((len(X_test_scaled), len(self.level0_models)))
        
        print(f"   Level-0 모델 훈련 (5-Fold CV):")
        for i, (name, model) in enumerate(self.level0_models.items()):
            fold_predictions = np.zeros(len(X_train_scaled))
            
            for train_idx, val_idx in kf.split(X_train_scaled):
                try:
                    fold_model = type(model)(**model.get_params())
                    fold_model.fit(X_train_scaled[train_idx], y_train[train_idx])
                    fold_predictions[val_idx] = fold_model.predict(X_train_scaled[val_idx])
                except Exception as e:
                    fold_predictions[val_idx] = np.mean(y_train[train_idx])
            
            meta_features_train[:, i] = fold_predictions
            
            # 전체 훈련 세트로 재훈련
            try:
                model.fit(X_train_scaled, y_train)
                meta_features_test[:, i] = model.predict(X_test_scaled)
                
                test_pred = model.predict(X_test_scaled)
                test_mae = mean_absolute_error(y_test, test_pred)
                test_r2 = r2_score(y_test, test_pred) if len(set(y_test)) > 1 else 0.0
                print(f"      {name}: MAE={test_mae:.4f}, R²={test_r2:.4f}")
                
            except Exception as e:
                meta_features_test[:, i] = np.mean(y_train)
        
        # Level-1 메타 모델 (선형 회귀)
        self.meta_model = LinearRegression()
        try:
            self.meta_model.fit(meta_features_train, y_train)
            final_pred = self.meta_model.predict(meta_features_test)
            final_mae = mean_absolute_error(y_test, final_pred)
            final_r2 = r2_score(y_test, final_pred) if len(set(y_test)) > 1 else 0.0
        except:
            final_pred = np.mean(meta_features_test, axis=1)
            final_mae = mean_absolute_error(y_test, final_pred)
            final_r2 = r2_score(y_test, final_pred) if len(set(y_test)) > 1 else 0.0
        
        print(f"      스태킹 앙상블 훈련 완료")
        print(f"      최종 MAE: {final_mae:.4f}")
        print(f"      최종 R²: {final_r2:.4f}")
        
        return {
            'final_mae': final_mae,
            'final_r2': final_r2,
            'level0_models': list(self.level0_models.keys()),
            'meta_model': 'LinearRegression',
            'n_samples': len(X),
            'n_features': X.shape[1],
            'sampling_optimized': True
        }

    def analyze_business_stability(self, volatility_results):
        """영업활동 안정성 분석"""
        print("\n  영업활동 안정성 분석 중...")
        
        if not volatility_results:
            return {}
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
        
        # 분위수 기반 등급 분류 (원래대로 3단계)
        p25, p75 = np.percentile(coefficients, [25, 75])
        
        stability_analysis = {}
        grade_counts = {'안정': 0, '보통': 0, '주의': 0}
        
        for customer_id, data in volatility_results.items():
            coeff = data['enhanced_volatility_coefficient']
            
            # 3단계 등급 분류 (원래대로)
            if coeff <= p25:
                grade = '안정'
                risk_level = 'low'
            elif coeff <= p75:
                grade = '보통'
                risk_level = 'medium'
            else:
                grade = '주의'
                risk_level = 'high'
            
            grade_counts[grade] += 1
            
            # 위험 요인 분석
            risk_factors = []
            if data.get('zero_ratio', 0) > 0.1:
                risk_factors.append('빈번한_사용중단')
            if data.get('load_factor', 1) < 0.3:
                risk_factors.append('낮은_부하율')
            if data.get('peak_cv', 0) > data.get('basic_cv', 0) * 2:
                risk_factors.append('피크시간_불안정')
            if data.get('weekend_diff', 0) > 0.3:
                risk_factors.append('주말_패턴_급변')
            if data.get('extreme_changes', 0) > data.get('data_points', 1) * 0.05:
                risk_factors.append('급격한_변화_빈발')
            
            # 안정성 점수 계산 (0-100점)
            stability_score = max(0, 100 - (coeff * 400))
            
            stability_analysis[customer_id] = {
                'enhanced_volatility_coefficient': round(coeff, 4),
                'stability_grade': grade,
                'risk_level': risk_level,
                'stability_score': round(stability_score, 1),
                'risk_factors': risk_factors,
                'load_factor': data.get('load_factor', 0.0),
                'peak_load_ratio': data.get('peak_load_ratio', 1.0),
                'zero_ratio': data.get('zero_ratio', 0.0),
                'extreme_changes': data.get('extreme_changes', 0)
            }
        
        print(f"   안정성 등급 분포:")
        total = len(stability_analysis)
        for grade, count in grade_counts.items():
            percentage = count / total * 100 if total > 0 else 0
            print(f"      {grade}: {count}명 ({percentage:.1f}%)")
        
        return stability_analysis

    def generate_report(self, volatility_results, model_performance, stability_analysis):
        """샘플링 최적화 리포트 생성"""
        print("\n  샘플링 최적화 리포트 생성 중...")
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()] if volatility_results else []
        
        # 위험 고객 식별
        high_risk_customers = [
            customer_id for customer_id, analysis in stability_analysis.items()
            if analysis['risk_level'] == 'high'
        ] if stability_analysis else []
        
        # 주요 위험 요인 집계
        all_risk_factors = []
        for analysis in stability_analysis.values():
            all_risk_factors.extend(analysis['risk_factors'])
        
        from collections import Counter
        risk_factor_counts = Counter(all_risk_factors)
        
        report = {
            'analysis_metadata': {
                'timestamp': pd.Timestamp.now().isoformat(),
                'algorithm_version': 'sampling_optimized_v2',
                'sampling_config': self.sampling_config,
                'total_customers_analyzed': len(volatility_results),
                'execution_mode': 'sampling_optimized'
            },
            
            'sampling_summary': {
                'customer_sample_ratio': self.sampling_config['customer_sample_ratio'],
                'time_sample_ratio': self.sampling_config['time_sample_ratio'],
                'stratified_sampling_used': self.sampling_config['stratified_sampling']
            },
            
            'volatility_coefficient_summary': {
                'total_customers': len(volatility_results),
                'mean_coefficient': round(np.mean(coefficients), 4) if coefficients else 0,
                'std_coefficient': round(np.std(coefficients), 4) if coefficients else 0,
                'percentiles': {
                    '25%': round(np.percentile(coefficients, 25), 4) if coefficients else 0,
                    '50%': round(np.percentile(coefficients, 50), 4) if coefficients else 0,
                    '75%': round(np.percentile(coefficients, 75), 4) if coefficients else 0
                }
            },
            
            'model_performance': model_performance or {},
            
            'business_stability_distribution': {
                grade: sum(1 for a in stability_analysis.values() if a['stability_grade'] == grade)
                for grade in ['안정', '보통', '주의']
            } if stability_analysis else {},
            
            'risk_analysis': {
                'high_risk_customers': len(high_risk_customers),
                'high_risk_percentage': round(len(high_risk_customers) / len(stability_analysis) * 100, 1) if stability_analysis else 0,
                'top_risk_factors': dict(risk_factor_counts.most_common(5))
            },
            
            'performance_optimization': {
                'data_reduction_achieved': True,
                'accuracy_maintained': model_performance['final_r2'] >= 0.3 if model_performance else False,
                'sampling_effective': True
            },
            
            'business_insights': [
                f"샘플링을 통해 {len(volatility_results)}명 고객 분석 완료",
                f"데이터 크기 {(1-self.sampling_config['customer_sample_ratio'])*100:.0f}% 감소로 속도 향상",
                f"모델 예측 정확도(R²): {model_performance['final_r2']:.3f}" if model_performance else "모델 성능 측정 불가",
                f"고위험 고객 {len(high_risk_customers)}명 식별",
                "실무 적용 가능한 효율적 분석 시스템 구축"
            ],
            
            'recommendations': [
                "샘플링 비율 조정을 통한 속도-정확도 균형 최적화",
                "계층별 샘플링으로 대표성 확보",
                "실시간 모니터링을 위한 효율적 분석 체계",
                "주기적 전체 데이터 검증으로 샘플링 편향 확인"
            ]
        }
        
        return report

    def create_volatility_components_radar_chart(self, volatility_results, save_path='./analysis_results'):
        """변동계수 구성요소 레이더 차트 생성"""
        import matplotlib.pyplot as plt
        import numpy as np
        from math import pi
        import os
        
        print("\n  변동계수 구성요소 레이더 차트 생성 중...")
        
        if not volatility_results:
            print("    변동계수 결과가 없습니다.")
            return None
        
        # 구성요소 이름 및 순서 정의
        components = ['기본 CV', '시간대별 CV', '피크 CV', '주말 차이', '계절별 CV']
        component_keys = ['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']
        
        # 데이터 추출 및 정규화
        customers_data = {}
        all_values = {key: [] for key in component_keys}
        
        # 모든 고객의 데이터 수집
        for customer_id, data in volatility_results.items():
            customer_values = []
            for key in component_keys:
                value = data.get(key, 0)
                # 이상값 처리
                if np.isnan(value) or np.isinf(value):
                    value = 0
                customer_values.append(value)
                all_values[key].append(value)
            customers_data[customer_id] = customer_values
        
        # 정규화를 위한 최대값 계산 (각 구성요소별)
        max_values = []
        for key in component_keys:
            values = all_values[key]
            if values:
                max_val = max(values) if max(values) > 0 else 1
                max_values.append(max_val)
            else:
                max_values.append(1)
        
        # 상위 5명의 고객 선택 (변동계수가 높은 순)
        top_customers = sorted(
            volatility_results.items(),
            key=lambda x: x[1].get('enhanced_volatility_coefficient', 0),
            reverse=True
        )[:5]
        
        # 레이더 차트 설정
        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))
        
        # 각도 계산 (5개 항목)
        angles = [n / float(len(components)) * 2 * pi for n in range(len(components))]
        angles += angles[:1]  # 원을 닫기 위해
        
        # 색상 팔레트
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
        
        # 각 고객별 레이더 차트 그리기
        for i, (customer_id, data) in enumerate(top_customers):
            if i >= 5:  # 최대 5명만
                break
                
            # 데이터 정규화 (0-1 범위)
            values = []
            for j, key in enumerate(component_keys):
                raw_value = data.get(key, 0)
                if np.isnan(raw_value) or np.isinf(raw_value):
                    raw_value = 0
                normalized_value = raw_value / max_values[j] if max_values[j] > 0 else 0
                values.append(min(normalized_value, 1.0))  # 1.0으로 클리핑
            
            values += values[:1]  # 원을 닫기 위해
            
            # 선 그리기
            ax.plot(angles, values, 'o-', linewidth=2, label=f'{customer_id}', color=colors[i], markersize=6)
            # 영역 채우기 (투명도 적용)
            ax.fill(angles, values, alpha=0.15, color=colors[i])
        
        # 라벨 설정
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(components, fontsize=11, fontweight='bold')
        
        # Y축 설정 (0-1 범위)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)
        ax.grid(True, alpha=0.3)
        
        # 제목 및 범례
        plt.title('변동계수 구성요소 분석 (상위 5개 고객)', 
                  fontsize=16, fontweight='bold', pad=30)
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=10)
        
        # 서브 제목 (정규화 설명)
        fig.text(0.5, 0.02, '※ 각 구성요소는 최대값으로 정규화됨 (0-1 범위)', 
                 ha='center', fontsize=9, style='italic')
        
        # 통계 정보 추가
        stats_text = f"분석 고객 수: {len(volatility_results)}명\n"
        stats_text += f"평균 변동계수: {np.mean([v.get('enhanced_volatility_coefficient', 0) for v in volatility_results.values()]):.4f}"
        fig.text(0.02, 0.95, stats_text, fontsize=9, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # 저장
        os.makedirs(save_path, exist_ok=True)
        chart_path = os.path.join(save_path, 'volatility_components_radar.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"    레이더 차트 저장: {chart_path}")
        
        return {
            'chart_path': chart_path,
            'top_customers': [customer_id for customer_id, _ in top_customers]
        }

def create_sampling_test_environment():
    """샘플링 테스트 환경 생성"""
    print(" 샘플링 테스트 환경 생성 중...")
    
    import json
    os.makedirs('./analysis_results', exist_ok=True)
    
    # 1단계, 2단계 결과 생성
    step1_results = {
        'metadata': {'timestamp': datetime.now().isoformat(), 'total_customers': 200}
    }
    with open('./analysis_results/analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(step1_results, f, ensure_ascii=False, indent=2, default=str)
    
    step2_results = {
        'temporal_patterns': {
            'peak_hours': [9, 10, 11, 14, 15, 18, 19],
            'off_peak_hours': [0, 1, 2, 3, 4, 5, 22, 23],
            'weekend_ratio': 0.75
        }
    }
    with open('./analysis_results/analysis_results2.json', 'w', encoding='utf-8') as f:
        json.dump(step2_results, f, ensure_ascii=False, indent=2, default=str)
    
    # 더 큰 LP 데이터 생성 (200명, 14일)
    print("    대용량 테스트 LP 데이터 생성 중...")
    
    np.random.seed(42)
    data = []
    
    for customer in range(1, 201):  # 200명
        base_power = 30 + customer * 0.8
        cv = 0.15 + (customer % 8) * 0.12  # 다양한 변동성
        
        for day in range(14):  # 14일
            for hour in range(24):
                for minute in [0, 15, 30, 45]:  # 15분 간격
                    timestamp = datetime(2024, 3, 1) + timedelta(days=day, hours=hour, minutes=minute)
                    
                    # 복잡한 패턴 생성
                    hour_factor = 1.0
                    if hour in [9, 10, 11, 14, 15, 18, 19]:
                        hour_factor = 1.3 + np.random.normal(0, 0.15)
                    elif hour in [0, 1, 2, 3, 4, 5, 22, 23]:
                        hour_factor = 0.6 + np.random.normal(0, 0.1)
                    
                    # 요일 효과
                    weekday = timestamp.weekday()
                    if weekday >= 5:  # 주말
                        hour_factor *= 0.75
                    
                    # 고객별 특성 반영
                    if customer % 3 == 0:  # 야간 운영 고객
                        if hour in [22, 23, 0, 1, 2]:
                            hour_factor *= 1.8
                    
                    power = base_power * hour_factor + np.random.normal(0, base_power * cv)
                    
                    # 간헐적 특수 패턴
                    if np.random.random() < 0.03:  # 3% 확률로 특수 상황
                        power = 0  # 정전 또는 휴업
                    else:
                        power = max(2, power)
                    
                    data.append({
                        '대체고객번호': f'SAMP_{customer:03d}',
                        'datetime': timestamp,
                        '순방향 유효전력': round(power, 1)
                    })
    
    df = pd.DataFrame(data)
    
    # CSV로 저장
    df.to_csv('./analysis_results/processed_lp_data.csv', index=False)
    print(f"    대용량 테스트 데이터 생성: {len(df):,}건, {df['대체고객번호'].nunique()}명")

def main_sampling():
    """메인 실행 함수 (샘플링 최적화 버전)"""
    print("KEPCO 변동계수 분석")
    
    start_time = datetime.now()
    
    # 샘플링 설정 (사용자 조정 가능)
    sampling_config = {
        'customer_sample_ratio': 0.25,    # 25% 고객만 샘플링
        'time_sample_ratio': 0.15,        # 15% 시간 데이터만 샘플링
        'min_customers': 30,              # 최소 30명
        'min_records_per_customer': 100,   # 고객당 최소 100개 레코드
        'stratified_sampling': True       # 계층 샘플링 사용
    }
    
    print(f"샘플링: 고객 {sampling_config['customer_sample_ratio']*100:.0f}%")
    print()
    
    try:
        # 1. 분석기 초기화
        analyzer = KEPCOSamplingVolatilityAnalyzer('./analysis_results', sampling_config)
        
        # 2. 데이터 로딩 + 샘플링
        if not analyzer.load_data():
            print(" 데이터 로딩 실패")
            return None
        
        # 3. 변동계수 계산
        volatility_results = analyzer.calculate_volatility()
        if not volatility_results:
            print(" 변동계수 계산 실패")
            return None
        
        # 4. 모델 훈련
        model_performance = analyzer.train_stacking_ensemble_model(volatility_results)
        
        # 5. 안정성 분석
        stability_analysis = analyzer.analyze_business_stability(volatility_results)
        
        # 6. 샘플링 리포트 생성
        report = analyzer.generate_report(volatility_results, model_performance, stability_analysis)
        
        # 7. 시각화 생성
        radar_result = analyzer.create_volatility_components_radar_chart(volatility_results)
        if radar_result:
            print(f"    레이더 차트 생성 완료: {radar_result['chart_path']}")
        else:
            print("차트 생성 실패")
        
        # 8. 결과 저장
        save_sampling_results(volatility_results, stability_analysis, report)
        
        # 실행 시간 계산
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"분석 완료 ({execution_time:.1f}초)")
        print(f"고객 {len(volatility_results)}명 분석")
        
        if volatility_results:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"    평균 변동계수: {np.mean(cv_values):.4f}")
        
        if model_performance:
            print(f"    모델 성능: R²={model_performance['final_r2']:.3f}")
        
        data_reduction = (1 - sampling_config['customer_sample_ratio'] * sampling_config['time_sample_ratio']) * 100
        print(f"    데이터 감소: 약 {data_reduction:.0f}%")
        
        return {
            'volatility_results': volatility_results,
            'model_performance': model_performance,
            'stability_analysis': stability_analysis,
            'report': report,
            'execution_time': execution_time,
            'sampling_config': sampling_config
        }
        
    except Exception as e:
        print(f" 시스템 실행 중 오류: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_sampling_results(volatility_results, stability_analysis, report):
    """샘플링 최적화 결과 저장"""
    try:
        os.makedirs('./analysis_results', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 변동계수 결과
        if volatility_results:
            df = pd.DataFrame.from_dict(volatility_results, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': '대체고객번호'}, inplace=True)
            csv_path = f'./analysis_results/volatility_sampling_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"    변동계수 (샘플링): {csv_path}")
        
        # 안정성 분석
        if stability_analysis:
            df = pd.DataFrame.from_dict(stability_analysis, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': '대체고객번호'}, inplace=True)
            if 'risk_factors' in df.columns:
                df['risk_factors_str'] = df['risk_factors'].apply(
                    lambda x: ', '.join(x) if isinstance(x, list) else ''
                )
            csv_path = f'./analysis_results/stability_sampling_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"    안정성 (샘플링): {csv_path}")
        
        # 샘플링 리포트
        if report:
            json_path = f'./analysis_results/sampling_report_{timestamp}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"    샘플링 리포트: {json_path}")
        
        return True
        
    except Exception as e:
        print(f"    결과 저장 실패: {e}")
        return False

if __name__ == "__main__":
    print(" 한국전력공사 변동계수 분석 시작!")
    print("=" * 80)
    print()
    
    # 데이터 파일 확인
    required_files = ['./analysis_results/processed_lp_data.csv']
    missing_files = [f for f in required_files if not os.path.exists(f)]
    
    if missing_files:
        print(" 데이터 파일이 없습니다. 대용량 테스트 데이터를 생성합니다.")
        create_sampling_test_environment()
        print()
    
    # 메인 실행
    results = main_sampling()
    
    if results:
        print(f"\n분석 성공")

    else:
        print(f"\n분석 실패")
