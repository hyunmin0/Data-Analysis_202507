{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ - ì™„ì „í•œ ë²„ì „\n",
    "# ëˆ„ë½ëœ ë©”ì„œë“œë“¤ì„ ëª¨ë‘ í¬í•¨í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityCoefficient:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜\n",
    "    ì™„ì „í•œ êµ¬í˜„ ë²„ì „\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, results_path='./analysis_results'):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        Args:\n",
    "            results_path: 1-2ë‹¨ê³„ ê²°ê³¼ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        self.results_path = results_path\n",
    "        \n",
    "        # ê¸°ë³¸ ì„¤ì •\n",
    "        self.config = {\n",
    "            'temporal_weights': {\n",
    "                'peak_hours': [9, 10, 11, 14, 15, 18, 19, 20],  \n",
    "                'peak_weight': 1.5,                               \n",
    "                'off_peak_weight': 0.8                            \n",
    "            },\n",
    "            'seasonal_adjustment': {\n",
    "                'summer_months': [6, 7, 8],      \n",
    "                'winter_months': [12, 1, 2],     \n",
    "                'summer_factor': 1.2,            \n",
    "                'winter_factor': 1.1             \n",
    "            },\n",
    "            'industry_baselines': {\n",
    "                '222': 0.25,  # ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A\n",
    "                '226': 0.30,  # ì¼ë°˜ìš©(ì„) ê³ ì••A  \n",
    "                '311': 0.35,  # ì‚°ì—…ìš©(ê°‘) ì €ì••\n",
    "                '322': 0.20,  # ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A\n",
    "                '726': 0.28   # ì‚°ì—…ìš©(ì„) ê³ ì••A\n",
    "            },\n",
    "            'usage_type_factors': {\n",
    "                '02': 1.1,    # ìƒì—…ìš©\n",
    "                '09': 0.9     # ê´‘ê³µì—…ìš©\n",
    "            },\n",
    "            'anomaly_thresholds': {\n",
    "                'cv_extreme': 1.0,           \n",
    "                'zero_ratio_max': 0.1,       \n",
    "                'night_day_ratio_max': 0.8   \n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ëª¨ë¸ êµ¬ì„±ìš”ì†Œ\n",
    "        self.level0_models = {}\n",
    "        self.level1_model = None\n",
    "        self.scalers = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # ê³¼ì í•© ë°©ì§€ ì„¤ì •\n",
    "        self.cv_folds = 5\n",
    "        self.random_state = 42\n",
    "        \n",
    "        print(\"ğŸ”§ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì´ˆê¸°í™”\")\n",
    "        print(f\"ê²°ê³¼ í´ë”: {self.results_path}\")\n",
    "        \n",
    "        # ìŠ¤íƒœí‚¹ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        self._initialize_stacking_models()\n",
    "        \n",
    "        # 1-2ë‹¨ê³„ ê²°ê³¼ ìë™ ë¡œë“œ\n",
    "        self._load_previous_results()\n",
    "    \n",
    "    def _initialize_stacking_models(self):\n",
    "        \"\"\"ìŠ¤íƒœí‚¹ ëª¨ë¸ ì´ˆê¸°í™”\"\"\"\n",
    "        # Level-0 ëª¨ë¸ë“¤ (ë‹¤ì–‘í•œ ê¸°ë²•ìœ¼ë¡œ ë³€ë™ì„± ì¸¡ì •)\n",
    "        self.level0_models = {\n",
    "            'rf_temporal': RandomForestRegressor(\n",
    "                n_estimators=100, \n",
    "                max_depth=10, \n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'gbm_seasonal': GradientBoostingRegressor(\n",
    "                n_estimators=100, \n",
    "                max_depth=6, \n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'ridge_basic': Ridge(alpha=1.0),\n",
    "            'elastic_pattern': ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        # Level-1 ë©”íƒ€ëª¨ë¸\n",
    "        self.level1_model = Ridge(alpha=0.1)\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¼ëŸ¬\n",
    "        self.scalers = {\n",
    "            'basic': StandardScaler(),\n",
    "            'temporal': RobustScaler(),\n",
    "            'seasonal': StandardScaler(),\n",
    "            'pattern': RobustScaler(),\n",
    "            'anomaly': StandardScaler()\n",
    "        }\n",
    "    \n",
    "    def _load_previous_results(self):\n",
    "        \"\"\"1-2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•´ì„œ ì„¤ì • ì—…ë°ì´íŠ¸\"\"\"\n",
    "        print(\"ğŸ“‚ 1-2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘...\")\n",
    "        \n",
    "        try:\n",
    "            # 1ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ\n",
    "            step1_file = os.path.join(self.results_path, 'analysis_results.json')\n",
    "            if os.path.exists(step1_file):\n",
    "                with open(step1_file, 'r', encoding='utf-8') as f:\n",
    "                    step1_results = json.load(f)\n",
    "                print(\"âœ… 1ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ\")\n",
    "                self._update_config_from_step1(step1_results)\n",
    "            else:\n",
    "                print(f\"âš ï¸ 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {step1_file}\")\n",
    "            \n",
    "            # 2ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ\n",
    "            step2_file = os.path.join(self.results_path, 'volatility_summary.csv')\n",
    "            if os.path.exists(step2_file):\n",
    "                step2_results = pd.read_csv(step2_file)\n",
    "                print(\"âœ… 2ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ\")\n",
    "                self._update_config_from_step2(step2_results)\n",
    "            else:\n",
    "                print(f\"âš ï¸ 2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {step2_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            print(\"ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    def _update_config_from_step1(self, step1_results):\n",
    "        \"\"\"1ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸\"\"\"\n",
    "        print(\"ğŸ”„ 1ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸...\")\n",
    "        \n",
    "        customer_summary = step1_results.get('customer_summary', {})\n",
    "        if customer_summary:\n",
    "            total_customers = customer_summary.get('total_customers', 3000)\n",
    "            print(f\"   ê³ ê° ìˆ˜: {total_customers:,}ëª…\")\n",
    "            \n",
    "            contract_types = customer_summary.get('contract_types', {})\n",
    "            if contract_types:\n",
    "                print(f\"   ê³„ì•½ì¢…ë³„: {len(contract_types)}ê°œ ìœ í˜•\")\n",
    "                \n",
    "                total_contracts = sum(contract_types.values())\n",
    "                for contract, count in contract_types.items():\n",
    "                    if str(contract) in self.config['industry_baselines']:\n",
    "                        ratio = count / total_contracts\n",
    "                        if ratio > 0.3:  \n",
    "                            self.config['industry_baselines'][str(contract)] *= 0.95\n",
    "    \n",
    "    def _update_config_from_step2(self, step2_results):\n",
    "        \"\"\"2ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸\"\"\"\n",
    "        print(\"ğŸ”„ 2ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸...\")\n",
    "        \n",
    "        for _, row in step2_results.iterrows():\n",
    "            metric = row['metric']\n",
    "            value = row['value']\n",
    "            \n",
    "            if metric == 'overall_cv':\n",
    "                baseline_adjustment = value / 0.25  \n",
    "                for contract_type in self.config['industry_baselines']:\n",
    "                    self.config['industry_baselines'][contract_type] *= baseline_adjustment\n",
    "                print(f\"   ì „ì²´ CV ê¸°ì¤€ ì¡°ì •: {baseline_adjustment:.3f}ë°°\")\n",
    "                \n",
    "            elif metric == 'weekday_cv' and 'weekend_cv' in step2_results['metric'].values:\n",
    "                weekend_row = step2_results[step2_results['metric'] == 'weekend_cv']\n",
    "                if not weekend_row.empty:\n",
    "                    weekend_cv = weekend_row['value'].iloc[0]\n",
    "                    weekend_factor = weekend_cv / value if value > 0 else 1.0\n",
    "                    self.config['weekend_factor'] = weekend_factor\n",
    "                    print(f\"   ì£¼ë§ íŒ©í„°: {weekend_factor:.3f}\")\n",
    "        \n",
    "        print(\"âœ… ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
    "    \n",
    "    def load_actual_data_for_features(self):\n",
    "        \"\"\"ì‹¤ì œ LP ë°ì´í„°ë¥¼ ë¡œë“œí•´ì„œ íŠ¹ì„± ìƒì„±\"\"\"\n",
    "        print(\"ğŸ“Š ì‹¤ì œ ë°ì´í„°ë¡œ íŠ¹ì„± ìƒì„±...\")\n",
    "        \n",
    "        try:\n",
    "            # ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "            processed_data_file = os.path.join(self.results_path, 'processed_lp_data.csv')\n",
    "            \n",
    "            if os.path.exists(processed_data_file):\n",
    "                print(\"ğŸ“‚ ì „ì²˜ë¦¬ëœ LP ë°ì´í„° ë¡œë“œ...\")\n",
    "                lp_data = pd.read_csv(processed_data_file)\n",
    "                lp_data['LP ìˆ˜ì‹ ì¼ì'] = pd.to_datetime(lp_data['LP ìˆ˜ì‹ ì¼ì'])\n",
    "            else:\n",
    "                # ì›ë³¸ LP ë°ì´í„° ë¡œë“œ\n",
    "                print(\"ğŸ“‚ ì›ë³¸ LP ë°ì´í„° ë¡œë“œ...\")\n",
    "                lp_data = self._load_original_lp_data()\n",
    "            \n",
    "            # ê³ ê° ë°ì´í„° ë¡œë“œ\n",
    "            customer_file = os.path.join(os.path.dirname(self.results_path), 'ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx')\n",
    "            if os.path.exists(customer_file):\n",
    "                customer_data = pd.read_excel(customer_file)\n",
    "            else:\n",
    "                customer_data = None\n",
    "                print(\"âš ï¸ ê³ ê° ë°ì´í„° íŒŒì¼ ì—†ìŒ\")\n",
    "            \n",
    "            # íŠ¹ì„± ìƒì„±\n",
    "            features_dict = self.create_features(lp_data, customer_data)\n",
    "            \n",
    "            print(f\"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features_dict)}ëª…\")\n",
    "            return features_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            print(\"ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\")\n",
    "            return self._create_sample_features()\n",
    "    \n",
    "    def _load_original_lp_data(self):\n",
    "        \"\"\"ì›ë³¸ LP ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "        import glob\n",
    "        \n",
    "        base_path = os.path.dirname(self.results_path)\n",
    "        lp_files = glob.glob(os.path.join(base_path, 'processed_LPData_*.csv'))\n",
    "        \n",
    "        if not lp_files:\n",
    "            raise FileNotFoundError(\"LP ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        dataframes = []\n",
    "        for file_path in sorted(lp_files):\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # ì»¬ëŸ¼ëª… í‘œì¤€í™”\n",
    "            column_mapping = {\n",
    "                'LPìˆ˜ì‹ ì¼ì': 'LP ìˆ˜ì‹ ì¼ì',\n",
    "                'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥',\n",
    "                'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'\n",
    "            }\n",
    "            \n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in df.columns:\n",
    "                    df = df.rename(columns={old_col: new_col})\n",
    "            \n",
    "            dataframes.append(df)\n",
    "        \n",
    "        lp_data = pd.concat(dataframes, ignore_index=True)\n",
    "        lp_data['LP ìˆ˜ì‹ ì¼ì'] = pd.to_datetime(lp_data['LP ìˆ˜ì‹ ì¼ì'])\n",
    "        \n",
    "        return lp_data\n",
    "    \n",
    "    def _create_sample_features(self):\n",
    "        \"\"\"ìƒ˜í”Œ íŠ¹ì„± ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\"\"\"\n",
    "        print(\"ğŸ§ª ìƒ˜í”Œ íŠ¹ì„± ë°ì´í„° ìƒì„±...\")\n",
    "        \n",
    "        sample_features = {}\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        for i in range(100):  \n",
    "            customer_id = f'SAMPLE_{i:04d}'\n",
    "            \n",
    "            sample_features[customer_id] = {\n",
    "                'basic': {\n",
    "                    'cv_basic': np.random.normal(0.25, 0.1),\n",
    "                    'range_volatility': np.random.normal(0.8, 0.3),\n",
    "                    'iqr_volatility': np.random.normal(0.6, 0.2),\n",
    "                    'skewness': np.random.normal(0.5, 0.3),\n",
    "                    'kurtosis': np.random.normal(0.2, 0.5),\n",
    "                    'mean_power': np.random.normal(50, 20),\n",
    "                    'load_factor': np.random.normal(0.6, 0.2)\n",
    "                },\n",
    "                'temporal': {\n",
    "                    'peak_cv': np.random.normal(0.3, 0.1),\n",
    "                    'off_peak_cv': np.random.normal(0.2, 0.1),\n",
    "                    'peak_mean': np.random.normal(60, 25),\n",
    "                    'off_peak_mean': np.random.normal(40, 15),\n",
    "                    'peak_off_peak_ratio': np.random.normal(1.5, 0.3),\n",
    "                    'temporal_cv_diff': np.random.normal(0.1, 0.05),\n",
    "                    'weekday_weekend_cv_ratio': np.random.normal(1.2, 0.3)\n",
    "                },\n",
    "                'seasonal': {\n",
    "                    'summer_cv': np.random.normal(0.35, 0.15),\n",
    "                    'winter_cv': np.random.normal(0.3, 0.12),\n",
    "                    'summer_mean': np.random.normal(65, 30),\n",
    "                    'winter_mean': np.random.normal(55, 25),\n",
    "                    'seasonal_cv_diff': np.random.normal(0.05, 0.03),\n",
    "                    'seasonal_stability': np.random.normal(0.1, 0.05)\n",
    "                },\n",
    "                'pattern': {\n",
    "                    'daily_cv_stability': np.random.normal(0.05, 0.02),\n",
    "                    'daily_cv_mean': np.random.normal(0.25, 0.1),\n",
    "                    'autocorrelation': np.random.normal(0.7, 0.2),\n",
    "                    'trend_volatility': np.random.normal(0.1, 0.05)\n",
    "                },\n",
    "                'anomaly': {\n",
    "                    'zero_ratio': np.random.beta(1, 10),\n",
    "                    'sudden_change_ratio': np.random.beta(1, 20),\n",
    "                    'night_day_ratio': np.random.normal(0.4, 0.2),\n",
    "                    'outlier_ratio': np.random.beta(1, 30)\n",
    "                },\n",
    "                'customer': {\n",
    "                    'baseline_cv': 0.25,\n",
    "                    'usage_factor': np.random.choice([0.9, 1.1]),\n",
    "                    'contract_power_log': np.random.normal(6.0, 1.0)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return sample_features\n",
    "    \n",
    "    def create_features(self, lp_data, customer_data=None):\n",
    "        \"\"\"ì‹¤ì œ LP ë°ì´í„°ë¡œ íŠ¹ì„± ìƒì„±\"\"\"\n",
    "        print(\"ğŸ”„ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        features_dict = {}\n",
    "        customers = lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ê³ ë ¤í•´ì„œ ìµœëŒ€ 1000ëª…ìœ¼ë¡œ ì œí•œ\n",
    "        if len(customers) > 1000:\n",
    "            customers = customers[:1000]\n",
    "            print(f\"   ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ {len(customers)}ëª…ìœ¼ë¡œ ì œí•œ\")\n",
    "        \n",
    "        for i, customer in enumerate(customers):\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"   ì§„í–‰: {i+1}/{len(customers)} ({(i+1)/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            # ì»¬ëŸ¼ëª… í™•ì¸ ë° í‘œì¤€í™”\n",
    "            power_col = None\n",
    "            for col in ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥']:\n",
    "                if col in lp_data.columns:\n",
    "                    power_col = col\n",
    "                    break\n",
    "            \n",
    "            if power_col is None:\n",
    "                print(\"âš ï¸ ì „ë ¥ ë°ì´í„° ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                continue\n",
    "            \n",
    "            customer_lp = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()\n",
    "            \n",
    "            if len(customer_lp) < 96:  # ìµœì†Œ 1ì¼ ë°ì´í„° í•„ìš”\n",
    "                continue\n",
    "            \n",
    "            # ì‹œê°„ ê´€ë ¨ íŒŒìƒ ë³€ìˆ˜\n",
    "            customer_lp['ì‹œê°„'] = customer_lp['LP ìˆ˜ì‹ ì¼ì'].dt.hour\n",
    "            customer_lp['ìš”ì¼'] = customer_lp['LP ìˆ˜ì‹ ì¼ì'].dt.weekday\n",
    "            customer_lp['ì›”'] = customer_lp['LP ìˆ˜ì‹ ì¼ì'].dt.month\n",
    "            customer_lp['ì£¼ë§ì—¬ë¶€'] = customer_lp['ìš”ì¼'].isin([5, 6])\n",
    "            \n",
    "            power_series = customer_lp[power_col]\n",
    "            \n",
    "            # 1. ê¸°ë³¸ ë³€ë™ì„± íŠ¹ì„±\n",
    "            basic_features = self._calculate_basic_volatility(power_series)\n",
    "            \n",
    "            # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŠ¹ì„±\n",
    "            temporal_features = self._calculate_temporal_volatility(customer_lp, power_col)\n",
    "            \n",
    "            # 3. ê³„ì ˆì„± ë³€ë™ì„± íŠ¹ì„±  \n",
    "            seasonal_features = self._calculate_seasonal_volatility(customer_lp, power_col)\n",
    "            \n",
    "            # 4. íŒ¨í„´ ì•ˆì •ì„± íŠ¹ì„±\n",
    "            pattern_features = self._calculate_pattern_stability(customer_lp, power_col)\n",
    "            \n",
    "            # 5. ì´ìƒ íŒ¨í„´ íŠ¹ì„±\n",
    "            anomaly_features = self._calculate_anomaly_features(customer_lp, power_col)\n",
    "            \n",
    "            # 6. ê³ ê° íŠ¹ì„±\n",
    "            customer_features = {}\n",
    "            if customer_data is not None:\n",
    "                customer_info = customer_data[customer_data['ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "                if not customer_info.empty:\n",
    "                    customer_features = self._get_customer_features(customer_info.iloc[0])\n",
    "                else:\n",
    "                    customer_features = {'baseline_cv': 0.25, 'usage_factor': 1.0, 'contract_power_log': 6.0}\n",
    "            else:\n",
    "                customer_features = {'baseline_cv': 0.25, 'usage_factor': 1.0, 'contract_power_log': 6.0}\n",
    "            \n",
    "            features_dict[customer] = {\n",
    "                'basic': basic_features,\n",
    "                'temporal': temporal_features,\n",
    "                'seasonal': seasonal_features,\n",
    "                'pattern': pattern_features,\n",
    "                'anomaly': anomaly_features,\n",
    "                'customer': customer_features\n",
    "            }\n",
    "        \n",
    "        print(f\"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features_dict)}ëª…\")\n",
    "        return features_dict\n",
    "    \n",
    "    def _calculate_basic_volatility(self, power_series):\n",
    "        \"\"\"ê¸°ë³¸ ë³€ë™ì„± íŠ¹ì„± ê³„ì‚°\"\"\"\n",
    "        if len(power_series) == 0 or power_series.mean() == 0:\n",
    "            return {\n",
    "                'cv_basic': 0, 'range_volatility': 0, 'iqr_volatility': 0,\n",
    "                'skewness': 0, 'kurtosis': 0, 'mean_power': 0, 'load_factor': 0\n",
    "            }\n",
    "        \n",
    "        mean_power = power_series.mean()\n",
    "        std_power = power_series.std()\n",
    "        \n",
    "        # ë³€ë™ê³„ìˆ˜\n",
    "        cv_basic = std_power / mean_power if mean_power > 0 else 0\n",
    "        \n",
    "        # ë²”ìœ„ ë³€ë™ì„±\n",
    "        range_volatility = (power_series.max() - power_series.min()) / mean_power if mean_power > 0 else 0\n",
    "        \n",
    "        # IQR ë³€ë™ì„±\n",
    "        q75, q25 = np.percentile(power_series, [75, 25])\n",
    "        median_power = np.median(power_series)\n",
    "        iqr_volatility = (q75 - q25) / median_power if median_power > 0 else 0\n",
    "        \n",
    "        # ì™œë„ì™€ ì²¨ë„\n",
    "        skewness = stats.skew(power_series)\n",
    "        kurtosis = stats.kurtosis(power_series)\n",
    "        \n",
    "        # ë¶€í•˜ìœ¨ (í‰ê· /ìµœëŒ€)\n",
    "        load_factor = mean_power / power_series.max() if power_series.max() > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'cv_basic': cv_basic,\n",
    "            'range_volatility': range_volatility,\n",
    "            'iqr_volatility': iqr_volatility,\n",
    "            'skewness': skewness,\n",
    "            'kurtosis': kurtosis,\n",
    "            'mean_power': mean_power,\n",
    "            'load_factor': load_factor\n",
    "        }\n",
    "    \n",
    "    def _calculate_temporal_volatility(self, customer_lp, power_col):\n",
    "        \"\"\"ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŠ¹ì„± ê³„ì‚°\"\"\"\n",
    "        # í”¼í¬/ì˜¤í”„í”¼í¬ êµ¬ë¶„\n",
    "        peak_mask = customer_lp['ì‹œê°„'].isin(self.config['temporal_weights']['peak_hours'])\n",
    "        \n",
    "        peak_data = customer_lp[peak_mask][power_col]\n",
    "        off_peak_data = customer_lp[~peak_mask][power_col]\n",
    "        \n",
    "        # ê°ê°ì˜ ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "        peak_cv = peak_data.std() / peak_data.mean() if len(peak_data) > 0 and peak_data.mean() > 0 else 0\n",
    "        off_peak_cv = off_peak_data.std() / off_peak_data.mean() if len(off_peak_data) > 0 and off_peak_data.mean() > 0 else 0\n",
    "        \n",
    "        # í‰ì¼/ì£¼ë§ êµ¬ë¶„\n",
    "        weekday_data = customer_lp[~customer_lp['ì£¼ë§ì—¬ë¶€']][power_col]\n",
    "        weekend_data = customer_lp[customer_lp['ì£¼ë§ì—¬ë¶€']][power_col]\n",
    "        \n",
    "        weekday_cv = weekday_data.std() / weekday_data.mean() if len(weekday_data) > 0 and weekday_data.mean() > 0 else 0\n",
    "        weekend_cv = weekend_data.std() / weekend_data.mean() if len(weekend_data) > 0 and weekend_data.mean() > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'peak_cv': peak_cv,\n",
    "            'off_peak_cv': off_peak_cv,\n",
    "            'peak_mean': peak_data.mean() if len(peak_data) > 0 else 0,\n",
    "            'off_peak_mean': off_peak_data.mean() if len(off_peak_data) > 0 else 0,\n",
    "            'peak_off_peak_ratio': (peak_data.mean() / off_peak_data.mean()) if len(off_peak_data) > 0 and off_peak_data.mean() > 0 else 1,\n",
    "            'temporal_cv_diff': abs(peak_cv - off_peak_cv),\n",
    "            'weekday_weekend_cv_ratio': (weekday_cv / weekend_cv) if weekend_cv > 0 else 1\n",
    "        }\n",
    "    \n",
    "    def _calculate_seasonal_volatility(self, customer_lp, power_col):\n",
    "        \"\"\"ê³„ì ˆì„± ë³€ë™ì„± íŠ¹ì„± ê³„ì‚°\"\"\"\n",
    "        # ì—¬ë¦„/ê²¨ìš¸/ê¸°íƒ€ êµ¬ë¶„\n",
    "        summer_mask = customer_lp['ì›”'].isin(self.config['seasonal_adjustment']['summer_months'])\n",
    "        winter_mask = customer_lp['ì›”'].isin(self.config['seasonal_adjustment']['winter_months'])\n",
    "        \n",
    "        summer_data = customer_lp[summer_mask][power_col]\n",
    "        winter_data = customer_lp[winter_mask][power_col]\n",
    "        other_data = customer_lp[~(summer_mask | winter_mask)][power_col]\n",
    "        \n",
    "        summer_cv = summer_data.std() / summer_data.mean() if len(summer_data) > 0 and summer_data.mean() > 0 else 0\n",
    "        winter_cv = winter_data.std() / winter_data.mean() if len(winter_data) > 0 and winter_data.mean() > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'summer_cv': summer_cv,\n",
    "            'winter_cv': winter_cv,\n",
    "            'summer_mean': summer_data.mean() if len(summer_data) > 0 else 0,\n",
    "            'winter_mean': winter_data.mean() if len(winter_data) > 0 else 0,\n",
    "            'seasonal_cv_diff': abs(summer_cv - winter_cv),\n",
    "            'seasonal_stability': 1 - (abs(summer_cv - winter_cv) / max(summer_cv, winter_cv)) if max(summer_cv, winter_cv) > 0 else 1\n",
    "        }\n",
    "    \n",
    "    def _calculate_pattern_stability(self, customer_lp, power_col):\n",
    "        \"\"\"íŒ¨í„´ ì•ˆì •ì„± íŠ¹ì„± ê³„ì‚°\"\"\"\n",
    "        power_series = customer_lp[power_col]\n",
    "        \n",
    "        # ì¼ë³„ ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "        customer_lp['ë‚ ì§œ'] = customer_lp['LP ìˆ˜ì‹ ì¼ì'].dt.date\n",
    "        daily_cvs = []\n",
    "        \n",
    "        for date in customer_lp['ë‚ ì§œ'].unique():\n",
    "            daily_data = customer_lp[customer_lp['ë‚ ì§œ'] == date][power_col]\n",
    "            if len(daily_data) > 1 and daily_data.mean() > 0:\n",
    "                daily_cv = daily_data.std() / daily_data.mean()\n",
    "                daily_cvs.append(daily_cv)\n",
    "        \n",
    "        daily_cv_stability = np.std(daily_cvs) if len(daily_cvs) > 1 else 0\n",
    "        daily_cv_mean = np.mean(daily_cvs) if len(daily_cvs) > 0 else 0\n",
    "        \n",
    "        # ìê¸°ìƒê´€\n",
    "        if len(power_series) > 1:\n",
    "            autocorr = power_series.autocorr(lag=1)\n",
    "            autocorr = autocorr if not np.isnan(autocorr) else 0\n",
    "        else:\n",
    "            autocorr = 0\n",
    "        \n",
    "        # ì¶”ì„¸ ë³€ë™ì„±\n",
    "        if len(power_series) > 2:\n",
    "            trend = np.polyfit(range(len(power_series)), power_series, 1)[0]\n",
    "            detrended = power_series - (trend * np.arange(len(power_series)))\n",
    "            trend_volatility = detrended.std() / power_series.mean() if power_series.mean() > 0 else 0\n",
    "        else:\n",
    "            trend_volatility = 0\n",
    "        \n",
    "        return {\n",
    "            'daily_cv_stability': daily_cv_stability,\n",
    "            'daily_cv_mean': daily_cv_mean,\n",
    "            'autocorrelation': autocorr,\n",
    "            'trend_volatility': trend_volatility\n",
    "        }\n",
    "    \n",
    "    def _calculate_anomaly_features(self, customer_lp, power_col):\n",
    "        \"\"\"ì´ìƒ íŒ¨í„´ íŠ¹ì„± ê³„ì‚°\"\"\"\n",
    "        power_series = customer_lp[power_col]\n",
    "        \n",
    "        # 0ê°’ ë¹„ìœ¨\n",
    "        zero_ratio = (power_series == 0).sum() / len(power_series)\n",
    "        \n",
    "        # ê¸‰ê²©í•œ ë³€í™” ë¹„ìœ¨\n",
    "        if len(power_series) > 1:\n",
    "            changes = np.abs(power_series.diff())\n",
    "            sudden_changes = changes > (changes.mean() + 2 * changes.std())\n",
    "            sudden_change_ratio = sudden_changes.sum() / len(power_series)\n",
    "        else:\n",
    "            sudden_change_ratio = 0\n",
    "        \n",
    "        # ì•¼ê°„/ì£¼ê°„ ë¹„ìœ¨\n",
    "        night_mask = customer_lp['ì‹œê°„'].isin([22, 23, 0, 1, 2, 3, 4, 5])\n",
    "        day_mask = customer_lp['ì‹œê°„'].isin([6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21])\n",
    "        \n",
    "        night_mean = customer_lp[night_mask][power_col].mean() if night_mask.sum() > 0 else 0\n",
    "        day_mean = customer_lp[day_mask][power_col].mean() if day_mask.sum() > 0 else 0\n",
    "        \n",
    "        night_day_ratio = night_mean / day_mean if day_mean > 0 else 0\n",
    "        \n",
    "        # ì´ìƒì¹˜ ë¹„ìœ¨ (IQR ë°©ë²•)\n",
    "        q75, q25 = np.percentile(power_series, [75, 25])\n",
    "        iqr = q75 - q25\n",
    "        outlier_mask = (power_series < q25 - 1.5*iqr) | (power_series > q75 + 1.5*iqr)\n",
    "        outlier_ratio = outlier_mask.sum() / len(power_series)\n",
    "        \n",
    "        return {\n",
    "            'zero_ratio': zero_ratio,\n",
    "            'sudden_change_ratio': sudden_change_ratio,\n",
    "            'night_day_ratio': night_day_ratio,\n",
    "            'outlier_ratio': outlier_ratio\n",
    "        }\n",
    "    \n",
    "    def _get_customer_features(self, customer_info):\n",
    "        \"\"\"ê³ ê° ì •ë³´ì—ì„œ íŠ¹ì„± ì¶”ì¶œ\"\"\"\n",
    "        # ê³„ì•½ì¢…ë³„ì— ë”°ë¥¸ ê¸°ì¤€ ë³€ë™ê³„ìˆ˜\n",
    "        contract_type = str(customer_info.get('ê³„ì•½ì¢…ë³„', '322'))\n",
    "        baseline_cv = self.config['industry_baselines'].get(contract_type, 0.25)\n",
    "        \n",
    "        # ìš©ë„ë³„ íŒ©í„°\n",
    "        usage_type = str(customer_info.get('ìš©ë„ë³„', '09'))\n",
    "        usage_factor = self.config['usage_type_factors'].get(usage_type, 1.0)\n",
    "        \n",
    "        # ê³„ì•½ì „ë ¥ (ë¡œê·¸ ë³€í™˜)\n",
    "        contract_power = customer_info.get('ê³„ì•½ì „ë ¥', 100)\n",
    "        contract_power_log = np.log10(max(contract_power, 1))\n",
    "        \n",
    "        return {\n",
    "            'baseline_cv': baseline_cv,\n",
    "            'usage_factor': usage_factor,\n",
    "            'contract_power_log': contract_power_log\n",
    "        }\n",
    "    \n",
    "    def _prepare_feature_matrix(self, features_dict):\n",
    "        \"\"\"íŠ¹ì„± ë”•ì…”ë„ˆë¦¬ë¥¼ ëª¨ë¸ í•™ìŠµìš© í–‰ë ¬ë¡œ ë³€í™˜\"\"\"\n",
    "        feature_matrices = {}\n",
    "        customer_ids = list(features_dict.keys())\n",
    "        \n",
    "        # ê° ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ì„± í–‰ë ¬ ìƒì„±\n",
    "        for category in ['basic', 'temporal', 'seasonal', 'pattern', 'anomaly', 'customer']:\n",
    "            category_features = []\n",
    "            \n",
    "            for customer_id in customer_ids:\n",
    "                customer_features = features_dict[customer_id][category]\n",
    "                feature_vector = list(customer_features.values())\n",
    "                category_features.append(feature_vector)\n",
    "            \n",
    "            feature_matrices[category] = np.array(category_features)\n",
    "            \n",
    "            # NaN ì²˜ë¦¬\n",
    "            feature_matrices[category] = np.nan_to_num(feature_matrices[category], 0)\n",
    "        \n",
    "        return feature_matrices, customer_ids\n",
    "    \n",
    "    def fit(self, features_dict):\n",
    "        \"\"\"ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "        print(\"ğŸš€ ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "        \n",
    "        # íŠ¹ì„± í–‰ë ¬ ì¤€ë¹„\n",
    "        feature_matrices, customer_ids = self._prepare_feature_matrix(features_dict)\n",
    "        \n",
    "        # íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„± (ê¸°ë³¸ ë³€ë™ê³„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ)\n",
    "        y_target = feature_matrices['basic'][:, 0]  # cv_basic\n",
    "        \n",
    "        print(f\"í•™ìŠµ ë°ì´í„°: {len(customer_ids)}ëª…\")\n",
    "        print(f\"íƒ€ê²Ÿ ë³€ìˆ˜ ë²”ìœ„: {y_target.min():.3f} ~ {y_target.max():.3f}\")\n",
    "        \n",
    "        # Level-0 ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡\n",
    "        level0_predictions = {}\n",
    "        \n",
    "        for category, model_name in [\n",
    "            ('temporal', 'rf_temporal'),\n",
    "            ('seasonal', 'gbm_seasonal'), \n",
    "            ('basic', 'ridge_basic'),\n",
    "            ('pattern', 'elastic_pattern')\n",
    "        ]:\n",
    "            print(f\"  Level-0 ëª¨ë¸ í•™ìŠµ: {model_name} ({category})\")\n",
    "            \n",
    "            X = feature_matrices[category]\n",
    "            \n",
    "            # ìŠ¤ì¼€ì¼ë§\n",
    "            if category in self.scalers:\n",
    "                X_scaled = self.scalers[category].fit_transform(X)\n",
    "            else:\n",
    "                X_scaled = X\n",
    "            \n",
    "            # êµì°¨ê²€ì¦ì„ í†µí•œ Level-0 ì˜ˆì¸¡ ìƒì„±\n",
    "            cv = TimeSeriesSplit(n_splits=self.cv_folds)\n",
    "            predictions = np.zeros(len(y_target))\n",
    "            \n",
    "            for train_idx, val_idx in cv.split(X_scaled):\n",
    "                X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "                y_train, y_val = y_target[train_idx], y_target[val_idx]\n",
    "                \n",
    "                # ëª¨ë¸ í•™ìŠµ\n",
    "                model = self.level0_models[model_name]\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡\n",
    "                val_pred = model.predict(X_val)\n",
    "                predictions[val_idx] = val_pred\n",
    "            \n",
    "            level0_predictions[model_name] = predictions\n",
    "            \n",
    "            # ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ ì¬í•™ìŠµ\n",
    "            self.level0_models[model_name].fit(X_scaled, y_target)\n",
    "        \n",
    "        # Level-1 ë©”íƒ€ íŠ¹ì„± êµ¬ì„±\n",
    "        meta_features = np.column_stack([\n",
    "            level0_predictions['rf_temporal'],\n",
    "            level0_predictions['gbm_seasonal'],\n",
    "            level0_predictions['ridge_basic'],\n",
    "            level0_predictions['elastic_pattern'],\n",
    "            feature_matrices['anomaly'].mean(axis=1),  # ì´ìƒ íŒ¨í„´ ìš”ì•½\n",
    "            feature_matrices['customer'][:, 0]         # baseline_cv\n",
    "        ])\n",
    "        \n",
    "        # Level-1 ëª¨ë¸ í•™ìŠµ\n",
    "        print(\"  Level-1 ë©”íƒ€ëª¨ë¸ í•™ìŠµ...\")\n",
    "        self.level1_model.fit(meta_features, y_target)\n",
    "        \n",
    "        # ì„±ëŠ¥ í‰ê°€\n",
    "        final_predictions = self.level1_model.predict(meta_features)\n",
    "        mae = mean_absolute_error(y_target, final_predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_target, final_predictions))\n",
    "        r2 = r2_score(y_target, final_predictions)\n",
    "        \n",
    "        print(f\"âœ… í•™ìŠµ ì™„ë£Œ - MAE: {mae:.4f}, RMSE: {rmse:.4f}, RÂ²: {r2:.4f}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'level0_predictions': level0_predictions,\n",
    "            'meta_features_shape': meta_features.shape\n",
    "        }\n",
    "    \n",
    "    def predict(self, features_dict):\n",
    "        \"\"\"ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "        \n",
    "        print(\"ğŸ”® ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡ ì¤‘...\")\n",
    "        \n",
    "        # íŠ¹ì„± í–‰ë ¬ ì¤€ë¹„\n",
    "        feature_matrices, customer_ids = self._prepare_feature_matrix(features_dict)\n",
    "        \n",
    "        # Level-0 ì˜ˆì¸¡\n",
    "        level0_predictions = {}\n",
    "        \n",
    "        for category, model_name in [\n",
    "            ('temporal', 'rf_temporal'),\n",
    "            ('seasonal', 'gbm_seasonal'),\n",
    "            ('basic', 'ridge_basic'),\n",
    "            ('pattern', 'elastic_pattern')\n",
    "        ]:\n",
    "            X = feature_matrices[category]\n",
    "            \n",
    "            # ìŠ¤ì¼€ì¼ë§ (í•™ìŠµì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©)\n",
    "            if category in self.scalers:\n",
    "                X_scaled = self.scalers[category].transform(X)\n",
    "            else:\n",
    "                X_scaled = X\n",
    "            \n",
    "            # ì˜ˆì¸¡\n",
    "            predictions = self.level0_models[model_name].predict(X_scaled)\n",
    "            level0_predictions[model_name] = predictions\n",
    "        \n",
    "        # Level-1 ë©”íƒ€ íŠ¹ì„± êµ¬ì„±\n",
    "        meta_features = np.column_stack([\n",
    "            level0_predictions['rf_temporal'],\n",
    "            level0_predictions['gbm_seasonal'],\n",
    "            level0_predictions['ridge_basic'],\n",
    "            level0_predictions['elastic_pattern'],\n",
    "            feature_matrices['anomaly'].mean(axis=1),\n",
    "            feature_matrices['customer'][:, 0]\n",
    "        ])\n",
    "        \n",
    "        # ìµœì¢… ì˜ˆì¸¡\n",
    "        final_predictions = self.level1_model.predict(meta_features)\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±\n",
    "        predictions_dict = {}\n",
    "        for i, customer_id in enumerate(customer_ids):\n",
    "            # ì‹ ë¢°ë„ ê³„ì‚° (Level-0 ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ ì¼ì¹˜ë„)\n",
    "            level0_values = [\n",
    "                level0_predictions['rf_temporal'][i],\n",
    "                level0_predictions['gbm_seasonal'][i],\n",
    "                level0_predictions['ridge_basic'][i],\n",
    "                level0_predictions['elastic_pattern'][i]\n",
    "            ]\n",
    "            confidence = 1 - (np.std(level0_values) / max(np.mean(level0_values), 0.01))\n",
    "            confidence = max(0, min(1, confidence))  # 0-1 ë²”ìœ„ë¡œ ì œí•œ\n",
    "            \n",
    "            predictions_dict[customer_id] = {\n",
    "                'volatility_coefficient': max(0, final_predictions[i]),  # ìŒìˆ˜ ë°©ì§€\n",
    "                'confidence_score': confidence,\n",
    "                'level0_predictions': {\n",
    "                    'temporal': level0_predictions['rf_temporal'][i],\n",
    "                    'seasonal': level0_predictions['gbm_seasonal'][i],\n",
    "                    'basic': level0_predictions['ridge_basic'][i],\n",
    "                    'pattern': level0_predictions['elastic_pattern'][i]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        print(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions_dict)}ëª…\")\n",
    "        return predictions_dict\n",
    "    \n",
    "    def classify_volatility_grade(self, volatility_coefficient, customer_features=None):\n",
    "        \"\"\"ë³€ë™ì„± ë“±ê¸‰ ë¶„ë¥˜\"\"\"\n",
    "        # ê¸°ë³¸ ê¸°ì¤€ê°’\n",
    "        baseline_cv = 0.25\n",
    "        \n",
    "        # ê³ ê°ë³„ ê¸°ì¤€ê°’ ì¡°ì •\n",
    "        if customer_features and 'customer' in customer_features:\n",
    "            baseline_cv = customer_features['customer'].get('baseline_cv', 0.25)\n",
    "        \n",
    "        # ìƒëŒ€ ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "        relative_cv = volatility_coefficient / baseline_cv\n",
    "        \n",
    "        # ë“±ê¸‰ ë¶„ë¥˜\n",
    "        if relative_cv <= 0.8:\n",
    "            grade = \"ë§¤ìš° ì•ˆì •\"\n",
    "            risk_level = \"ë‚®ìŒ\"\n",
    "            recommendation = \"í˜„ì¬ ì•ˆì •ì ì¸ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ì„ ìœ ì§€í•˜ì„¸ìš”.\"\n",
    "        elif relative_cv <= 1.2:\n",
    "            grade = \"ì•ˆì •\"\n",
    "            risk_level = \"ë‚®ìŒ\"\n",
    "            recommendation = \"ì–‘í˜¸í•œ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ì…ë‹ˆë‹¤.\"\n",
    "        elif relative_cv <= 1.8:\n",
    "            grade = \"ë³´í†µ\"\n",
    "            risk_level = \"ì¤‘ê°„\"\n",
    "            recommendation = \"ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.\"\n",
    "        elif relative_cv <= 2.5:\n",
    "            grade = \"ë¶ˆì•ˆì •\"\n",
    "            risk_level = \"ë†’ìŒ\"\n",
    "            recommendation = \"ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
    "        else:\n",
    "            grade = \"ë§¤ìš° ë¶ˆì•ˆì •\"\n",
    "            risk_level = \"ë§¤ìš° ë†’ìŒ\"\n",
    "            recommendation = \"ì¦‰ì‹œ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ì ê²€ ë° ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
    "        \n",
    "        return {\n",
    "            'grade': grade,\n",
    "            'risk_level': risk_level,\n",
    "            'relative_cv': relative_cv,\n",
    "            'baseline_cv': baseline_cv,\n",
    "            'recommendation': recommendation\n",
    "        }\n",
    "    \n",
    "    def generate_report(self, predictions, save_path=None):\n",
    "        \"\"\"ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        print(\"ğŸ“‹ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        cv_values = [pred['volatility_coefficient'] for pred in predictions.values()]\n",
    "        confidence_scores = [pred['confidence_score'] for pred in predictions.values()]\n",
    "        \n",
    "        # ë“±ê¸‰ë³„ ë¶„í¬\n",
    "        grade_distribution = {}\n",
    "        for customer_id, pred in predictions.items():\n",
    "            grade_info = self.classify_volatility_grade(pred['volatility_coefficient'])\n",
    "            grade = grade_info['grade']\n",
    "            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1\n",
    "        \n",
    "        # ë¦¬í¬íŠ¸ êµ¬ì„±\n",
    "        report = {\n",
    "            'analysis_summary': {\n",
    "                'total_customers': len(predictions),\n",
    "                'analysis_date': datetime.now().isoformat(),\n",
    "                'model_type': 'Stacking Ensemble'\n",
    "            },\n",
    "            'volatility_statistics': {\n",
    "                'mean_cv': np.mean(cv_values),\n",
    "                'std_cv': np.std(cv_values),\n",
    "                'min_cv': np.min(cv_values),\n",
    "                'max_cv': np.max(cv_values),\n",
    "                'median_cv': np.median(cv_values),\n",
    "                'percentiles': {\n",
    "                    'p25': np.percentile(cv_values, 25),\n",
    "                    'p75': np.percentile(cv_values, 75),\n",
    "                    'p90': np.percentile(cv_values, 90),\n",
    "                    'p95': np.percentile(cv_values, 95)\n",
    "                }\n",
    "            },\n",
    "            'confidence_statistics': {\n",
    "                'mean_confidence': np.mean(confidence_scores),\n",
    "                'min_confidence': np.min(confidence_scores),\n",
    "                'max_confidence': np.max(confidence_scores)\n",
    "            },\n",
    "            'grade_distribution': grade_distribution,\n",
    "            'high_risk_customers': [],\n",
    "            'recommendations': {\n",
    "                'immediate_attention': [],\n",
    "                'monitoring_required': [],\n",
    "                'stable_customers': []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ê³ ìœ„í—˜ ê³ ê° ì‹ë³„\n",
    "        for customer_id, pred in predictions.items():\n",
    "            cv = pred['volatility_coefficient']\n",
    "            grade_info = self.classify_volatility_grade(cv)\n",
    "            \n",
    "            if grade_info['risk_level'] in ['ë†’ìŒ', 'ë§¤ìš° ë†’ìŒ']:\n",
    "                report['high_risk_customers'].append({\n",
    "                    'customer_id': customer_id,\n",
    "                    'volatility_coefficient': cv,\n",
    "                    'grade': grade_info['grade'],\n",
    "                    'risk_level': grade_info['risk_level'],\n",
    "                    'confidence': pred['confidence_score']\n",
    "                })\n",
    "            \n",
    "            # ê¶Œì¥ì‚¬í•­ ë¶„ë¥˜\n",
    "            if grade_info['risk_level'] == 'ë§¤ìš° ë†’ìŒ':\n",
    "                report['recommendations']['immediate_attention'].append(customer_id)\n",
    "            elif grade_info['risk_level'] in ['ë†’ìŒ', 'ì¤‘ê°„']:\n",
    "                report['recommendations']['monitoring_required'].append(customer_id)\n",
    "            else:\n",
    "                report['recommendations']['stable_customers'].append(customer_id)\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        if save_path:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"ğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥: {save_path}\")\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"ëª¨ë¸ ì €ì¥\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            print(\"âš ï¸ í•™ìŠµë˜ì§€ ì•Šì€ ëª¨ë¸ì€ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            model_data = {\n",
    "                'level0_models': self.level0_models,\n",
    "                'level1_model': self.level1_model,\n",
    "                'scalers': self.scalers,\n",
    "                'config': self.config,\n",
    "                'is_fitted': self.is_fitted\n",
    "            }\n",
    "            \n",
    "            joblib.dump(model_data, file_path)\n",
    "            print(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {file_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_model(self, file_path):\n",
    "        \"\"\"ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            model_data = joblib.load(file_path)\n",
    "            \n",
    "            self.level0_models = model_data['level0_models']\n",
    "            self.level1_model = model_data['level1_model']\n",
    "            self.scalers = model_data['scalers']\n",
    "            self.config = model_data['config']\n",
    "            self.is_fitted = model_data['is_fitted']\n",
    "            \n",
    "            print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {file_path}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"ì „ì²´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"ğŸš€ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ì‹œì‘\")\n",
    "        print(f\"ì‹œì‘ ì‹œê°„: {start_time}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. íŠ¹ì„± ë°ì´í„° ì¤€ë¹„\n",
    "            features_dict = self.load_actual_data_for_features()\n",
    "            \n",
    "            if not features_dict:\n",
    "                print(\"âŒ íŠ¹ì„± ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨\")\n",
    "                return False\n",
    "            \n",
    "            # 2. ëª¨ë¸ í•™ìŠµ\n",
    "            print(\"ğŸš€ ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ...\")\n",
    "            training_results = self.fit(features_dict)\n",
    "            \n",
    "            # 3. ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "            print(\"ğŸ”® ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡...\")\n",
    "            predictions = self.predict(features_dict)\n",
    "            \n",
    "            # 4. ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸\n",
    "            print(\"ğŸ“‹ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±...\")\n",
    "            report_path = os.path.join(self.results_path, 'volatility_coefficient_report.json')\n",
    "            final_report = self.generate_report(predictions, save_path=report_path)\n",
    "            \n",
    "            # 5. ëª¨ë¸ ì €ì¥\n",
    "            model_path = os.path.join(self.results_path, 'kepco_volatility_model.pkl')\n",
    "            self.save_model(model_path)\n",
    "            \n",
    "            # 6. ë“±ê¸‰ë³„ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥\n",
    "            self._save_classification_results(predictions)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ğŸ† ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì™„ë£Œ!\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"ì†Œìš” ì‹œê°„: {duration}\")\n",
    "            print(f\"ë¶„ì„ ê³ ê°: {len(predictions)}ëª…\")\n",
    "            print(f\"í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean([p['volatility_coefficient'] for p in predictions.values()]):.4f}\")\n",
    "            \n",
    "            # ë“±ê¸‰ë³„ ë¶„í¬ ì¶œë ¥\n",
    "            grades = {}\n",
    "            for pred in predictions.values():\n",
    "                grade_info = self.classify_volatility_grade(pred['volatility_coefficient'])\n",
    "                grade = grade_info['grade']\n",
    "                grades[grade] = grades.get(grade, 0) + 1\n",
    "            \n",
    "            print(\"\\nğŸ¯ ë³€ë™ì„± ë“±ê¸‰ë³„ ë¶„í¬:\")\n",
    "            for grade, count in grades.items():\n",
    "                pct = count / len(predictions) * 100\n",
    "                print(f\"   {grade}: {count}ëª… ({pct:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nğŸ“ ê²°ê³¼ íŒŒì¼:\")\n",
    "            print(f\"   âœ… ëª¨ë¸: {model_path}\")\n",
    "            print(f\"   âœ… ë¦¬í¬íŠ¸: {report_path}\")\n",
    "            print(f\"   âœ… ë¶„ë¥˜ê²°ê³¼: {os.path.join(self.results_path, 'customer_volatility_grades.csv')}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def _save_classification_results(self, predictions):\n",
    "        \"\"\"ê³ ê°ë³„ ë³€ë™ì„± ë“±ê¸‰ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for customer_id, pred in predictions.items():\n",
    "            grade_info = self.classify_volatility_grade(pred['volatility_coefficient'])\n",
    "            \n",
    "            results.append({\n",
    "                'ê³ ê°ë²ˆí˜¸': customer_id,\n",
    "                'ë³€ë™ê³„ìˆ˜': pred['volatility_coefficient'],\n",
    "                'ë³€ë™ì„±ë“±ê¸‰': grade_info['grade'],\n",
    "                'ìœ„í—˜ìˆ˜ì¤€': grade_info['risk_level'],\n",
    "                'ìƒëŒ€ë³€ë™ê³„ìˆ˜': grade_info['relative_cv'],\n",
    "                'ê¸°ì¤€ë³€ë™ê³„ìˆ˜': grade_info['baseline_cv'],\n",
    "                'ì‹ ë¢°ë„': pred['confidence_score'],\n",
    "                'ê¶Œì¥ì‚¬í•­': grade_info['recommendation']\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        output_path = os.path.join(self.results_path, 'customer_volatility_grades.csv')\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"ğŸ’¾ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥: {output_path}\")\n",
    "\n",
    "# ì‹¤í–‰ í•¨ìˆ˜\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ - ì™„ì „í•œ ë²„ì „\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ê²°ê³¼ í´ë” í™•ì¸\n",
    "    results_folders = ['./analysis_results', './results', './output']\n",
    "    results_path = None\n",
    "    \n",
    "    for folder in results_folders:\n",
    "        if os.path.exists(folder):\n",
    "            results_path = folder\n",
    "            break\n",
    "    \n",
    "    if results_path is None:\n",
    "        print(\"âš ï¸ ê²°ê³¼ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ í´ë”ì— 1-2ë‹¨ê³„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì„¸ìš”:\")\n",
    "        for folder in results_folders:\n",
    "            print(f\"   - {folder}\")\n",
    "        results_path = './analysis_results'\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        print(f\"âœ… ê¸°ë³¸ í´ë” ìƒì„±: {results_path}\")\n",
    "    \n",
    "    # ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰\n",
    "    algorithm = KEPCOVolatilityCoefficient(results_path=results_path)\n",
    "    success = algorithm.run_complete_analysis()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nğŸ‰ ë³€ë™ê³„ìˆ˜ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(\"\\nğŸ’¥ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    return algorithm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
