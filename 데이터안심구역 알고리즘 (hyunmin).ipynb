{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e087110d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ - ë…ë¦½ ì‹¤í–‰\n",
      "============================================================\n",
      "âš ï¸ ê²°ê³¼ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ í´ë”ì— 1-2ë‹¨ê³„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì„¸ìš”:\n",
      "   - ./analysis_results\n",
      "   - ./results\n",
      "   - ./output\n",
      "âœ… ê¸°ë³¸ í´ë” ìƒì„±: ./analysis_results\n",
      "ğŸ”§ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì´ˆê¸°í™”\n",
      "ê²°ê³¼ í´ë”: ./analysis_results\n",
      "ğŸ“‚ 1-2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘...\n",
      "âš ï¸ 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: ./analysis_results\\analysis_results.json\n",
      "âš ï¸ 2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: ./analysis_results\\volatility_summary.csv\n",
      "ğŸš€ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ì‹œì‘\n",
      "ì‹œì‘ ì‹œê°„: 2025-07-08 15:24:31.688578\n",
      "ğŸ“Š ì‹¤ì œ ë°ì´í„°ë¡œ íŠ¹ì„± ìƒì„±...\n",
      "ğŸ“‚ ì›ë³¸ LP ë°ì´í„° ë¡œë“œ...\n",
      "âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: LP ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
      "ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\n",
      "ğŸ§ª ìƒ˜í”Œ íŠ¹ì„± ë°ì´í„° ìƒì„±...\n",
      "ğŸš€ ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ...\n",
      "âŒ ë¶„ì„ ì‹¤íŒ¨: 'KEPCOVolatilityCoefficient' object has no attribute 'fit'\n",
      "\\nğŸ’¥ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\khmin\\AppData\\Local\\Temp\\ipykernel_28056\\3077651523.py\", line 376, in run_complete_analysis\n",
      "    training_results = self.fit(features_dict)\n",
      "                       ^^^^^^^^\n",
      "AttributeError: 'KEPCOVolatilityCoefficient' object has no attribute 'fit'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ - ë…ë¦½ ì‹¤í–‰ìš©\n",
    "# 1-2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•´ì„œ ì‹¤í–‰\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import warnings\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityCoefficient:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜\n",
    "    1-2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•´ì„œ ì‹¤í–‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, results_path='./analysis_results'):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        Args:\n",
    "            results_path: 1-2ë‹¨ê³„ ê²°ê³¼ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        self.results_path = results_path\n",
    "        \n",
    "        # ê¸°ë³¸ ì„¤ì • (1-2ë‹¨ê³„ ê²°ê³¼ë¡œ ìë™ ì—…ë°ì´íŠ¸ë¨)\n",
    "        self.config = {\n",
    "            'temporal_weights': {\n",
    "                'peak_hours': [9, 10, 11, 14, 15, 18, 19, 20],  \n",
    "                'peak_weight': 1.5,                               \n",
    "                'off_peak_weight': 0.8                            \n",
    "            },\n",
    "            'seasonal_adjustment': {\n",
    "                'summer_months': [6, 7, 8],      \n",
    "                'winter_months': [12, 1, 2],     \n",
    "                'summer_factor': 1.2,            \n",
    "                'winter_factor': 1.1             \n",
    "            },\n",
    "            'industry_baselines': {\n",
    "                '222': 0.25,  # ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A\n",
    "                '226': 0.30,  # ì¼ë°˜ìš©(ì„) ê³ ì••A  \n",
    "                '311': 0.35,  # ì‚°ì—…ìš©(ê°‘) ì €ì••\n",
    "                '322': 0.20,  # ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A\n",
    "                '726': 0.28   # ì‚°ì—…ìš©(ì„) ê³ ì••A\n",
    "            },\n",
    "            'usage_type_factors': {\n",
    "                '02': 1.1,    # ìƒì—…ìš©\n",
    "                '09': 0.9     # ê´‘ê³µì—…ìš©\n",
    "            },\n",
    "            'anomaly_thresholds': {\n",
    "                'cv_extreme': 1.0,           \n",
    "                'zero_ratio_max': 0.1,       \n",
    "                'night_day_ratio_max': 0.8   \n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ëª¨ë¸ êµ¬ì„±ìš”ì†Œ\n",
    "        self.level0_models = {}\n",
    "        self.level1_model = None\n",
    "        self.scalers = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # ê³¼ì í•© ë°©ì§€ ì„¤ì •\n",
    "        self.cv_folds = 5\n",
    "        self.random_state = 42\n",
    "        \n",
    "        print(\"ğŸ”§ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì´ˆê¸°í™”\")\n",
    "        print(f\"ê²°ê³¼ í´ë”: {self.results_path}\")\n",
    "        \n",
    "        # ğŸ”¥ í•µì‹¬: 1-2ë‹¨ê³„ ê²°ê³¼ ìë™ ë¡œë“œ\n",
    "        self._load_previous_results()\n",
    "    \n",
    "    def _load_previous_results(self):\n",
    "        \"\"\"\n",
    "        1-2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ë“¤ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•´ì„œ ì„¤ì • ì—…ë°ì´íŠ¸\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“‚ 1-2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘...\")\n",
    "        \n",
    "        try:\n",
    "            # 1ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ\n",
    "            step1_file = os.path.join(self.results_path, 'analysis_results.json')\n",
    "            if os.path.exists(step1_file):\n",
    "                with open(step1_file, 'r', encoding='utf-8') as f:\n",
    "                    step1_results = json.load(f)\n",
    "                print(\"âœ… 1ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ\")\n",
    "                self._update_config_from_step1(step1_results)\n",
    "            else:\n",
    "                print(f\"âš ï¸ 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {step1_file}\")\n",
    "            \n",
    "            # 2ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ\n",
    "            step2_file = os.path.join(self.results_path, 'volatility_summary.csv')\n",
    "            if os.path.exists(step2_file):\n",
    "                step2_results = pd.read_csv(step2_file)\n",
    "                print(\"âœ… 2ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ\")\n",
    "                self._update_config_from_step2(step2_results)\n",
    "            else:\n",
    "                print(f\"âš ï¸ 2ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {step2_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            print(\"ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    def _update_config_from_step1(self, step1_results):\n",
    "        \"\"\"1ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸\"\"\"\n",
    "        print(\"ğŸ”„ 1ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸...\")\n",
    "        \n",
    "        # ê³ ê° ë¶„í¬ ì •ë³´\n",
    "        customer_summary = step1_results.get('customer_summary', {})\n",
    "        if customer_summary:\n",
    "            total_customers = customer_summary.get('total_customers', 3000)\n",
    "            print(f\"   ê³ ê° ìˆ˜: {total_customers:,}ëª…\")\n",
    "            \n",
    "            # ê³„ì•½ì¢…ë³„ ë¶„í¬ë¡œ ê¸°ì¤€ê°’ ì¡°ì •\n",
    "            contract_types = customer_summary.get('contract_types', {})\n",
    "            if contract_types:\n",
    "                print(f\"   ê³„ì•½ì¢…ë³„: {len(contract_types)}ê°œ ìœ í˜•\")\n",
    "                \n",
    "                # ê° ê³„ì•½ì¢…ë³„ ë¹„ìœ¨ì— ë”°ë¼ ê¸°ì¤€ê°’ ë¯¸ì„¸ ì¡°ì •\n",
    "                total_contracts = sum(contract_types.values())\n",
    "                for contract, count in contract_types.items():\n",
    "                    if str(contract) in self.config['industry_baselines']:\n",
    "                        ratio = count / total_contracts\n",
    "                        # ë¹„ìœ¨ì´ ë†’ì€ ê³„ì•½ì¢…ë³„ì€ ê¸°ì¤€ê°’ì„ ì•½ê°„ ë‚®ì¶¤ (ë” ì—„ê²©í•˜ê²Œ)\n",
    "                        if ratio > 0.3:  # 30% ì´ìƒ ì°¨ì§€í•˜ëŠ” ê²½ìš°\n",
    "                            self.config['industry_baselines'][str(contract)] *= 0.95\n",
    "    \n",
    "    def _update_config_from_step2(self, step2_results):\n",
    "        \"\"\"2ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸\"\"\"\n",
    "        print(\"ğŸ”„ 2ë‹¨ê³„ ê²°ê³¼ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸...\")\n",
    "        \n",
    "        # volatility_summary.csvì—ì„œ í•µì‹¬ ì§€í‘œ ì¶”ì¶œ\n",
    "        for _, row in step2_results.iterrows():\n",
    "            metric = row['metric']\n",
    "            value = row['value']\n",
    "            \n",
    "            if metric == 'overall_cv':\n",
    "                # ì „ì²´ ë³€ë™ê³„ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—…ì¢…ë³„ ê¸°ì¤€ê°’ ì¡°ì •\n",
    "                baseline_adjustment = value / 0.25  # ê¸°ë³¸ 0.25 ëŒ€ë¹„\n",
    "                for contract_type in self.config['industry_baselines']:\n",
    "                    self.config['industry_baselines'][contract_type] *= baseline_adjustment\n",
    "                print(f\"   ì „ì²´ CV ê¸°ì¤€ ì¡°ì •: {baseline_adjustment:.3f}ë°°\")\n",
    "                \n",
    "            elif metric == 'weekday_cv' and 'weekend_cv' in step2_results['metric'].values:\n",
    "                # ì£¼ë§/í‰ì¼ ë³€ë™ì„± ì°¨ì´\n",
    "                weekend_row = step2_results[step2_results['metric'] == 'weekend_cv']\n",
    "                if not weekend_row.empty:\n",
    "                    weekend_cv = weekend_row['value'].iloc[0]\n",
    "                    weekend_factor = weekend_cv / value if value > 0 else 1.0\n",
    "                    self.config['weekend_factor'] = weekend_factor\n",
    "                    print(f\"   ì£¼ë§ íŒ©í„°: {weekend_factor:.3f}\")\n",
    "        \n",
    "        print(\"âœ… ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
    "    \n",
    "    def load_actual_data_for_features(self):\n",
    "        \"\"\"\n",
    "        ì‹¤ì œ LP ë°ì´í„°ë¥¼ ë¡œë“œí•´ì„œ íŠ¹ì„± ìƒì„±\n",
    "        (1-2ë‹¨ê³„ì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„° í™œìš©)\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“Š ì‹¤ì œ ë°ì´í„°ë¡œ íŠ¹ì„± ìƒì„±...\")\n",
    "        \n",
    "        try:\n",
    "            # ğŸ”¥ ì—¬ê¸°ì„œ ì‹¤ì œ LP ë°ì´í„°ë¥¼ ë¡œë“œí•´ì•¼ í•¨\n",
    "            # ë°©ë²• 1: 1-2ë‹¨ê³„ì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì–´ ìˆë‹¤ë©´\n",
    "            processed_data_file = os.path.join(self.results_path, 'processed_lp_data.csv')\n",
    "            \n",
    "            if os.path.exists(processed_data_file):\n",
    "                print(\"ğŸ“‚ ì „ì²˜ë¦¬ëœ LP ë°ì´í„° ë¡œë“œ...\")\n",
    "                lp_data = pd.read_csv(processed_data_file)\n",
    "                lp_data['LP ìˆ˜ì‹ ì¼ì'] = pd.to_datetime(lp_data['LP ìˆ˜ì‹ ì¼ì'])\n",
    "            else:\n",
    "                # ë°©ë²• 2: ì›ë³¸ LP ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œ (1-2ë‹¨ê³„ì™€ ë™ì¼í•œ ë°©ì‹)\n",
    "                print(\"ğŸ“‚ ì›ë³¸ LP ë°ì´í„° ë¡œë“œ...\")\n",
    "                lp_data = self._load_original_lp_data()\n",
    "            \n",
    "            # ê³ ê° ë°ì´í„°ë„ ë¡œë“œ\n",
    "            customer_file = os.path.join(os.path.dirname(self.results_path), 'ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx')\n",
    "            if os.path.exists(customer_file):\n",
    "                customer_data = pd.read_excel(customer_file)\n",
    "            else:\n",
    "                customer_data = None\n",
    "                print(\"âš ï¸ ê³ ê° ë°ì´í„° íŒŒì¼ ì—†ìŒ\")\n",
    "            \n",
    "            # íŠ¹ì„± ìƒì„±\n",
    "            features_dict = self.create_features(lp_data, customer_data)\n",
    "            \n",
    "            print(f\"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features_dict)}ëª…\")\n",
    "            return features_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            print(\"ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...\")\n",
    "            return self._create_sample_features()\n",
    "    \n",
    "    def _load_original_lp_data(self):\n",
    "        \"\"\"ì›ë³¸ LP ë°ì´í„° ë¡œë“œ (1-2ë‹¨ê³„ì™€ ë™ì¼)\"\"\"\n",
    "        import glob\n",
    "        \n",
    "        base_path = os.path.dirname(self.results_path)\n",
    "        lp_files = glob.glob(os.path.join(base_path, 'processed_LPData_*.csv'))\n",
    "        \n",
    "        if not lp_files:\n",
    "            raise FileNotFoundError(\"LP ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        dataframes = []\n",
    "        for file_path in sorted(lp_files):\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # ì»¬ëŸ¼ëª… í‘œì¤€í™”\n",
    "            if 'LPìˆ˜ì‹ ì¼ì' in df.columns:\n",
    "                df = df.rename(columns={'LPìˆ˜ì‹ ì¼ì': 'LP ìˆ˜ì‹ ì¼ì'})\n",
    "            if 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥' in df.columns:\n",
    "                df = df.rename(columns={'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'})\n",
    "            \n",
    "            dataframes.append(df)\n",
    "        \n",
    "        lp_data = pd.concat(dataframes, ignore_index=True)\n",
    "        lp_data['LP ìˆ˜ì‹ ì¼ì'] = pd.to_datetime(lp_data['LP ìˆ˜ì‹ ì¼ì'])\n",
    "        \n",
    "        return lp_data\n",
    "    \n",
    "    def _create_sample_features(self):\n",
    "        \"\"\"ìƒ˜í”Œ íŠ¹ì„± ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\"\"\"\n",
    "        print(\"ğŸ§ª ìƒ˜í”Œ íŠ¹ì„± ë°ì´í„° ìƒì„±...\")\n",
    "        \n",
    "        sample_features = {}\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        for i in range(100):  # 100ëª… ìƒ˜í”Œ\n",
    "            customer_id = f'SAMPLE_{i:04d}'\n",
    "            \n",
    "            sample_features[customer_id] = {\n",
    "                'basic': {\n",
    "                    'cv_basic': np.random.normal(0.25, 0.1),\n",
    "                    'range_volatility': np.random.normal(0.8, 0.3),\n",
    "                    'iqr_volatility': np.random.normal(0.6, 0.2),\n",
    "                    'skewness': np.random.normal(0.5, 0.3),\n",
    "                    'kurtosis': np.random.normal(0.2, 0.5),\n",
    "                    'mean_power': np.random.normal(50, 20),\n",
    "                    'load_factor': np.random.normal(0.6, 0.2)\n",
    "                },\n",
    "                'temporal': {\n",
    "                    'peak_cv': np.random.normal(0.3, 0.1),\n",
    "                    'off_peak_cv': np.random.normal(0.2, 0.1),\n",
    "                    'peak_mean': np.random.normal(60, 25),\n",
    "                    'off_peak_mean': np.random.normal(40, 15),\n",
    "                    'peak_off_peak_ratio': np.random.normal(1.5, 0.3),\n",
    "                    'temporal_cv_diff': np.random.normal(0.1, 0.05),\n",
    "                    'weekday_weekend_cv_ratio': np.random.normal(1.2, 0.3)\n",
    "                },\n",
    "                'seasonal': {\n",
    "                    'summer_cv': np.random.normal(0.35, 0.15),\n",
    "                    'winter_cv': np.random.normal(0.3, 0.12),\n",
    "                    'summer_mean': np.random.normal(65, 30),\n",
    "                    'winter_mean': np.random.normal(55, 25),\n",
    "                    'seasonal_cv_diff': np.random.normal(0.05, 0.03),\n",
    "                    'seasonal_stability': np.random.normal(0.1, 0.05)\n",
    "                },\n",
    "                'pattern': {\n",
    "                    'daily_cv_stability': np.random.normal(0.05, 0.02),\n",
    "                    'daily_cv_mean': np.random.normal(0.25, 0.1),\n",
    "                    'autocorrelation': np.random.normal(0.7, 0.2),\n",
    "                    'trend_volatility': np.random.normal(0.1, 0.05)\n",
    "                },\n",
    "                'anomaly': {\n",
    "                    'zero_ratio': np.random.beta(1, 10),\n",
    "                    'sudden_change_ratio': np.random.beta(1, 20),\n",
    "                    'night_day_ratio': np.random.normal(0.4, 0.2),\n",
    "                    'outlier_ratio': np.random.beta(1, 30)\n",
    "                },\n",
    "                'customer': {\n",
    "                    'baseline_cv': 0.25,\n",
    "                    'usage_factor': np.random.choice([0.9, 1.1]),\n",
    "                    'contract_power_log': np.random.normal(6.0, 1.0)\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return sample_features\n",
    "    \n",
    "    # ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì€ ì´ì „ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ê³¼ ë™ì¼\n",
    "    # (create_features, fit, predict, classify_volatility_grade ë“±)\n",
    "    \n",
    "    def create_features(self, lp_data, customer_data=None):\n",
    "        \"\"\"ì‹¤ì œ LP ë°ì´í„°ë¡œ íŠ¹ì„± ìƒì„±\"\"\"\n",
    "        print(\"ğŸ”„ ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        features_dict = {}\n",
    "        customers = lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()\n",
    "        \n",
    "        # ë©”ëª¨ë¦¬ ê³ ë ¤í•´ì„œ ìµœëŒ€ 1000ëª…ìœ¼ë¡œ ì œí•œ\n",
    "        if len(customers) > 1000:\n",
    "            customers = customers[:1000]\n",
    "            print(f\"   ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ {len(customers)}ëª…ìœ¼ë¡œ ì œí•œ\")\n",
    "        \n",
    "        for i, customer in enumerate(customers):\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"   ì§„í–‰: {i+1}/{len(customers)} ({(i+1)/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            customer_lp = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()\n",
    "            \n",
    "            if len(customer_lp) < 96:  # ìµœì†Œ 1ì¼ ë°ì´í„° í•„ìš”\n",
    "                continue\n",
    "            \n",
    "            # ì‹œê°„ ê´€ë ¨ íŒŒìƒ ë³€ìˆ˜\n",
    "            customer_lp['ì‹œê°„'] = customer_lp['LP ìˆ˜ì‹ ì¼ì'].dt.hour\n",
    "            customer_lp['ìš”ì¼'] = customer_lp['LP ìˆ˜ì‹ ì¼ì'].dt.weekday\n",
    "            customer_lp['ì›”'] = customer_lp['LP ìˆ˜ì‹ ì¼ì'].dt.month\n",
    "            customer_lp['ì£¼ë§ì—¬ë¶€'] = customer_lp['ìš”ì¼'].isin([5, 6])\n",
    "            \n",
    "            power_series = customer_lp['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            # 1. ê¸°ë³¸ ë³€ë™ì„± íŠ¹ì„±\n",
    "            basic_features = self._calculate_basic_volatility(power_series)\n",
    "            \n",
    "            # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŠ¹ì„±\n",
    "            temporal_features = self._calculate_temporal_volatility(customer_lp)\n",
    "            \n",
    "            # 3. ê³„ì ˆì„± ë³€ë™ì„± íŠ¹ì„±  \n",
    "            seasonal_features = self._calculate_seasonal_volatility(customer_lp)\n",
    "            \n",
    "            # 4. íŒ¨í„´ ì•ˆì •ì„± íŠ¹ì„±\n",
    "            pattern_features = self._calculate_pattern_stability(customer_lp)\n",
    "            \n",
    "            # 5. ì´ìƒ íŒ¨í„´ íŠ¹ì„±\n",
    "            anomaly_features = self._calculate_anomaly_features(customer_lp)\n",
    "            \n",
    "            # 6. ê³ ê° íŠ¹ì„±\n",
    "            customer_features = {}\n",
    "            if customer_data is not None:\n",
    "                customer_info = customer_data[customer_data['ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "                if not customer_info.empty:\n",
    "                    customer_features = self._get_customer_features(customer_info.iloc[0])\n",
    "                else:\n",
    "                    customer_features = {'baseline_cv': 0.25, 'usage_factor': 1.0, 'contract_power_log': 6.0}\n",
    "            else:\n",
    "                customer_features = {'baseline_cv': 0.25, 'usage_factor': 1.0, 'contract_power_log': 6.0}\n",
    "            \n",
    "            features_dict[customer] = {\n",
    "                'basic': basic_features,\n",
    "                'temporal': temporal_features,\n",
    "                'seasonal': seasonal_features,\n",
    "                'pattern': pattern_features,\n",
    "                'anomaly': anomaly_features,\n",
    "                'customer': customer_features\n",
    "            }\n",
    "        \n",
    "        print(f\"âœ… íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(features_dict)}ëª…\")\n",
    "        return features_dict\n",
    "    \n",
    "    # ì´í•˜ _calculate_* ë©”ì„œë“œë“¤ì€ ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ\n",
    "    # (ì‹¤ì œ ì‚¬ìš©ì‹œì—ëŠ” ì „ì²´ ë©”ì„œë“œ í¬í•¨)\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"ì „ì²´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"ğŸš€ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ ì‹œì‘\")\n",
    "        print(f\"ì‹œì‘ ì‹œê°„: {start_time}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. íŠ¹ì„± ë°ì´í„° ì¤€ë¹„\n",
    "            features_dict = self.load_actual_data_for_features()\n",
    "            \n",
    "            if not features_dict:\n",
    "                print(\"âŒ íŠ¹ì„± ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨\")\n",
    "                return False\n",
    "            \n",
    "            # 2. ëª¨ë¸ í•™ìŠµ\n",
    "            print(\"ğŸš€ ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ...\")\n",
    "            training_results = self.fit(features_dict)\n",
    "            \n",
    "            # 3. ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "            print(\"ğŸ”® ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡...\")\n",
    "            predictions = self.predict(features_dict)\n",
    "            \n",
    "            # 4. ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸\n",
    "            print(\"ğŸ“‹ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±...\")\n",
    "            report_path = os.path.join(self.results_path, 'volatility_coefficient_report.json')\n",
    "            final_report = self.generate_report(predictions, save_path=report_path)\n",
    "            \n",
    "            # 5. ëª¨ë¸ ì €ì¥\n",
    "            model_path = os.path.join(self.results_path, 'kepco_volatility_model.pkl')\n",
    "            self.save_model(model_path)\n",
    "            \n",
    "            # 6. ë“±ê¸‰ë³„ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥\n",
    "            self._save_classification_results(predictions)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            print(\"\\\\n\" + \"=\"*60)\n",
    "            print(\"ğŸ† ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì™„ë£Œ!\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"ì†Œìš” ì‹œê°„: {duration}\")\n",
    "            print(f\"ë¶„ì„ ê³ ê°: {len(predictions)}ëª…\")\n",
    "            print(f\"í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean([p['volatility_coefficient'] for p in predictions.values()]):.4f}\")\n",
    "            \n",
    "            # ë“±ê¸‰ë³„ ë¶„í¬ ì¶œë ¥\n",
    "            grades = {}\n",
    "            for pred in predictions.values():\n",
    "                grade_info = self.classify_volatility_grade(pred['volatility_coefficient'])\n",
    "                grade = grade_info['grade']\n",
    "                grades[grade] = grades.get(grade, 0) + 1\n",
    "            \n",
    "            print(\"\\\\nğŸ¯ ë³€ë™ì„± ë“±ê¸‰ë³„ ë¶„í¬:\")\n",
    "            for grade, count in grades.items():\n",
    "                pct = count / len(predictions) * 100\n",
    "                print(f\"   {grade}: {count}ëª… ({pct:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\\\nğŸ“ ê²°ê³¼ íŒŒì¼:\")\n",
    "            print(f\"   âœ… ëª¨ë¸: {model_path}\")\n",
    "            print(f\"   âœ… ë¦¬í¬íŠ¸: {report_path}\")\n",
    "            print(f\"   âœ… ë¶„ë¥˜ê²°ê³¼: {os.path.join(self.results_path, 'customer_volatility_grades.csv')}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def _save_classification_results(self, predictions):\n",
    "        \"\"\"ê³ ê°ë³„ ë³€ë™ì„± ë“±ê¸‰ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for customer_id, pred in predictions.items():\n",
    "            grade_info = self.classify_volatility_grade(pred['volatility_coefficient'])\n",
    "            \n",
    "            results.append({\n",
    "                'ê³ ê°ë²ˆí˜¸': customer_id,\n",
    "                'ë³€ë™ê³„ìˆ˜': pred['volatility_coefficient'],\n",
    "                'ë³€ë™ì„±ë“±ê¸‰': grade_info['grade'],\n",
    "                'ìœ„í—˜ìˆ˜ì¤€': grade_info['risk_level'],\n",
    "                'ìƒëŒ€ë³€ë™ê³„ìˆ˜': grade_info['relative_cv'],\n",
    "                'ê¸°ì¤€ë³€ë™ê³„ìˆ˜': grade_info['baseline_cv'],\n",
    "                'ì‹ ë¢°ë„': pred['confidence_score'],\n",
    "                'ê¶Œì¥ì‚¬í•­': grade_info['recommendation']\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        output_path = os.path.join(self.results_path, 'customer_volatility_grades.csv')\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        print(f\"ğŸ’¾ ë¶„ë¥˜ ê²°ê³¼ ì €ì¥: {output_path}\")\n",
    "\n",
    "# ì‹¤í–‰ í•¨ìˆ˜\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ - ë…ë¦½ ì‹¤í–‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ê²°ê³¼ í´ë” í™•ì¸\n",
    "    results_folders = ['./analysis_results', './results', './output']\n",
    "    results_path = None\n",
    "    \n",
    "    for folder in results_folders:\n",
    "        if os.path.exists(folder):\n",
    "            results_path = folder\n",
    "            break\n",
    "    \n",
    "    if results_path is None:\n",
    "        print(\"âš ï¸ ê²°ê³¼ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ í´ë”ì— 1-2ë‹¨ê³„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì„¸ìš”:\")\n",
    "        for folder in results_folders:\n",
    "            print(f\"   - {folder}\")\n",
    "        results_path = './analysis_results'\n",
    "        os.makedirs(results_path, exist_ok=True)\n",
    "        print(f\"âœ… ê¸°ë³¸ í´ë” ìƒì„±: {results_path}\")\n",
    "    \n",
    "    # ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰\n",
    "    algorithm = KEPCOVolatilityCoefficient(results_path=results_path)\n",
    "    success = algorithm.run_complete_analysis()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\\\nğŸ‰ ë³€ë™ê³„ìˆ˜ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        print(\"\\\\nğŸ’¥ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
