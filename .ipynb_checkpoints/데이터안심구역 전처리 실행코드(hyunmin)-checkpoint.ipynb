{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd85c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\n",
      "데이터안심구역 전용 - 실제 데이터 분석\n",
      "============================================================\n",
      "\\n[1단계] 고객 기본정보 로딩 및 분석\n",
      "=== 고객 기본정보 로딩 ===\n",
      "고객 데이터 로딩 실패: [Errno 2] No such file or directory: '제13회 산업부 공모전 대상고객.xlsx'\n",
      "\\n[2단계] LP 데이터 로딩 및 품질 분석\n",
      "\\n=== LP 데이터 로딩 ===\n",
      "LP 데이터 파일을 찾을 수 없습니다.\n",
      "\\n[3단계] 이상치 탐지 및 데이터 정제\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 258\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# 3단계: 이상치 탐지\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn[3단계] 이상치 탐지 및 데이터 정제\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 258\u001b[0m outliers \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mdetect_outliers(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miqr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# 4단계: 종합 리포트\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn[4단계] 데이터 품질 종합 평가\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m, in \u001b[0;36mKEPCODataAnalyzer.detect_outliers\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    177\u001b[0m numeric_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m순방향 유효전력\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m지상무효\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m진상무효\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m피상전력\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numeric_columns:\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlp_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miqr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    182\u001b[0m             Q1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlp_data[col]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.25\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "    def load_customer_data(self, file_path='제13회 산업부 공모전 대상고객/제13회 산업부 공모전 대상고객.xlsx'):\n",
    "        \"\"\"실제 고객 기본정보 로딩 및 기본 분석\"\"\"\n",
    "        print(\"=== 고객 기본정보 로딩 ===\")\n",
    "        \n",
    "        try:\n",
    "            # 실제 Excel 파일 읽기\n",
    "            self.customer_data = pd.read_excel(file_path, header=1)\n",
    "            \n",
    "            print(f\"총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"컬럼: {list(self.customer_data.columns)}\")\n",
    "            print(\"\\\\n기본 정보:\")\n",
    "            print(self.customer_data.head())\n",
    "            \n",
    "            return self._analyze_customer_distribution()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"고객 데이터 로딩 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"고객 분포 분석\"\"\"\n",
    "        print(\"\\\\n=== 고객 분포 분석 ===\")\n",
    "        \n",
    "        # 계약종별 분포\n",
    "        contract_counts = self.customer_data['계약종별'].value_counts()\n",
    "        print(\"\\\\n📊 계약종별 분포:\")\n",
    "        for contract, count in contract_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {contract}: {count}명 ({pct:.1f}%)\")\n",
    "        \n",
    "        # 사용용도별 분포\n",
    "        usage_counts = self.customer_data['사용용도'].value_counts()\n",
    "        print(\"\\\\n🏭 사용용도별 분포:\")\n",
    "        for usage, count in usage_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {usage}: {count}명 ({pct:.1f}%)\")\n",
    "        \n",
    "        # 계약전력 분포\n",
    "        print(\"\\\\n⚡ 계약전력 분포:\")\n",
    "        power_stats = self.customer_data['계약전력'].describe()\n",
    "        print(power_stats)\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_counts,\n",
    "            'usage_distribution': usage_counts,\n",
    "            'power_stats': power_stats\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, data_directory='./제13회 산업부 공모전 대상고객 LP데이터/'):\n",
    "        \"\"\"실제 LP 데이터 로딩 (여러 CSV 파일)\"\"\"\n",
    "        print(\"\\\\n=== LP 데이터 로딩 ===\")\n",
    "        \n",
    "        try:\n",
    "            # processed_LPData_YYYYMMDD_DD.csv 패턴의 파일들 찾기\n",
    "            lp_files = glob.glob(os.path.join(data_directory, 'processed_LPData_*.csv'))\n",
    "            \n",
    "            if not lp_files:\n",
    "                print(\"LP 데이터 파일을 찾을 수 없습니다.\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"발견된 LP 파일 수: {len(lp_files)}개\")\n",
    "            \n",
    "            # 모든 LP 파일 읽기 및 결합\n",
    "            lp_dataframes = []\n",
    "            total_records = 0\n",
    "            \n",
    "            for i, file_path in enumerate(sorted(lp_files)):\n",
    "                try:\n",
    "                    print(f\"파일 {i+1} 로딩: {os.path.basename(file_path)}\")\n",
    "                    \n",
    "                    # CSV 파일 읽기\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # 컬럼명 확인 및 표준화 (실제 파일 구조에 맞춰 조정)\n",
    "                    expected_columns = ['대체고객번호', 'LP 수신일자', '순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "                    \n",
    "                    if all(col in df.columns for col in expected_columns):\n",
    "                        lp_dataframes.append(df)\n",
    "                        total_records += len(df)\n",
    "                        \n",
    "                        # 기간 정보 출력\n",
    "                        if 'LP 수신일자' in df.columns:\n",
    "                            min_date = df['LP 수신일자'].min()\n",
    "                            max_date = df['LP 수신일자'].max()\n",
    "                            print(f\"  레코드 수: {len(df):,}\")\n",
    "                            print(f\"  고객 수: {df['대체고객번호'].nunique()}\")\n",
    "                            print(f\"  기간: {min_date} ~ {max_date}\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠️ 컬럼 구조가 예상과 다름: {list(df.columns)}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ 파일 로딩 실패: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not lp_dataframes:\n",
    "                print(\"유효한 LP 데이터가 없습니다.\")\n",
    "                return None\n",
    "            \n",
    "            # 모든 데이터 결합\n",
    "            self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "            \n",
    "            print(f\"\\\\n✅ 전체 LP 데이터 결합 완료:\")\n",
    "            print(f\"  총 레코드: {len(self.lp_data):,}\")\n",
    "            print(f\"  총 고객: {self.lp_data['대체고객번호'].nunique()}\")\n",
    "            \n",
    "            return self._analyze_lp_quality()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LP 데이터 로딩 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP 데이터 품질 분석\"\"\"\n",
    "        print(\"\\\\n=== LP 데이터 품질 분석 ===\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        print(\"📈 기본 통계:\")\n",
    "        print(self.lp_data[numeric_columns].describe())\n",
    "        \n",
    "        # 시간 간격 체크 (LP 수신일자를 datetime으로 변환)\n",
    "        print(\"\\\\n⏰ 시간 간격 체크:\")\n",
    "        #수정전\n",
    "        #self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP 수신일자'])\n",
    "        \n",
    "        #수정후\n",
    "        # 24:00을 다음날 00:00으로 정확히 변환\n",
    "        self.lp_data['LP 수신일자'] = self.lp_data['LP 수신일자'].str.replace(' 24:00', ' 00:00')\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP 수신일자'], errors='coerce')\n",
    "\n",
    "        # 원래 24:00이었던 행들을 다음날로 이동\n",
    "        mask_24 = self.lp_data['LP 수신일자'].str.contains(' 00:00')\n",
    "        self.lp_data.loc[mask_24, 'datetime'] += pd.Timedelta(days=1)\n",
    "        \n",
    "        # 고객별 샘플 체크 (상위 3개 고객)\n",
    "        sample_customers = self.lp_data['대체고객번호'].unique()[:3]\n",
    "        \n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('datetime')\n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['datetime'].diff().dt.total_seconds() / 60  # 분 단위\n",
    "                time_diffs = time_diffs.dropna()\n",
    "                \n",
    "                avg_interval = time_diffs.mean()\n",
    "                std_interval = time_diffs.std()\n",
    "                print(f\"  {customer}: 평균 간격 {avg_interval:.1f}분, 표준편차 {std_interval:.1f}분\")\n",
    "        \n",
    "        # 데이터 품질 체크\n",
    "        print(\"\\\\n🔍 데이터 품질 체크:\")\n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                missing_count = self.lp_data[col].isnull().sum()\n",
    "                missing_pct = (missing_count / len(self.lp_data)) * 100\n",
    "                zero_count = (self.lp_data[col] == 0).sum()\n",
    "                zero_pct = (zero_count / len(self.lp_data)) * 100\n",
    "                \n",
    "                print(f\"  {col}:\")\n",
    "                print(f\"    결측치: {missing_count}건 ({missing_pct:.2f}%)\")\n",
    "                print(f\"    0값: {zero_count}건 ({zero_pct:.2f}%)\")\n",
    "        \n",
    "        # 이상치 탐지 (IQR 방법)\n",
    "        print(\"\\\\n🚨 이상치 탐지:\")\n",
    "        return self.detect_outliers('iqr')\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"이상치 탐지\"\"\"\n",
    "        outlier_summary = {}\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                if method == 'iqr':\n",
    "                    Q1 = self.lp_data[col].quantile(0.25)\n",
    "                    Q3 = self.lp_data[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    \n",
    "                    outliers = self.lp_data[\n",
    "                        (self.lp_data[col] < lower_bound) | \n",
    "                        (self.lp_data[col] > upper_bound)\n",
    "                    ]\n",
    "                    \n",
    "                    outlier_count = len(outliers)\n",
    "                    outlier_pct = (outlier_count / len(self.lp_data)) * 100\n",
    "                    \n",
    "                    print(f\"  {col}: {outlier_count}건 ({outlier_pct:.2f}%)\")\n",
    "                    outlier_summary[col] = {\n",
    "                        'count': outlier_count,\n",
    "                        'percentage': outlier_pct,\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound\n",
    "                    }\n",
    "        \n",
    "        return outlier_summary\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"데이터 품질 종합 리포트\"\"\"\n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(\"📋 데이터 품질 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 고객 데이터 요약\n",
    "        if self.customer_data is not None:\n",
    "            print(f\"\\\\n👥 고객 데이터:\")\n",
    "            print(f\"  총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"  계약종별 유형: {self.customer_data['계약종별'].nunique()}개\")\n",
    "            print(f\"  사용용도 유형: {self.customer_data['사용용도'].nunique()}개\")\n",
    "        \n",
    "        # LP 데이터 요약\n",
    "        if self.lp_data is not None:\n",
    "            print(f\"\\\\n⚡ LP 데이터:\")\n",
    "            print(f\"  총 레코드: {len(self.lp_data):,}건\")\n",
    "            print(f\"  측정 기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"  데이터 커버리지: {(self.lp_data['datetime'].max() - self.lp_data['datetime'].min()).days}일\")\n",
    "            \n",
    "            # 평균 전력 사용량\n",
    "            avg_power = self.lp_data['순방향 유효전력'].mean()\n",
    "            print(f\"  평균 유효전력: {avg_power:.2f}kW\")\n",
    "        \n",
    "        # 권장사항\n",
    "        print(\"\\\\n💡 다음 단계 권장사항:\")\n",
    "        print(\"  1. 시계열 패턴 분석 (일/주/월별 사용 패턴)\")\n",
    "        print(\"  2. 고객별 사용량 프로파일링\")\n",
    "        print(\"  3. 변동성 지표 계산 및 비교\")\n",
    "        print(\"  4. 이상 패턴 탐지 알고리즘 개발\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# 사용 예제 (실제 데이터안심구역에서 실행)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\")\n",
    "    print(\"데이터안심구역 전용 - 실제 데이터 분석\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1단계: 고객 기본정보 분석\n",
    "    print(\"\\\\n[1단계] 고객 기본정보 로딩 및 분석\")\n",
    "    customer_analysis = analyzer.load_customer_data('제13회 산업부 공모전 대상고객/제13회 산업부 공모전 대상고객.xlsx')\n",
    "    \n",
    "    # 2단계: LP 데이터 분석\n",
    "    print(\"\\\\n[2단계] LP 데이터 로딩 및 품질 분석\")\n",
    "    lp_analysis = analyzer.load_lp_data('./제13회 산업부 공모전 대상고객 LP데이터/')  # 현재 디렉터리에서 LP 파일 찾기\n",
    "    \n",
    "    # 3단계: 이상치 탐지\n",
    "    print(\"\\\\n[3단계] 이상치 탐지 및 데이터 정제\")\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4단계: 종합 리포트\n",
    "    print(\"\\\\n[4단계] 데이터 품질 종합 평가\")\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\\\n🎯 1단계 데이터 품질 점검 완료!\")\n",
    "    print(\"다음: 2단계 시계열 패턴 분석 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    \"\"\"한국전력공사 LP 데이터 시계열 패턴 분석 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path='./'):\n",
    "        \"\"\"\n",
    "        초기화\n",
    "        Args:\n",
    "            base_path: 데이터가 저장된 기본 경로\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "        # 결과 저장 디렉토리 생성\n",
    "        self.output_dir = os.path.join(base_path, 'analysis_results')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\")\n",
    "        print(\"2단계: 시계열 패턴 분석 및 변동성 지표 개발\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"작업 디렉토리: {self.base_path}\")\n",
    "        print(f\"결과 저장: {self.output_dir}\")\n",
    "        print()\n",
    "\n",
    "    def load_customer_data(self, filename='제13회 산업부 공모전 대상고객.xlsx'):\n",
    "        \"\"\"실제 고객 기본정보 로딩\"\"\"\n",
    "        print(\"🔄 1단계: 고객 기본정보 로딩...\")\n",
    "        \n",
    "        try:\n",
    "            file_path = os.path.join(self.base_path, filename)\n",
    "            self.customer_data = pd.read_excel(file_path)\n",
    "            \n",
    "            print(f\"✅ 고객 데이터 로딩 완료\")\n",
    "            print(f\"   - 총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"   - 컬럼: {list(self.customer_data.columns)}\")\n",
    "            \n",
    "            # 고객 분포 분석\n",
    "            contract_dist = self.customer_data['계약종별'].value_counts()\n",
    "            usage_dist = self.customer_data['사용용도'].value_counts()\n",
    "            \n",
    "            print(f\"\\\\n📊 고객 분포:\")\n",
    "            print(f\"   - 계약종별: {len(contract_dist)}개 유형\")\n",
    "            print(f\"   - 사용용도: {len(usage_dist)}개 유형\")\n",
    "            \n",
    "            self.analysis_results['customer_summary'] = {\n",
    "                'total_customers': len(self.customer_data),\n",
    "                'contract_types': contract_dist.to_dict(),\n",
    "                'usage_types': usage_dist.to_dict()\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 고객 데이터 로딩 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_lp_data(self, data_directory=None):\n",
    "        \"\"\"실제 LP 데이터 로딩 (대용량 처리)\"\"\"\n",
    "        print(\"\\\\n🔄 2단계: LP 데이터 로딩...\")\n",
    "        \n",
    "        if data_directory is None:\n",
    "            data_directory = self.base_path\n",
    "        \n",
    "        try:\n",
    "            # processed_LPData_YYYYMMDD_DD.csv 패턴의 파일들 찾기\n",
    "            lp_files = glob.glob(os.path.join(data_directory, 'processed_LPData_*.csv'))\n",
    "            \n",
    "            if not lp_files:\n",
    "                print(\"❌ LP 데이터 파일을 찾을 수 없습니다.\")\n",
    "                print(f\"   경로: {data_directory}\")\n",
    "                print(\"   파일명 패턴: processed_LPData_*.csv\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"📁 발견된 LP 파일 수: {len(lp_files)}개\")\n",
    "            \n",
    "            # 메모리 효율적 로딩을 위한 청크 처리\n",
    "            lp_dataframes = []\n",
    "            total_records = 0\n",
    "            processed_files = 0\n",
    "            \n",
    "            # 파일을 날짜 순으로 정렬\n",
    "            lp_files.sort()\n",
    "            \n",
    "            for i, file_path in enumerate(lp_files):\n",
    "                try:\n",
    "                    filename = os.path.basename(file_path)\n",
    "                    print(f\"   [{i+1}/{len(lp_files)}] {filename} 처리 중...\")\n",
    "                    \n",
    "                    # CSV 파일 읽기 (실제 컬럼명에 맞춰 조정)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # 컬럼명 확인 및 표준화\n",
    "                    expected_columns = ['대체고객번호', 'LP 수신일자', '순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "                    \n",
    "                    # 컬럼명이 다를 수 있으므로 유연하게 처리\n",
    "                    if 'LP수신일자' in df.columns:\n",
    "                        df = df.rename(columns={'LP수신일자': 'LP 수신일자'})\n",
    "                    if '순방향유효전력' in df.columns:\n",
    "                        df = df.rename(columns={'순방향유효전력': '순방향 유효전력'})\n",
    "                    \n",
    "                    # 필수 컬럼 존재 확인\n",
    "                    required_cols = ['대체고객번호', 'LP 수신일자', '순방향 유효전력']\n",
    "                    if all(col in df.columns for col in required_cols):\n",
    "                        # 날짜 컬럼 변환\n",
    "                        df['LP 수신일자'] = pd.to_datetime(df['LP 수신일자'])\n",
    "                        \n",
    "                        # 데이터 품질 기본 체크\n",
    "                        df = df.dropna(subset=required_cols)  # 필수 컬럼 결측치 제거\n",
    "                        df = df[df['순방향 유효전력'] >= 0]     # 음수 전력값 제거\n",
    "                        \n",
    "                        lp_dataframes.append(df)\n",
    "                        total_records += len(df)\n",
    "                        processed_files += 1\n",
    "                        \n",
    "                        # 진행상황 출력\n",
    "                        print(f\"      레코드: {len(df):,}개, 고객: {df['대체고객번호'].nunique()}명\")\n",
    "                        \n",
    "                        # 메모리 사용량 체크 (500만 레코드마다)\n",
    "                        if total_records % 5000000 == 0:\n",
    "                            print(f\"      📊 누적 레코드: {total_records:,}개\")\n",
    "                            \n",
    "                    else:\n",
    "                        print(f\"      ⚠️ 컬럼 구조 불일치: {list(df.columns)}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"      ❌ 파일 처리 실패: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not lp_dataframes:\n",
    "                print(\"❌ 유효한 LP 데이터가 없습니다.\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"\\\\n🔄 데이터 결합 중... ({processed_files}개 파일)\")\n",
    "            \n",
    "            # 모든 데이터 결합\n",
    "            self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "            \n",
    "            # 시간 순서로 정렬\n",
    "            self.lp_data = self.lp_data.sort_values(['대체고객번호', 'LP 수신일자']).reset_index(drop=True)\n",
    "            \n",
    "            print(f\"✅ LP 데이터 로딩 완료:\")\n",
    "            print(f\"   - 총 레코드: {len(self.lp_data):,}개\")\n",
    "            print(f\"   - 총 고객: {self.lp_data['대체고객번호'].nunique()}명\")\n",
    "            print(f\"   - 기간: {self.lp_data['LP 수신일자'].min()} ~ {self.lp_data['LP 수신일자'].max()}\")\n",
    "            \n",
    "            # 메모리 정리\n",
    "            del lp_dataframes\n",
    "            \n",
    "            return self._validate_data_quality()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ LP 데이터 로딩 실패: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def _validate_data_quality(self):\n",
    "        \"\"\"데이터 품질 검증\"\"\"\n",
    "        print(\"\\\\n🔍 데이터 품질 검증 중...\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        available_numeric_cols = [col for col in numeric_columns if col in self.lp_data.columns]\n",
    "        \n",
    "        print(f\"   📈 수치형 컬럼: {len(available_numeric_cols)}개\")\n",
    "        \n",
    "        # 결측치 확인\n",
    "        null_counts = self.lp_data[available_numeric_cols].isnull().sum()\n",
    "        total_nulls = null_counts.sum()\n",
    "        \n",
    "        if total_nulls > 0:\n",
    "            print(f\"   ⚠️ 결측치: {total_nulls:,}개 ({total_nulls/len(self.lp_data)*100:.2f}%)\")\n",
    "            for col, count in null_counts.items():\n",
    "                if count > 0:\n",
    "                    print(f\"      {col}: {count:,}개\")\n",
    "        else:\n",
    "            print(\"   ✅ 결측치 없음\")\n",
    "        \n",
    "        # 시간 간격 체크 (샘플 고객으로)\n",
    "        sample_customers = self.lp_data['대체고객번호'].unique()[:3]\n",
    "        \n",
    "        print(\"   ⏰ 시간 간격 검증:\")\n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('LP 수신일자')\n",
    "            \n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['LP 수신일자'].diff().dt.total_seconds() / 60\n",
    "                time_diffs = time_diffs.dropna()\n",
    "                \n",
    "                if len(time_diffs) > 0:\n",
    "                    avg_interval = time_diffs.mean()\n",
    "                    std_interval = time_diffs.std()\n",
    "                    print(f\"      {customer}: 평균 {avg_interval:.1f}분 (표준편차: {std_interval:.1f})\")\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['data_quality'] = {\n",
    "            'total_records': len(self.lp_data),\n",
    "            'customers': self.lp_data['대체고객번호'].nunique(),\n",
    "            'null_counts': null_counts.to_dict(),\n",
    "            'date_range': {\n",
    "                'start': str(self.lp_data['LP 수신일자'].min()),\n",
    "                'end': str(self.lp_data['LP 수신일자'].max())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def analyze_temporal_patterns(self):\n",
    "        \"\"\"시계열 패턴 분석\"\"\"\n",
    "        print(\"\\\\n📈 3단계: 시계열 패턴 분석...\")\n",
    "        \n",
    "        # 시간 관련 파생 변수 생성\n",
    "        print(\"   🕐 시간 파생 변수 생성 중...\")\n",
    "        self.lp_data['날짜'] = self.lp_data['LP 수신일자'].dt.date\n",
    "        self.lp_data['시간'] = self.lp_data['LP 수신일자'].dt.hour\n",
    "        self.lp_data['요일'] = self.lp_data['LP 수신일자'].dt.weekday  # 0=월요일\n",
    "        self.lp_data['월'] = self.lp_data['LP 수신일자'].dt.month\n",
    "        self.lp_data['주'] = self.lp_data['LP 수신일자'].dt.isocalendar().week\n",
    "        self.lp_data['주말여부'] = self.lp_data['요일'].isin([5, 6])  # 토, 일\n",
    "        \n",
    "        # 1. 시간대별 패턴 분석\n",
    "        print(\"   📊 시간대별 패턴 분석...\")\n",
    "        hourly_patterns = self.lp_data.groupby('시간')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'min', 'max', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        # 피크/비피크 시간대 식별\n",
    "        avg_by_hour = hourly_patterns['mean']\n",
    "        peak_threshold = avg_by_hour.quantile(0.75)\n",
    "        off_peak_threshold = avg_by_hour.quantile(0.25)\n",
    "        \n",
    "        peak_hours = avg_by_hour[avg_by_hour >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = avg_by_hour[avg_by_hour <= off_peak_threshold].index.tolist()\n",
    "        \n",
    "        print(f\"      피크 시간대: {peak_hours}\")\n",
    "        print(f\"      비피크 시간대: {off_peak_hours}\")\n",
    "        \n",
    "        # 2. 요일별 패턴 분석\n",
    "        print(\"   📅 요일별 패턴 분석...\")\n",
    "        daily_patterns = self.lp_data.groupby('요일')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        # 평일 vs 주말 비교\n",
    "        weekday_avg = self.lp_data[~self.lp_data['주말여부']]['순방향 유효전력'].mean()\n",
    "        weekend_avg = self.lp_data[self.lp_data['주말여부']]['순방향 유효전력'].mean()\n",
    "        weekend_ratio = weekend_avg / weekday_avg if weekday_avg > 0 else 0\n",
    "        \n",
    "        print(f\"      평일 평균: {weekday_avg:.2f}kW\")\n",
    "        print(f\"      주말 평균: {weekend_avg:.2f}kW\")\n",
    "        print(f\"      주말/평일 비율: {weekend_ratio:.3f}\")\n",
    "        \n",
    "        # 3. 월별 계절성 패턴\n",
    "        print(\"   🗓️ 월별 계절성 분석...\")\n",
    "        monthly_patterns = self.lp_data.groupby('월')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        # 계절 구분 (한국 기준)\n",
    "        season_map = {12: '겨울', 1: '겨울', 2: '겨울',\n",
    "                     3: '봄', 4: '봄', 5: '봄',\n",
    "                     6: '여름', 7: '여름', 8: '여름',\n",
    "                     9: '가을', 10: '가을', 11: '가을'}\n",
    "        \n",
    "        self.lp_data['계절'] = self.lp_data['월'].map(season_map)\n",
    "        seasonal_patterns = self.lp_data.groupby('계절')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "        \n",
    "        print(f\"      계절별 평균 사용량:\")\n",
    "        for season, values in seasonal_patterns.iterrows():\n",
    "            print(f\"        {season}: {values['mean']:.2f}kW\")\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['temporal_patterns'] = {\n",
    "            'hourly_patterns': hourly_patterns.to_dict(),\n",
    "            'daily_patterns': daily_patterns.to_dict(),\n",
    "            'monthly_patterns': monthly_patterns.to_dict(),\n",
    "            'seasonal_patterns': seasonal_patterns.to_dict(),\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def analyze_volatility_indicators(self):\n",
    "        \"\"\"변동성 지표 분석 (집계 중심)\"\"\"\n",
    "        print(\"\\\\n📊 4단계: 변동성 지표 분석...\")\n",
    "        \n",
    "        customers = self.lp_data['대체고객번호'].unique()\n",
    "        print(f\"   🔄 {len(customers)}명 고객 변동성 분석 중...\")\n",
    "        \n",
    "        # 전체 데이터에 대한 집계 분석\n",
    "        \n",
    "        # 1. 전체 변동성 통계\n",
    "        overall_power = self.lp_data['순방향 유효전력']\n",
    "        overall_cv = overall_power.std() / overall_power.mean() if overall_power.mean() > 0 else 0\n",
    "        \n",
    "        print(f\"   📈 전체 데이터 변동성:\")\n",
    "        print(f\"      전체 변동계수: {overall_cv:.4f}\")\n",
    "        print(f\"      평균 전력: {overall_power.mean():.2f}kW\")\n",
    "        print(f\"      표준편차: {overall_power.std():.2f}kW\")\n",
    "        \n",
    "        # 2. 시간대별 변동성 패턴\n",
    "        hourly_volatility = self.lp_data.groupby('시간')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        hourly_volatility['cv'] = hourly_volatility['std'] / hourly_volatility['mean']\n",
    "        \n",
    "        print(f\"\\\\n   ⏰ 시간대별 변동성 패턴:\")\n",
    "        high_volatility_hours = hourly_volatility.nlargest(3, 'cv').index.tolist()\n",
    "        low_volatility_hours = hourly_volatility.nsmallest(3, 'cv').index.tolist()\n",
    "        print(f\"      고변동성 시간대: {high_volatility_hours}시 (CV: {hourly_volatility.loc[high_volatility_hours, 'cv'].mean():.4f})\")\n",
    "        print(f\"      저변동성 시간대: {low_volatility_hours}시 (CV: {hourly_volatility.loc[low_volatility_hours, 'cv'].mean():.4f})\")\n",
    "        \n",
    "        # 3. 요일별 변동성 패턴\n",
    "        daily_volatility = self.lp_data.groupby('요일')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        daily_volatility['cv'] = daily_volatility['std'] / daily_volatility['mean']\n",
    "        \n",
    "        weekday_cv = daily_volatility.loc[0:4, 'cv'].mean()  # 월-금\n",
    "        weekend_cv = daily_volatility.loc[5:6, 'cv'].mean()  # 토-일\n",
    "        \n",
    "        print(f\"\\\\n   📅 요일별 변동성 패턴:\")\n",
    "        print(f\"      평일 평균 변동계수: {weekday_cv:.4f}\")\n",
    "        print(f\"      주말 평균 변동계수: {weekend_cv:.4f}\")\n",
    "        print(f\"      주말/평일 변동성 비율: {weekend_cv/weekday_cv:.3f}\")\n",
    "        \n",
    "        # 4. 월별 변동성 패턴\n",
    "        monthly_volatility = self.lp_data.groupby('월')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        monthly_volatility['cv'] = monthly_volatility['std'] / monthly_volatility['mean']\n",
    "        \n",
    "        print(f\"\\\\n   🗓️ 월별 변동성 패턴:\")\n",
    "        high_var_months = monthly_volatility.nlargest(2, 'cv').index.tolist()\n",
    "        low_var_months = monthly_volatility.nsmallest(2, 'cv').index.tolist()\n",
    "        print(f\"      고변동성 월: {high_var_months}월\")\n",
    "        print(f\"      저변동성 월: {low_var_months}월\")\n",
    "        \n",
    "        # 5. 고객별 변동성 분포 (요약 통계만)\n",
    "        print(f\"\\\\n   👥 고객별 변동성 분포 분석...\")\n",
    "        \n",
    "        # 청크 단위로 고객별 변동계수 계산 (메모리 효율성)\n",
    "        chunk_size = 100\n",
    "        customer_cvs = []\n",
    "        \n",
    "        for i in range(0, len(customers), chunk_size):\n",
    "            chunk_customers = customers[i:i+chunk_size]\n",
    "            if (i // chunk_size + 1) % 5 == 0:  # 500명마다 진행상황 출력\n",
    "                print(f\"      진행: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            for customer in chunk_customers:\n",
    "                customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "                power_series = customer_data['순방향 유효전력']\n",
    "                \n",
    "                if len(power_series) >= 96 and power_series.mean() > 0:  # 최소 1일 데이터\n",
    "                    cv = power_series.std() / power_series.mean()\n",
    "                    customer_cvs.append(cv)\n",
    "        \n",
    "        # 고객별 변동계수 분포 통계\n",
    "        cv_array = np.array(customer_cvs)\n",
    "        cv_percentiles = np.percentile(cv_array, [10, 25, 50, 75, 90])\n",
    "        \n",
    "        print(f\"   📊 고객별 변동계수 분포 ({len(customer_cvs)}명):\")\n",
    "        print(f\"      평균: {cv_array.mean():.4f}\")\n",
    "        print(f\"      표준편차: {cv_array.std():.4f}\")\n",
    "        print(f\"      10%ile: {cv_percentiles[0]:.4f}\")\n",
    "        print(f\"      25%ile: {cv_percentiles[1]:.4f}\")\n",
    "        print(f\"      50%ile: {cv_percentiles[2]:.4f}\")\n",
    "        print(f\"      75%ile: {cv_percentiles[3]:.4f}\")\n",
    "        print(f\"      90%ile: {cv_percentiles[4]:.4f}\")\n",
    "        \n",
    "        # 변동계수 구간별 고객 수\n",
    "        cv_bins = [0, 0.1, 0.2, 0.3, 0.5, 1.0, float('inf')]\n",
    "        cv_labels = ['매우 안정 (<0.1)', '안정 (0.1-0.2)', '보통 (0.2-0.3)', \n",
    "                    '높음 (0.3-0.5)', '매우 높음 (0.5-1.0)', '극히 높음 (>1.0)']\n",
    "        \n",
    "        cv_counts = pd.cut(cv_array, bins=cv_bins, labels=cv_labels, include_lowest=True).value_counts()\n",
    "        \n",
    "        print(f\"\\\\n   🎯 변동성 등급별 고객 분포:\")\n",
    "        for grade, count in cv_counts.items():\n",
    "            percentage = count / len(customer_cvs) * 100\n",
    "            print(f\"      {grade}: {count}명 ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['volatility_analysis'] = {\n",
    "            'overall_cv': overall_cv,\n",
    "            'hourly_volatility': hourly_volatility.to_dict(),\n",
    "            'daily_volatility': daily_volatility.to_dict(),\n",
    "            'monthly_volatility': monthly_volatility.to_dict(),\n",
    "            'customer_cv_stats': {\n",
    "                'count': len(customer_cvs),\n",
    "                'mean': float(cv_array.mean()),\n",
    "                'std': float(cv_array.std()),\n",
    "                'percentiles': {\n",
    "                    '10%': float(cv_percentiles[0]),\n",
    "                    '25%': float(cv_percentiles[1]),\n",
    "                    '50%': float(cv_percentiles[2]),\n",
    "                    '75%': float(cv_percentiles[3]),\n",
    "                    '90%': float(cv_percentiles[4])\n",
    "                }\n",
    "            },\n",
    "            'volatility_distribution': cv_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        # 요약 데이터만 CSV로 저장 (개별 고객 데이터는 제외)\n",
    "        summary_data = {\n",
    "            'metric': ['overall_cv', 'weekday_cv', 'weekend_cv', 'customer_cv_mean', \n",
    "                      'customer_cv_std', 'customer_cv_median'],\n",
    "            'value': [overall_cv, weekday_cv, weekend_cv, cv_array.mean(), \n",
    "                     cv_array.std(), cv_percentiles[2]]\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        output_file = os.path.join(self.output_dir, 'volatility_summary.csv')\n",
    "        summary_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\\\n   💾 변동성 요약 저장: {output_file}\")\n",
    "        \n",
    "        return cv_array\n",
    "\n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"이상 패턴 탐지 (집계 중심)\"\"\"\n",
    "        print(\"\\\\n🚨 5단계: 이상 패턴 탐지...\")\n",
    "        \n",
    "        customers = self.lp_data['대체고객번호'].unique()\n",
    "        print(f\"   🔍 {len(customers)}명 고객 이상 패턴 탐지 중...\")\n",
    "        \n",
    "        # 전체 데이터 기반 이상 패턴 탐지\n",
    "        \n",
    "        # 1. 전체 데이터의 통계적 이상치 임계값 설정\n",
    "        overall_power = self.lp_data['순방향 유효전력']\n",
    "        q1, q3 = overall_power.quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # 전체 통계적 이상치\n",
    "        total_outliers = ((overall_power < lower_bound) | (overall_power > upper_bound)).sum()\n",
    "        outlier_rate = total_outliers / len(overall_power) * 100\n",
    "        \n",
    "        print(f\"   📊 전체 데이터 이상치 현황:\")\n",
    "        print(f\"      통계적 이상치: {total_outliers:,}개 ({outlier_rate:.2f}%)\")\n",
    "        print(f\"      정상 범위: {lower_bound:.1f} ~ {upper_bound:.1f}kW\")\n",
    "        \n",
    "        # 2. 시간대별 이상 패턴\n",
    "        night_hours = [0, 1, 2, 3, 4, 5]  # 야간 시간대\n",
    "        day_hours = [9, 10, 11, 12, 13, 14, 15, 16, 17]  # 주간 시간대\n",
    "        \n",
    "        night_data = self.lp_data[self.lp_data['시간'].isin(night_hours)]\n",
    "        day_data = self.lp_data[self.lp_data['시간'].isin(day_hours)]\n",
    "        \n",
    "        night_avg = night_data['순방향 유효전력'].mean()\n",
    "        day_avg = day_data['순방향 유효전력'].mean()\n",
    "        night_day_ratio = night_avg / day_avg if day_avg > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\n   🌙 시간대별 사용 패턴:\")\n",
    "        print(f\"      야간 평균: {night_avg:.2f}kW\")\n",
    "        print(f\"      주간 평균: {day_avg:.2f}kW\")\n",
    "        print(f\"      야간/주간 비율: {night_day_ratio:.3f}\")\n",
    "        \n",
    "        # 3. 0값 패턴 분석\n",
    "        zero_count = (overall_power == 0).sum()\n",
    "        zero_rate = zero_count / len(overall_power) * 100\n",
    "        \n",
    "        print(f\"\\\\n   ⚫ 0값 패턴 분석:\")\n",
    "        print(f\"      0값 측정: {zero_count:,}개 ({zero_rate:.2f}%)\")\n",
    "        \n",
    "        # 4. 급격한 변화 패턴 (전체 데이터 기준)\n",
    "        power_changes = self.lp_data.sort_values(['대체고객번호', 'LP 수신일자'])['순방향 유효전력'].pct_change().abs()\n",
    "        sudden_changes = power_changes[power_changes > 2.0]  # 200% 이상 변화\n",
    "        sudden_change_rate = len(sudden_changes) / len(power_changes.dropna()) * 100\n",
    "        \n",
    "        print(f\"\\\\n   ⚡ 급격한 변화 패턴:\")\n",
    "        print(f\"      급격한 변화: {len(sudden_changes):,}건 ({sudden_change_rate:.2f}%)\")\n",
    "        \n",
    "        # 5. 고객별 이상 패턴 요약 통계 (개별 출력 없이)\n",
    "        anomaly_customers = {\n",
    "            'high_night_usage': 0,      # 야간 과다 사용\n",
    "            'excessive_zeros': 0,        # 과도한 0값\n",
    "            'high_volatility': 0,        # 높은 변동성\n",
    "            'statistical_outliers': 0    # 통계적 이상치 다수\n",
    "        }\n",
    "        \n",
    "        chunk_size = 100\n",
    "        processed_customers = 0\n",
    "        \n",
    "        for i in range(0, len(customers), chunk_size):\n",
    "            chunk_customers = customers[i:i+chunk_size]\n",
    "            if (i // chunk_size + 1) % 5 == 0:\n",
    "                print(f\"      진행: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            for customer in chunk_customers:\n",
    "                customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "                power_series = customer_data['순방향 유효전력']\n",
    "                \n",
    "                if len(power_series) < 96:  # 최소 1일 데이터 필요\n",
    "                    continue\n",
    "                \n",
    "                processed_customers += 1\n",
    "                \n",
    "                # 야간 과다 사용 체크\n",
    "                customer_night = customer_data[customer_data['시간'].isin(night_hours)]['순방향 유효전력'].mean()\n",
    "                customer_day = customer_data[customer_data['시간'].isin(day_hours)]['순방향 유효전력'].mean()\n",
    "                if customer_day > 0 and customer_night / customer_day > 0.8:\n",
    "                    anomaly_customers['high_night_usage'] += 1\n",
    "                \n",
    "                # 과도한 0값 체크\n",
    "                zero_ratio = (power_series == 0).sum() / len(power_series)\n",
    "                if zero_ratio > 0.1:  # 10% 이상이 0값\n",
    "                    anomaly_customers['excessive_zeros'] += 1\n",
    "                \n",
    "                # 높은 변동성 체크\n",
    "                if power_series.mean() > 0:\n",
    "                    cv = power_series.std() / power_series.mean()\n",
    "                    if cv > 1.0:  # 변동계수 1.0 이상\n",
    "                        anomaly_customers['high_volatility'] += 1\n",
    "                \n",
    "                # 통계적 이상치 다수 체크\n",
    "                customer_outliers = ((power_series < lower_bound) | (power_series > upper_bound)).sum()\n",
    "                outlier_ratio = customer_outliers / len(power_series)\n",
    "                if outlier_ratio > 0.05:  # 5% 이상이 이상치\n",
    "                    anomaly_customers['statistical_outliers'] += 1\n",
    "        \n",
    "        # 종합 이상 패턴 고객 (중복 제거를 위해 실제로는 근사치)\n",
    "        total_anomaly_customers = max(anomaly_customers.values())  # 단순 근사\n",
    "        anomaly_rate = total_anomaly_customers / processed_customers * 100 if processed_customers > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\n   📊 이상 패턴 고객 요약 ({processed_customers}명 분석):\")\n",
    "        print(f\"      야간 과다 사용: {anomaly_customers['high_night_usage']}명\")\n",
    "        print(f\"      과도한 0값: {anomaly_customers['excessive_zeros']}명\")\n",
    "        print(f\"      높은 변동성: {anomaly_customers['high_volatility']}명\")\n",
    "        print(f\"      통계적 이상치 다수: {anomaly_customers['statistical_outliers']}명\")\n",
    "        print(f\"      전체 이상 패턴 비율: 약 {anomaly_rate:.1f}%\")\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['anomaly_analysis'] = {\n",
    "            'processed_customers': processed_customers,\n",
    "            'total_outliers': int(total_outliers),\n",
    "            'outlier_rate': float(outlier_rate),\n",
    "            'zero_count': int(zero_count),\n",
    "            'zero_rate': float(zero_rate),\n",
    "            'sudden_changes': len(sudden_changes),\n",
    "            'sudden_change_rate': float(sudden_change_rate),\n",
    "            'night_day_ratio': float(night_day_ratio),\n",
    "            'anomaly_customers': anomaly_customers,\n",
    "            'estimated_anomaly_rate': float(anomaly_rate)\n",
    "        }\n",
    "        \n",
    "        return anomaly_customers\n",
    "        }\n",
    "        \n",
    "        return anomaly_customers\n",
    "\n",
    "    def create_summary_visualizations(self):\n",
    "        \"\"\"요약 시각화 생성 (집계 데이터 중심)\"\"\"\n",
    "        print(\"\\\\n📊 6단계: 분석 결과 시각화...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. 시간대별/요일별 패턴 시각화\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # 시간대별 패턴\n",
    "            hourly_avg = self.lp_data.groupby('시간')['순방향 유효전력'].mean()\n",
    "            axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, color='blue')\n",
    "            axes[0, 0].set_title('시간대별 평균 전력 사용량', fontsize=14, fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('시간')\n",
    "            axes[0, 0].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_xticks(range(0, 24, 3))\n",
    "            \n",
    "            # 요일별 패턴\n",
    "            daily_avg = self.lp_data.groupby('요일')['순방향 유효전력'].mean()\n",
    "            weekday_names = ['월', '화', '수', '목', '금', '토', '일']\n",
    "            axes[0, 1].bar(range(len(daily_avg)), daily_avg.values, color='skyblue')\n",
    "            axes[0, 1].set_title('요일별 평균 전력 사용량', fontsize=14, fontweight='bold')\n",
    "            axes[0, 1].set_xlabel('요일')\n",
    "            axes[0, 1].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[0, 1].set_xticks(range(7))\n",
    "            axes[0, 1].set_xticklabels(weekday_names)\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 시간대별 변동성\n",
    "            hourly_std = self.lp_data.groupby('시간')['순방향 유효전력'].std()\n",
    "            axes[1, 0].plot(hourly_std.index, hourly_std.values, marker='s', linewidth=2, color='red')\n",
    "            axes[1, 0].set_title('시간대별 전력 사용량 변동성', fontsize=14, fontweight='bold')\n",
    "            axes[1, 0].set_xlabel('시간')\n",
    "            axes[1, 0].set_ylabel('표준편차 (kW)')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].set_xticks(range(0, 24, 3))\n",
    "            \n",
    "            # 월별 계절성 패턴\n",
    "            monthly_avg = self.lp_data.groupby('월')['순방향 유효전력'].mean()\n",
    "            axes[1, 1].plot(monthly_avg.index, monthly_avg.values, marker='s', linewidth=2, color='orange')\n",
    "            axes[1, 1].set_title('월별 평균 전력 사용량 (계절성)', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('월')\n",
    "            axes[1, 1].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            axes[1, 1].set_xticks(range(1, 13))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # 이미지 저장\n",
    "            output_file = os.path.join(self.output_dir, 'temporal_patterns_summary.png')\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"   💾 시계열 패턴 시각화 저장: {output_file}\")\n",
    "            \n",
    "            # 2. 변동성 및 이상치 분포 시각화\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # 전체 전력 사용량 분포\n",
    "            axes[0, 0].hist(self.lp_data['순방향 유효전력'], bins=50, alpha=0.7, color='lightblue')\n",
    "            axes[0, 0].set_title('전력 사용량 분포', fontsize=14, fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('순방향 유효전력 (kW)')\n",
    "            axes[0, 0].set_ylabel('빈도')\n",
    "            axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 시간대별 변동계수\n",
    "            hourly_volatility = self.analysis_results.get('volatility_analysis', {}).get('hourly_volatility', {})\n",
    "            if hourly_volatility and 'cv' in hourly_volatility:\n",
    "                cv_data = hourly_volatility['cv']\n",
    "                hours = list(cv_data.keys())\n",
    "                cv_values = list(cv_data.values())\n",
    "                axes[0, 1].bar(hours, cv_values, color='lightgreen')\n",
    "                axes[0, 1].set_title('시간대별 변동계수', fontsize=14, fontweight='bold')\n",
    "                axes[0, 1].set_xlabel('시간')\n",
    "                axes[0, 1].set_ylabel('변동계수')\n",
    "                axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 요일별 변동계수\n",
    "            daily_volatility = self.analysis_results.get('volatility_analysis', {}).get('daily_volatility', {})\n",
    "            if daily_volatility and 'cv' in daily_volatility:\n",
    "                cv_data = daily_volatility['cv']\n",
    "                weekdays = list(cv_data.keys())\n",
    "                cv_values = list(cv_data.values())\n",
    "                weekday_names = ['월', '화', '수', '목', '금', '토', '일']\n",
    "                axes[1, 0].bar(range(len(cv_values)), cv_values, color='purple')\n",
    "                axes[1, 0].set_title('요일별 변동계수', fontsize=14, fontweight='bold')\n",
    "                axes[1, 0].set_xlabel('요일')\n",
    "                axes[1, 0].set_ylabel('변동계수')\n",
    "                axes[1, 0].set_xticks(range(7))\n",
    "                axes[1, 0].set_xticklabels(weekday_names)\n",
    "                axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 월별 변동계수\n",
    "            monthly_volatility = self.analysis_results.get('volatility_analysis', {}).get('monthly_volatility', {})\n",
    "            if monthly_volatility and 'cv' in monthly_volatility:\n",
    "                cv_data = monthly_volatility['cv']\n",
    "                months = list(cv_data.keys())\n",
    "                cv_values = list(cv_data.values())\n",
    "                axes[1, 1].plot(months, cv_values, marker='o', linewidth=2, color='red')\n",
    "                axes[1, 1].set_title('월별 변동계수 (계절성)', fontsize=14, fontweight='bold')\n",
    "                axes[1, 1].set_xlabel('월')\n",
    "                axes[1, 1].set_ylabel('변동계수')\n",
    "                axes[1, 1].grid(True, alpha=0.3)\n",
    "                axes[1, 1].set_xticks(range(1, 13))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # 이미지 저장\n",
    "            output_file = os.path.join(self.output_dir, 'volatility_analysis_summary.png')\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"   💾 변동성 분석 시각화 저장: {output_file}\")\n",
    "            \n",
    "            # 3. 변동성 등급별 분포 시각화 (있는 경우)\n",
    "            volatility_dist = self.analysis_results.get('volatility_analysis', {}).get('volatility_distribution', {})\n",
    "            if volatility_dist:\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "                \n",
    "                grades = list(volatility_dist.keys())\n",
    "                counts = list(volatility_dist.values())\n",
    "                \n",
    "                bars = ax.bar(range(len(grades)), counts, color=['green', 'lightgreen', 'yellow', 'orange', 'red', 'darkred'])\n",
    "                ax.set_title('변동성 등급별 고객 분포', fontsize=16, fontweight='bold')\n",
    "                ax.set_xlabel('변동성 등급', fontsize=12)\n",
    "                ax.set_ylabel('고객 수', fontsize=12)\n",
    "                ax.set_xticks(range(len(grades)))\n",
    "                ax.set_xticklabels(grades, rotation=45, ha='right')\n",
    "                ax.grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                # 각 막대 위에 수치 표시\n",
    "                for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "                    height = bar.get_height()\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                           f'{count}명', ha='center', va='bottom', fontweight='bold')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # 이미지 저장\n",
    "                output_file = os.path.join(self.output_dir, 'volatility_distribution.png')\n",
    "                plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                print(f\"   💾 변동성 분포 시각화 저장: {output_file}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 시각화 생성 실패: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "    def create_summary_visualizations(self):\n",
    "        \"\"\"요약 시각화 생성\"\"\"\n",
    "        print(\"\\\\n📊 6단계: 분석 결과 시각화...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. 시간대별 평균 전력 사용 패턴\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # 시간대별 패턴\n",
    "            hourly_avg = self.lp_data.groupby('시간')['순방향 유효전력'].mean()\n",
    "            axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2)\n",
    "            axes[0, 0].set_title('시간대별 평균 전력 사용량', fontsize=14, fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('시간')\n",
    "            axes[0, 0].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_xticks(range(0, 24, 3))\n",
    "            \n",
    "            # 요일별 패턴\n",
    "            daily_avg = self.lp_data.groupby('요일')['순방향 유효전력'].mean()\n",
    "            weekday_names = ['월', '화', '수', '목', '금', '토', '일']\n",
    "            axes[0, 1].bar(range(len(daily_avg)), daily_avg.values, color='skyblue')\n",
    "            axes[0, 1].set_title('요일별 평균 전력 사용량', fontsize=14, fontweight='bold')\n",
    "            axes[0, 1].set_xlabel('요일')\n",
    "            axes[0, 1].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[0, 1].set_xticks(range(7))\n",
    "            axes[0, 1].set_xticklabels(weekday_names)\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 변동계수 분포 (변동성 분석이 완료된 경우)\n",
    "            if 'volatility_analysis' in self.analysis_results:\n",
    "                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')\n",
    "                if os.path.exists(volatility_file):\n",
    "                    volatility_df = pd.read_csv(volatility_file)\n",
    "                    axes[1, 0].hist(volatility_df['cv_basic'].dropna(), bins=30, alpha=0.7, color='lightgreen')\n",
    "                    axes[1, 0].set_title('변동계수 분포', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 0].set_xlabel('변동계수 (CV)')\n",
    "                    axes[1, 0].set_ylabel('고객 수')\n",
    "                    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 월별 계절성 패턴\n",
    "            monthly_avg = self.lp_data.groupby('월')['순방향 유효전력'].mean()\n",
    "            month_names = ['1월', '2월', '3월', '4월', '5월', '6월', \n",
    "                          '7월', '8월', '9월', '10월', '11월', '12월']\n",
    "            axes[1, 1].plot(monthly_avg.index, monthly_avg.values, marker='s', linewidth=2, color='orange')\n",
    "            axes[1, 1].set_title('월별 평균 전력 사용량 (계절성)', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('월')\n",
    "            axes[1, 1].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            axes[1, 1].set_xticks(range(1, 13))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # 이미지 저장\n",
    "            output_file = os.path.join(self.output_dir, 'temporal_patterns_summary.png')\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"   💾 시계열 패턴 시각화 저장: {output_file}\")\n",
    "            \n",
    "            # 2. 변동성 관련 시각화 (추가)\n",
    "            if 'volatility_analysis' in self.analysis_results:\n",
    "                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')\n",
    "                if os.path.exists(volatility_file):\n",
    "                    volatility_df = pd.read_csv(volatility_file)\n",
    "                    \n",
    "                    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "                    \n",
    "                    # 평균 사용량 vs 변동계수\n",
    "                    axes[0, 0].scatter(volatility_df['mean_power'], volatility_df['cv_basic'], alpha=0.6, s=20)\n",
    "                    axes[0, 0].set_title('평균 사용량 vs 변동계수', fontsize=14, fontweight='bold')\n",
    "                    axes[0, 0].set_xlabel('평균 전력 (kW)')\n",
    "                    axes[0, 0].set_ylabel('변동계수')\n",
    "                    axes[0, 0].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # 시간대별 변동성 vs 일별 변동성\n",
    "                    axes[0, 1].scatter(volatility_df['hourly_cv_mean'], volatility_df['daily_cv_mean'], alpha=0.6, s=20, color='red')\n",
    "                    axes[0, 1].set_title('시간대별 vs 일별 변동성', fontsize=14, fontweight='bold')\n",
    "                    axes[0, 1].set_xlabel('시간대별 평균 변동계수')\n",
    "                    axes[0, 1].set_ylabel('일별 평균 변동계수')\n",
    "                    axes[0, 1].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # 주말/평일 변동계수 비교\n",
    "                    weekend_weekday_ratio = volatility_df['weekend_weekday_cv_ratio'].dropna()\n",
    "                    axes[1, 0].hist(weekend_weekday_ratio, bins=20, alpha=0.7, color='purple')\n",
    "                    axes[1, 0].set_title('주말/평일 변동계수 비율 분포', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 0].set_xlabel('주말/평일 변동계수 비율')\n",
    "                    axes[1, 0].set_ylabel('고객 수')\n",
    "                    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    # 변동계수 상위/하위 분포\n",
    "                    cv_top10 = volatility_df.nlargest(10, 'cv_basic')['cv_basic']\n",
    "                    cv_bottom10 = volatility_df.nsmallest(10, 'cv_basic')['cv_basic']\n",
    "                    \n",
    "                    x_pos = range(10)\n",
    "                    width = 0.35\n",
    "                    axes[1, 1].bar([x - width/2 for x in x_pos], cv_top10.values, width, \n",
    "                                  label='상위 10명', alpha=0.8, color='red')\n",
    "                    axes[1, 1].bar([x + width/2 for x in x_pos], cv_bottom10.values, width, \n",
    "                                  label='하위 10명', alpha=0.8, color='blue')\n",
    "                    axes[1, 1].set_title('변동계수 상위/하위 10명 비교', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 1].set_xlabel('순위')\n",
    "                    axes[1, 1].set_ylabel('변동계수')\n",
    "                    axes[1, 1].legend()\n",
    "                    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # 이미지 저장\n",
    "                    output_file = os.path.join(self.output_dir, 'volatility_analysis_summary.png')\n",
    "                    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    print(f\"   💾 변동성 분석 시각화 저장: {output_file}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 시각화 생성 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"종합 분석 리포트 생성\"\"\"\n",
    "        print(\"\\\\n📋 7단계: 종합 분석 리포트 생성...\")\n",
    "        \n",
    "        report_file = os.path.join(self.output_dir, 'comprehensive_analysis_report.txt')\n",
    "        \n",
    "        try:\n",
    "            with open(report_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\\\\n\")\n",
    "                f.write(\"시계열 패턴 분석 및 변동성 지표 개발 결과 리포트\\\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\\\n\\\\n\")\n",
    "                \n",
    "                # 1. 분석 개요\n",
    "                f.write(\"1. 분석 개요\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(f\"분석 일시: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "                f.write(f\"고객 수: {self.analysis_results.get('customer_summary', {}).get('total_customers', 'N/A'):,}명\\\\n\")\n",
    "                f.write(f\"LP 레코드: {self.analysis_results.get('data_quality', {}).get('total_records', 'N/A'):,}개\\\\n\")\n",
    "                f.write(f\"분석 대상 고객: {self.analysis_results.get('data_quality', {}).get('customers', 'N/A')}명\\\\n\")\n",
    "                \n",
    "                date_range = self.analysis_results.get('data_quality', {}).get('date_range', {})\n",
    "                if date_range:\n",
    "                    f.write(f\"데이터 기간: {date_range.get('start', 'N/A')} ~ {date_range.get('end', 'N/A')}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 2. 시계열 패턴 분석 결과\n",
    "                f.write(\"2. 시계열 패턴 분석 결과\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                temporal = self.analysis_results.get('temporal_patterns', {})\n",
    "                if temporal:\n",
    "                    f.write(f\"피크 시간대: {temporal.get('peak_hours', [])}\\\\n\")\n",
    "                    f.write(f\"비피크 시간대: {temporal.get('off_peak_hours', [])}\\\\n\")\n",
    "                    f.write(f\"주말/평일 사용량 비율: {temporal.get('weekend_ratio', 0):.3f}\\\\n\")\n",
    "                    \n",
    "                    # 계절별 패턴\n",
    "                    seasonal = temporal.get('seasonal_patterns', {})\n",
    "                    if seasonal:\n",
    "                        f.write(\"\\\\n계절별 평균 사용량:\\\\n\")\n",
    "                        for season in ['봄', '여름', '가을', '겨울']:\n",
    "                            if season in seasonal and 'mean' in seasonal[season]:\n",
    "                                f.write(f\"  {season}: {seasonal[season]['mean']:.2f}kW\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 3. 변동성 지표 분석 결과\n",
    "                f.write(\"3. 변동성 지표 분석 결과\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                volatility = self.analysis_results.get('volatility_analysis', {})\n",
    "                if volatility:\n",
    "                    summary_stats = volatility.get('summary_stats', {})\n",
    "                    cv_stats = summary_stats.get('cv_basic', {})\n",
    "                    \n",
    "                    if cv_stats:\n",
    "                        f.write(\"기본 변동계수(CV) 통계:\\\\n\")\n",
    "                        f.write(f\"  평균: {cv_stats.get('mean', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  표준편차: {cv_stats.get('std', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  최솟값: {cv_stats.get('min', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  최댓값: {cv_stats.get('max', 0):.4f}\\\\n\")\n",
    "                        \n",
    "                    quartiles = volatility.get('quartiles', {})\n",
    "                    if quartiles:\n",
    "                        f.write(\"\\\\n변동계수 사분위수:\\\\n\")\n",
    "                        f.write(f\"  Q1 (25%): {quartiles.get(0.25, 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  Q2 (50%): {quartiles.get(0.5, 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  Q3 (75%): {quartiles.get(0.75, 0):.4f}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 4. 이상 패턴 탐지 결과\n",
    "                f.write(\"4. 이상 패턴 탐지 결과\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                anomaly = self.analysis_results.get('anomaly_analysis', {})\n",
    "                if anomaly:\n",
    "                    total_anomaly = anomaly.get('total_anomaly_customers', 0)\n",
    "                    anomaly_rate = anomaly.get('anomaly_rate', 0) * 100\n",
    "                    f.write(f\"이상 패턴 고객: {total_anomaly}명 ({anomaly_rate:.1f}%)\\\\n\")\n",
    "                    \n",
    "                    anomaly_types = anomaly.get('anomaly_types', {})\n",
    "                    f.write(\"\\\\n이상 패턴 유형별 분포:\\\\n\")\n",
    "                    for pattern_type, count in anomaly_types.items():\n",
    "                        f.write(f\"  {pattern_type}: {count}명\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 5. 변동계수 개발을 위한 인사이트\n",
    "                f.write(\"5. 변동계수 개발을 위한 핵심 인사이트\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"가. 시간대별 차별화 필요성:\\\\n\")\n",
    "                f.write(\"   - 피크/비피크 시간대별 가중치 적용\\\\n\")\n",
    "                f.write(\"   - 야간 시간대 이상 사용 패턴 별도 처리\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"나. 요일별 패턴 고려:\\\\n\")\n",
    "                f.write(\"   - 주말/평일 사용 패턴 차이 반영\\\\n\")\n",
    "                f.write(\"   - 요일별 변동성 가중치 조정\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"다. 계절성 보정:\\\\n\")\n",
    "                f.write(\"   - 월별/계절별 기준값 차별화\\\\n\")\n",
    "                f.write(\"   - 외부 기상 데이터 연계 고려\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"라. 다차원 변동성 지표:\\\\n\")\n",
    "                f.write(\"   - 기본 변동계수(CV) 외 추가 지표 활용\\\\n\")\n",
    "                f.write(\"   - 시간 윈도우별 변동성 조합\\\\n\")\n",
    "                f.write(\"   - 방향성 변동성 고려\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"마. 이상 패턴 필터링:\\\\n\")\n",
    "                f.write(\"   - 급격한 변화 및 장기간 0값 처리\\\\n\")\n",
    "                f.write(\"   - 통계적 이상치 제거 알고리즘\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 6. 다음 단계 권장사항\n",
    "                f.write(\"6. 다음 단계 권장사항\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"1. 업종별 변동계수 기준값 설정\\\\n\")\n",
    "                f.write(\"   - 계약종별/사용용도별 임계값 차별화\\\\n\")\n",
    "                f.write(\"   - 업종 특성 반영한 가중치 설계\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"2. 스태킹 알고리즘 개발\\\\n\")\n",
    "                f.write(\"   - Level-0: 개별 변동성 지표 모델\\\\n\")\n",
    "                f.write(\"   - Level-1: 메타모델을 통한 통합 변동계수\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"3. 외부 데이터 연계\\\\n\")\n",
    "                f.write(\"   - 기상청 기상 데이터 (온도, 습도 등)\\\\n\")\n",
    "                f.write(\"   - 경제 지표 및 업종별 운영 현황\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"4. 실시간 모니터링 시스템\\\\n\")\n",
    "                f.write(\"   - 변동계수 임계값 기반 알림 시스템\\\\n\")\n",
    "                f.write(\"   - 이상 패턴 자동 탐지 및 보고\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"5. 성능 검증 및 최적화\\\\n\")\n",
    "                f.write(\"   - 교차검증을 통한 모델 성능 평가\\\\n\")\n",
    "                f.write(\"   - 하이퍼파라미터 튜닝 및 최적화\\\\n\")\n",
    "                \n",
    "            print(f\"   💾 종합 리포트 저장: {report_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 리포트 생성 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def save_analysis_results(self):\n",
    "        \"\"\"분석 결과를 JSON 파일로 저장\"\"\"\n",
    "        print(\"\\\\n💾 8단계: 분석 결과 저장...\")\n",
    "        \n",
    "        try:\n",
    "            # JSON으로 저장 가능한 형태로 변환\n",
    "            results_for_json = {}\n",
    "            \n",
    "            for key, value in self.analysis_results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    results_for_json[key] = {}\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        if hasattr(sub_value, 'to_dict'):  # pandas 객체인 경우\n",
    "                            results_for_json[key][sub_key] = sub_value.to_dict()\n",
    "                        else:\n",
    "                            results_for_json[key][sub_key] = sub_value\n",
    "                else:\n",
    "                    results_for_json[key] = value\n",
    "            \n",
    "            # JSON 파일로 저장\n",
    "            output_file = os.path.join(self.output_dir, 'analysis_results.json')\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results_for_json, f, ensure_ascii=False, indent=2, default=str)\n",
    "            \n",
    "            print(f\"   💾 분석 결과 JSON 저장: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 분석 결과 저장 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"전체 분석 프로세스 실행\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"🚀 한국전력공사 LP 데이터 시계열 패턴 분석 시작\")\n",
    "        print(f\"시작 시간: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # 1. 고객 데이터 로딩\n",
    "            if not self.load_customer_data():\n",
    "                print(\"❌ 고객 데이터 로딩 실패로 분석을 중단합니다.\")\n",
    "                return False\n",
    "            \n",
    "            # 2. LP 데이터 로딩\n",
    "            if not self.load_lp_data():\n",
    "                print(\"❌ LP 데이터 로딩 실패로 분석을 중단합니다.\")\n",
    "                return False\n",
    "            \n",
    "            # 3. 시계열 패턴 분석\n",
    "            if not self.analyze_temporal_patterns():\n",
    "                print(\"❌ 시계열 패턴 분석 실패\")\n",
    "                return False\n",
    "            \n",
    "            # 4. 변동성 지표 분석\n",
    "            cv_array = self.analyze_volatility_indicators()\n",
    "            if cv_array is None or len(cv_array) == 0:\n",
    "                print(\"❌ 변동성 지표 분석 실패\")\n",
    "                return False\n",
    "            \n",
    "            # 5. 이상 패턴 탐지\n",
    "            anomaly_summary = self.detect_anomalies()\n",
    "            if anomaly_summary is None:\n",
    "                print(\"❌ 이상 패턴 탐지 실패\")\n",
    "                return False\n",
    "            \n",
    "            # 6. 시각화 생성\n",
    "            self.create_summary_visualizations()\n",
    "            \n",
    "            # 7. 종합 리포트 생성\n",
    "            self.generate_comprehensive_report()\n",
    "            \n",
    "            # 8. 결과 저장\n",
    "            self.save_analysis_results()\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            print(\"\\\\n\" + \"=\" * 80)\n",
    "            print(\"🎉 시계열 패턴 분석 완료!\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"소요 시간: {duration}\")\n",
    "            print(f\"결과 저장 위치: {self.output_dir}\")\n",
    "            \n",
    "            print(\"\\\\n📁 생성된 파일:\")\n",
    "            output_files = [\n",
    "                'comprehensive_analysis_report.txt',\n",
    "                'analysis_results.json', \n",
    "                'volatility_summary.csv',\n",
    "                'temporal_patterns_summary.png',\n",
    "                'volatility_analysis_summary.png',\n",
    "                'volatility_distribution.png'\n",
    "            ]\n",
    "            \n",
    "            for file in output_files:\n",
    "                file_path = os.path.join(self.output_dir, file)\n",
    "                if os.path.exists(file_path):\n",
    "                    file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "                    print(f\"   ✅ {file} ({file_size:.1f}KB)\")\n",
    "                else:\n",
    "                    print(f\"   ❌ {file}\")\n",
    "            \n",
    "            print(\"\\\\n🎯 다음 단계: 변동계수 알고리즘 설계\")\n",
    "            print(\"   1. 업종별 특성 반영한 가중치 설계\")\n",
    "            print(\"   2. 스태킹 모델 Level-0 구현\")\n",
    "            print(\"   3. 메타모델 개발 및 최적화\")\n",
    "            print(\"   4. 외부 데이터 연계 방안 수립\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\\\n❌ 분석 중 오류 발생: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    # 데이터안심구역 환경에 맞는 경로 설정\n",
    "    base_paths = [\n",
    "        '/data/kepco',           # 데이터안심구역 기본 경로\n",
    "        '/home/user/kepco_data', # 사용자 홈 디렉토리\n",
    "        './data',                # 상대 경로\n",
    "        '.'                      # 현재 디렉토리\n",
    "    ]\n",
    "    \n",
    "    # 존재하는 경로 찾기\n",
    "    data_path = None\n",
    "    for path in base_paths:\n",
    "        if os.path.exists(path):\n",
    "            data_path = path\n",
    "            break\n",
    "    \n",
    "    if data_path is None:\n",
    "        print(\"❌ 데이터 디렉토리를 찾을 수 없습니다.\")\n",
    "        print(\"다음 경로 중 하나에 데이터를 배치해주세요:\")\n",
    "        for path in base_paths:\n",
    "            print(f\"   - {path}\")\n",
    "        print(\"\\\\n필요한 파일:\")\n",
    "        print(\"   - 제13회 산업부 공모전 대상고객.xlsx\")\n",
    "        print(\"   - processed_LPData_*.csv (여러 파일)\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📂 데이터 경로: {data_path}\")\n",
    "    \n",
    "    # 필수 파일 존재 확인\n",
    "    customer_file = os.path.join(data_path, '제13회 산업부 공모전 대상고객.xlsx')\n",
    "    lp_files = glob.glob(os.path.join(data_path, 'processed_LPData_*.csv'))\n",
    "    \n",
    "    if not os.path.exists(customer_file):\n",
    "        print(f\"❌ 고객 데이터 파일이 없습니다: {customer_file}\")\n",
    "        return\n",
    "    \n",
    "    if not lp_files:\n",
    "        print(f\"❌ LP 데이터 파일이 없습니다: {data_path}/processed_LPData_*.csv\")\n",
    "        return\n",
    "    \n",
    "    print(f\"✅ 고객 데이터 파일 확인: {customer_file}\")\n",
    "    print(f\"✅ LP 데이터 파일 확인: {len(lp_files)}개\")\n",
    "    \n",
    "    # 분석 실행\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer(base_path=data_path)\n",
    "    success = analyzer.run_complete_analysis()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\\\n🏆 분석이 성공적으로 완료되었습니다!\")\n",
    "    else:\n",
    "        print(\"\\\\n💥 분석 중 오류가 발생했습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
