{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c60571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ì‹¤ì œ í™˜ê²½ì—ì„œ ì‚¬ìš© ë°©ë²• (3000í˜¸):\n",
      "\n",
      "    # ì‹¤ì œ LP ë°ì´í„°ë§Œ ë¡œë”©í•˜ë©´ ë!\n",
      "    lp_files = ['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv']\n",
      "    analyzer.load_real_lp_data(lp_files)\n",
      "    \n",
      "    # ì‹¤ì œ ë°ì´í„°ë¡œ íŒ¨í„´ ë¶„ì„\n",
      "    enhanced_summary = analyzer.generate_enhanced_pattern_summary()\n",
      "    \n",
      "\n",
      "==================================================\n",
      "ğŸ§ª í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹œì—°\n",
      "==================================================\n",
      "=== í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± ===\n",
      "âš ï¸  ì£¼ì˜: ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” load_real_lp_data() ì‚¬ìš©\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: 29,760ë ˆì½”ë“œ\n",
      "ê¸°ê°„: 2024-03-01 00:00:00 ~ 2024-03-31 23:45:00\n",
      "ê³ ê° ìˆ˜: 10ëª…\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\n",
      "============================================================\n",
      "\n",
      "=== ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ===\n",
      "ğŸ“Š ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (kW):\n",
      "ì‹œê°„\tí‰ê· \tí‘œì¤€í¸ì°¨\tìµœì†Œ\tìµœëŒ€\n",
      "00ì‹œ\t58.3\t23.8\t7.4\t122.1\n",
      "01ì‹œ\t66.2\t27.4\t8.6\t133.2\n",
      "02ì‹œ\t73.7\t30.4\t10.1\t159.4\n",
      "03ì‹œ\t79.8\t32.5\t9.8\t159.2\n",
      "04ì‹œ\t84.9\t34.9\t11.7\t170.7\n",
      "05ì‹œ\t88.1\t36.3\t10.6\t180.2\n",
      "06ì‹œ\t92.8\t37.2\t12.3\t190.1\n",
      "07ì‹œ\t104.0\t56.2\t16.1\t287.0\n",
      "08ì‹œ\t111.3\t54.2\t15.5\t262.6\n",
      "09ì‹œ\t107.8\t51.5\t15.4\t255.9\n",
      "10ì‹œ\t93.6\t38.3\t10.3\t183.9\n",
      "11ì‹œ\t79.8\t34.9\t8.2\t152.7\n",
      "12ì‹œ\t68.0\t34.5\t11.3\t180.6\n",
      "13ì‹œ\t62.8\t31.1\t9.4\t154.7\n",
      "14ì‹œ\t55.2\t26.5\t8.1\t135.4\n",
      "15ì‹œ\t45.7\t18.5\t4.7\t91.4\n",
      "16ì‹œ\t35.0\t14.1\t4.3\t77.3\n",
      "17ì‹œ\t32.9\t14.9\t5.4\t90.5\n",
      "18ì‹œ\t32.4\t15.9\t4.8\t89.6\n",
      "19ì‹œ\t37.7\t19.1\t5.4\t83.4\n",
      "20ì‹œ\t38.2\t19.0\t3.9\t87.0\n",
      "21ì‹œ\t42.4\t21.2\t5.4\t98.7\n",
      "22ì‹œ\t44.0\t17.7\t6.0\t85.8\n",
      "23ì‹œ\t50.3\t20.8\t6.2\t110.5\n",
      "\n",
      "âš¡ í”¼í¬ ì‹œê°„ëŒ€ (ìƒìœ„ 20%): [6, 7, 8, 9, 10]ì‹œ\n",
      "ğŸ’¤ ë¹„í”¼í¬ ì‹œê°„ëŒ€ (í•˜ìœ„ 30%): [16, 17, 18, 19, 20, 21, 22]ì‹œ\n",
      "\n",
      "=== ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ===\n",
      "ğŸ“… ìš”ì¼ë³„ í‰ê·  ì¼ê°„ ì‚¬ìš©ëŸ‰ (kWh):\n",
      "ì›”ìš”ì¼: 7,064.2 Â± 2006.4\n",
      "í™”ìš”ì¼: 7,066.2 Â± 1988.1\n",
      "ìˆ˜ìš”ì¼: 7,073.5 Â± 2008.9\n",
      "ëª©ìš”ì¼: 7,075.3 Â± 2010.7\n",
      "ê¸ˆìš”ì¼: 7,076.1 Â± 2001.6\n",
      "í† ìš”ì¼: 4,802.9 Â± 2989.1\n",
      "ì¼ìš”ì¼: 4,799.4 Â± 2982.6\n",
      "\n",
      "ğŸ“Š í‰ì¼ vs ì£¼ë§ ë¹„êµ:\n",
      "í‰ì¼ í‰ê· : 7,071.3 kWh\n",
      "ì£¼ë§ í‰ê· : 4,801.1 kWh\n",
      "ì£¼ë§/í‰ì¼ ë¹„ìœ¨: 0.68\n",
      "\n",
      "=== ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„ ===\n",
      "ğŸ‘¥ ê³ ê°ë³„ ê¸°ë³¸ í†µê³„ (kW):\n",
      "ê³ ê°ë²ˆí˜¸\tí‰ê· \tí‘œì¤€í¸ì°¨\të³€ë™ê³„ìˆ˜\tìµœì†Œ\tìµœëŒ€\n",
      "A1001\t85.2\t36.3\t0.426\t23.6\t216.2\n",
      "A1002\t45.7\t29.6\t0.648\t5.5\t143.5\n",
      "A1003\t78.9\t28.9\t0.366\t23.9\t183.9\n",
      "A1004\t81.1\t63.8\t0.787\t3.9\t287.0\n",
      "A1005\t66.0\t24.4\t0.37\t23.5\t131.3\n",
      "A1006\t57.9\t26.3\t0.454\t18.6\t156.4\n",
      "A1007\t59.7\t41.3\t0.692\t4.8\t203.4\n",
      "A1008\t95.3\t44.5\t0.467\t28.4\t246.4\n",
      "A1009\t36.7\t19.7\t0.537\t7.6\t107.7\n",
      "A1010\t53.9\t26.0\t0.482\t11.4\t148.7\n",
      "\n",
      "ğŸ“ˆ ëŒ€ìš©ëŸ‰ ì‚¬ìš©ì (ìƒìœ„ 20%): ['A1001', 'A1008']\n",
      "ğŸ“‰ ì†Œìš©ëŸ‰ ì‚¬ìš©ì (í•˜ìœ„ 20%): ['A1002', 'A1009']\n",
      "\n",
      "ğŸŒŠ ê³ ë³€ë™ì„± ê³ ê°: ['A1004', 'A1007']\n",
      "ğŸ“Š ì €ë³€ë™ì„± ê³ ê°: ['A1003', 'A1005']\n",
      "\n",
      "=== ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ===\n",
      "âš¡ ê³ ê°ë³„ ë¶€í•˜ìœ¨ ë° í”¼í¬ ì§‘ì¤‘ë„:\n",
      "ê³ ê°ë²ˆí˜¸\të¶€í•˜ìœ¨\tí”¼í¬ì§‘ì¤‘ë„\tí‰ê· ë¶€í•˜\tìµœëŒ€ë¶€í•˜\n",
      "A1001\t0.394\t1.041\t85.2\t216.2\n",
      "A1002\t0.319\t1.064\t45.7\t143.5\n",
      "A1003\t0.429\t0.923\t78.9\t183.9\n",
      "A1004\t0.283\t1.103\t81.1\t287.0\n",
      "A1005\t0.502\t0.851\t66.0\t131.3\n",
      "A1006\t0.37\t0.803\t57.9\t156.4\n",
      "A1007\t0.294\t1.072\t59.7\t203.4\n",
      "A1008\t0.387\t0.958\t95.3\t246.4\n",
      "A1009\t0.34\t0.967\t36.7\t107.7\n",
      "A1010\t0.362\t1.081\t53.9\t148.7\n",
      "\n",
      "ğŸ“Š ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬:\n",
      "í‰ê·  ë¶€í•˜ìœ¨: 0.368\n",
      "ë¶€í•˜ìœ¨ ë²”ìœ„: 0.283 ~ 0.502\n",
      "\n",
      "=== ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€ ===\n",
      "ğŸš¨ ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼:\n",
      "ê³ ê°ë²ˆí˜¸\tê¸‰ê²©ë³€í™”\tì¥ê¸°0ê°’\tí†µê³„ì´ìƒì¹˜\n",
      "A1001\t0\t0\t7\n",
      "A1002\t4\t0\t6\n",
      "A1003\t0\t0\t4\n",
      "A1004\t4\t0\t3\n",
      "A1006\t0\t0\t35\n",
      "A1007\t4\t0\t1\n",
      "A1008\t0\t0\t9\n",
      "A1009\t0\t0\t17\n",
      "A1010\t0\t0\t7\n",
      "\n",
      "ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\n",
      "  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: [6, 7, 8, 9, 10]ì‹œ\n",
      "  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: 0.68\n",
      "  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: 0.366 ~ 0.787\n",
      "  â€¢ í‰ê·  ë¶€í•˜ìœ¨: 0.368\n",
      "  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: 9ëª…\n",
      "\n",
      "ğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\n",
      "  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\n",
      "  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\n",
      "  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\n",
      "  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\n",
      "  5. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \n",
      "\n",
      "ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\n",
      "  âœ… ì‹¤ì œ í™˜ê²½: ì‹¤ì œ LP ë°ì´í„° ê·¸ëŒ€ë¡œ ë¶„ì„\n",
      "  âœ… í…ŒìŠ¤íŠ¸ìš©: ìƒ˜í”Œ ë°ì´í„°ë¡œ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦\n",
      "  âœ… 3000í˜¸: ì‹¤ì œ ë°ì´í„° ìˆìœ¼ë‹ˆ í”„ë¡œíŒŒì¼ ìƒì„± ë¶ˆí•„ìš”\n",
      "  âœ… íŒ¨í„´ ë¶„ì„: ì‹¤ì œ ì‚¬ìš©ëŸ‰ì—ì„œ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.weather_data = None\n",
    "        self.calendar_data = None\n",
    "        \n",
    "    def load_sample_data(self, customer_df=None):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ LP ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨)\"\"\"\n",
    "        print(\"=== í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± ===\")\n",
    "        print(\"âš ï¸  ì£¼ì˜: ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” load_real_lp_data() ì‚¬ìš©\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "        self.lp_data = self._create_comprehensive_sample_data(customer_df)\n",
    "        \n",
    "        # ë‚ ì§œ/ì‹œê°„ ì „ì²˜ë¦¬\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "        \n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "        print(f\"ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "        print(f\"ê³ ê° ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def load_real_lp_data(self, lp_files):\n",
    "        \"\"\"ì‹¤ì œ LP ë°ì´í„° ë¡œë”© (ì‹¤ì œ í™˜ê²½ì—ì„œ ì‚¬ìš©)\"\"\"\n",
    "        print(\"=== ì‹¤ì œ LP ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        lp_data_list = []\n",
    "        \n",
    "        for file_path in lp_files:\n",
    "            print(f\"ğŸ“‚ ë¡œë”© ì¤‘: {file_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                lp_data_list.append(df)\n",
    "                print(f\"   âœ… ì™„ë£Œ: {len(df):,}ë ˆì½”ë“œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        if lp_data_list:\n",
    "            self.lp_data = pd.concat(lp_data_list, ignore_index=True)\n",
    "            \n",
    "            # ë‚ ì§œ/ì‹œê°„ ì „ì²˜ë¦¬\n",
    "            self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "            self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "            self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "            self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "            self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "            self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "            \n",
    "            print(f\"âœ… ì „ì²´ ë°ì´í„° ê²°í•© ì™„ë£Œ: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "            print(f\"ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"ê³ ê° ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def load_external_data(self):\n",
    "        \"\"\"ê¸°ìƒ ë° ë‹¬ë ¥ ë°ì´í„° ë¡œë”©\"\"\"\n",
    "        print(\"\\n=== ì™¸ë¶€ ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        try:\n",
    "            # ê¸°ìƒ ë°ì´í„° ë¡œë”©\n",
    "            print(\"ğŸ“Š ê¸°ìƒ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "            self.weather_data = pd.read_csv('weather_daily_processed.csv')\n",
    "            \n",
    "            # ë‚ ì§œ ì»¬ëŸ¼ ì „ì²˜ë¦¬\n",
    "            self.weather_data['date'] = pd.to_datetime(self.weather_data['ë‚ ì§œ'])\n",
    "            \n",
    "            print(f\"âœ… ê¸°ìƒ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.weather_data):,}ì¼\")\n",
    "            print(f\"   ê¸°ê°„: {self.weather_data['date'].min().date()} ~ {self.weather_data['date'].max().date()}\")\n",
    "            print(f\"   ì»¬ëŸ¼: {len(self.weather_data.columns)}ê°œ - {list(self.weather_data.columns[:5])}...\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"âš ï¸  ê¸°ìƒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (weather_daily_processed.csv)\")\n",
    "            self.weather_data = None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê¸°ìƒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.weather_data = None\n",
    "        \n",
    "        try:\n",
    "            # ë‹¬ë ¥ ë°ì´í„° ë¡œë”©\n",
    "            print(\"\\nğŸ“… ë‹¬ë ¥ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "            self.calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')\n",
    "            \n",
    "            # ë‚ ì§œ ì»¬ëŸ¼ ì „ì²˜ë¦¬\n",
    "            self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date'])\n",
    "            \n",
    "            print(f\"âœ… ë‹¬ë ¥ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.calendar_data):,}ì¼\")\n",
    "            print(f\"   ê¸°ê°„: {self.calendar_data['date'].min().date()} ~ {self.calendar_data['date'].max().date()}\")\n",
    "            print(f\"   ì»¬ëŸ¼: {len(self.calendar_data.columns)}ê°œ - {list(self.calendar_data.columns[:5])}...\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"âš ï¸  ë‹¬ë ¥ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (power_analysis_calendar_2022_2025.csv)\")\n",
    "            self.calendar_data = None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë‹¬ë ¥ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.calendar_data = None\n",
    "    \n",
    "    def _create_customer_profiles_from_data(self, customer_df=None):\n",
    "        \"\"\"ì‹¤ì œ ê³ ê° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ê° í”„ë¡œíŒŒì¼ ìƒì„±\"\"\"\n",
    "        profiles = {}\n",
    "        \n",
    "        if customer_df is not None:\n",
    "            # ì‹¤ì œ ê³ ê° ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°\n",
    "            for _, customer in customer_df.iterrows():\n",
    "                customer_id = customer['ê³ ê°ë²ˆí˜¸']\n",
    "                \n",
    "                # ê³„ì•½ì „ë ¥ì—ì„œ base_power ì¶”ì •\n",
    "                contract_power = customer['ê³„ì•½ì „ë ¥']\n",
    "                if '100~199' in str(contract_power):\n",
    "                    base_power = np.random.uniform(80, 120)\n",
    "                elif '200~299' in str(contract_power):\n",
    "                    base_power = np.random.uniform(120, 180)\n",
    "                elif '400~499' in str(contract_power):\n",
    "                    base_power = np.random.uniform(200, 280)\n",
    "                elif '500~599' in str(contract_power):\n",
    "                    base_power = np.random.uniform(280, 360)\n",
    "                elif '700~799' in str(contract_power):\n",
    "                    base_power = np.random.uniform(400, 500)\n",
    "                elif '800~899' in str(contract_power):\n",
    "                    base_power = np.random.uniform(500, 650)\n",
    "                else:\n",
    "                    base_power = np.random.uniform(100, 200)\n",
    "                \n",
    "                # ì‚¬ìš©ìš©ë„ë³„ íŒ¨í„´ ì •ì˜\n",
    "                usage_type = customer['ì‚¬ìš©ìš©ë„']\n",
    "                if 'ìƒì—…ìš©' in str(usage_type):\n",
    "                    peak_hours = [9, 14, 19]  # ìƒì—…ìš©: ì˜¤ì „, ì˜¤í›„, ì €ë… í”¼í¬\n",
    "                    weekend_factor = np.random.uniform(0.7, 1.3)  # ìƒì—…ìš©ì€ ì£¼ë§ ë³€ë™ í¼\n",
    "                else:  # ê´‘ê³µì—…ìš©\n",
    "                    peak_hours = [8, 13, 18]  # ì‚°ì—…ìš©: ì‘ì—…ì‹œê°„ í”¼í¬\n",
    "                    weekend_factor = np.random.uniform(0.1, 0.4)  # ì‚°ì—…ìš©ì€ ì£¼ë§ ë‚®ìŒ\n",
    "                \n",
    "                # ê³„ì•½ì¢…ë³„ ì„¸ë¶€ ì¡°ì •\n",
    "                contract_type = customer['ê³„ì•½ì¢…ë³„']\n",
    "                if 'ì¼ë°˜ìš©' in str(contract_type):\n",
    "                    # ì¼ë°˜ìš©ì€ ë” ë¶ˆê·œì¹™í•œ íŒ¨í„´\n",
    "                    weekend_factor *= np.random.uniform(0.8, 1.5)\n",
    "                \n",
    "                # ì‚°ì—…ë¶„ë¥˜ë³„ íŠ¹ì„± ë°˜ì˜ (ìˆëŠ” ê²½ìš°)\n",
    "                industry = customer.get('ì‚°ì—…ë¶„ë¥˜(ì†Œ)', '')\n",
    "                if 'ì œì¡°' in str(industry) or 'ìƒì‚°' in str(industry):\n",
    "                    # ì œì¡°ì—…ì€ ë” ì¼ì •í•œ íŒ¨í„´\n",
    "                    weekend_factor *= 0.3\n",
    "                    peak_hours = [8, 13, 18, 22]  # êµëŒ€ê·¼ë¬´ ê³ ë ¤\n",
    "                \n",
    "                profiles[customer_id] = {\n",
    "                    'type': self._classify_business_type(customer),\n",
    "                    'base_power': round(base_power, 1),\n",
    "                    'peak_hours': peak_hours,\n",
    "                    'weekend_factor': round(weekend_factor, 2),\n",
    "                    'contract_power': contract_power,\n",
    "                    'usage_type': usage_type\n",
    "                }\n",
    "        else:\n",
    "            # ìƒ˜í”Œ ë°ì´í„°ìš© ê¸°ë³¸ í”„ë¡œíŒŒì¼\n",
    "            profiles = self._get_default_sample_profiles()\n",
    "        \n",
    "        return profiles\n",
    "    \n",
    "    def _classify_business_type(self, customer):\n",
    "        \"\"\"ê³ ê° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—…ì¢… ë¶„ë¥˜\"\"\"\n",
    "        usage_type = str(customer['ì‚¬ìš©ìš©ë„'])\n",
    "        contract_type = str(customer['ê³„ì•½ì¢…ë³„'])\n",
    "        industry = str(customer.get('ì‚°ì—…ë¶„ë¥˜(ì†Œ)', ''))\n",
    "        product = str(customer.get('ì£¼ìƒì‚°í’ˆ', ''))\n",
    "        \n",
    "        # ì‚¬ìš©ìš©ë„ ê¸°ë°˜ 1ì°¨ ë¶„ë¥˜\n",
    "        if 'ìƒì—…ìš©' in usage_type:\n",
    "            if 'ì¼ë°˜ìš©' in contract_type:\n",
    "                return 'commercial'  # ìƒê°€, ì‚¬ë¬´ì†Œ ë“±\n",
    "            else:\n",
    "                return 'service'     # ëŒ€í˜• ì„œë¹„ìŠ¤ì—…\n",
    "        else:  # ê´‘ê³µì—…ìš©\n",
    "            if any(keyword in industry.lower() for keyword in ['ì œì¡°', 'ìƒì‚°', 'ê³µì¥']):\n",
    "                return 'manufacturing'\n",
    "            else:\n",
    "                return 'industrial'\n",
    "        \n",
    "    def _get_default_sample_profiles(self):\n",
    "        \"\"\"ìƒ˜í”Œ ë°ì´í„°ìš© ê¸°ë³¸ í”„ë¡œíŒŒì¼\"\"\"\n",
    "        return {\n",
    "            'A1001': {'type': 'hospital', 'base_power': 120, 'peak_hours': [9, 14, 20], 'weekend_factor': 0.8},\n",
    "            'A1002': {'type': 'office', 'base_power': 80, 'peak_hours': [9, 14], 'weekend_factor': 0.3},\n",
    "            'A1003': {'type': 'retail', 'base_power': 100, 'peak_hours': [11, 15, 19], 'weekend_factor': 1.2},\n",
    "            'A1004': {'type': 'factory', 'base_power': 150, 'peak_hours': [8, 13, 18], 'weekend_factor': 0.1},\n",
    "            'A1005': {'type': 'restaurant', 'base_power': 90, 'peak_hours': [12, 18], 'weekend_factor': 1.1},\n",
    "            'A1006': {'type': 'gym', 'base_power': 70, 'peak_hours': [7, 18, 21], 'weekend_factor': 1.3},\n",
    "            'A1007': {'type': 'school', 'base_power': 110, 'peak_hours': [10, 14], 'weekend_factor': 0.2},\n",
    "            'A1008': {'type': 'hotel', 'base_power': 130, 'peak_hours': [8, 20], 'weekend_factor': 1.0},\n",
    "            'A1009': {'type': 'warehouse', 'base_power': 60, 'peak_hours': [9, 16], 'weekend_factor': 0.5},\n",
    "            'A1010': {'type': 'clinic', 'base_power': 85, 'peak_hours': [10, 15], 'weekend_factor': 0.6}\n",
    "        }\n",
    "\n",
    "    def _create_comprehensive_sample_data(self, customer_df=None):\n",
    "        \"\"\"í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31  # 3ì›” ì „ì²´\n",
    "        \n",
    "        # ë™ì  ê³ ê° í”„ë¡œíŒŒì¼ ìƒì„±\n",
    "        customer_profiles = self._create_customer_profiles_from_data(customer_df)\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = customer_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                is_weekend = current_date.weekday() >= 5\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ê¸°ë³¸ ì „ë ¥ ê³„ì‚°\n",
    "                        base_power = profile['base_power']\n",
    "                        \n",
    "                        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ (ì‚¬ì¸íŒŒ ê¸°ë°˜)\n",
    "                        time_factor = 0.3 + 0.7 * (np.sin(2 * np.pi * hour / 24) + 1) / 2\n",
    "                        \n",
    "                        # í”¼í¬ ì‹œê°„ ë³´ì •\n",
    "                        peak_factor = 1.0\n",
    "                        for peak_hour in profile['peak_hours']:\n",
    "                            if abs(hour - peak_hour) <= 1:\n",
    "                                peak_factor = 1.5\n",
    "                        \n",
    "                        # ì£¼ë§ ë³´ì •\n",
    "                        weekend_factor = profile['weekend_factor'] if is_weekend else 1.0\n",
    "                        \n",
    "                        # ìµœì¢… ì „ë ¥ ê³„ì‚°\n",
    "                        power = base_power * time_factor * peak_factor * weekend_factor\n",
    "                        power += np.random.normal(0, power * 0.1)  # 10% ë…¸ì´ì¦ˆ\n",
    "                        power = max(0, power)\n",
    "                        \n",
    "                        # ë¬´íš¨ì „ë ¥ ê³„ì‚°\n",
    "                        reactive_lag = power * np.random.uniform(0.1, 0.3)\n",
    "                        reactive_lead = power * np.random.uniform(0.05, 0.15)\n",
    "                        apparent_power = np.sqrt(power**2 + (reactive_lag - reactive_lead)**2)\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(reactive_lag, 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(reactive_lead, 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(apparent_power, 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def merge_with_external_data(self):\n",
    "        \"\"\"LP ë°ì´í„°ì™€ ì™¸ë¶€ ë°ì´í„° ê²°í•©\"\"\"\n",
    "        if self.lp_data is None:\n",
    "            print(\"âŒ LP ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•ŠìŒ\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n=== ì™¸ë¶€ ë°ì´í„°ì™€ ê²°í•© ===\")\n",
    "        \n",
    "        # ì¼ë³„ LP ë°ì´í„° ì§‘ê³„\n",
    "        daily_lp = self.lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date'])['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "        daily_lp.columns = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date', 'daily_sum', 'daily_mean', 'daily_std', 'daily_min', 'daily_max']\n",
    "        daily_lp['date'] = pd.to_datetime(daily_lp['date'])\n",
    "        \n",
    "        # ê¸°ìƒ ë°ì´í„°ì™€ ê²°í•©\n",
    "        if self.weather_data is not None:\n",
    "            print(\"ğŸŒ¤ï¸  ê¸°ìƒ ë°ì´í„° ê²°í•© ì¤‘...\")\n",
    "            merged_data = daily_lp.merge(\n",
    "                self.weather_data[['date', 'í‰ê· ê¸°ì˜¨', 'ìµœê³ ê¸°ì˜¨', 'ìµœì €ê¸°ì˜¨', 'í‰ê· ìŠµë„', 'ì´ê°•ìˆ˜ëŸ‰', 'ë¶ˆì¾Œì§€ìˆ˜', 'ëƒ‰ë°©í•„ìš”ë„', 'ë‚œë°©í•„ìš”ë„']],\n",
    "                on='date',\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"   âœ… ê¸°ìƒ ë°ì´í„° ê²°í•© ì™„ë£Œ: {merged_data['í‰ê· ê¸°ì˜¨'].notna().sum():,}ì¼ ë§¤ì¹­\")\n",
    "        else:\n",
    "            merged_data = daily_lp.copy()\n",
    "        \n",
    "        # ë‹¬ë ¥ ë°ì´í„°ì™€ ê²°í•©\n",
    "        if self.calendar_data is not None:\n",
    "            print(\"ğŸ“… ë‹¬ë ¥ ë°ì´í„° ê²°í•© ì¤‘...\")\n",
    "            merged_data = merged_data.merge(\n",
    "                self.calendar_data[['date', 'is_workday', 'is_weekend', 'is_holiday', 'is_consecutive_holiday', 'day_type']],\n",
    "                on='date',\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"   âœ… ë‹¬ë ¥ ë°ì´í„° ê²°í•© ì™„ë£Œ: {merged_data['is_workday'].notna().sum():,}ì¼ ë§¤ì¹­\")\n",
    "        \n",
    "        # ê²°í•©ëœ ë°ì´í„° ì €ì¥\n",
    "        self.merged_daily_data = merged_data\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ê²°í•©ëœ ë°ì´í„° ìš”ì•½:\")\n",
    "        print(f\"   ì´ ë ˆì½”ë“œ: {len(merged_data):,}ê°œ\")\n",
    "        print(f\"   ê³ ê° ìˆ˜: {merged_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique():,}ëª…\")\n",
    "        print(f\"   ê¸°ê°„: {merged_data['date'].min().date()} ~ {merged_data['date'].max().date()}\")\n",
    "        print(f\"   ì»¬ëŸ¼ ìˆ˜: {len(merged_data.columns)}ê°œ\")\n",
    "        \n",
    "        return merged_data\n",
    "    \n",
    "    def analyze_weather_impact(self):\n",
    "        \"\"\"ê¸°ìƒ ìš”ì¸ì´ ì „ë ¥ ì‚¬ìš©ëŸ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„\"\"\"\n",
    "        if not hasattr(self, 'merged_daily_data') or self.weather_data is None:\n",
    "            print(\"âš ï¸  ê¸°ìƒ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ìƒ ì˜í–¥ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n=== ê¸°ìƒ ìš”ì¸ ì˜í–¥ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê¸°ìƒ ë³€ìˆ˜ë³„ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "        weather_cols = ['í‰ê· ê¸°ì˜¨', 'ìµœê³ ê¸°ì˜¨', 'ìµœì €ê¸°ì˜¨', 'í‰ê· ìŠµë„', 'ì´ê°•ìˆ˜ëŸ‰', 'ë¶ˆì¾Œì§€ìˆ˜', 'ëƒ‰ë°©í•„ìš”ë„', 'ë‚œë°©í•„ìš”ë„']\n",
    "        power_cols = ['daily_sum', 'daily_mean']\n",
    "        \n",
    "        correlations = {}\n",
    "        for weather_col in weather_cols:\n",
    "            if weather_col in self.merged_daily_data.columns:\n",
    "                corr_sum = self.merged_daily_data[weather_col].corr(self.merged_daily_data['daily_sum'])\n",
    "                corr_mean = self.merged_daily_data[weather_col].corr(self.merged_daily_data['daily_mean'])\n",
    "                correlations[weather_col] = {'sum': corr_sum, 'mean': corr_mean}\n",
    "        \n",
    "        print(\"ğŸŒ¡ï¸  ê¸°ìƒ ìš”ì¸ë³„ ìƒê´€ê´€ê³„ (ì „ë ¥ì‚¬ìš©ëŸ‰):\")\n",
    "        print(\"ê¸°ìƒìš”ì¸\\t\\tì¼ì´ì‚¬ìš©ëŸ‰\\tì¼í‰ê· ì‚¬ìš©ëŸ‰\")\n",
    "        for weather_factor, corrs in correlations.items():\n",
    "            print(f\"{weather_factor}\\t{corrs['sum']:.3f}\\t\\t{corrs['mean']:.3f}\")\n",
    "        \n",
    "        # ì˜¨ë„ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´\n",
    "        temp_bins = [-10, 0, 10, 20, 25, 30, 35, 50]\n",
    "        temp_labels = ['ê·¹í•œì €ì˜¨', 'ì €ì˜¨', 'ì„œëŠ˜', 'ì ì •', 'ë”°ëœ»', 'ë”ì›€', 'ê³ ì˜¨']\n",
    "        \n",
    "        if 'í‰ê· ê¸°ì˜¨' in self.merged_daily_data.columns:\n",
    "            self.merged_daily_data['temp_category'] = pd.cut(\n",
    "                self.merged_daily_data['í‰ê· ê¸°ì˜¨'], \n",
    "                bins=temp_bins, \n",
    "                labels=temp_labels, \n",
    "                include_lowest=True\n",
    "            )\n",
    "            \n",
    "            temp_power = self.merged_daily_data.groupby('temp_category')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "            \n",
    "            print(f\"\\nğŸŒ¡ï¸  ì˜¨ë„ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´:\")\n",
    "            print(\"ì˜¨ë„êµ¬ê°„\\tì¼ìˆ˜\\tí‰ê· ì‚¬ìš©ëŸ‰\\tí‘œì¤€í¸ì°¨\")\n",
    "            for temp_cat in temp_power.index:\n",
    "                stats = temp_power.loc[temp_cat]\n",
    "                print(f\"{temp_cat}\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        return correlations\n",
    "    \n",
    "    def analyze_calendar_patterns(self):\n",
    "        \"\"\"ë‹¬ë ¥ ìš”ì¸ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        if not hasattr(self, 'merged_daily_data') or self.calendar_data is None:\n",
    "            print(\"âš ï¸  ë‹¬ë ¥ ë°ì´í„°ê°€ ì—†ì–´ ë‹¬ë ¥ íŒ¨í„´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n=== ë‹¬ë ¥ íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # í‰ì¼/ì£¼ë§/íœ´ì¼ë³„ íŒ¨í„´\n",
    "        if 'day_type' in self.merged_daily_data.columns:\n",
    "            day_type_stats = self.merged_daily_data.groupby('day_type')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "            \n",
    "            print(\"ğŸ“… ì¼ì ìœ í˜•ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´:\")\n",
    "            print(\"ì¼ììœ í˜•\\tì¼ìˆ˜\\tí‰ê· ì‚¬ìš©ëŸ‰\\tí‘œì¤€í¸ì°¨\")\n",
    "            for day_type in day_type_stats.index:\n",
    "                stats = day_type_stats.loc[day_type]\n",
    "                print(f\"{day_type}\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        # ì—°íœ´ íš¨ê³¼ ë¶„ì„\n",
    "        if 'is_consecutive_holiday' in self.merged_daily_data.columns:\n",
    "            holiday_effect = self.merged_daily_data.groupby('is_consecutive_holiday')['daily_mean'].agg(['count', 'mean']).round(1)\n",
    "            \n",
    "            print(f\"\\nğŸŠ ì—°íœ´ íš¨ê³¼ ë¶„ì„:\")\n",
    "            for is_holiday, stats in holiday_effect.iterrows():\n",
    "                holiday_label = \"ì—°íœ´ê¸°ê°„\" if is_holiday else \"ì¼ë°˜ê¸°ê°„\"\n",
    "                print(f\"{holiday_label}: ì¼ìˆ˜ {stats['count']}ì¼, í‰ê·  {stats['mean']}kW\")\n",
    "        \n",
    "        # ì›”ë³„ íŒ¨í„´ (ê³„ì ˆì„±)\n",
    "        self.merged_daily_data['month'] = self.merged_daily_data['date'].dt.month\n",
    "        monthly_stats = self.merged_daily_data.groupby('month')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì›”ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´:\")\n",
    "        print(\"ì›”\\tì¼ìˆ˜\\tí‰ê· ì‚¬ìš©ëŸ‰\\tí‘œì¤€í¸ì°¨\")\n",
    "        for month in monthly_stats.index:\n",
    "            stats = monthly_stats.loc[month]\n",
    "            print(f\"{month}ì›”\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        return {\n",
    "            'day_type_stats': day_type_stats if 'day_type' in self.merged_daily_data.columns else None,\n",
    "            'holiday_effect': holiday_effect if 'is_consecutive_holiday' in self.merged_daily_data.columns else None,\n",
    "            'monthly_stats': monthly_stats\n",
    "        }\n",
    "    \n",
    "    def analyze_hourly_patterns(self):\n",
    "        \"\"\"ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "        hourly_avg = self.lp_data.groupby('hour')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "        \n",
    "        print(\"ğŸ“Š ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (kW):\")\n",
    "        print(\"ì‹œê°„\\tí‰ê· \\tí‘œì¤€í¸ì°¨\\tìµœì†Œ\\tìµœëŒ€\")\n",
    "        for hour in range(24):\n",
    "            stats = hourly_avg.loc[hour]\n",
    "            print(f\"{hour:02d}ì‹œ\\t{stats['mean']}\\t{stats['std']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # í”¼í¬/ë¹„í”¼í¬ ì‹œê°„ëŒ€ ì‹ë³„\n",
    "        peak_threshold = hourly_avg['mean'].quantile(0.8)\n",
    "        peak_hours = hourly_avg[hourly_avg['mean'] >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = hourly_avg[hourly_avg['mean'] < hourly_avg['mean'].quantile(0.3)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nâš¡ í”¼í¬ ì‹œê°„ëŒ€ (ìƒìœ„ 20%): {peak_hours}ì‹œ\")\n",
    "        print(f\"ğŸ’¤ ë¹„í”¼í¬ ì‹œê°„ëŒ€ (í•˜ìœ„ 30%): {off_peak_hours}ì‹œ\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_stats': hourly_avg,\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours\n",
    "        }\n",
    "    \n",
    "    def analyze_daily_patterns(self):\n",
    "        \"\"\"ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ì¼ë³„ ì´ ì‚¬ìš©ëŸ‰\n",
    "        daily_usage = self.lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date'])['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].sum().reset_index()\n",
    "        daily_usage['weekday'] = pd.to_datetime(daily_usage['date']).dt.weekday\n",
    "        daily_usage['is_weekend'] = daily_usage['weekday'].isin([5, 6])\n",
    "        \n",
    "        # ìš”ì¼ë³„ í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "        weekday_avg = daily_usage.groupby('weekday')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['mean', 'std']).round(1)\n",
    "        weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']\n",
    "        \n",
    "        print(\"ğŸ“… ìš”ì¼ë³„ í‰ê·  ì¼ê°„ ì‚¬ìš©ëŸ‰ (kWh):\")\n",
    "        for i, day_name in enumerate(weekday_names):\n",
    "            stats = weekday_avg.loc[i]\n",
    "            print(f\"{day_name}ìš”ì¼: {stats['mean']:,.1f} Â± {stats['std']:.1f}\")\n",
    "        \n",
    "        # í‰ì¼ vs ì£¼ë§ ë¹„êµ\n",
    "        weekday_mean = daily_usage[~daily_usage['is_weekend']]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "        weekend_mean = daily_usage[daily_usage['is_weekend']]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "        weekend_ratio = weekend_mean / weekday_mean\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í‰ì¼ vs ì£¼ë§ ë¹„êµ:\")\n",
    "        print(f\"í‰ì¼ í‰ê· : {weekday_mean:,.1f} kWh\")\n",
    "        print(f\"ì£¼ë§ í‰ê· : {weekend_mean:,.1f} kWh\")\n",
    "        print(f\"ì£¼ë§/í‰ì¼ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'daily_usage': daily_usage,\n",
    "            'weekday_stats': weekday_avg,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "    \n",
    "    def analyze_customer_profiles(self):\n",
    "        \"\"\"ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ê¸°ë³¸ í†µê³„\n",
    "        customer_stats = self.lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max'\n",
    "        ]).round(1)\n",
    "        customer_stats['cv'] = (customer_stats['std'] / customer_stats['mean']).round(3)  # ë³€ë™ê³„ìˆ˜\n",
    "        \n",
    "        print(\"ğŸ‘¥ ê³ ê°ë³„ ê¸°ë³¸ í†µê³„ (kW):\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tí‰ê· \\tí‘œì¤€í¸ì°¨\\të³€ë™ê³„ìˆ˜\\tìµœì†Œ\\tìµœëŒ€\")\n",
    "        for customer in customer_stats.index:\n",
    "            stats = customer_stats.loc[customer]\n",
    "            print(f\"{customer}\\t{stats['mean']}\\t{stats['std']}\\t{stats['cv']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë¶„ë¥˜\n",
    "        mean_usage = customer_stats['mean']\n",
    "        high_users = mean_usage[mean_usage >= mean_usage.quantile(0.8)].index.tolist()\n",
    "        low_users = mean_usage[mean_usage <= mean_usage.quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ëŒ€ìš©ëŸ‰ ì‚¬ìš©ì (ìƒìœ„ 20%): {high_users}\")\n",
    "        print(f\"ğŸ“‰ ì†Œìš©ëŸ‰ ì‚¬ìš©ì (í•˜ìœ„ 20%): {low_users}\")\n",
    "        \n",
    "        # ë³€ë™ì„±ë³„ ë¶„ë¥˜\n",
    "        high_volatility = customer_stats[customer_stats['cv'] >= customer_stats['cv'].quantile(0.8)].index.tolist()\n",
    "        low_volatility = customer_stats[customer_stats['cv'] <= customer_stats['cv'].quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nğŸŒŠ ê³ ë³€ë™ì„± ê³ ê°: {high_volatility}\")\n",
    "        print(f\"ğŸ“Š ì €ë³€ë™ì„± ê³ ê°: {low_volatility}\")\n",
    "        \n",
    "        return {\n",
    "            'customer_stats': customer_stats,\n",
    "            'usage_segments': {\n",
    "                'high_users': high_users,\n",
    "                'low_users': low_users,\n",
    "                'high_volatility': high_volatility,\n",
    "                'low_volatility': low_volatility\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_load_factors(self):\n",
    "        \"\"\"ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ===\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ë¶€í•˜ìœ¨ ê³„ì‚°\n",
    "        customer_load_factors = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "            \n",
    "            avg_load = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            max_load = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].max()\n",
    "            load_factor = avg_load / max_load if max_load > 0 else 0\n",
    "            \n",
    "            # í”¼í¬ ì§‘ì¤‘ë„ (í”¼í¬ ì‹œê°„ëŒ€ ì‚¬ìš©ëŸ‰ ë¹„ì¤‘)\n",
    "            peak_hours = [9, 14, 18]  # ëŒ€í‘œ í”¼í¬ ì‹œê°„\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            total_avg = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            peak_concentration = peak_usage / total_avg if total_avg > 0 else 0\n",
    "            \n",
    "            customer_load_factors[customer] = {\n",
    "                'load_factor': round(load_factor, 3),\n",
    "                'peak_concentration': round(peak_concentration, 3),\n",
    "                'avg_load': round(avg_load, 1),\n",
    "                'max_load': round(max_load, 1)\n",
    "            }\n",
    "        \n",
    "        print(\"âš¡ ê³ ê°ë³„ ë¶€í•˜ìœ¨ ë° í”¼í¬ ì§‘ì¤‘ë„:\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\të¶€í•˜ìœ¨\\tí”¼í¬ì§‘ì¤‘ë„\\tí‰ê· ë¶€í•˜\\tìµœëŒ€ë¶€í•˜\")\n",
    "        for customer, metrics in customer_load_factors.items():\n",
    "            print(f\"{customer}\\t{metrics['load_factor']}\\t{metrics['peak_concentration']}\\t{metrics['avg_load']}\\t{metrics['max_load']}\")\n",
    "        \n",
    "        # ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬\n",
    "        load_factors = [metrics['load_factor'] for metrics in customer_load_factors.values()]\n",
    "        avg_load_factor = np.mean(load_factors)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬:\")\n",
    "        print(f\"í‰ê·  ë¶€í•˜ìœ¨: {avg_load_factor:.3f}\")\n",
    "        print(f\"ë¶€í•˜ìœ¨ ë²”ìœ„: {min(load_factors):.3f} ~ {max(load_factors):.3f}\")\n",
    "        \n",
    "        return customer_load_factors\n",
    "    \n",
    "    def detect_usage_anomalies(self):\n",
    "        \"\"\"ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€\"\"\"\n",
    "        print(\"\\n=== ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€ ===\")\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()\n",
    "            customer_data = customer_data.sort_values('datetime')\n",
    "            \n",
    "            # 1. ê¸‰ê²©í•œ ë³€í™” íƒì§€ (ì „ì‹œì  ëŒ€ë¹„ 200% ì´ìƒ ë³€í™”)\n",
    "            customer_data['power_change'] = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['power_change']) > 2.0]\n",
    "            \n",
    "            # 2. ì—°ì†ì ì¸ 0ê°’ íƒì§€ (2ì‹œê°„ ì´ìƒ)\n",
    "            customer_data['is_zero'] = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] == 0\n",
    "            customer_data['zero_group'] = (customer_data['is_zero'] != customer_data['is_zero'].shift()).cumsum()\n",
    "            zero_periods = customer_data[customer_data['is_zero']].groupby('zero_group').size()\n",
    "            long_zero_periods = zero_periods[zero_periods >= 8]  # 2ì‹œê°„ = 8ê°œ 15ë¶„ êµ¬ê°„\n",
    "            \n",
    "            # 3. í†µê³„ì  ì´ìƒì¹˜ (Z-score > 3)\n",
    "            mean_power = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            std_power = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].std()\n",
    "            if std_power > 0:\n",
    "                customer_data['z_score'] = abs(customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] - mean_power) / std_power\n",
    "                statistical_outliers = customer_data[customer_data['z_score'] > 3]\n",
    "            else:\n",
    "                statistical_outliers = pd.DataFrame()\n",
    "            \n",
    "            # ì´ìƒì¹˜ ì •ë³´ ì €ì¥\n",
    "            if len(sudden_changes) > 0 or len(long_zero_periods) > 0 or len(statistical_outliers) > 0:\n",
    "                anomalies.append({\n",
    "                    'customer': customer,\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'long_zero_periods': len(long_zero_periods),\n",
    "                    'statistical_outliers': len(statistical_outliers)\n",
    "                })\n",
    "        \n",
    "        print(\"ğŸš¨ ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼:\")\n",
    "        if anomalies:\n",
    "            print(\"ê³ ê°ë²ˆí˜¸\\tê¸‰ê²©ë³€í™”\\tì¥ê¸°0ê°’\\tí†µê³„ì´ìƒì¹˜\")\n",
    "            for anomaly in anomalies:\n",
    "                print(f\"{anomaly['customer']}\\t{anomaly['sudden_changes']}\\t{anomaly['long_zero_periods']}\\t{anomaly['statistical_outliers']}\")\n",
    "        else:\n",
    "            print(\"âœ… ì‹¬ê°í•œ ì´ìƒ íŒ¨í„´ ì—†ìŒ\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def generate_enhanced_pattern_summary(self):\n",
    "        \"\"\"ê°•í™”ëœ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š ê°•í™”ëœ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ì™¸ë¶€ ë°ì´í„° ë¡œë”©\n",
    "        self.load_external_data()\n",
    "        \n",
    "        # ë°ì´í„° ê²°í•©\n",
    "        merged_data = self.merge_with_external_data()\n",
    "        \n",
    "        # ê¸°ì¡´ íŒ¨í„´ ë¶„ì„\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        # ê°•í™”ëœ ë¶„ì„ (ì™¸ë¶€ ë°ì´í„° í™œìš©)\n",
    "        weather_impact = self.analyze_weather_impact()\n",
    "        calendar_patterns = self.analyze_calendar_patterns()\n",
    "        \n",
    "        print(\"\\nğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "        \n",
    "        # 1. ì‹œê°„ íŒ¨í„´\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: {peak_hours}ì‹œ\")\n",
    "        \n",
    "        # 2. ìš”ì¼ íŒ¨í„´  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. ê³ ê° ë‹¤ì–‘ì„±\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. ë¶€í•˜ìœ¨\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  â€¢ í‰ê·  ë¶€í•˜ìœ¨: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. ì´ìƒ íŒ¨í„´\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: {anomaly_customers}ëª…\")\n",
    "        \n",
    "        # 6. ê¸°ìƒ ì˜í–¥ (ìˆëŠ” ê²½ìš°)\n",
    "        if weather_impact:\n",
    "            temp_corr = weather_impact.get('í‰ê· ê¸°ì˜¨', {}).get('mean', 0)\n",
    "            humidity_corr = weather_impact.get('í‰ê· ìŠµë„', {}).get('mean', 0)\n",
    "            print(f\"  â€¢ ê¸°ì˜¨ê³¼ ì „ë ¥ì‚¬ìš©ëŸ‰ ìƒê´€ê´€ê³„: {temp_corr:.3f}\")\n",
    "            print(f\"  â€¢ ìŠµë„ì™€ ì „ë ¥ì‚¬ìš©ëŸ‰ ìƒê´€ê´€ê³„: {humidity_corr:.3f}\")\n",
    "        \n",
    "        # 7. ë‹¬ë ¥ íš¨ê³¼ (ìˆëŠ” ê²½ìš°)\n",
    "        if calendar_patterns and calendar_patterns.get('holiday_effect') is not None:\n",
    "            holiday_effect = calendar_patterns['holiday_effect']\n",
    "            if len(holiday_effect) >= 2:\n",
    "                normal_avg = holiday_effect.loc[False, 'mean'] if False in holiday_effect.index else 0\n",
    "                holiday_avg = holiday_effect.loc[True, 'mean'] if True in holiday_effect.index else 0\n",
    "                if normal_avg > 0:\n",
    "                    holiday_ratio = holiday_avg / normal_avg\n",
    "                    print(f\"  â€¢ ì—°íœ´/ì¼ë°˜ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {holiday_ratio:.3f}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ê°•í™”ëœ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\")\n",
    "        print(\"  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\") \n",
    "        print(\"  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\")\n",
    "        print(\"  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
    "        print(\"  5. ê¸°ìƒ ìš”ì¸ ë³´ì • (ì˜¨ë„, ìŠµë„, ê°•ìˆ˜ëŸ‰)\")\n",
    "        print(\"  6. ë‹¬ë ¥ íš¨ê³¼ ë°˜ì˜ (íœ´ì¼, ì—°íœ´, ê³„ì ˆì„±)\")\n",
    "        print(\"  7. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \")\n",
    "        print(\"  8. ì´ìƒ íŒ¨í„´ í•„í„°ë§ ë©”ì»¤ë‹ˆì¦˜\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies,\n",
    "            'weather_impact': weather_impact,\n",
    "            'calendar_patterns': calendar_patterns,\n",
    "            'merged_data': merged_data\n",
    "        }\n",
    "    \n",
    "    def generate_pattern_summary(self):\n",
    "        \"\"\"ê¸°ì¡´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½ (í•˜ìœ„ í˜¸í™˜ì„±)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ì£¼ìš” íŒ¨í„´ íŠ¹ì„±\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        print(\"\\nğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "        \n",
    "        # 1. ì‹œê°„ íŒ¨í„´\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: {peak_hours}ì‹œ\")\n",
    "        \n",
    "        # 2. ìš”ì¼ íŒ¨í„´  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. ê³ ê° ë‹¤ì–‘ì„±\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. ë¶€í•˜ìœ¨\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  â€¢ í‰ê·  ë¶€í•˜ìœ¨: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. ì´ìƒ íŒ¨í„´\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: {anomaly_customers}ëª…\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\")\n",
    "        print(\"  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\") \n",
    "        print(\"  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\")\n",
    "        print(\"  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
    "        print(\"  5. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies\n",
    "        }\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer()\n",
    "    \n",
    "    print(\"ğŸ”§ ì‹¤ì œ í™˜ê²½ì—ì„œ ì‚¬ìš© ë°©ë²• (3000í˜¸):\")\n",
    "    print(\"\"\"\n",
    "    # ì‹¤ì œ LP ë°ì´í„°ë§Œ ë¡œë”©í•˜ë©´ ë!\n",
    "    lp_files = ['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv']\n",
    "    analyzer.load_real_lp_data(lp_files)\n",
    "    \n",
    "    # ì‹¤ì œ ë°ì´í„°ë¡œ íŒ¨í„´ ë¶„ì„\n",
    "    enhanced_summary = analyzer.generate_enhanced_pattern_summary()\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ§ª í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹œì—°\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    lp_data = analyzer.load_sample_data()\n",
    "    \n",
    "    # íŒ¨í„´ ë¶„ì„ ì‹¤í–‰\n",
    "    pattern_summary = analyzer.generate_pattern_summary()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\n",
    "    print(\"  âœ… ì‹¤ì œ í™˜ê²½: ì‹¤ì œ LP ë°ì´í„° ê·¸ëŒ€ë¡œ ë¶„ì„\")\n",
    "    print(\"  âœ… í…ŒìŠ¤íŠ¸ìš©: ìƒ˜í”Œ ë°ì´í„°ë¡œ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦\")\n",
    "    print(\"  âœ… 3000í˜¸: ì‹¤ì œ ë°ì´í„° ìˆìœ¼ë‹ˆ í”„ë¡œíŒŒì¼ ìƒì„± ë¶ˆí•„ìš”\")\n",
    "    print(\"  âœ… íŒ¨í„´ ë¶„ì„: ì‹¤ì œ ì‚¬ìš©ëŸ‰ì—ì„œ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
