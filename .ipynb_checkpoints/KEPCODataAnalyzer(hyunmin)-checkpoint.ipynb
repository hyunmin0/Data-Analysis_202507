{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7203f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 고객 기본정보 로딩 ===\n",
      "총 고객 수: 200명\n",
      "컬럼: ['순번', '고객번호', '계약전력', '계약종별', '사용용도', '주생산품', '산업분류']\n",
      "\n",
      "기본 정보:\n",
      "   순번   고객번호     계약전력            계약종별     사용용도 주생산품     산업분류\n",
      "0   1  A1001  500~599  222 일반용(갑)‖고압A  09 광공업용   마트  996기타업종\n",
      "1   2  A1002  400~499  222 일반용(갑)‖고압A   02 상업용   병원  391기타업종\n",
      "2   3  A1003  200~299  222 일반용(갑)‖고압A  09 광공업용   교회  304기타업종\n",
      "3   4  A1004  200~299  726 산업용(을) 고압A  09 광공업용  제조업  457기타업종\n",
      "4   5  A1005  200~299  222 일반용(갑)‖고압A  09 광공업용  온천탕  825기타업종\n",
      "\n",
      "=== 고객 분포 분석 ===\n",
      "\n",
      "📊 계약종별 분포:\n",
      "  222 일반용(갑)‖고압A: 60명 (30.0%)\n",
      "  322 산업용(갑)‖고압A: 55명 (27.5%)\n",
      "  226 일반용(을) 고압A: 45명 (22.5%)\n",
      "  726 산업용(을) 고압A: 40명 (20.0%)\n",
      "\n",
      "🏭 사용용도별 분포:\n",
      "  09 광공업용: 104명 (52.0%)\n",
      "  02 상업용: 96명 (48.0%)\n",
      "\n",
      "⚡ 계약전력 분포:\n",
      "  100~199kW: 39명 (19.5%)\n",
      "  200~299kW: 35명 (17.5%)\n",
      "  400~499kW: 30명 (15.0%)\n",
      "  500~599kW: 25명 (12.5%)\n",
      "  700~799kW: 34명 (17.0%)\n",
      "  800~899kW: 37명 (18.5%)\n",
      "\n",
      "=== LP 데이터 로딩 ===\n",
      "파일 1 로딩: LP데이터1.csv\n",
      "  레코드 수: 14,400\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "파일 2 로딩: LP데이터2.csv\n",
      "  레코드 수: 15,360\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-16-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "✅ 전체 LP 데이터 결합 완료:\n",
      "  총 레코드: 29,760\n",
      "  총 고객: 10\n",
      "\n",
      "=== LP 데이터 품질 분석 ===\n",
      "📈 기본 통계:\n",
      "            순방향유효전력          지상무효          진상무효          피상전력\n",
      "count  29760.000000  29760.000000  29760.000000  29760.000000\n",
      "mean      79.940981     10.069745      5.137708     79.981331\n",
      "std       24.013893      4.828842      2.803947     23.695466\n",
      "min        4.600000      0.000000      0.000000      3.900000\n",
      "25%       60.200000      6.600000      3.000000     60.200000\n",
      "50%       79.900000     10.000000      5.000000     80.200000\n",
      "75%       99.600000     13.300000      7.000000     99.700000\n",
      "max      151.200000     29.900000     16.200000    147.600000\n",
      "\n",
      "⏰ 시간 간격 체크:\n",
      "  A1001: 평균 간격 15.0분, 표준편차 0.0분\n",
      "  A1002: 평균 간격 15.0분, 표준편차 0.0분\n",
      "  A1003: 평균 간격 15.0분, 표준편차 0.0분\n",
      "\n",
      "🔍 데이터 품질 체크:\n",
      "  순방향유효전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "    음수: 0건 (0.00%)\n",
      "  지상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 34건 (0.11%)\n",
      "    음수: 0건 (0.00%)\n",
      "  진상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 105건 (0.35%)\n",
      "    음수: 0건 (0.00%)\n",
      "  피상전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "    음수: 0건 (0.00%)\n",
      "\n",
      "👥 고객별 데이터 완정성:\n",
      "  예상 레코드 수: 2,976개/고객\n",
      "  실제 레코드 수: 2,976~2,976개/고객\n",
      "  ✅ 모든 고객 데이터 완정성 양호\n",
      "\n",
      "=== 이상치 탐지 (IQR 방법) ===\n",
      "\n",
      "📊 순방향유효전력:\n",
      "  이상치: 0건 (0.00%)\n",
      "\n",
      "📊 지상무효:\n",
      "  이상치: 114건 (0.38%)\n",
      "  범위: 23.4 ~ 29.9\n",
      "\n",
      "📊 진상무효:\n",
      "  이상치: 105건 (0.35%)\n",
      "  범위: 13.1 ~ 16.2\n",
      "\n",
      "📊 피상전력:\n",
      "  이상치: 0건 (0.00%)\n",
      "\n",
      "============================================================\n",
      "📋 데이터 품질 종합 리포트\n",
      "============================================================\n",
      "\n",
      "🔢 데이터 규모:\n",
      "  • 고객 기본정보: 200명\n",
      "  • LP 데이터: 29,760레코드\n",
      "  • 분석 대상 고객: 10명\n",
      "  • 분석 기간: 2024-03-01-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "✅ 데이터 품질 상태:\n",
      "  • 결측치 비율: 0.00%\n",
      "  • 0값 비율: 0.12%\n",
      "  • 음수값 비율: 0.00%\n",
      "\n",
      "💡 다음 단계 권장사항:\n",
      "  1. 시계열 패턴 분석 (일/주/월별 사용 패턴)\n",
      "  2. 고객별 사용량 프로파일링\n",
      "  3. 변동성 지표 계산 및 비교\n",
      "  4. 이상 패턴 탐지 알고리즘 개발\n",
      "\n",
      "🎯 1단계 데이터 품질 점검 완료!\n",
      "다음: 2단계 시계열 패턴 분석 준비 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "    def load_customer_data(self, file_path):\n",
    "        \"\"\"고객 기본정보 로딩 및 기본 분석\"\"\"\n",
    "        print(\"=== 고객 기본정보 로딩 ===\")\n",
    "        \n",
    "        # Excel 파일 읽기 (실제 환경에서는 이 부분 사용)\n",
    "        # self.customer_data = pd.read_excel(file_path)\n",
    "        \n",
    "        # 샘플 데이터 생성 (테스트용)\n",
    "        self.customer_data = self._create_sample_customer_data()\n",
    "        \n",
    "        print(f\"총 고객 수: {len(self.customer_data):,}명\")\n",
    "        print(f\"컬럼: {list(self.customer_data.columns)}\")\n",
    "        print(\"\\n기본 정보:\")\n",
    "        print(self.customer_data.head())\n",
    "        \n",
    "        return self._analyze_customer_distribution()\n",
    "    \n",
    "    def _create_sample_customer_data(self):\n",
    "        \"\"\"테스트용 샘플 고객 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        contract_types = ['222 일반용(갑)‖고압A', '226 일반용(을) 고압A', '322 산업용(갑)‖고압A', '726 산업용(을) 고압A']\n",
    "        usage_types = ['02 상업용', '09 광공업용']\n",
    "        power_ranges = ['100~199', '200~299', '400~499', '500~599', '700~799', '800~899']\n",
    "        industries = ['병원', '교회', '상가', 'CNG충전', '금속가공', '점포', '온천탕', '제조업', '마트']\n",
    "        \n",
    "        for i in range(1, 201):  # 200명\n",
    "            data.append({\n",
    "                '순번': i,\n",
    "                '고객번호': f'A{1000+i}',\n",
    "                '계약전력': np.random.choice(power_ranges),\n",
    "                '계약종별': np.random.choice(contract_types),\n",
    "                '사용용도': np.random.choice(usage_types),\n",
    "                '주생산품': np.random.choice(industries),\n",
    "                '산업분류': f'{np.random.randint(100,999)}기타업종'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"고객 분포 분석\"\"\"\n",
    "        print(\"\\n=== 고객 분포 분석 ===\")\n",
    "        \n",
    "        # 계약종별 분포\n",
    "        contract_dist = self.customer_data['계약종별'].value_counts()\n",
    "        print(\"\\n📊 계약종별 분포:\")\n",
    "        for contract, count in contract_dist.items():\n",
    "            print(f\"  {contract}: {count}명 ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # 사용용도별 분포\n",
    "        usage_dist = self.customer_data['사용용도'].value_counts()\n",
    "        print(\"\\n🏭 사용용도별 분포:\")\n",
    "        for usage, count in usage_dist.items():\n",
    "            print(f\"  {usage}: {count}명 ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # 계약전력 분포\n",
    "        power_dist = self.customer_data['계약전력'].value_counts().sort_index()\n",
    "        print(\"\\n⚡ 계약전력 분포:\")\n",
    "        for power, count in power_dist.items():\n",
    "            print(f\"  {power}kW: {count}명 ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_dist,\n",
    "            'usage_distribution': usage_dist,\n",
    "            'power_distribution': power_dist\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, file_paths):\n",
    "        \"\"\"LP 데이터 로딩 및 결합\"\"\"\n",
    "        print(\"\\n=== LP 데이터 로딩 ===\")\n",
    "        \n",
    "        lp_dataframes = []\n",
    "        \n",
    "        for i, file_path in enumerate(file_paths, 1):\n",
    "            print(f\"파일 {i} 로딩: {file_path}\")\n",
    "            \n",
    "            # CSV 파일 읽기 (실제 환경에서는 이 부분 사용)\n",
    "            # df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            \n",
    "            # 샘플 데이터 생성 (테스트용)\n",
    "            df = self._create_sample_lp_data(i)\n",
    "            \n",
    "            print(f\"  레코드 수: {len(df):,}\")\n",
    "            print(f\"  고객 수: {df['대체고객번호'].nunique()}\")\n",
    "            print(f\"  기간: {df['LP수신일자'].min()} ~ {df['LP수신일자'].max()}\")\n",
    "            \n",
    "            lp_dataframes.append(df)\n",
    "        \n",
    "        # 데이터 결합\n",
    "        self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "        print(f\"\\n✅ 전체 LP 데이터 결합 완료:\")\n",
    "        print(f\"  총 레코드: {len(self.lp_data):,}\")\n",
    "        print(f\"  총 고객: {self.lp_data['대체고객번호'].nunique()}\")\n",
    "        \n",
    "        return self._analyze_lp_quality()\n",
    "    \n",
    "    def _create_sample_lp_data(self, file_num):\n",
    "        \"\"\"테스트용 샘플 LP 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        \n",
    "        # 파일별로 다른 기간 설정\n",
    "        if file_num == 1:\n",
    "            start_date = datetime(2024, 3, 1)\n",
    "            days = 15\n",
    "        else:\n",
    "            start_date = datetime(2024, 3, 16) \n",
    "            days = 16\n",
    "        \n",
    "        for customer in customers:\n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # 시간대별 패턴을 반영한 가상 데이터\n",
    "                        base_power = 80 + 30 * np.sin(2 * np.pi * hour / 24) + np.random.normal(0, 10)\n",
    "                        base_power = max(0, base_power)\n",
    "                        \n",
    "                        data.append({\n",
    "                            '대체고객번호': customer,\n",
    "                            'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            '순방향유효전력': round(base_power + np.random.normal(0, 5), 1),\n",
    "                            '지상무효': round(abs(np.random.normal(10, 5)), 1),\n",
    "                            '진상무효': round(abs(np.random.normal(5, 3)), 1),\n",
    "                            '피상전력': round(base_power + np.random.normal(0, 3), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP 데이터 품질 분석\"\"\"\n",
    "        print(\"\\n=== LP 데이터 품질 분석 ===\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        print(\"📈 기본 통계:\")\n",
    "        numeric_cols = ['순방향유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        print(self.lp_data[numeric_cols].describe())\n",
    "        \n",
    "        # 시간 간격 체크\n",
    "        print(\"\\n⏰ 시간 간격 체크:\")\n",
    "        self.lp_data['LP수신일자_dt'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique()[:3]:  # 처음 3명만 체크\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('LP수신일자_dt')\n",
    "            time_diffs = customer_data['LP수신일자_dt'].diff().dt.total_seconds() / 60  # 분 단위\n",
    "            \n",
    "            print(f\"  {customer}: 평균 간격 {time_diffs.mean():.1f}분, 표준편차 {time_diffs.std():.1f}분\")\n",
    "            \n",
    "            # 15분이 아닌 간격 찾기\n",
    "            non_15min = time_diffs[(time_diffs != 15.0) & (~time_diffs.isna())]\n",
    "            if len(non_15min) > 0:\n",
    "                print(f\"    ⚠️ 비정상 간격: {len(non_15min)}건\")\n",
    "        \n",
    "        # 결측치 및 이상치 체크\n",
    "        print(\"\\n🔍 데이터 품질 체크:\")\n",
    "        for col in numeric_cols:\n",
    "            null_count = self.lp_data[col].isnull().sum()\n",
    "            zero_count = (self.lp_data[col] == 0).sum()\n",
    "            negative_count = (self.lp_data[col] < 0).sum()\n",
    "            \n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    결측치: {null_count:,}건 ({null_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    0값: {zero_count:,}건 ({zero_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    음수: {negative_count:,}건 ({negative_count/len(self.lp_data)*100:.2f}%)\")\n",
    "        \n",
    "        # 고객별 데이터 완정성\n",
    "        print(\"\\n👥 고객별 데이터 완정성:\")\n",
    "        customer_counts = self.lp_data['대체고객번호'].value_counts()\n",
    "        expected_records = 24 * 4 * 31  # 15분 간격 × 1개월\n",
    "        \n",
    "        print(f\"  예상 레코드 수: {expected_records:,}개/고객\")\n",
    "        print(f\"  실제 레코드 수: {customer_counts.min():,}~{customer_counts.max():,}개/고객\")\n",
    "        \n",
    "        incomplete_customers = customer_counts[customer_counts < expected_records * 0.95]  # 95% 미만\n",
    "        if len(incomplete_customers) > 0:\n",
    "            print(f\"  ⚠️ 불완전한 고객: {len(incomplete_customers)}명\")\n",
    "        else:\n",
    "            print(\"  ✅ 모든 고객 데이터 완정성 양호\")\n",
    "        \n",
    "        return {\n",
    "            'total_records': len(self.lp_data),\n",
    "            'customers': self.lp_data['대체고객번호'].nunique(),\n",
    "            'date_range': (self.lp_data['LP수신일자_dt'].min(), self.lp_data['LP수신일자_dt'].max()),\n",
    "            'data_quality': {col: {'null': self.lp_data[col].isnull().sum(), \n",
    "                                  'zero': (self.lp_data[col] == 0).sum(),\n",
    "                                  'negative': (self.lp_data[col] < 0).sum()} for col in numeric_cols}\n",
    "        }\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"이상치 탐지\"\"\"\n",
    "        print(f\"\\n=== 이상치 탐지 ({method.upper()} 방법) ===\")\n",
    "        \n",
    "        numeric_cols = ['순방향유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        outliers_summary = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            data = self.lp_data[col].dropna()\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs((data - data.mean()) / data.std())\n",
    "                outliers = data[z_scores > 3]\n",
    "            \n",
    "            outliers_summary[col] = {\n",
    "                'count': len(outliers),\n",
    "                'percentage': len(outliers) / len(data) * 100,\n",
    "                'min_outlier': outliers.min() if len(outliers) > 0 else None,\n",
    "                'max_outlier': outliers.max() if len(outliers) > 0 else None\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n📊 {col}:\")\n",
    "            print(f\"  이상치: {len(outliers):,}건 ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"  범위: {outliers.min():.1f} ~ {outliers.max():.1f}\")\n",
    "        \n",
    "        return outliers_summary\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"데이터 품질 종합 리포트 생성\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 데이터 품질 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. 데이터 규모\n",
    "        print(\"\\n🔢 데이터 규모:\")\n",
    "        print(f\"  • 고객 기본정보: {len(self.customer_data):,}명\")\n",
    "        print(f\"  • LP 데이터: {len(self.lp_data):,}레코드\")\n",
    "        print(f\"  • 분석 대상 고객: {self.lp_data['대체고객번호'].nunique()}명\")\n",
    "        print(f\"  • 분석 기간: {self.lp_data['LP수신일자'].min()} ~ {self.lp_data['LP수신일자'].max()}\")\n",
    "        \n",
    "        # 2. 데이터 품질 요약\n",
    "        print(\"\\n✅ 데이터 품질 상태:\")\n",
    "        numeric_cols = ['순방향유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        \n",
    "        total_records = len(self.lp_data)\n",
    "        total_nulls = sum(self.lp_data[col].isnull().sum() for col in numeric_cols)\n",
    "        total_zeros = sum((self.lp_data[col] == 0).sum() for col in numeric_cols)\n",
    "        total_negatives = sum((self.lp_data[col] < 0).sum() for col in numeric_cols)\n",
    "        \n",
    "        print(f\"  • 결측치 비율: {total_nulls/(total_records*4)*100:.2f}%\")\n",
    "        print(f\"  • 0값 비율: {total_zeros/(total_records*4)*100:.2f}%\") \n",
    "        print(f\"  • 음수값 비율: {total_negatives/(total_records*4)*100:.2f}%\")\n",
    "        \n",
    "        # 3. 권장사항\n",
    "        print(\"\\n💡 다음 단계 권장사항:\")\n",
    "        print(\"  1. 시계열 패턴 분석 (일/주/월별 사용 패턴)\")\n",
    "        print(\"  2. 고객별 사용량 프로파일링\")\n",
    "        print(\"  3. 변동성 지표 계산 및 비교\")\n",
    "        print(\"  4. 이상 패턴 탐지 알고리즘 개발\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1단계: 고객 기본정보 분석\n",
    "    customer_analysis = analyzer.load_customer_data('고객번호.xlsx')\n",
    "    \n",
    "    # 2단계: LP 데이터 분석  \n",
    "    lp_analysis = analyzer.load_lp_data(['LP데이터1.csv', 'LP데이터2.csv'])\n",
    "    \n",
    "    # 3단계: 이상치 탐지\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4단계: 종합 리포트\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\n🎯 1단계 데이터 품질 점검 완료!\")\n",
    "    print(\"다음: 2단계 시계열 패턴 분석 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a4c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LP 데이터 로딩 및 전처리 ===\n",
      "✅ 데이터 로딩 완료: 29,760레코드\n",
      "기간: 2024-03-01 00:00:00 ~ 2024-03-31 23:45:00\n",
      "고객 수: 10명\n",
      "\n",
      "============================================================\n",
      "📊 시계열 패턴 분석 종합 요약\n",
      "============================================================\n",
      "\n",
      "=== 시간대별 전력 사용 패턴 분석 ===\n",
      "📊 시간대별 평균 전력 사용량 (kW):\n",
      "시간\t평균\t표준편차\t최소\t최대\n",
      "00시\t58.3\t24.3\t7.8\t120.5\n",
      "01시\t66.2\t27.2\t9.3\t137.0\n",
      "02시\t73.6\t30.3\t9.3\t154.6\n",
      "03시\t80.0\t33.0\t9.0\t167.9\n",
      "04시\t85.0\t35.1\t10.9\t180.6\n",
      "05시\t87.9\t36.3\t12.3\t196.4\n",
      "06시\t93.1\t37.6\t11.6\t184.0\n",
      "07시\t103.7\t55.5\t17.9\t268.5\n",
      "08시\t111.0\t54.1\t14.4\t265.8\n",
      "09시\t108.2\t51.8\t16.5\t252.3\n",
      "10시\t93.9\t38.6\t8.8\t181.5\n",
      "11시\t79.1\t34.4\t8.7\t164.4\n",
      "12시\t68.0\t34.9\t11.8\t183.4\n",
      "13시\t63.1\t31.1\t9.6\t155.6\n",
      "14시\t55.4\t26.4\t8.6\t125.2\n",
      "15시\t45.7\t18.7\t4.7\t93.9\n",
      "16시\t34.9\t14.2\t3.9\t79.4\n",
      "17시\t32.9\t14.9\t5.1\t86.9\n",
      "18시\t32.6\t15.8\t5.2\t87.8\n",
      "19시\t37.6\t19.2\t5.2\t86.5\n",
      "20시\t38.4\t19.3\t3.7\t90.4\n",
      "21시\t42.2\t21.0\t5.1\t98.3\n",
      "22시\t44.2\t17.9\t5.2\t93.5\n",
      "23시\t50.1\t20.6\t6.8\t105.2\n",
      "\n",
      "⚡ 피크 시간대 (상위 20%): [6, 7, 8, 9, 10]시\n",
      "💤 비피크 시간대 (하위 30%): [16, 17, 18, 19, 20, 21, 22]시\n",
      "\n",
      "=== 일별/요일별 패턴 분석 ===\n",
      "📅 요일별 평균 일간 사용량 (kWh):\n",
      "월요일: 7,085.5 ± 2014.9\n",
      "화요일: 7,075.1 ± 2014.0\n",
      "수요일: 7,044.2 ± 1986.6\n",
      "목요일: 7,063.5 ± 2006.7\n",
      "금요일: 7,087.1 ± 2046.0\n",
      "토요일: 4,800.1 ± 2987.5\n",
      "일요일: 4,804.2 ± 2982.3\n",
      "\n",
      "📊 평일 vs 주말 비교:\n",
      "평일 평균: 7,071.9 kWh\n",
      "주말 평균: 4,802.2 kWh\n",
      "주말/평일 비율: 0.68\n",
      "\n",
      "=== 고객별 사용량 프로파일 분석 ===\n",
      "👥 고객별 기본 통계 (kW):\n",
      "고객번호\t평균\t표준편차\t변동계수\t최소\t최대\n",
      "A1001\t85.1\t36.3\t0.427\t21.4\t205.5\n",
      "A1002\t45.7\t29.6\t0.648\t5.7\t141.9\n",
      "A1003\t79.0\t28.7\t0.363\t21.8\t181.2\n",
      "A1004\t81.7\t64.2\t0.786\t3.7\t268.5\n",
      "A1005\t66.0\t24.2\t0.367\t22.6\t138.6\n",
      "A1006\t58.0\t26.8\t0.462\t18.0\t172.3\n",
      "A1007\t59.7\t41.5\t0.695\t5.1\t186.5\n",
      "A1008\t94.9\t43.9\t0.463\t28.6\t244.2\n",
      "A1009\t36.6\t19.9\t0.544\t6.8\t109.5\n",
      "A1010\t53.8\t25.9\t0.481\t11.2\t146.4\n",
      "\n",
      "📈 대용량 사용자 (상위 20%): ['A1001', 'A1008']\n",
      "📉 소용량 사용자 (하위 20%): ['A1002', 'A1009']\n",
      "\n",
      "🌊 고변동성 고객: ['A1004', 'A1007']\n",
      "📊 저변동성 고객: ['A1003', 'A1005']\n",
      "\n",
      "=== 부하율 및 효율성 지표 계산 ===\n",
      "⚡ 고객별 부하율 및 피크 집중도:\n",
      "고객번호\t부하율\t피크집중도\t평균부하\t최대부하\n",
      "A1001\t0.414\t1.039\t85.1\t205.5\n",
      "A1002\t0.322\t1.076\t45.7\t141.9\n",
      "A1003\t0.436\t0.928\t79.0\t181.2\n",
      "A1004\t0.304\t1.099\t81.7\t268.5\n",
      "A1005\t0.476\t0.858\t66.0\t138.6\n",
      "A1006\t0.337\t0.806\t58.0\t172.3\n",
      "A1007\t0.32\t1.081\t59.7\t186.5\n",
      "A1008\t0.389\t0.963\t94.9\t244.2\n",
      "A1009\t0.334\t0.977\t36.6\t109.5\n",
      "A1010\t0.367\t1.083\t53.8\t146.4\n",
      "\n",
      "📊 전체 부하율 분포:\n",
      "평균 부하율: 0.370\n",
      "부하율 범위: 0.304 ~ 0.476\n",
      "\n",
      "=== 사용량 이상 패턴 탐지 ===\n",
      "🚨 이상 패턴 탐지 결과:\n",
      "고객번호\t급격변화\t장기0값\t통계이상치\n",
      "A1001\t0\t0\t10\n",
      "A1002\t4\t0\t6\n",
      "A1003\t0\t0\t9\n",
      "A1004\t4\t0\t0\n",
      "A1005\t0\t0\t1\n",
      "A1006\t0\t0\t54\n",
      "A1007\t4\t0\t2\n",
      "A1008\t0\t0\t9\n",
      "A1009\t0\t0\t21\n",
      "A1010\t0\t0\t6\n",
      "\n",
      "🔍 주요 발견사항:\n",
      "  • 주요 피크 시간: [6, 7, 8, 9, 10]시\n",
      "  • 주말/평일 사용량 비율: 0.68\n",
      "  • 고객별 변동계수 범위: 0.363 ~ 0.786\n",
      "  • 평균 부하율: 0.370\n",
      "  • 이상 패턴 고객: 10명\n",
      "\n",
      "💡 변동계수 설계를 위한 인사이트:\n",
      "  1. 시간대별 가중치 필요 (피크/비피크 구분)\n",
      "  2. 요일별 보정 계수 고려\n",
      "  3. 고객별 기준 변동성 설정\n",
      "  4. 부하율과 변동성의 상관관계 분석\n",
      "  5. 다차원 변동성 지표 조합 검토\n",
      "\n",
      "🎯 2단계 시계열 패턴 분석 완료!\n",
      "다음: 3단계 변동성 지표 계산\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.weather_data = None\n",
    "        self.calendar_data = None\n",
    "        \n",
    "    def load_sample_data(self, customer_df=None):\n",
    "        \"\"\"테스트용 샘플 LP 데이터 생성 (실제 환경에서는 사용 안함)\"\"\"\n",
    "        print(\"=== 테스트용 샘플 데이터 생성 ===\")\n",
    "        print(\"⚠️  주의: 실제 환경에서는 load_real_lp_data() 사용\")\n",
    "        \n",
    "        # 테스트용 샘플 데이터 생성\n",
    "        self.lp_data = self._create_comprehensive_sample_data(customer_df)\n",
    "        \n",
    "        # 날짜/시간 전처리\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday  # 0=월요일\n",
    "        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "        \n",
    "        print(f\"✅ 테스트 데이터 생성 완료: {len(self.lp_data):,}레코드\")\n",
    "        print(f\"기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "        print(f\"고객 수: {self.lp_data['대체고객번호'].nunique()}명\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def load_real_lp_data(self, lp_files):\n",
    "        \"\"\"실제 LP 데이터 로딩 (실제 환경에서 사용)\"\"\"\n",
    "        print(\"=== 실제 LP 데이터 로딩 ===\")\n",
    "        \n",
    "        lp_data_list = []\n",
    "        \n",
    "        for file_path in lp_files:\n",
    "            print(f\"📂 로딩 중: {file_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                lp_data_list.append(df)\n",
    "                print(f\"   ✅ 완료: {len(df):,}레코드\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ 실패: {e}\")\n",
    "        \n",
    "        if lp_data_list:\n",
    "            self.lp_data = pd.concat(lp_data_list, ignore_index=True)\n",
    "            \n",
    "            # 날짜/시간 전처리\n",
    "            self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "            self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "            self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "            self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "            self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "            self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "            \n",
    "            print(f\"✅ 전체 데이터 결합 완료: {len(self.lp_data):,}레코드\")\n",
    "            print(f\"기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"고객 수: {self.lp_data['대체고객번호'].nunique()}명\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def load_external_data(self):\n",
    "        \"\"\"기상 및 달력 데이터 로딩\"\"\"\n",
    "        print(\"\\n=== 외부 데이터 로딩 ===\")\n",
    "        \n",
    "        try:\n",
    "            # 기상 데이터 로딩\n",
    "            print(\"📊 기상 데이터 로딩 중...\")\n",
    "            self.weather_data = pd.read_csv('weather_daily_processed.csv')\n",
    "            \n",
    "            # 날짜 컬럼 전처리\n",
    "            self.weather_data['date'] = pd.to_datetime(self.weather_data['날짜'])\n",
    "            \n",
    "            print(f\"✅ 기상 데이터 로딩 완료: {len(self.weather_data):,}일\")\n",
    "            print(f\"   기간: {self.weather_data['date'].min().date()} ~ {self.weather_data['date'].max().date()}\")\n",
    "            print(f\"   컬럼: {len(self.weather_data.columns)}개 - {list(self.weather_data.columns[:5])}...\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"⚠️  기상 데이터 파일을 찾을 수 없음 (weather_daily_processed.csv)\")\n",
    "            self.weather_data = None\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 기상 데이터 로딩 실패: {e}\")\n",
    "            self.weather_data = None\n",
    "        \n",
    "        try:\n",
    "            # 달력 데이터 로딩\n",
    "            print(\"\\n📅 달력 데이터 로딩 중...\")\n",
    "            self.calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')\n",
    "            \n",
    "            # 날짜 컬럼 전처리\n",
    "            self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date'])\n",
    "            \n",
    "            print(f\"✅ 달력 데이터 로딩 완료: {len(self.calendar_data):,}일\")\n",
    "            print(f\"   기간: {self.calendar_data['date'].min().date()} ~ {self.calendar_data['date'].max().date()}\")\n",
    "            print(f\"   컬럼: {len(self.calendar_data.columns)}개 - {list(self.calendar_data.columns[:5])}...\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"⚠️  달력 데이터 파일을 찾을 수 없음 (power_analysis_calendar_2022_2025.csv)\")\n",
    "            self.calendar_data = None\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 달력 데이터 로딩 실패: {e}\")\n",
    "            self.calendar_data = None\n",
    "    \n",
    "    def _create_customer_profiles_from_data(self, customer_df=None):\n",
    "        \"\"\"실제 고객 정보를 기반으로 고객 프로파일 생성\"\"\"\n",
    "        profiles = {}\n",
    "        \n",
    "        if customer_df is not None:\n",
    "            # 실제 고객 데이터가 있는 경우\n",
    "            for _, customer in customer_df.iterrows():\n",
    "                customer_id = customer['고객번호']\n",
    "                \n",
    "                # 계약전력에서 base_power 추정\n",
    "                contract_power = customer['계약전력']\n",
    "                if '100~199' in str(contract_power):\n",
    "                    base_power = np.random.uniform(80, 120)\n",
    "                elif '200~299' in str(contract_power):\n",
    "                    base_power = np.random.uniform(120, 180)\n",
    "                elif '400~499' in str(contract_power):\n",
    "                    base_power = np.random.uniform(200, 280)\n",
    "                elif '500~599' in str(contract_power):\n",
    "                    base_power = np.random.uniform(280, 360)\n",
    "                elif '700~799' in str(contract_power):\n",
    "                    base_power = np.random.uniform(400, 500)\n",
    "                elif '800~899' in str(contract_power):\n",
    "                    base_power = np.random.uniform(500, 650)\n",
    "                else:\n",
    "                    base_power = np.random.uniform(100, 200)\n",
    "                \n",
    "                # 사용용도별 패턴 정의\n",
    "                usage_type = customer['사용용도']\n",
    "                if '상업용' in str(usage_type):\n",
    "                    peak_hours = [9, 14, 19]  # 상업용: 오전, 오후, 저녁 피크\n",
    "                    weekend_factor = np.random.uniform(0.7, 1.3)  # 상업용은 주말 변동 큼\n",
    "                else:  # 광공업용\n",
    "                    peak_hours = [8, 13, 18]  # 산업용: 작업시간 피크\n",
    "                    weekend_factor = np.random.uniform(0.1, 0.4)  # 산업용은 주말 낮음\n",
    "                \n",
    "                # 계약종별 세부 조정\n",
    "                contract_type = customer['계약종별']\n",
    "                if '일반용' in str(contract_type):\n",
    "                    # 일반용은 더 불규칙한 패턴\n",
    "                    weekend_factor *= np.random.uniform(0.8, 1.5)\n",
    "                \n",
    "                # 산업분류별 특성 반영 (있는 경우)\n",
    "                industry = customer.get('산업분류(소)', '')\n",
    "                if '제조' in str(industry) or '생산' in str(industry):\n",
    "                    # 제조업은 더 일정한 패턴\n",
    "                    weekend_factor *= 0.3\n",
    "                    peak_hours = [8, 13, 18, 22]  # 교대근무 고려\n",
    "                \n",
    "                profiles[customer_id] = {\n",
    "                    'type': self._classify_business_type(customer),\n",
    "                    'base_power': round(base_power, 1),\n",
    "                    'peak_hours': peak_hours,\n",
    "                    'weekend_factor': round(weekend_factor, 2),\n",
    "                    'contract_power': contract_power,\n",
    "                    'usage_type': usage_type\n",
    "                }\n",
    "        else:\n",
    "            # 샘플 데이터용 기본 프로파일\n",
    "            profiles = self._get_default_sample_profiles()\n",
    "        \n",
    "        return profiles\n",
    "    \n",
    "    def _classify_business_type(self, customer):\n",
    "        \"\"\"고객 정보를 바탕으로 업종 분류\"\"\"\n",
    "        usage_type = str(customer['사용용도'])\n",
    "        contract_type = str(customer['계약종별'])\n",
    "        industry = str(customer.get('산업분류(소)', ''))\n",
    "        product = str(customer.get('주생산품', ''))\n",
    "        \n",
    "        # 사용용도 기반 1차 분류\n",
    "        if '상업용' in usage_type:\n",
    "            if '일반용' in contract_type:\n",
    "                return 'commercial'  # 상가, 사무소 등\n",
    "            else:\n",
    "                return 'service'     # 대형 서비스업\n",
    "        else:  # 광공업용\n",
    "            if any(keyword in industry.lower() for keyword in ['제조', '생산', '공장']):\n",
    "                return 'manufacturing'\n",
    "            else:\n",
    "                return 'industrial'\n",
    "        \n",
    "    def _get_default_sample_profiles(self):\n",
    "        \"\"\"샘플 데이터용 기본 프로파일\"\"\"\n",
    "        return {\n",
    "            'A1001': {'type': 'hospital', 'base_power': 120, 'peak_hours': [9, 14, 20], 'weekend_factor': 0.8},\n",
    "            'A1002': {'type': 'office', 'base_power': 80, 'peak_hours': [9, 14], 'weekend_factor': 0.3},\n",
    "            'A1003': {'type': 'retail', 'base_power': 100, 'peak_hours': [11, 15, 19], 'weekend_factor': 1.2},\n",
    "            'A1004': {'type': 'factory', 'base_power': 150, 'peak_hours': [8, 13, 18], 'weekend_factor': 0.1},\n",
    "            'A1005': {'type': 'restaurant', 'base_power': 90, 'peak_hours': [12, 18], 'weekend_factor': 1.1},\n",
    "            'A1006': {'type': 'gym', 'base_power': 70, 'peak_hours': [7, 18, 21], 'weekend_factor': 1.3},\n",
    "            'A1007': {'type': 'school', 'base_power': 110, 'peak_hours': [10, 14], 'weekend_factor': 0.2},\n",
    "            'A1008': {'type': 'hotel', 'base_power': 130, 'peak_hours': [8, 20], 'weekend_factor': 1.0},\n",
    "            'A1009': {'type': 'warehouse', 'base_power': 60, 'peak_hours': [9, 16], 'weekend_factor': 0.5},\n",
    "            'A1010': {'type': 'clinic', 'base_power': 85, 'peak_hours': [10, 15], 'weekend_factor': 0.6}\n",
    "        }\n",
    "\n",
    "    def _create_comprehensive_sample_data(self, customer_df=None):\n",
    "        \"\"\"포괄적인 테스트용 샘플 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31  # 3월 전체\n",
    "        \n",
    "        # 동적 고객 프로파일 생성\n",
    "        customer_profiles = self._create_customer_profiles_from_data(customer_df)\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = customer_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                is_weekend = current_date.weekday() >= 5\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # 기본 전력 계산\n",
    "                        base_power = profile['base_power']\n",
    "                        \n",
    "                        # 시간대별 패턴 (사인파 기반)\n",
    "                        time_factor = 0.3 + 0.7 * (np.sin(2 * np.pi * hour / 24) + 1) / 2\n",
    "                        \n",
    "                        # 피크 시간 보정\n",
    "                        peak_factor = 1.0\n",
    "                        for peak_hour in profile['peak_hours']:\n",
    "                            if abs(hour - peak_hour) <= 1:\n",
    "                                peak_factor = 1.5\n",
    "                        \n",
    "                        # 주말 보정\n",
    "                        weekend_factor = profile['weekend_factor'] if is_weekend else 1.0\n",
    "                        \n",
    "                        # 최종 전력 계산\n",
    "                        power = base_power * time_factor * peak_factor * weekend_factor\n",
    "                        power += np.random.normal(0, power * 0.1)  # 10% 노이즈\n",
    "                        power = max(0, power)\n",
    "                        \n",
    "                        # 무효전력 계산\n",
    "                        reactive_lag = power * np.random.uniform(0.1, 0.3)\n",
    "                        reactive_lead = power * np.random.uniform(0.05, 0.15)\n",
    "                        apparent_power = np.sqrt(power**2 + (reactive_lag - reactive_lead)**2)\n",
    "                        \n",
    "                        data.append({\n",
    "                            '대체고객번호': customer,\n",
    "                            'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            '순방향유효전력': round(power, 1),\n",
    "                            '지상무효': round(reactive_lag, 1),\n",
    "                            '진상무효': round(reactive_lead, 1),\n",
    "                            '피상전력': round(apparent_power, 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def merge_with_external_data(self):\n",
    "        \"\"\"LP 데이터와 외부 데이터 결합\"\"\"\n",
    "        if self.lp_data is None:\n",
    "            print(\"❌ LP 데이터가 로딩되지 않음\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n=== 외부 데이터와 결합 ===\")\n",
    "        \n",
    "        # 일별 LP 데이터 집계\n",
    "        daily_lp = self.lp_data.groupby(['대체고객번호', 'date'])['순방향유효전력'].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "        daily_lp.columns = ['대체고객번호', 'date', 'daily_sum', 'daily_mean', 'daily_std', 'daily_min', 'daily_max']\n",
    "        daily_lp['date'] = pd.to_datetime(daily_lp['date'])\n",
    "        \n",
    "        # 기상 데이터와 결합\n",
    "        if self.weather_data is not None:\n",
    "            print(\"🌤️  기상 데이터 결합 중...\")\n",
    "            merged_data = daily_lp.merge(\n",
    "                self.weather_data[['date', '평균기온', '최고기온', '최저기온', '평균습도', '총강수량', '불쾌지수', '냉방필요도', '난방필요도']],\n",
    "                on='date',\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"   ✅ 기상 데이터 결합 완료: {merged_data['평균기온'].notna().sum():,}일 매칭\")\n",
    "        else:\n",
    "            merged_data = daily_lp.copy()\n",
    "        \n",
    "        # 달력 데이터와 결합\n",
    "        if self.calendar_data is not None:\n",
    "            print(\"📅 달력 데이터 결합 중...\")\n",
    "            merged_data = merged_data.merge(\n",
    "                self.calendar_data[['date', 'is_workday', 'is_weekend', 'is_holiday', 'is_consecutive_holiday', 'day_type']],\n",
    "                on='date',\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"   ✅ 달력 데이터 결합 완료: {merged_data['is_workday'].notna().sum():,}일 매칭\")\n",
    "        \n",
    "        # 결합된 데이터 저장\n",
    "        self.merged_daily_data = merged_data\n",
    "        \n",
    "        print(f\"\\n📊 결합된 데이터 요약:\")\n",
    "        print(f\"   총 레코드: {len(merged_data):,}개\")\n",
    "        print(f\"   고객 수: {merged_data['대체고객번호'].nunique():,}명\")\n",
    "        print(f\"   기간: {merged_data['date'].min().date()} ~ {merged_data['date'].max().date()}\")\n",
    "        print(f\"   컬럼 수: {len(merged_data.columns)}개\")\n",
    "        \n",
    "        return merged_data\n",
    "    \n",
    "    def analyze_weather_impact(self):\n",
    "        \"\"\"기상 요인이 전력 사용량에 미치는 영향 분석\"\"\"\n",
    "        if not hasattr(self, 'merged_daily_data') or self.weather_data is None:\n",
    "            print(\"⚠️  기상 데이터가 없어 기상 영향 분석을 건너뜁니다.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n=== 기상 요인 영향 분석 ===\")\n",
    "        \n",
    "        # 기상 변수별 상관관계 분석\n",
    "        weather_cols = ['평균기온', '최고기온', '최저기온', '평균습도', '총강수량', '불쾌지수', '냉방필요도', '난방필요도']\n",
    "        power_cols = ['daily_sum', 'daily_mean']\n",
    "        \n",
    "        correlations = {}\n",
    "        for weather_col in weather_cols:\n",
    "            if weather_col in self.merged_daily_data.columns:\n",
    "                corr_sum = self.merged_daily_data[weather_col].corr(self.merged_daily_data['daily_sum'])\n",
    "                corr_mean = self.merged_daily_data[weather_col].corr(self.merged_daily_data['daily_mean'])\n",
    "                correlations[weather_col] = {'sum': corr_sum, 'mean': corr_mean}\n",
    "        \n",
    "        print(\"🌡️  기상 요인별 상관관계 (전력사용량):\")\n",
    "        print(\"기상요인\\t\\t일총사용량\\t일평균사용량\")\n",
    "        for weather_factor, corrs in correlations.items():\n",
    "            print(f\"{weather_factor}\\t{corrs['sum']:.3f}\\t\\t{corrs['mean']:.3f}\")\n",
    "        \n",
    "        # 온도별 전력 사용 패턴\n",
    "        temp_bins = [-10, 0, 10, 20, 25, 30, 35, 50]\n",
    "        temp_labels = ['극한저온', '저온', '서늘', '적정', '따뜻', '더움', '고온']\n",
    "        \n",
    "        if '평균기온' in self.merged_daily_data.columns:\n",
    "            self.merged_daily_data['temp_category'] = pd.cut(\n",
    "                self.merged_daily_data['평균기온'], \n",
    "                bins=temp_bins, \n",
    "                labels=temp_labels, \n",
    "                include_lowest=True\n",
    "            )\n",
    "            \n",
    "            temp_power = self.merged_daily_data.groupby('temp_category')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "            \n",
    "            print(f\"\\n🌡️  온도별 전력 사용 패턴:\")\n",
    "            print(\"온도구간\\t일수\\t평균사용량\\t표준편차\")\n",
    "            for temp_cat in temp_power.index:\n",
    "                stats = temp_power.loc[temp_cat]\n",
    "                print(f\"{temp_cat}\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        return correlations\n",
    "    \n",
    "    def analyze_calendar_patterns(self):\n",
    "        \"\"\"달력 요인별 전력 사용 패턴 분석\"\"\"\n",
    "        if not hasattr(self, 'merged_daily_data') or self.calendar_data is None:\n",
    "            print(\"⚠️  달력 데이터가 없어 달력 패턴 분석을 건너뜁니다.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n=== 달력 패턴 분석 ===\")\n",
    "        \n",
    "        # 평일/주말/휴일별 패턴\n",
    "        if 'day_type' in self.merged_daily_data.columns:\n",
    "            day_type_stats = self.merged_daily_data.groupby('day_type')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "            \n",
    "            print(\"📅 일자 유형별 전력 사용 패턴:\")\n",
    "            print(\"일자유형\\t일수\\t평균사용량\\t표준편차\")\n",
    "            for day_type in day_type_stats.index:\n",
    "                stats = day_type_stats.loc[day_type]\n",
    "                print(f\"{day_type}\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        # 연휴 효과 분석\n",
    "        if 'is_consecutive_holiday' in self.merged_daily_data.columns:\n",
    "            holiday_effect = self.merged_daily_data.groupby('is_consecutive_holiday')['daily_mean'].agg(['count', 'mean']).round(1)\n",
    "            \n",
    "            print(f\"\\n🎊 연휴 효과 분석:\")\n",
    "            for is_holiday, stats in holiday_effect.iterrows():\n",
    "                holiday_label = \"연휴기간\" if is_holiday else \"일반기간\"\n",
    "                print(f\"{holiday_label}: 일수 {stats['count']}일, 평균 {stats['mean']}kW\")\n",
    "        \n",
    "        # 월별 패턴 (계절성)\n",
    "        self.merged_daily_data['month'] = self.merged_daily_data['date'].dt.month\n",
    "        monthly_stats = self.merged_daily_data.groupby('month')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "        \n",
    "        print(f\"\\n📊 월별 전력 사용 패턴:\")\n",
    "        print(\"월\\t일수\\t평균사용량\\t표준편차\")\n",
    "        for month in monthly_stats.index:\n",
    "            stats = monthly_stats.loc[month]\n",
    "            print(f\"{month}월\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        return {\n",
    "            'day_type_stats': day_type_stats if 'day_type' in self.merged_daily_data.columns else None,\n",
    "            'holiday_effect': holiday_effect if 'is_consecutive_holiday' in self.merged_daily_data.columns else None,\n",
    "            'monthly_stats': monthly_stats\n",
    "        }\n",
    "    \n",
    "    def analyze_hourly_patterns(self):\n",
    "        \"\"\"시간대별 전력 사용 패턴 분석\"\"\"\n",
    "        print(\"\\n=== 시간대별 전력 사용 패턴 분석 ===\")\n",
    "        \n",
    "        # 시간대별 평균 사용량\n",
    "        hourly_avg = self.lp_data.groupby('hour')['순방향유효전력'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "        \n",
    "        print(\"📊 시간대별 평균 전력 사용량 (kW):\")\n",
    "        print(\"시간\\t평균\\t표준편차\\t최소\\t최대\")\n",
    "        for hour in range(24):\n",
    "            stats = hourly_avg.loc[hour]\n",
    "            print(f\"{hour:02d}시\\t{stats['mean']}\\t{stats['std']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # 피크/비피크 시간대 식별\n",
    "        peak_threshold = hourly_avg['mean'].quantile(0.8)\n",
    "        peak_hours = hourly_avg[hourly_avg['mean'] >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = hourly_avg[hourly_avg['mean'] < hourly_avg['mean'].quantile(0.3)].index.tolist()\n",
    "        \n",
    "        print(f\"\\n⚡ 피크 시간대 (상위 20%): {peak_hours}시\")\n",
    "        print(f\"💤 비피크 시간대 (하위 30%): {off_peak_hours}시\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_stats': hourly_avg,\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours\n",
    "        }\n",
    "    \n",
    "    def analyze_daily_patterns(self):\n",
    "        \"\"\"일별/요일별 패턴 분석\"\"\"\n",
    "        print(\"\\n=== 일별/요일별 패턴 분석 ===\")\n",
    "        \n",
    "        # 일별 총 사용량\n",
    "        daily_usage = self.lp_data.groupby(['대체고객번호', 'date'])['순방향유효전력'].sum().reset_index()\n",
    "        daily_usage['weekday'] = pd.to_datetime(daily_usage['date']).dt.weekday\n",
    "        daily_usage['is_weekend'] = daily_usage['weekday'].isin([5, 6])\n",
    "        \n",
    "        # 요일별 평균 사용량\n",
    "        weekday_avg = daily_usage.groupby('weekday')['순방향유효전력'].agg(['mean', 'std']).round(1)\n",
    "        weekday_names = ['월', '화', '수', '목', '금', '토', '일']\n",
    "        \n",
    "        print(\"📅 요일별 평균 일간 사용량 (kWh):\")\n",
    "        for i, day_name in enumerate(weekday_names):\n",
    "            stats = weekday_avg.loc[i]\n",
    "            print(f\"{day_name}요일: {stats['mean']:,.1f} ± {stats['std']:.1f}\")\n",
    "        \n",
    "        # 평일 vs 주말 비교\n",
    "        weekday_mean = daily_usage[~daily_usage['is_weekend']]['순방향유효전력'].mean()\n",
    "        weekend_mean = daily_usage[daily_usage['is_weekend']]['순방향유효전력'].mean()\n",
    "        weekend_ratio = weekend_mean / weekday_mean\n",
    "        \n",
    "        print(f\"\\n📊 평일 vs 주말 비교:\")\n",
    "        print(f\"평일 평균: {weekday_mean:,.1f} kWh\")\n",
    "        print(f\"주말 평균: {weekend_mean:,.1f} kWh\")\n",
    "        print(f\"주말/평일 비율: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'daily_usage': daily_usage,\n",
    "            'weekday_stats': weekday_avg,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "    \n",
    "    def analyze_customer_profiles(self):\n",
    "        \"\"\"고객별 사용량 프로파일 분석\"\"\"\n",
    "        print(\"\\n=== 고객별 사용량 프로파일 분석 ===\")\n",
    "        \n",
    "        # 고객별 기본 통계\n",
    "        customer_stats = self.lp_data.groupby('대체고객번호')['순방향유효전력'].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max'\n",
    "        ]).round(1)\n",
    "        customer_stats['cv'] = (customer_stats['std'] / customer_stats['mean']).round(3)  # 변동계수\n",
    "        \n",
    "        print(\"👥 고객별 기본 통계 (kW):\")\n",
    "        print(\"고객번호\\t평균\\t표준편차\\t변동계수\\t최소\\t최대\")\n",
    "        for customer in customer_stats.index:\n",
    "            stats = customer_stats.loc[customer]\n",
    "            print(f\"{customer}\\t{stats['mean']}\\t{stats['std']}\\t{stats['cv']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # 사용량 규모별 분류\n",
    "        mean_usage = customer_stats['mean']\n",
    "        high_users = mean_usage[mean_usage >= mean_usage.quantile(0.8)].index.tolist()\n",
    "        low_users = mean_usage[mean_usage <= mean_usage.quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\n📈 대용량 사용자 (상위 20%): {high_users}\")\n",
    "        print(f\"📉 소용량 사용자 (하위 20%): {low_users}\")\n",
    "        \n",
    "        # 변동성별 분류\n",
    "        high_volatility = customer_stats[customer_stats['cv'] >= customer_stats['cv'].quantile(0.8)].index.tolist()\n",
    "        low_volatility = customer_stats[customer_stats['cv'] <= customer_stats['cv'].quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\n🌊 고변동성 고객: {high_volatility}\")\n",
    "        print(f\"📊 저변동성 고객: {low_volatility}\")\n",
    "        \n",
    "        return {\n",
    "            'customer_stats': customer_stats,\n",
    "            'usage_segments': {\n",
    "                'high_users': high_users,\n",
    "                'low_users': low_users,\n",
    "                'high_volatility': high_volatility,\n",
    "                'low_volatility': low_volatility\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_load_factors(self):\n",
    "        \"\"\"부하율 및 효율성 지표 계산\"\"\"\n",
    "        print(\"\\n=== 부하율 및 효율성 지표 계산 ===\")\n",
    "        \n",
    "        # 고객별 부하율 계산\n",
    "        customer_load_factors = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "            \n",
    "            avg_load = customer_data['순방향유효전력'].mean()\n",
    "            max_load = customer_data['순방향유효전력'].max()\n",
    "            load_factor = avg_load / max_load if max_load > 0 else 0\n",
    "            \n",
    "            # 피크 집중도 (피크 시간대 사용량 비중)\n",
    "            peak_hours = [9, 14, 18]  # 대표 피크 시간\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['순방향유효전력'].mean()\n",
    "            total_avg = customer_data['순방향유효전력'].mean()\n",
    "            peak_concentration = peak_usage / total_avg if total_avg > 0 else 0\n",
    "            \n",
    "            customer_load_factors[customer] = {\n",
    "                'load_factor': round(load_factor, 3),\n",
    "                'peak_concentration': round(peak_concentration, 3),\n",
    "                'avg_load': round(avg_load, 1),\n",
    "                'max_load': round(max_load, 1)\n",
    "            }\n",
    "        \n",
    "        print(\"⚡ 고객별 부하율 및 피크 집중도:\")\n",
    "        print(\"고객번호\\t부하율\\t피크집중도\\t평균부하\\t최대부하\")\n",
    "        for customer, metrics in customer_load_factors.items():\n",
    "            print(f\"{customer}\\t{metrics['load_factor']}\\t{metrics['peak_concentration']}\\t{metrics['avg_load']}\\t{metrics['max_load']}\")\n",
    "        \n",
    "        # 전체 부하율 분포\n",
    "        load_factors = [metrics['load_factor'] for metrics in customer_load_factors.values()]\n",
    "        avg_load_factor = np.mean(load_factors)\n",
    "        \n",
    "        print(f\"\\n📊 전체 부하율 분포:\")\n",
    "        print(f\"평균 부하율: {avg_load_factor:.3f}\")\n",
    "        print(f\"부하율 범위: {min(load_factors):.3f} ~ {max(load_factors):.3f}\")\n",
    "        \n",
    "        return customer_load_factors\n",
    "    \n",
    "    def detect_usage_anomalies(self):\n",
    "        \"\"\"사용량 이상 패턴 탐지\"\"\"\n",
    "        print(\"\\n=== 사용량 이상 패턴 탐지 ===\")\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].copy()\n",
    "            customer_data = customer_data.sort_values('datetime')\n",
    "            \n",
    "            # 1. 급격한 변화 탐지 (전시점 대비 200% 이상 변화)\n",
    "            customer_data['power_change'] = customer_data['순방향유효전력'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['power_change']) > 2.0]\n",
    "            \n",
    "            # 2. 연속적인 0값 탐지 (2시간 이상)\n",
    "            customer_data['is_zero'] = customer_data['순방향유효전력'] == 0\n",
    "            customer_data['zero_group'] = (customer_data['is_zero'] != customer_data['is_zero'].shift()).cumsum()\n",
    "            zero_periods = customer_data[customer_data['is_zero']].groupby('zero_group').size()\n",
    "            long_zero_periods = zero_periods[zero_periods >= 8]  # 2시간 = 8개 15분 구간\n",
    "            \n",
    "            # 3. 통계적 이상치 (Z-score > 3)\n",
    "            mean_power = customer_data['순방향유효전력'].mean()\n",
    "            std_power = customer_data['순방향유효전력'].std()\n",
    "            if std_power > 0:\n",
    "                customer_data['z_score'] = abs(customer_data['순방향유효전력'] - mean_power) / std_power\n",
    "                statistical_outliers = customer_data[customer_data['z_score'] > 3]\n",
    "            else:\n",
    "                statistical_outliers = pd.DataFrame()\n",
    "            \n",
    "            # 이상치 정보 저장\n",
    "            if len(sudden_changes) > 0 or len(long_zero_periods) > 0 or len(statistical_outliers) > 0:\n",
    "                anomalies.append({\n",
    "                    'customer': customer,\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'long_zero_periods': len(long_zero_periods),\n",
    "                    'statistical_outliers': len(statistical_outliers)\n",
    "                })\n",
    "        \n",
    "        print(\"🚨 이상 패턴 탐지 결과:\")\n",
    "        if anomalies:\n",
    "            print(\"고객번호\\t급격변화\\t장기0값\\t통계이상치\")\n",
    "            for anomaly in anomalies:\n",
    "                print(f\"{anomaly['customer']}\\t{anomaly['sudden_changes']}\\t{anomaly['long_zero_periods']}\\t{anomaly['statistical_outliers']}\")\n",
    "        else:\n",
    "            print(\"✅ 심각한 이상 패턴 없음\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def generate_enhanced_pattern_summary(self):\n",
    "        \"\"\"강화된 패턴 분석 종합 요약\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 강화된 시계열 패턴 분석 종합 요약\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 외부 데이터 로딩\n",
    "        self.load_external_data()\n",
    "        \n",
    "        # 데이터 결합\n",
    "        merged_data = self.merge_with_external_data()\n",
    "        \n",
    "        # 기존 패턴 분석\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        # 강화된 분석 (외부 데이터 활용)\n",
    "        weather_impact = self.analyze_weather_impact()\n",
    "        calendar_patterns = self.analyze_calendar_patterns()\n",
    "        \n",
    "        print(\"\\n🔍 주요 발견사항:\")\n",
    "        \n",
    "        # 1. 시간 패턴\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  • 주요 피크 시간: {peak_hours}시\")\n",
    "        \n",
    "        # 2. 요일 패턴  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  • 주말/평일 사용량 비율: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. 고객 다양성\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  • 고객별 변동계수 범위: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. 부하율\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  • 평균 부하율: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. 이상 패턴\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  • 이상 패턴 고객: {anomaly_customers}명\")\n",
    "        \n",
    "        # 6. 기상 영향 (있는 경우)\n",
    "        if weather_impact:\n",
    "            temp_corr = weather_impact.get('평균기온', {}).get('mean', 0)\n",
    "            humidity_corr = weather_impact.get('평균습도', {}).get('mean', 0)\n",
    "            print(f\"  • 기온과 전력사용량 상관관계: {temp_corr:.3f}\")\n",
    "            print(f\"  • 습도와 전력사용량 상관관계: {humidity_corr:.3f}\")\n",
    "        \n",
    "        # 7. 달력 효과 (있는 경우)\n",
    "        if calendar_patterns and calendar_patterns.get('holiday_effect') is not None:\n",
    "            holiday_effect = calendar_patterns['holiday_effect']\n",
    "            if len(holiday_effect) >= 2:\n",
    "                normal_avg = holiday_effect.loc[False, 'mean'] if False in holiday_effect.index else 0\n",
    "                holiday_avg = holiday_effect.loc[True, 'mean'] if True in holiday_effect.index else 0\n",
    "                if normal_avg > 0:\n",
    "                    holiday_ratio = holiday_avg / normal_avg\n",
    "                    print(f\"  • 연휴/일반 사용량 비율: {holiday_ratio:.3f}\")\n",
    "        \n",
    "        print(\"\\n💡 강화된 변동계수 설계를 위한 인사이트:\")\n",
    "        print(\"  1. 시간대별 가중치 필요 (피크/비피크 구분)\")\n",
    "        print(\"  2. 요일별 보정 계수 고려\") \n",
    "        print(\"  3. 고객별 기준 변동성 설정\")\n",
    "        print(\"  4. 부하율과 변동성의 상관관계 분석\")\n",
    "        print(\"  5. 기상 요인 보정 (온도, 습도, 강수량)\")\n",
    "        print(\"  6. 달력 효과 반영 (휴일, 연휴, 계절성)\")\n",
    "        print(\"  7. 다차원 변동성 지표 조합 검토\")\n",
    "        print(\"  8. 이상 패턴 필터링 메커니즘\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies,\n",
    "            'weather_impact': weather_impact,\n",
    "            'calendar_patterns': calendar_patterns,\n",
    "            'merged_data': merged_data\n",
    "        }\n",
    "    \n",
    "    def generate_pattern_summary(self):\n",
    "        \"\"\"기존 패턴 분석 종합 요약 (하위 호환성)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 시계열 패턴 분석 종합 요약\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 주요 패턴 특성\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        print(\"\\n🔍 주요 발견사항:\")\n",
    "        \n",
    "        # 1. 시간 패턴\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  • 주요 피크 시간: {peak_hours}시\")\n",
    "        \n",
    "        # 2. 요일 패턴  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  • 주말/평일 사용량 비율: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. 고객 다양성\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  • 고객별 변동계수 범위: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. 부하율\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  • 평균 부하율: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. 이상 패턴\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  • 이상 패턴 고객: {anomaly_customers}명\")\n",
    "        \n",
    "        print(\"\\n💡 변동계수 설계를 위한 인사이트:\")\n",
    "        print(\"  1. 시간대별 가중치 필요 (피크/비피크 구분)\")\n",
    "        print(\"  2. 요일별 보정 계수 고려\") \n",
    "        print(\"  3. 고객별 기준 변동성 설정\")\n",
    "        print(\"  4. 부하율과 변동성의 상관관계 분석\")\n",
    "        print(\"  5. 다차원 변동성 지표 조합 검토\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies\n",
    "        }\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer()\n",
    "    \n",
    "    print(\"🔧 실제 환경에서 사용 방법 (3000호):\")\n",
    "    print(\"\"\"\n",
    "    # 1. 실제 LP 데이터 로딩\n",
    "    lp_files = ['LP데이터1.csv', 'LP데이터2.csv']\n",
    "    analyzer.load_real_lp_data(lp_files)\n",
    "    \n",
    "    # 2. 기상 및 달력 데이터 자동 로딩하여 강화된 분석\n",
    "    enhanced_summary = analyzer.generate_enhanced_pattern_summary()\n",
    "    \n",
    "    # generate_enhanced_pattern_summary() 내부에서 자동으로:\n",
    "    # - weather_daily_processed.csv 로딩\n",
    "    # - power_analysis_calendar_2022_2025.csv 로딩\n",
    "    # - LP 데이터와 결합하여 분석\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🧪 현재는 테스트용 샘플 데이터로 시연\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 테스트용으로만 샘플 데이터 생성\n",
    "    lp_data = analyzer.load_sample_data()\n",
    "    \n",
    "    # 강화된 패턴 분석 실행 (weather + calendar 데이터 포함)\n",
    "    print(\"\\n🌤️📅 기상 및 달력 데이터를 포함한 강화된 분석:\")\n",
    "    enhanced_summary = analyzer.generate_enhanced_pattern_summary()\n",
    "    \n",
    "    print(\"\\n💡 핵심 포인트:\")\n",
    "    print(\"  ✅ 실제 환경: 실제 LP 데이터 + 기상 데이터 + 달력 데이터\")\n",
    "    print(\"  ✅ weather_daily_processed.csv: 온도, 습도, 강수량 등 기상 요인\")\n",
    "    print(\"  ✅ power_analysis_calendar_2022_2025.csv: 휴일, 연휴, 평일/주말\")\n",
    "    print(\"  ✅ 결합 분석: 기상/달력 요인이 전력 사용량에 미치는 영향\")\n",
    "    print(\"  ✅ 변동성 지표: 외부 요인을 고려한 정교한 변동계수 개발\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77981da6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 변동성 분석을 위한 데이터 준비 ===\n",
      "✅ 데이터 준비 완료: 29,760레코드\n",
      "\n",
      "============================================================\n",
      "📊 변동성 분석 종합 리포트\n",
      "============================================================\n",
      "\n",
      "=== 기본 변동성 지표 계산 ===\n",
      "고객번호\tCV\t범위변동성\tIQR변동성\tMAD변동성\t수익률변동성\n",
      "A1001\t0.049\t0.3511\t0.0651\t0.0385\t0.07\n",
      "A1002\t0.4806\t4.2717\t0.4656\t0.3194\tnan\n",
      "A1003\t0.2469\t1.4009\t0.4039\t0.2095\t0.1471\n",
      "A1004\t0.1861\t1.1296\t0.2589\t0.1497\t0.229\n",
      "A1005\t0.242\t1.5726\t0.3375\t0.1944\t0.3202\n",
      "A1006\t0.1341\t1.9821\t0.1355\t0.0888\t0.1678\n",
      "A1007\t0.2917\t2.3547\t0.2276\t0.1914\tnan\n",
      "A1008\t0.4125\t2.3961\t0.5556\t0.3295\tnan\n",
      "A1009\t0.1757\t1.2397\t0.235\t0.1402\t0.2752\n",
      "A1010\t0.3447\t2.3695\t0.4607\t0.2746\tnan\n",
      "\n",
      "=== 시간 윈도우별 변동성 분석 ===\n",
      "윈도우별 평균 변동계수:\n",
      "고객번호\t시간별\t일별\t주별\n",
      "A1001\t0.0453\t0.0489\t0.049\n",
      "A1002\t0.3818\t0.4716\t0.4805\n",
      "A1003\t0.0941\t0.2393\t0.2462\n",
      "A1004\t0.138\t0.1473\t0.1495\n",
      "A1005\t0.1825\t0.1958\t0.2031\n",
      "A1006\t0.0985\t0.1259\t0.1332\n",
      "A1007\t0.2312\t0.2437\t0.2844\n",
      "A1008\t0.3991\t0.4132\t0.4118\n",
      "A1009\t0.163\t0.1749\t0.1753\n",
      "A1010\t0.3284\t0.3449\t0.3452\n",
      "\n",
      "=== 방향성 변동성 분석 ===\n",
      "고객번호\t상승변동성\t하락변동성\t비대칭비율\t급증횟수\t급감횟수\n",
      "A1001\t0.0456\t0.0386\t1.1793\t0\t0\n",
      "A1002\tnan\t0.2268\tnan\t676\t370\n",
      "A1003\t0.1041\t0.0767\t1.3563\t13\t0\n",
      "A1004\t0.1792\t0.1065\t1.6822\t105\t4\n",
      "A1005\t0.2735\t0.132\t2.0712\t233\t33\n",
      "A1006\t0.1446\t0.0819\t1.7652\t21\t6\n",
      "A1007\tnan\t0.2183\tnan\t314\t181\n",
      "A1008\tnan\t0.2381\tnan\t782\t409\n",
      "A1009\t0.2223\t0.1193\t1.8638\t160\t15\n",
      "A1010\tnan\t0.2079\tnan\t641\t300\n",
      "\n",
      "=== 패턴 안정성 분석 ===\n",
      "고객번호\t패턴일관성\t일일주기성\t자기상관\n",
      "A1001\t0.0302\t0.6329\t0.9976\n",
      "A1002\t0.0039\t2.0012\t0.8167\n",
      "A1003\t0.9462\t68.7362\t0.9899\n",
      "A1004\t-0.0045\t0.3761\t0.9779\n",
      "A1005\t-0.01\t0.5237\t0.9634\n",
      "A1006\t0.0461\t3.0904\t0.988\n",
      "A1007\t-0.0079\t1.0726\t0.9217\n",
      "A1008\t-0.0115\t1.4058\t0.8554\n",
      "A1009\t0.0084\t1.1471\t0.9701\n",
      "A1010\t-0.0024\t0.6642\t0.8931\n",
      "\n",
      "=== 복합 변동성 스코어 계산 ===\n",
      "고객번호\t기본\t윈도우\t방향성\t안정성\t복합점수\n",
      "A1001\t0.049\t0.0477\t0.0421\t0.9698\t0.2314\n",
      "A1002\t0.4806\t0.4446\tnan\t0.9961\tnan\n",
      "A1003\t0.2469\t0.1932\t0.0904\t0.0538\t0.1609\n",
      "A1004\t0.1861\t0.1449\t0.1429\t1.0045\t0.3288\n",
      "A1005\t0.242\t0.1938\t0.2028\t1.01\t0.3733\n",
      "A1006\t0.1341\t0.1192\t0.1133\t0.9539\t0.2894\n",
      "A1007\t0.2917\t0.2531\tnan\t1.0079\tnan\n",
      "A1008\t0.4125\t0.408\tnan\t1.0115\tnan\n",
      "A1009\t0.1757\t0.1711\t0.1708\t0.9916\t0.3365\n",
      "A1010\t0.3447\t0.3395\tnan\t1.0024\tnan\n",
      "\n",
      "📊 변동성 등급 기준:\n",
      "  저변동성: < nan\n",
      "  중변동성: nan ~ nan\n",
      "  고변동성: > nan\n",
      "\n",
      "등급별 고객 분류:\n",
      "  A1001: 0.231 (고변동성)\n",
      "  A1002: nan (고변동성)\n",
      "  A1003: 0.161 (고변동성)\n",
      "  A1004: 0.329 (고변동성)\n",
      "  A1005: 0.373 (고변동성)\n",
      "  A1006: 0.289 (고변동성)\n",
      "  A1007: nan (고변동성)\n",
      "  A1008: nan (고변동성)\n",
      "  A1009: 0.337 (고변동성)\n",
      "  A1010: nan (고변동성)\n",
      "\n",
      "🎯 변동계수 알고리즘 설계 인사이트:\n",
      "  1. 다차원 변동성 지표의 필요성 확인\n",
      "  2. 시간 윈도우별 차별화된 가중치 적용\n",
      "  3. 방향성 변동성으로 리스크 비대칭성 포착\n",
      "  4. 패턴 안정성으로 예측가능성 평가\n",
      "  5. 복합 스코어를 통한 종합적 변동성 평가\n",
      "\n",
      "💡 스태킹 알고리즘 설계 방향:\n",
      "  • Level-0 모델: 각 변동성 지표를 개별 모델로 구성\n",
      "  • Level-1 메타모델: 가중 결합으로 최종 변동계수 산출\n",
      "  • 과적합 방지: 교차검증 및 정규화 적용\n",
      "\n",
      "🎯 3단계 변동성 지표 계산 완료!\n",
      "다음: 4단계 스태킹 알고리즘 개발\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.volatility_metrics = {}\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"데이터 로딩 및 전처리\"\"\"\n",
    "        print(\"=== 변동성 분석을 위한 데이터 준비 ===\")\n",
    "        \n",
    "        # 실제 환경에서는 CSV 파일 읽기\n",
    "        # self.lp_data = pd.concat([\n",
    "        #     pd.read_csv('LP데이터1.csv'),\n",
    "        #     pd.read_csv('LP데이터2.csv')\n",
    "        # ], ignore_index=True)\n",
    "        \n",
    "        # 테스트용 데이터 생성\n",
    "        self.lp_data = self._create_volatility_test_data()\n",
    "        \n",
    "        # 시간 관련 컬럼 추가\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "        \n",
    "        print(f\"✅ 데이터 준비 완료: {len(self.lp_data):,}레코드\")\n",
    "        return self.lp_data\n",
    "    \n",
    "    def _create_volatility_test_data(self):\n",
    "        \"\"\"변동성 테스트용 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        # 다양한 변동성 패턴을 가진 고객 정의\n",
    "        volatility_profiles = {\n",
    "            'A1001': {'type': 'stable', 'base': 100, 'noise': 0.05},      # 안정적\n",
    "            'A1002': {'type': 'high_volatility', 'base': 80, 'noise': 0.3}, # 고변동성\n",
    "            'A1003': {'type': 'periodic', 'base': 120, 'noise': 0.1},     # 주기적\n",
    "            'A1004': {'type': 'trending', 'base': 90, 'noise': 0.15},     # 트렌드\n",
    "            'A1005': {'type': 'seasonal', 'base': 110, 'noise': 0.2},     # 계절성\n",
    "            'A1006': {'type': 'jumpy', 'base': 85, 'noise': 0.1},         # 점프형\n",
    "            'A1007': {'type': 'clustered', 'base': 95, 'noise': 0.25},    # 클러스터형\n",
    "            'A1008': {'type': 'low_usage', 'base': 30, 'noise': 0.4},     # 저사용량\n",
    "            'A1009': {'type': 'medium', 'base': 70, 'noise': 0.18},       # 중간\n",
    "            'A1010': {'type': 'irregular', 'base': 105, 'noise': 0.35}    # 불규칙\n",
    "        }\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = volatility_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # 기본 패턴 생성\n",
    "                        base_power = profile['base']\n",
    "                        \n",
    "                        # 타입별 패턴 적용\n",
    "                        if profile['type'] == 'stable':\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'high_volatility':\n",
    "                            # 높은 변동성 - 큰 무작위 변화\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                            if np.random.random() < 0.1:  # 10% 확률로 큰 점프\n",
    "                                power *= np.random.choice([0.3, 2.5])\n",
    "                        \n",
    "                        elif profile['type'] == 'periodic':\n",
    "                            # 주기적 패턴\n",
    "                            daily_cycle = np.sin(2 * np.pi * hour / 24)\n",
    "                            weekly_cycle = np.sin(2 * np.pi * day / 7)\n",
    "                            power = base_power * (1 + 0.3 * daily_cycle + 0.1 * weekly_cycle)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'trending':\n",
    "                            # 트렌드 패턴\n",
    "                            trend = 0.5 * day / days  # 30일간 50% 증가\n",
    "                            power = base_power * (1 + trend)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'seasonal':\n",
    "                            # 계절성 패턴\n",
    "                            seasonal = 0.2 * np.sin(2 * np.pi * day / 30)\n",
    "                            power = base_power * (1 + seasonal)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'jumpy':\n",
    "                            # 급격한 변화가 있는 패턴\n",
    "                            power = base_power\n",
    "                            if day % 7 == 0 and hour == 9:  # 주 1회 큰 변화\n",
    "                                power *= 2.0\n",
    "                            elif day % 7 == 3 and hour == 15:\n",
    "                                power *= 0.4\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'clustered':\n",
    "                            # 변동성 클러스터링 (변동성이 높은 구간과 낮은 구간)\n",
    "                            if day % 10 < 3:  # 30% 기간 동안 높은 변동성\n",
    "                                noise_factor = profile['noise'] * 2\n",
    "                            else:\n",
    "                                noise_factor = profile['noise'] * 0.5\n",
    "                            power = base_power + np.random.normal(0, base_power * noise_factor)\n",
    "                        \n",
    "                        else:  # irregular, low_usage, medium\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        power = max(0, power)  # 음수 방지\n",
    "                        \n",
    "                        data.append({\n",
    "                            '대체고객번호': customer,\n",
    "                            'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            '순방향유효전력': round(power, 1),\n",
    "                            '지상무효': round(power * np.random.uniform(0.1, 0.3), 1),\n",
    "                            '진상무효': round(power * np.random.uniform(0.05, 0.15), 1),\n",
    "                            '피상전력': round(power * np.random.uniform(1.0, 1.1), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_basic_volatility_metrics(self):\n",
    "        \"\"\"기본 변동성 지표 계산\"\"\"\n",
    "        print(\"\\n=== 기본 변동성 지표 계산 ===\")\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]['순방향유효전력']\n",
    "            \n",
    "            # 1. 전통적 변동계수 (CV)\n",
    "            cv = customer_data.std() / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 2. 범위 기반 변동성\n",
    "            range_volatility = (customer_data.max() - customer_data.min()) / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 3. 분위수 기반 변동성 (IQR/Median)\n",
    "            q75, q25 = np.percentile(customer_data, [75, 25])\n",
    "            iqr_volatility = (q75 - q25) / np.median(customer_data) if np.median(customer_data) > 0 else 0\n",
    "            \n",
    "            # 4. 평균절대편차 (MAD)\n",
    "            mad = np.mean(np.abs(customer_data - customer_data.mean()))\n",
    "            mad_volatility = mad / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 5. 변화율 기반 변동성\n",
    "            returns = customer_data.pct_change().dropna()\n",
    "            return_volatility = returns.std() if len(returns) > 0 else 0\n",
    "            \n",
    "            metrics[customer] = {\n",
    "                'cv': round(cv, 4),\n",
    "                'range_vol': round(range_volatility, 4),\n",
    "                'iqr_vol': round(iqr_volatility, 4),\n",
    "                'mad_vol': round(mad_volatility, 4),\n",
    "                'return_vol': round(return_volatility, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\tCV\\t범위변동성\\tIQR변동성\\tMAD변동성\\t수익률변동성\")\n",
    "        for customer, metrics_dict in metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['cv']}\\t{metrics_dict['range_vol']}\\t{metrics_dict['iqr_vol']}\\t{metrics_dict['mad_vol']}\\t{metrics_dict['return_vol']}\")\n",
    "        \n",
    "        self.volatility_metrics['basic'] = metrics\n",
    "        return metrics\n",
    "    \n",
    "    def calculate_time_window_volatility(self):\n",
    "        \"\"\"시간 윈도우별 변동성 계산\"\"\"\n",
    "        print(\"\\n=== 시간 윈도우별 변동성 분석 ===\")\n",
    "        \n",
    "        window_metrics = {}\n",
    "        windows = {\n",
    "            'hourly': 4,    # 1시간 (4개 15분 구간)\n",
    "            'daily': 96,    # 1일 (96개 15분 구간)\n",
    "            'weekly': 672   # 1주 (672개 15분 구간)\n",
    "        }\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['순방향유효전력']\n",
    "            \n",
    "            window_metrics[customer] = {}\n",
    "            \n",
    "            for window_name, window_size in windows.items():\n",
    "                # 롤링 윈도우로 변동계수 계산\n",
    "                rolling_std = power_series.rolling(window=window_size, min_periods=window_size//2).std()\n",
    "                rolling_mean = power_series.rolling(window=window_size, min_periods=window_size//2).mean()\n",
    "                rolling_cv = rolling_std / rolling_mean\n",
    "                \n",
    "                # 윈도우별 변동성 통계\n",
    "                window_metrics[customer][window_name] = {\n",
    "                    'mean_cv': round(rolling_cv.mean(), 4),\n",
    "                    'std_cv': round(rolling_cv.std(), 4),\n",
    "                    'max_cv': round(rolling_cv.max(), 4),\n",
    "                    'min_cv': round(rolling_cv.min(), 4)\n",
    "                }\n",
    "        \n",
    "        print(\"윈도우별 평균 변동계수:\")\n",
    "        print(\"고객번호\\t시간별\\t일별\\t주별\")\n",
    "        for customer in window_metrics.keys():\n",
    "            hourly_cv = window_metrics[customer]['hourly']['mean_cv']\n",
    "            daily_cv = window_metrics[customer]['daily']['mean_cv']\n",
    "            weekly_cv = window_metrics[customer]['weekly']['mean_cv']\n",
    "            print(f\"{customer}\\t{hourly_cv}\\t{daily_cv}\\t{weekly_cv}\")\n",
    "        \n",
    "        self.volatility_metrics['time_windows'] = window_metrics\n",
    "        return window_metrics\n",
    "    \n",
    "    def calculate_directional_volatility(self):\n",
    "        \"\"\"방향성 변동성 분석 (상승/하락 비대칭성)\"\"\"\n",
    "        print(\"\\n=== 방향성 변동성 분석 ===\")\n",
    "        \n",
    "        directional_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['순방향유효전력']\n",
    "            \n",
    "            # 변화율 계산\n",
    "            returns = power_series.pct_change().dropna()\n",
    "            \n",
    "            # 상승/하락 분리\n",
    "            upside_returns = returns[returns > 0]\n",
    "            downside_returns = returns[returns < 0]\n",
    "            \n",
    "            # 방향별 변동성\n",
    "            upside_volatility = upside_returns.std() if len(upside_returns) > 0 else 0\n",
    "            downside_volatility = abs(downside_returns.std()) if len(downside_returns) > 0 else 0\n",
    "            \n",
    "            # 비대칭성 지수\n",
    "            asymmetry_ratio = upside_volatility / downside_volatility if downside_volatility > 0 else 0\n",
    "            \n",
    "            # 급격한 변화 횟수\n",
    "            large_increases = len(returns[returns > 0.5])  # 50% 이상 증가\n",
    "            large_decreases = len(returns[returns < -0.5]) # 50% 이상 감소\n",
    "            \n",
    "            directional_metrics[customer] = {\n",
    "                'upside_vol': round(upside_volatility, 4),\n",
    "                'downside_vol': round(downside_volatility, 4),\n",
    "                'asymmetry_ratio': round(asymmetry_ratio, 4),\n",
    "                'large_increases': large_increases,\n",
    "                'large_decreases': large_decreases\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\t상승변동성\\t하락변동성\\t비대칭비율\\t급증횟수\\t급감횟수\")\n",
    "        for customer, metrics_dict in directional_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['upside_vol']}\\t{metrics_dict['downside_vol']}\\t{metrics_dict['asymmetry_ratio']}\\t{metrics_dict['large_increases']}\\t{metrics_dict['large_decreases']}\")\n",
    "        \n",
    "        self.volatility_metrics['directional'] = directional_metrics\n",
    "        return directional_metrics\n",
    "    \n",
    "    def calculate_pattern_stability(self):\n",
    "        \"\"\"패턴 안정성 분석\"\"\"\n",
    "        print(\"\\n=== 패턴 안정성 분석 ===\")\n",
    "        \n",
    "        stability_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "            \n",
    "            # 시간대별 패턴 일관성\n",
    "            hourly_patterns = []\n",
    "            for day in customer_data['date'].unique():\n",
    "                day_data = customer_data[customer_data['date'] == day]\n",
    "                hourly_avg = day_data.groupby('hour')['순방향유효전력'].mean()\n",
    "                hourly_patterns.append(hourly_avg.values)\n",
    "            \n",
    "            # 패턴 간 상관관계 (일관성 측정)\n",
    "            if len(hourly_patterns) > 1:\n",
    "                correlations = []\n",
    "                for i in range(len(hourly_patterns)):\n",
    "                    for j in range(i+1, len(hourly_patterns)):\n",
    "                        if len(hourly_patterns[i]) == len(hourly_patterns[j]):\n",
    "                            corr = np.corrcoef(hourly_patterns[i], hourly_patterns[j])[0,1]\n",
    "                            if not np.isnan(corr):\n",
    "                                correlations.append(corr)\n",
    "                \n",
    "                pattern_consistency = np.mean(correlations) if correlations else 0\n",
    "            else:\n",
    "                pattern_consistency = 0\n",
    "            \n",
    "            # 주기성 강도 (FFT 기반)\n",
    "            power_series = customer_data.sort_values('datetime')['순방향유효전력'].values\n",
    "            if len(power_series) > 100:  # 충분한 데이터가 있을 때만\n",
    "                fft = np.fft.fft(power_series)\n",
    "                fft_magnitude = np.abs(fft)\n",
    "                \n",
    "                # 일일 주기 (96포인트) 강도\n",
    "                daily_freq_idx = len(fft_magnitude) // 96 if len(fft_magnitude) >= 96 else 1\n",
    "                daily_periodicity = fft_magnitude[daily_freq_idx] / np.mean(fft_magnitude) if np.mean(fft_magnitude) > 0 else 0\n",
    "            else:\n",
    "                daily_periodicity = 0\n",
    "            \n",
    "            # 예측가능성 (자기상관)\n",
    "            autocorr_1lag = power_series[1:].dot(power_series[:-1]) / (np.linalg.norm(power_series[1:]) * np.linalg.norm(power_series[:-1])) if len(power_series) > 1 else 0\n",
    "            \n",
    "            stability_metrics[customer] = {\n",
    "                'pattern_consistency': round(pattern_consistency, 4),\n",
    "                'daily_periodicity': round(daily_periodicity, 4),\n",
    "                'autocorrelation': round(autocorr_1lag, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\t패턴일관성\\t일일주기성\\t자기상관\")\n",
    "        for customer, metrics_dict in stability_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['pattern_consistency']}\\t{metrics_dict['daily_periodicity']}\\t{metrics_dict['autocorrelation']}\")\n",
    "        \n",
    "        self.volatility_metrics['stability'] = stability_metrics\n",
    "        return stability_metrics\n",
    "    \n",
    "    def create_composite_volatility_score(self):\n",
    "        \"\"\"복합 변동성 스코어 생성\"\"\"\n",
    "        print(\"\\n=== 복합 변동성 스코어 계산 ===\")\n",
    "        \n",
    "        if not all(key in self.volatility_metrics for key in ['basic', 'time_windows', 'directional', 'stability']):\n",
    "            print(\"❌ 모든 변동성 지표를 먼저 계산해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        composite_scores = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            # 각 카테고리별 점수 계산 (0-1 정규화)\n",
    "            basic_score = self.volatility_metrics['basic'][customer]['cv']\n",
    "            window_score = np.mean([\n",
    "                self.volatility_metrics['time_windows'][customer]['hourly']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['daily']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['weekly']['mean_cv']\n",
    "            ])\n",
    "            directional_score = (\n",
    "                self.volatility_metrics['directional'][customer]['upside_vol'] + \n",
    "                self.volatility_metrics['directional'][customer]['downside_vol']\n",
    "            ) / 2\n",
    "            \n",
    "            # 안정성은 역수로 (낮은 안정성 = 높은 변동성)\n",
    "            stability_score = 1 - self.volatility_metrics['stability'][customer]['pattern_consistency']\n",
    "            \n",
    "            # 가중 평균으로 최종 스코어 계산\n",
    "            weights = {\n",
    "                'basic': 0.3,       # 기본 변동계수\n",
    "                'window': 0.3,      # 시간 윈도우 변동성\n",
    "                'directional': 0.2, # 방향성 변동성\n",
    "                'stability': 0.2    # 패턴 불안정성\n",
    "            }\n",
    "            \n",
    "            composite_score = (\n",
    "                weights['basic'] * basic_score +\n",
    "                weights['window'] * window_score +\n",
    "                weights['directional'] * directional_score +\n",
    "                weights['stability'] * stability_score\n",
    "            )\n",
    "            \n",
    "            composite_scores[customer] = {\n",
    "                'basic_score': round(basic_score, 4),\n",
    "                'window_score': round(window_score, 4),\n",
    "                'directional_score': round(directional_score, 4),\n",
    "                'stability_score': round(stability_score, 4),\n",
    "                'composite_score': round(composite_score, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\t기본\\t윈도우\\t방향성\\t안정성\\t복합점수\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            print(f\"{customer}\\t{scores['basic_score']}\\t{scores['window_score']}\\t{scores['directional_score']}\\t{scores['stability_score']}\\t{scores['composite_score']}\")\n",
    "        \n",
    "        # 변동성 등급 분류\n",
    "        composite_values = [scores['composite_score'] for scores in composite_scores.values()]\n",
    "        percentiles = np.percentile(composite_values, [33, 67])\n",
    "        \n",
    "        print(f\"\\n📊 변동성 등급 기준:\")\n",
    "        print(f\"  저변동성: < {percentiles[0]:.3f}\")\n",
    "        print(f\"  중변동성: {percentiles[0]:.3f} ~ {percentiles[1]:.3f}\")\n",
    "        print(f\"  고변동성: > {percentiles[1]:.3f}\")\n",
    "        \n",
    "        print(\"\\n등급별 고객 분류:\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            score = scores['composite_score']\n",
    "            if score < percentiles[0]:\n",
    "                grade = \"저변동성\"\n",
    "            elif score < percentiles[1]:\n",
    "                grade = \"중변동성\"\n",
    "            else:\n",
    "                grade = \"고변동성\"\n",
    "            print(f\"  {customer}: {score:.3f} ({grade})\")\n",
    "        \n",
    "        self.volatility_metrics['composite'] = composite_scores\n",
    "        return composite_scores\n",
    "    \n",
    "    def generate_volatility_report(self):\n",
    "        \"\"\"변동성 분석 종합 리포트\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 변동성 분석 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 모든 지표 계산\n",
    "        basic_metrics = self.calculate_basic_volatility_metrics()\n",
    "        window_metrics = self.calculate_time_window_volatility()\n",
    "        directional_metrics = self.calculate_directional_volatility()\n",
    "        stability_metrics = self.calculate_pattern_stability()\n",
    "        composite_scores = self.create_composite_volatility_score()\n",
    "        \n",
    "        print(\"\\n🎯 변동계수 알고리즘 설계 인사이트:\")\n",
    "        print(\"  1. 다차원 변동성 지표의 필요성 확인\")\n",
    "        print(\"  2. 시간 윈도우별 차별화된 가중치 적용\")\n",
    "        print(\"  3. 방향성 변동성으로 리스크 비대칭성 포착\")\n",
    "        print(\"  4. 패턴 안정성으로 예측가능성 평가\")\n",
    "        print(\"  5. 복합 스코어를 통한 종합적 변동성 평가\")\n",
    "        \n",
    "        print(\"\\n💡 스태킹 알고리즘 설계 방향:\")\n",
    "        print(\"  • Level-0 모델: 각 변동성 지표를 개별 모델로 구성\")\n",
    "        print(\"  • Level-1 메타모델: 가중 결합으로 최종 변동계수 산출\")\n",
    "        print(\"  • 과적합 방지: 교차검증 및 정규화 적용\")\n",
    "        \n",
    "        return {\n",
    "            'basic': basic_metrics,\n",
    "            'time_windows': window_metrics,\n",
    "            'directional': directional_metrics,\n",
    "            'stability': stability_metrics,\n",
    "            'composite': composite_scores\n",
    "        }\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCOVolatilityAnalyzer()\n",
    "    \n",
    "    # 데이터 로딩\n",
    "    data = analyzer.load_and_prepare_data()\n",
    "    \n",
    "    # 종합 변동성 분석\n",
    "    volatility_report = analyzer.generate_volatility_report()\n",
    "    \n",
    "    print(\"\\n🎯 3단계 변동성 지표 계산 완료!\")\n",
    "    print(\"다음: 4단계 스태킹 알고리즘 개발\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityStackingModel:\n",
    "    \"\"\"\n",
    "    한국전력공사 전력 사용패턴 변동계수 스태킹 알고리즘\n",
    "    \n",
    "    Level-0: 다양한 변동성 지표들을 개별 모델로 구성\n",
    "    Level-1: 메타모델로 최종 변동계수 산출\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.level0_models = {}\n",
    "        self.level1_model = None\n",
    "        self.scaler = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Level-0 모델 정의\n",
    "        self._initialize_level0_models()\n",
    "        \n",
    "        # 과적합 방지 설정\n",
    "        self.cv_folds = 5\n",
    "        self.random_state = 42\n",
    "        \n",
    "    def _initialize_level0_models(self):\n",
    "        \"\"\"Level-0 기본 모델들 초기화\"\"\"\n",
    "        self.level0_models = {\n",
    "            'traditional_cv': self._traditional_cv_model,\n",
    "            'range_volatility': self._range_volatility_model,\n",
    "            'iqr_volatility': self._iqr_volatility_model,\n",
    "            'mad_volatility': self._mad_volatility_model,\n",
    "            'return_volatility': self._return_volatility_model,\n",
    "            'window_volatility': self._window_volatility_model,\n",
    "            'percentile_volatility': self._percentile_volatility_model\n",
    "        }\n",
    "        \n",
    "    def _traditional_cv_model(self, data):\n",
    "        \"\"\"전통적 변동계수 모델\"\"\"\n",
    "        return data.std() / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _range_volatility_model(self, data):\n",
    "        \"\"\"범위 기반 변동성 모델\"\"\"\n",
    "        return (data.max() - data.min()) / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _iqr_volatility_model(self, data):\n",
    "        \"\"\"분위수 기반 변동성 모델\"\"\"\n",
    "        q75, q25 = np.percentile(data, [75, 25])\n",
    "        median = np.median(data)\n",
    "        return (q75 - q25) / median if median > 0 else 0\n",
    "    \n",
    "    def _mad_volatility_model(self, data):\n",
    "        \"\"\"평균절대편차 기반 변동성 모델\"\"\"\n",
    "        mad = np.mean(np.abs(data - data.mean()))\n",
    "        return mad / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _return_volatility_model(self, data):\n",
    "        \"\"\"수익률 변동성 모델\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return 0\n",
    "        returns = data.pct_change().dropna()\n",
    "        return returns.std() if len(returns) > 0 else 0\n",
    "    \n",
    "    def _window_volatility_model(self, data, window_size=96):\n",
    "        \"\"\"윈도우 기반 변동성 모델 (일별)\"\"\"\n",
    "        if len(data) < window_size:\n",
    "            return self._traditional_cv_model(data)\n",
    "        \n",
    "        rolling_cv = []\n",
    "        for i in range(window_size, len(data) + 1):\n",
    "            window_data = data.iloc[i-window_size:i]\n",
    "            cv = self._traditional_cv_model(window_data)\n",
    "            rolling_cv.append(cv)\n",
    "        \n",
    "        return np.mean(rolling_cv) if rolling_cv else 0\n",
    "    \n",
    "    def _percentile_volatility_model(self, data):\n",
    "        \"\"\"백분위수 기반 변동성 모델\"\"\"\n",
    "        p90 = np.percentile(data, 90)\n",
    "        p10 = np.percentile(data, 10)\n",
    "        p50 = np.percentile(data, 50)\n",
    "        return (p90 - p10) / p50 if p50 > 0 else 0\n",
    "    \n",
    "    def extract_features(self, lp_data):\n",
    "        \"\"\"\n",
    "        LP 데이터에서 변동성 특성 추출\n",
    "        \n",
    "        Parameters:\n",
    "        lp_data: DataFrame with columns ['대체고객번호', 'LP수신일자', '순방향유효전력', ...]\n",
    "        \n",
    "        Returns:\n",
    "        features_df: DataFrame with volatility features for each customer\n",
    "        \"\"\"\n",
    "        print(\"🔧 변동성 특성 추출 중...\")\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for customer in lp_data['대체고객번호'].unique():\n",
    "            customer_data = lp_data[lp_data['대체고객번호'] == customer]\n",
    "            power_data = customer_data['순방향유효전력']\n",
    "            \n",
    "            # 시간 정보 추가\n",
    "            if 'datetime' not in customer_data.columns:\n",
    "                customer_data = customer_data.copy()\n",
    "                customer_data['datetime'] = pd.to_datetime(customer_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "                customer_data['hour'] = customer_data['datetime'].dt.hour\n",
    "                customer_data['weekday'] = customer_data['datetime'].dt.weekday\n",
    "            \n",
    "            # Level-0 모델들로 특성 계산\n",
    "            features = {'customer_id': customer}\n",
    "            \n",
    "            for model_name, model_func in self.level0_models.items():\n",
    "                try:\n",
    "                    if model_name == 'window_volatility':\n",
    "                        features[model_name] = model_func(power_data, window_size=96)\n",
    "                    else:\n",
    "                        features[model_name] = model_func(power_data)\n",
    "                except:\n",
    "                    features[model_name] = 0\n",
    "            \n",
    "            # 추가 컨텍스트 특성\n",
    "            features.update({\n",
    "                'mean_usage': power_data.mean(),\n",
    "                'total_usage': power_data.sum(),\n",
    "                'peak_ratio': power_data.max() / power_data.mean() if power_data.mean() > 0 else 0,\n",
    "                'zero_ratio': (power_data == 0).sum() / len(power_data),\n",
    "                'weekend_effect': self._calculate_weekend_effect(customer_data),\n",
    "                'peak_hour_concentration': self._calculate_peak_concentration(customer_data)\n",
    "            })\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        print(f\"✅ 특성 추출 완료: {len(features_df)}명 고객, {len(features_df.columns)-1}개 특성\")\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def _calculate_weekend_effect(self, customer_data):\n",
    "        \"\"\"주말 효과 계산\"\"\"\n",
    "        try:\n",
    "            weekday_avg = customer_data[customer_data['weekday'] < 5]['순방향유효전력'].mean()\n",
    "            weekend_avg = customer_data[customer_data['weekday'] >= 5]['순방향유효전력'].mean()\n",
    "            \n",
    "            if weekday_avg > 0:\n",
    "                return abs(weekend_avg / weekday_avg - 1)\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _calculate_peak_concentration(self, customer_data):\n",
    "        \"\"\"피크 시간 집중도 계산\"\"\"\n",
    "        try:\n",
    "            peak_hours = [9, 14, 18]  # 일반적인 피크 시간\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['순방향유효전력'].mean()\n",
    "            total_avg = customer_data['순방향유효전력'].mean()\n",
    "            \n",
    "            return peak_usage / total_avg if total_avg > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def prepare_training_data(self, features_df, target_column='traditional_cv'):\n",
    "        \"\"\"\n",
    "        훈련 데이터 준비\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: 특성 데이터프레임\n",
    "        target_column: 타겟 변수로 사용할 컬럼명\n",
    "        \n",
    "        Returns:\n",
    "        X, y: 특성 행렬과 타겟 벡터\n",
    "        \"\"\"\n",
    "        # 특성 선택 (Level-0 모델 출력들)\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        y = features_df[target_column].copy()\n",
    "        \n",
    "        # 결측치 처리\n",
    "        X = X.fillna(0)\n",
    "        y = y.fillna(0)\n",
    "        \n",
    "        # 이상치 처리 (상위 5%, 하위 5% 클리핑)\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                lower_bound = X[col].quantile(0.05)\n",
    "                upper_bound = X[col].quantile(0.95)\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"📊 훈련 데이터 준비 완료: {X.shape[0]}개 샘플, {X.shape[1]}개 특성\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        스태킹 모델 훈련\n",
    "        \n",
    "        Parameters:\n",
    "        X: 특성 행렬\n",
    "        y: 타겟 벡터 (실제 변동계수)\n",
    "        \"\"\"\n",
    "        print(\"🚀 스태킹 모델 훈련 시작...\")\n",
    "        \n",
    "        # 데이터 정규화\n",
    "        self.scaler = RobustScaler()  # 이상치에 강건한 스케일러\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Level-1 메타모델 후보들\n",
    "        meta_models = {\n",
    "            'linear': LinearRegression(),\n",
    "            'ridge': Ridge(alpha=1.0, random_state=self.random_state),\n",
    "            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state),\n",
    "            'rf': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=self.random_state),\n",
    "            'gbm': GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        # 교차검증으로 최적 메타모델 선택\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "        best_model_name = None\n",
    "        \n",
    "        # 시계열 교차검증\n",
    "        tscv = TimeSeriesSplit(n_splits=self.cv_folds)\n",
    "        \n",
    "        print(\"🔍 메타모델 성능 비교:\")\n",
    "        print(\"모델명\\t\\tMAE\\t\\tMSE\\t\\tR²\")\n",
    "        \n",
    "        for model_name, model in meta_models.items():\n",
    "            try:\n",
    "                # 교차검증 점수 계산\n",
    "                mae_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "                mse_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "                r2_scores = cross_val_score(model, X_scaled, y, cv=tscv, scoring='r2')\n",
    "                \n",
    "                avg_mae = np.mean(mae_scores)\n",
    "                avg_mse = np.mean(mse_scores)\n",
    "                avg_r2 = np.mean(r2_scores)\n",
    "                \n",
    "                print(f\"{model_name:<12}\\t{avg_mae:.4f}\\t\\t{avg_mse:.4f}\\t\\t{avg_r2:.4f}\")\n",
    "                \n",
    "                # R² 점수 기준으로 최적 모델 선택\n",
    "                if avg_r2 > best_score:\n",
    "                    best_score = avg_r2\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{model_name:<12}\\t에러: {str(e)[:30]}...\")\n",
    "        \n",
    "        # 최적 모델로 전체 데이터 훈련\n",
    "        if best_model is not None:\n",
    "            self.level1_model = best_model\n",
    "            self.level1_model.fit(X_scaled, y)\n",
    "            \n",
    "            print(f\"\\n🏆 최적 메타모델: {best_model_name} (R² = {best_score:.4f})\")\n",
    "            \n",
    "            # 특성 중요도 출력 (가능한 경우)\n",
    "            if hasattr(self.level1_model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': self.level1_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\n📊 특성 중요도 (상위 5개):\")\n",
    "                for _, row in importance_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "            \n",
    "            elif hasattr(self.level1_model, 'coef_'):\n",
    "                coef_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'coefficient': self.level1_model.coef_\n",
    "                }).sort_values('coefficient', key=abs, ascending=False)\n",
    "                \n",
    "                print(\"\\n📊 모델 계수 (절댓값 상위 5개):\")\n",
    "                for _, row in coef_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['coefficient']:.4f}\")\n",
    "            \n",
    "            self.is_fitted = True\n",
    "            print(\"✅ 스태킹 모델 훈련 완료!\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"❌ 적합한 메타모델을 찾을 수 없습니다.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, features_df):\n",
    "        \"\"\"\n",
    "        변동계수 예측\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: 특성 데이터프레임\n",
    "        \n",
    "        Returns:\n",
    "        predictions: 예측된 변동계수\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다. fit() 메서드를 먼저 호출하세요.\")\n",
    "        \n",
    "        # 훈련 시와 동일한 특성 선택\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # 정규화\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # 예측\n",
    "        predictions = self.level1_model.predict(X_scaled)\n",
    "        \n",
    "        # 음수 방지\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"모델 성능 평가\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다.\")\n",
    "        \n",
    "        # 예측\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        y_pred = self.level1_model.predict(X_test_scaled)\n",
    "        y_pred = np.maximum(y_pred, 0)  # 음수 방지\n",
    "        \n",
    "        # 성능 지표 계산\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # 상대 오차 계산\n",
    "        relative_errors = np.abs((y_test - y_pred) / (y_test + 1e-8))\n",
    "        mape = np.mean(relative_errors) * 100\n",
    "        \n",
    "        print(\"📊 모델 성능 평가:\")\n",
    "        print(f\"  MAE (평균절대오차): {mae:.4f}\")\n",
    "        print(f\"  MSE (평균제곱오차): {mse:.4f}\")\n",
    "        print(f\"  RMSE (제곱근평균제곱오차): {rmse:.4f}\")\n",
    "        print(f\"  R² (결정계수): {r2:.4f}\")\n",
    "        print(f\"  MAPE (평균절대백분율오차): {mape:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape\n",
    "        }\n",
    "    \n",
    "    def detect_anomalous_volatility(self, features_df, threshold_percentile=95):\n",
    "        \"\"\"\n",
    "        이상 변동성 탐지\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: 특성 데이터프레임\n",
    "        threshold_percentile: 이상치 기준 백분위수\n",
    "        \n",
    "        Returns:\n",
    "        anomaly_results: 이상치 탐지 결과\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다.\")\n",
    "        \n",
    "        print(\"🚨 이상 변동성 탐지 중...\")\n",
    "        \n",
    "        # 변동계수 예측\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # 이상치 기준 설정\n",
    "        threshold = np.percentile(predicted_volatility, threshold_percentile)\n",
    "        \n",
    "        # 이상치 탐지\n",
    "        anomaly_mask = predicted_volatility > threshold\n",
    "        \n",
    "        anomaly_results = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'is_anomaly': anomaly_mask,\n",
    "            'anomaly_score': (predicted_volatility - threshold) / threshold\n",
    "        })\n",
    "        \n",
    "        anomaly_customers = anomaly_results[anomaly_results['is_anomaly']]\n",
    "        \n",
    "        print(f\"🎯 탐지 결과:\")\n",
    "        print(f\"  전체 고객: {len(anomaly_results)}명\")\n",
    "        print(f\"  이상 변동성 고객: {len(anomaly_customers)}명\")\n",
    "        print(f\"  이상치 비율: {len(anomaly_customers)/len(anomaly_results)*100:.1f}%\")\n",
    "        \n",
    "        if len(anomaly_customers) > 0:\n",
    "            print(f\"\\n⚠️ 주의 필요 고객:\")\n",
    "            for _, row in anomaly_customers.sort_values('anomaly_score', ascending=False).iterrows():\n",
    "                print(f\"  {row['customer_id']}: 변동계수 {row['predicted_volatility']:.3f} (점수: {row['anomaly_score']:.2f})\")\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def generate_volatility_report(self, features_df):\n",
    "        \"\"\"종합 변동성 리포트 생성\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 전력 사용패턴 변동계수 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 변동계수 예측\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # 통계 요약\n",
    "        print(\"\\n📊 변동계수 분포:\")\n",
    "        print(f\"  평균: {np.mean(predicted_volatility):.3f}\")\n",
    "        print(f\"  표준편차: {np.std(predicted_volatility):.3f}\")\n",
    "        print(f\"  최소값: {np.min(predicted_volatility):.3f}\")\n",
    "        print(f\"  최대값: {np.max(predicted_volatility):.3f}\")\n",
    "        print(f\"  중위수: {np.median(predicted_volatility):.3f}\")\n",
    "        \n",
    "        # 등급별 분류\n",
    "        p33 = np.percentile(predicted_volatility, 33)\n",
    "        p67 = np.percentile(predicted_volatility, 67)\n",
    "        \n",
    "        print(f\"\\n🏷️ 변동성 등급:\")\n",
    "        print(f\"  안정형 (< {p33:.3f}): {np.sum(predicted_volatility < p33)}명\")\n",
    "        print(f\"  보통형 ({p33:.3f} ~ {p67:.3f}): {np.sum((predicted_volatility >= p33) & (predicted_volatility < p67))}명\")\n",
    "        print(f\"  변동형 (≥ {p67:.3f}): {np.sum(predicted_volatility >= p67)}명\")\n",
    "        \n",
    "        # 고객별 결과\n",
    "        results_df = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'actual_cv': features_df['traditional_cv'] if 'traditional_cv' in features_df.columns else np.nan\n",
    "        })\n",
    "        \n",
    "        results_df['grade'] = pd.cut(results_df['predicted_volatility'], \n",
    "                                   bins=[0, p33, p67, np.inf], \n",
    "                                   labels=['안정형', '보통형', '변동형'])\n",
    "        \n",
    "        print(f\"\\n👥 고객별 변동계수:\")\n",
    "        print(\"고객번호\\t예측변동계수\\t등급\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['customer_id']}\\t{row['predicted_volatility']:.3f}\\t\\t{row['grade']}\")\n",
    "        \n",
    "        # 이상 변동성 탐지\n",
    "        anomaly_results = self.detect_anomalous_volatility(features_df)\n",
    "        \n",
    "        print(f\"\\n💡 활용 방안:\")\n",
    "        print(\"  1. 비정상적 전력 사용 패턴 조기 감지\")\n",
    "        print(\"  2. 고객별 맞춤형 전력 관리 서비스\")\n",
    "        print(\"  3. 영업 리스크 평가 및 관리\")\n",
    "        print(\"  4. 전력 수요 예측 정확도 향상\")\n",
    "        \n",
    "        return {\n",
    "            'predictions': results_df,\n",
    "            'anomalies': anomaly_results,\n",
    "            'statistics': {\n",
    "                'mean': np.mean(predicted_volatility),\n",
    "                'std': np.std(predicted_volatility),\n",
    "                'percentiles': {'p33': p33, 'p67': p67}\n",
    "            }\n",
    "        }\n",
    "\n",
    "# 사용 예제 및 테스트\n",
    "def create_sample_lp_data():\n",
    "    \"\"\"테스트용 샘플 LP 데이터 생성\"\"\"\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    data = []\n",
    "    customers = [f'A{1001+i}' for i in range(10)]\n",
    "    start_date = datetime(2024, 3, 1)\n",
    "    \n",
    "    # 다양한 변동성 패턴의 고객들\n",
    "    volatility_patterns = {\n",
    "        'A1001': 0.3,   # 안정적\n",
    "        'A1002': 1.2,   # 높은 변동성\n",
    "        'A1003': 0.25,  # 매우 안정적\n",
    "        'A1004': 0.8,   # 중간 변동성\n",
    "        'A1005': 0.9,   # 중간 변동성\n",
    "        'A1006': 0.2,   # 안정적\n",
    "        'A1007': 1.5,   # 매우 높은 변동성\n",
    "        'A1008': 0.7,   # 중간 변동성\n",
    "        'A1009': 0.6,   # 중간 변동성\n",
    "        'A1010': 0.4    # 안정적\n",
    "    }\n",
    "    \n",
    "    for customer in customers:\n",
    "        target_cv = volatility_patterns[customer]\n",
    "        base_power = random.uniform(50, 150)\n",
    "        \n",
    "        for day in range(31):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            for hour in range(24):\n",
    "                for minute in [0, 15, 30, 45]:\n",
    "                    timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                    \n",
    "                    # 변동성에 따른 노이즈 생성\n",
    "                    noise_std = base_power * target_cv\n",
    "                    power = base_power + random.gauss(0, noise_std)\n",
    "                    power = max(0, power)\n",
    "                    \n",
    "                    data.append({\n",
    "                        '대체고객번호': customer,\n",
    "                        'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                        '순방향유효전력': round(power, 1)\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 한국전력공사 전력 사용패턴 변동계수 스태킹 알고리즘 테스트\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. 샘플 데이터 생성\n",
    "    print(\"1️⃣ 샘플 데이터 생성...\")\n",
    "    lp_data = create_sample_lp_data()\n",
    "    print(f\"   생성 완료: {len(lp_data):,}레코드\")\n",
    "    \n",
    "    # 2. 스태킹 모델 초기화\n",
    "    print(\"\\n2️⃣ 스태킹 모델 초기화...\")\n",
    "    stacking_model = KEPCOVolatilityStackingModel()\n",
    "    \n",
    "    # 3. 특성 추출\n",
    "    print(\"\\n3️⃣ 변동성 특성 추출...\")\n",
    "    features_df = stacking_model.extract_features(lp_data)\n",
    "    \n",
    "    # 4. 훈련 데이터 준비\n",
    "    print(\"\\n4️⃣ 훈련 데이터 준비...\")\n",
    "    X, y = stacking_model.prepare_training_data(features_df)\n",
    "    \n",
    "    # 5. 모델 훈련\n",
    "    print(\"\\n5️⃣ 스태킹 모델 훈련...\")\n",
    "    stacking_model.fit(X, y)\n",
    "    \n",
    "    # 6. 종합 리포트 생성\n",
    "    print(\"\\n6️⃣ 종합 변동성 리포트 생성...\")\n",
    "    report = stacking_model.generate_volatility_report(features_df)\n",
    "    \n",
    "    print(\"\\n🎯 스태킹 알고리즘 구현 완료!\")\n",
    "    print(\"   • Level-0: 7개 기본 변동성 모델\")\n",
    "    print(\"   • Level-1: 최적화된 메타모델\")\n",
    "    print(\"   • 과적합 방지: 교차검증 + 정규화\")\n",
    "    print(\"   • 이상 탐지: 자동 임계값 설정\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
