{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd85c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\n",
      "데이터안심구역 전용 - 실제 데이터 분석\n",
      "============================================================\n",
      "\\n[1단계] 고객 기본정보 로딩 및 분석\n",
      "=== 고객 기본정보 로딩 ===\n",
      "총 고객 수: 200명\n",
      "컬럼: ['순번', '고객번호', '계약전력', '계약종별', '사용용도', '주생산품', '산업분류']\n",
      "\\n기본 정보:\n",
      "   순번   고객번호     계약전력            계약종별    사용용도 주생산품                      산업분류\n",
      "0   1  A1001    1~199  322 산업용(갑)‖고압A  02 상업용   병원  721건축기술,엔지니어링및기타과학기술서비스업\n",
      "1   2  A1002  400~499  222 일반용(갑)‖고압A  02 상업용   교회               241제1차철강제조업\n",
      "2   3  A1003  400~499  222 일반용(갑)‖고압A  02 상업용   병원                 681부동산임대업\n",
      "3   4  A1004  500~599  322 산업용(갑)‖고압A  02 상업용   상가               241제1차철강제조업\n",
      "4   5  A1005  700~799  726 산업용(을) 고압A  02 상업용   상가            631창고및운송관련서비스업\n",
      "\\n=== 고객 분포 분석 ===\n",
      "\\n📊 계약종별 분포:\n",
      "  226 일반용(을) 고압A: 50명 (25.0%)\n",
      "  322 산업용(갑)‖고압A: 41명 (20.5%)\n",
      "  311 산업용(갑) 저압: 39명 (19.5%)\n",
      "  222 일반용(갑)‖고압A: 37명 (18.5%)\n",
      "  726 산업용(을) 고압A: 33명 (16.5%)\n",
      "\\n🏭 사용용도별 분포:\n",
      "  02 상업용: 105명 (52.5%)\n",
      "  09 광공업용: 95명 (47.5%)\n",
      "\\n⚡ 계약전력 분포:\n",
      "count       200\n",
      "unique        9\n",
      "top       1~199\n",
      "freq         27\n",
      "Name: 계약전력, dtype: object\n",
      "\\n[2단계] LP 데이터 로딩 및 품질 분석\n",
      "\\n=== LP 데이터 로딩 ===\n",
      "발견된 LP 파일 수: 4개\n",
      "파일 1 로딩: processed_LPData_20220301_15.csv\n",
      "  레코드 수: 14,400\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "파일 2 로딩: processed_LPData_20220316_31.csv\n",
      "  레코드 수: 14,400\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-16-00:00 ~ 2024-03-30-23:45\n",
      "파일 3 로딩: processed_LPData_20220401_15.csv\n",
      "  레코드 수: 14,400\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "파일 4 로딩: processed_LPData_20220416_29.csv\n",
      "  레코드 수: 14,400\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-16-00:00 ~ 2024-03-30-23:45\n",
      "\\n✅ 전체 LP 데이터 결합 완료:\n",
      "  총 레코드: 57,600\n",
      "  총 고객: 10\n",
      "\\n=== LP 데이터 품질 분석 ===\n",
      "📈 기본 통계:\n",
      "           순방향 유효전력          지상무효          진상무효          피상전력\n",
      "count  57600.000000  57600.000000  57600.000000  57600.000000\n",
      "mean      53.658823      7.500729      4.015514     54.492017\n",
      "std       52.880036     12.419111      8.295770     53.722780\n",
      "min        0.200000      0.000000      0.000000      0.200000\n",
      "25%        7.575000      0.000000      0.000000      7.700000\n",
      "50%       37.500000      1.400000      0.000000     38.100000\n",
      "75%       88.300000     10.100000      3.600000     89.900000\n",
      "max      284.500000    107.100000     77.300000    291.200000\n",
      "\\n⏰ 시간 간격 체크:\n",
      "  A1001: 평균 간격 7.5분, 표준편차 7.5분\n",
      "  A1002: 평균 간격 7.5분, 표준편차 7.5분\n",
      "  A1003: 평균 간격 7.5분, 표준편차 7.5분\n",
      "\\n🔍 데이터 품질 체크:\n",
      "  순방향 유효전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "  지상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 18118건 (31.45%)\n",
      "  진상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 29804건 (51.74%)\n",
      "  피상전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "\\n🚨 이상치 탐지:\n",
      "  순방향 유효전력: 508건 (0.88%)\n",
      "  지상무효: 5548건 (9.63%)\n",
      "  진상무효: 9002건 (15.63%)\n",
      "  피상전력: 508건 (0.88%)\n",
      "\\n[3단계] 이상치 탐지 및 데이터 정제\n",
      "  순방향 유효전력: 508건 (0.88%)\n",
      "  지상무효: 5548건 (9.63%)\n",
      "  진상무효: 9002건 (15.63%)\n",
      "  피상전력: 508건 (0.88%)\n",
      "\\n[4단계] 데이터 품질 종합 평가\n",
      "\n",
      "============================================================\n",
      "📋 데이터 품질 종합 리포트\n",
      "============================================================\n",
      "\n",
      "👥 고객 데이터:\n",
      "  총 고객 수: 200명\n",
      "  계약종별 유형: 5개\n",
      "  사용용도 유형: 2개\n",
      "\n",
      "⚡ LP 데이터:\n",
      "  총 레코드: 57,600건\n",
      "  측정 기간: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "  데이터 커버리지: 29일\n",
      "  평균 유효전력: 53.66kW\n",
      "\n",
      "💾 전처리된 LP 데이터 저장 중...\n",
      "   📊 저장 대상: 57,600개 레코드\n",
      "   💾 저장 중... (잠시만 기다려주세요)\n",
      "      📄 CSV 저장 중...\n",
      "      📦 Parquet 저장 중...\n",
      "   ✅ 전처리된 데이터 저장 완료!\n",
      "      📄 CSV: ./analysis_results/processed_lp_data.csv (0.00GB)\n",
      "      📦 Parquet: ./analysis_results/processed_lp_data.parquet (0.00GB)\n",
      "      🚀 크기 절약: 89.9%\n",
      "      ⚡ 로딩 속도 향상: 약 2-3배 빨라짐!\n",
      "   🚀 2-3단계에서 30분 → 3-5분으로 시간 단축 예상!\n",
      "\n",
      "💾 분석 결과 JSON 저장 중...\n",
      "✅ 분석 결과 JSON 저장: ./analysis_results\\analysis_results.json\n",
      "   저장된 항목: 4개\n",
      "   📁 저장된 구조:\n",
      "      - customer_summary: 고객 기본 정보\n",
      "      - lp_data_summary: LP 데이터 요약\n",
      "      - processed_lp_data: 전처리된 데이터 메타정보\n",
      "      - metadata: 시간정보 및 버전\n",
      "\n",
      "💡 다음 단계 권장사항:\n",
      "  1. 시계열 패턴 분석 (전처리된 데이터 활용)\n",
      "  2. 고객별 사용량 프로파일링\n",
      "  3. 변동성 지표 계산 및 비교\n",
      "  4. 이상 패턴 탐지 알고리즘 개발\n",
      "\n",
      "🎯 1단계 최적화 완료!\n",
      "   📁 생성 파일:\n",
      "      - analysis_results.json (2-3단계 연계용)\n",
      "      - processed_lp_data.csv (전처리된 LP 데이터)\n",
      "      - processed_lp_data.parquet (고성능 전처리된 데이터)\n",
      "\\n🎯 1단계 데이터 품질 점검 완료!\n",
      "다음: 2단계 시계열 패턴 분석 준비 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "        self.analysis_results = {}\n",
    "        \n",
    "    def load_customer_data(self, file_path='제13회 산업부 공모전 대상고객/제13회 산업부 공모전 대상고객.xlsx'):\n",
    "        \"\"\"실제 고객 기본정보 로딩 및 기본 분석\"\"\"\n",
    "        print(\"=== 고객 기본정보 로딩 ===\")\n",
    "        \n",
    "        try:\n",
    "            # 실제 Excel 파일 읽기\n",
    "            self.customer_data = pd.read_excel(file_path, header=1)\n",
    "            \n",
    "            print(f\"총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"컬럼: {list(self.customer_data.columns)}\")\n",
    "            print(\"\\\\n기본 정보:\")\n",
    "            print(self.customer_data.head())\n",
    "            \n",
    "            return self._analyze_customer_distribution()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"고객 데이터 로딩 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"고객 분포 분석\"\"\"\n",
    "        print(\"\\\\n=== 고객 분포 분석 ===\")\n",
    "        \n",
    "        # 계약종별 분포\n",
    "        contract_counts = self.customer_data['계약종별'].value_counts()\n",
    "        print(\"\\\\n📊 계약종별 분포:\")\n",
    "        for contract, count in contract_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {contract}: {count}명 ({pct:.1f}%)\")\n",
    "        \n",
    "        # 사용용도별 분포\n",
    "        usage_counts = self.customer_data['사용용도'].value_counts()\n",
    "        print(\"\\\\n🏭 사용용도별 분포:\")\n",
    "        for usage, count in usage_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {usage}: {count}명 ({pct:.1f}%)\")\n",
    "        \n",
    "        # 계약전력 분포\n",
    "        print(\"\\\\n⚡ 계약전력 분포:\")\n",
    "        power_stats = self.customer_data['계약전력'].describe()\n",
    "        print(power_stats)\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_counts,\n",
    "            'usage_distribution': usage_counts,\n",
    "            'power_stats': power_stats\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, data_directory='./제13회 산업부 공모전 대상고객 LP데이터/'):\n",
    "        \"\"\"실제 LP 데이터 로딩 (여러 CSV 파일)\"\"\"\n",
    "        print(\"\\\\n=== LP 데이터 로딩 ===\")\n",
    "        \n",
    "        try:\n",
    "            # processed_LPData_YYYYMMDD_DD.csv 패턴의 파일들 찾기\n",
    "            lp_files = glob.glob(os.path.join(data_directory, 'processed_LPData_*.csv'))\n",
    "            \n",
    "            if not lp_files:\n",
    "                print(\"LP 데이터 파일을 찾을 수 없습니다.\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"발견된 LP 파일 수: {len(lp_files)}개\")\n",
    "            \n",
    "            # 모든 LP 파일 읽기 및 결합\n",
    "            lp_dataframes = []\n",
    "            total_records = 0\n",
    "            \n",
    "            for i, file_path in enumerate(sorted(lp_files)):\n",
    "                try:\n",
    "                    print(f\"파일 {i+1} 로딩: {os.path.basename(file_path)}\")\n",
    "                    \n",
    "                    # CSV 파일 읽기\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # 컬럼명 확인 및 표준화 (실제 파일 구조에 맞춰 조정)\n",
    "                    expected_columns = ['대체고객번호', 'LP 수신일자', '순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "                    \n",
    "                    if all(col in df.columns for col in expected_columns):\n",
    "                        lp_dataframes.append(df)\n",
    "                        total_records += len(df)\n",
    "                        \n",
    "                        # 기간 정보 출력\n",
    "                        if 'LP 수신일자' in df.columns:\n",
    "                            min_date = df['LP 수신일자'].min()\n",
    "                            max_date = df['LP 수신일자'].max()\n",
    "                            print(f\"  레코드 수: {len(df):,}\")\n",
    "                            print(f\"  고객 수: {df['대체고객번호'].nunique()}\")\n",
    "                            print(f\"  기간: {min_date} ~ {max_date}\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠️ 컬럼 구조가 예상과 다름: {list(df.columns)}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ 파일 로딩 실패: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not lp_dataframes:\n",
    "                print(\"유효한 LP 데이터가 없습니다.\")\n",
    "                return None\n",
    "            \n",
    "            # 모든 데이터 결합\n",
    "            self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "            \n",
    "            print(f\"\\\\n✅ 전체 LP 데이터 결합 완료:\")\n",
    "            print(f\"  총 레코드: {len(self.lp_data):,}\")\n",
    "            print(f\"  총 고객: {self.lp_data['대체고객번호'].nunique()}\")\n",
    "            \n",
    "            return self._analyze_lp_quality()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LP 데이터 로딩 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP 데이터 품질 분석\"\"\"\n",
    "        print(\"\\\\n=== LP 데이터 품질 분석 ===\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        print(\"📈 기본 통계:\")\n",
    "        print(self.lp_data[numeric_columns].describe())\n",
    "        \n",
    "        # 시간 간격 체크 (LP 수신일자를 datetime으로 변환)\n",
    "        print(\"\\\\n⏰ 시간 간격 체크:\")\n",
    "        #수정전\n",
    "        #self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP 수신일자'])\n",
    "        \n",
    "        #수정후\n",
    "        # 24:00을 다음날 00:00으로 정확히 변환\n",
    "        self.lp_data['LP 수신일자'] = self.lp_data['LP 수신일자'].str.replace(' 24:00', ' 00:00')\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP 수신일자'], errors='coerce')\n",
    "\n",
    "        # 원래 24:00이었던 행들을 다음날로 이동\n",
    "        mask_24 = self.lp_data['LP 수신일자'].str.contains(' 00:00')\n",
    "        self.lp_data.loc[mask_24, 'datetime'] += pd.Timedelta(days=1)\n",
    "        \n",
    "        # 고객별 샘플 체크 (상위 3개 고객)\n",
    "        sample_customers = self.lp_data['대체고객번호'].unique()[:3]\n",
    "        \n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('datetime')\n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['datetime'].diff().dt.total_seconds() / 60  # 분 단위\n",
    "                time_diffs = time_diffs.dropna()\n",
    "                \n",
    "                avg_interval = time_diffs.mean()\n",
    "                std_interval = time_diffs.std()\n",
    "                print(f\"  {customer}: 평균 간격 {avg_interval:.1f}분, 표준편차 {std_interval:.1f}분\")\n",
    "        \n",
    "        # 데이터 품질 체크\n",
    "        print(\"\\\\n🔍 데이터 품질 체크:\")\n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                missing_count = self.lp_data[col].isnull().sum()\n",
    "                missing_pct = (missing_count / len(self.lp_data)) * 100\n",
    "                zero_count = (self.lp_data[col] == 0).sum()\n",
    "                zero_pct = (zero_count / len(self.lp_data)) * 100\n",
    "                \n",
    "                print(f\"  {col}:\")\n",
    "                print(f\"    결측치: {missing_count}건 ({missing_pct:.2f}%)\")\n",
    "                print(f\"    0값: {zero_count}건 ({zero_pct:.2f}%)\")\n",
    "        \n",
    "        # 이상치 탐지 (IQR 방법)\n",
    "        print(\"\\\\n🚨 이상치 탐지:\")\n",
    "        return self.detect_outliers('iqr')\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"이상치 탐지\"\"\"\n",
    "        outlier_summary = {}\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                if method == 'iqr':\n",
    "                    Q1 = self.lp_data[col].quantile(0.25)\n",
    "                    Q3 = self.lp_data[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    \n",
    "                    outliers = self.lp_data[\n",
    "                        (self.lp_data[col] < lower_bound) | \n",
    "                        (self.lp_data[col] > upper_bound)\n",
    "                    ]\n",
    "                    \n",
    "                    outlier_count = len(outliers)\n",
    "                    outlier_pct = (outlier_count / len(self.lp_data)) * 100\n",
    "                    \n",
    "                    print(f\"  {col}: {outlier_count}건 ({outlier_pct:.2f}%)\")\n",
    "                    outlier_summary[col] = {\n",
    "                        'count': outlier_count,\n",
    "                        'percentage': outlier_pct,\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound\n",
    "                    }\n",
    "        \n",
    "        return outlier_summary\n",
    "    \n",
    "\n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"데이터 품질 종합 리포트 생성 및 전처리된 데이터 저장\"\"\"\n",
    "        import json\n",
    "        from datetime import datetime\n",
    "        import os\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 데이터 품질 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # 데이터 존재 여부 확인\n",
    "        if self.customer_data is None or self.lp_data is None:\n",
    "            print(\"❌ 데이터가 로딩되지 않았습니다.\")\n",
    "            return False\n",
    "\n",
    "        # 고객 데이터 요약\n",
    "        if self.customer_data is not None:\n",
    "            print(f\"\\n👥 고객 데이터:\")\n",
    "            print(f\"  총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"  계약종별 유형: {self.customer_data['계약종별'].nunique()}개\")\n",
    "            print(f\"  사용용도 유형: {self.customer_data['사용용도'].nunique()}개\")\n",
    "\n",
    "            # ⭐ analysis_results에 고객 정보 저장\n",
    "            self.analysis_results['customer_summary'] = {\n",
    "                'total_customers': len(self.customer_data),\n",
    "                'contract_types': self.customer_data['계약종별'].value_counts().to_dict(),\n",
    "                'usage_types': self.customer_data['사용용도'].value_counts().to_dict()\n",
    "            }\n",
    "\n",
    "        # LP 데이터 요약\n",
    "        if self.lp_data is not None:\n",
    "            print(f\"\\n⚡ LP 데이터:\")\n",
    "            print(f\"  총 레코드: {len(self.lp_data):,}건\")\n",
    "            print(f\"  측정 기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"  데이터 커버리지: {(self.lp_data['datetime'].max() - self.lp_data['datetime'].min()).days}일\")\n",
    "\n",
    "            # 평균 전력 사용량\n",
    "            avg_power = self.lp_data['순방향 유효전력'].mean()\n",
    "            print(f\"  평균 유효전력: {avg_power:.2f}kW\")\n",
    "\n",
    "            # ⭐ analysis_results에 LP 데이터 정보 저장\n",
    "            self.analysis_results['lp_data_summary'] = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['대체고객번호'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'avg_power': float(avg_power)\n",
    "            }\n",
    "\n",
    "        # ⭐⭐⭐ 핵심: 전처리된 데이터 저장\n",
    "        print(f\"\\n💾 전처리된 LP 데이터 저장 중...\")\n",
    "\n",
    "        try:\n",
    "            # 출력 디렉토리 생성\n",
    "            import os\n",
    "            os.makedirs('./analysis_results', exist_ok=True)\n",
    "\n",
    "            # 전처리된 데이터 저장\n",
    "            processed_csv = './analysis_results/processed_lp_data.csv'\n",
    "            processed_parquet = './analysis_results/processed_lp_data.parquet'\n",
    "\n",
    "            print(f\"   📊 저장 대상: {len(self.lp_data):,}개 레코드\")\n",
    "            print(f\"   💾 저장 중... (잠시만 기다려주세요)\")\n",
    "\n",
    "            # 1. CSV 저장 (호환성용)\n",
    "            print(f\"      📄 CSV 저장 중...\")\n",
    "            self.lp_data.to_csv(processed_csv, index=False, encoding='utf-8-sig')\n",
    "            csv_size_gb = os.path.getsize(processed_csv) / 1024**3\n",
    "\n",
    "            # 2. ⭐ Parquet 저장 (성능 최적화용)\n",
    "            print(f\"      📦 Parquet 저장 중...\")\n",
    "            try:\n",
    "                self.lp_data.to_parquet(processed_parquet, compression='snappy')\n",
    "                parquet_size_gb = os.path.getsize(processed_parquet) / 1024**3\n",
    "                parquet_success = True\n",
    "            except Exception as parquet_error:\n",
    "                print(f\"         ⚠️ Parquet 저장 실패: {parquet_error}\")\n",
    "                print(f\"         💡 해결방법: pip install pyarrow\")\n",
    "                parquet_success = False\n",
    "\n",
    "            print(f\"   ✅ 전처리된 데이터 저장 완료!\")\n",
    "            print(f\"      📄 CSV: {processed_csv} ({csv_size_gb:.2f}GB)\")\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"      📦 Parquet: {processed_parquet} ({parquet_size_gb:.2f}GB)\")\n",
    "                print(f\"      🚀 크기 절약: {((csv_size_gb - parquet_size_gb) / csv_size_gb * 100):.1f}%\")\n",
    "                print(f\"      ⚡ 로딩 속도 향상: 약 2-3배 빨라짐!\")\n",
    "\n",
    "            # 메타 정보 저장 (⭐ Parquet 정보 추가)\n",
    "            meta_info = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['대체고객번호'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'file_info': {\n",
    "                    'csv_file': 'processed_lp_data.csv',\n",
    "                    'csv_size_gb': csv_size_gb,\n",
    "                    'parquet_file': 'processed_lp_data.parquet' if parquet_success else None,\n",
    "                    'parquet_size_gb': parquet_size_gb if parquet_success else None,\n",
    "                    'parquet_available': parquet_success,\n",
    "                    'encoding': 'utf-8-sig'\n",
    "                },\n",
    "                'processed_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            # analysis_results에 메타 정보 추가\n",
    "            self.analysis_results['processed_lp_data'] = meta_info\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"   🚀 2-3단계에서 30분 → 3-5분으로 시간 단축 예상!\")\n",
    "            else:\n",
    "                print(f\"   📄 CSV로 저장 완료 (30분 → 8분 시간 단축)\")\n",
    "\n",
    "        except Exception as save_error:\n",
    "            print(f\"   ❌ 전처리된 데이터 저장 실패: {save_error}\")\n",
    "            print(f\"      (분석은 계속 진행됩니다)\")\n",
    "\n",
    "        # ⭐⭐⭐ 필수: JSON 결과 저장 (2-3단계 연계용)\n",
    "        print(f\"\\n💾 분석 결과 JSON 저장 중...\")\n",
    "\n",
    "        try:\n",
    "            # 타임스탬프 추가\n",
    "            self.analysis_results['metadata'] = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'analysis_stage': 'step1_preprocessing_optimized',\n",
    "                'version': '2.0',\n",
    "                'total_customers': len(self.customer_data) if self.customer_data is not None else 0,\n",
    "                'total_lp_records': len(self.lp_data) if self.lp_data is not None else 0\n",
    "            }\n",
    "\n",
    "            # JSON 파일로 저장\n",
    "            output_file = os.path.join('./analysis_results', 'analysis_results.json')\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.analysis_results, f, \n",
    "                         ensure_ascii=False, \n",
    "                         indent=2, \n",
    "                         default=str)\n",
    "\n",
    "            print(f\"✅ 분석 결과 JSON 저장: {output_file}\")\n",
    "            print(f\"   저장된 항목: {len(self.analysis_results)}개\")\n",
    "\n",
    "            # 저장된 구조 확인\n",
    "            print(f\"   📁 저장된 구조:\")\n",
    "            for key in self.analysis_results.keys():\n",
    "                if key == 'metadata':\n",
    "                    print(f\"      - metadata: 시간정보 및 버전\")\n",
    "                elif key == 'customer_summary':\n",
    "                    print(f\"      - customer_summary: 고객 기본 정보\")\n",
    "                elif key == 'lp_data_summary':\n",
    "                    print(f\"      - lp_data_summary: LP 데이터 요약\")\n",
    "                elif key == 'processed_lp_data':\n",
    "                    print(f\"      - processed_lp_data: 전처리된 데이터 메타정보\")\n",
    "                else:\n",
    "                    print(f\"      - {key}: {type(self.analysis_results[key])}\")\n",
    "\n",
    "        except Exception as json_error:\n",
    "            print(f\"❌ JSON 저장 실패: {json_error}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # 권장사항\n",
    "        print(\"\\n💡 다음 단계 권장사항:\")\n",
    "        print(\"  1. 시계열 패턴 분석 (전처리된 데이터 활용)\")\n",
    "        print(\"  2. 고객별 사용량 프로파일링\")\n",
    "        print(\"  3. 변동성 지표 계산 및 비교\")\n",
    "        print(\"  4. 이상 패턴 탐지 알고리즘 개발\")\n",
    "\n",
    "        print(f\"\\n🎯 1단계 최적화 완료!\")\n",
    "        print(f\"   📁 생성 파일:\")\n",
    "        print(f\"      - analysis_results.json (2-3단계 연계용)\")\n",
    "        print(f\"      - processed_lp_data.csv (전처리된 LP 데이터)\")\n",
    "        if 'processed_lp_data' in self.analysis_results and self.analysis_results['processed_lp_data']['file_info']['parquet_available']:\n",
    "            print(f\"      - processed_lp_data.parquet (고성능 전처리된 데이터)\")\n",
    "\n",
    "        return True\n",
    "\n",
    "# 사용 예제 (실제 데이터안심구역에서 실행)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\")\n",
    "    print(\"데이터안심구역 전용 - 실제 데이터 분석\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1단계: 고객 기본정보 분석\n",
    "    print(\"\\\\n[1단계] 고객 기본정보 로딩 및 분석\")\n",
    "    customer_analysis = analyzer.load_customer_data('제13회 산업부 공모전 대상고객/제13회 산업부 공모전 대상고객.xlsx')\n",
    "    \n",
    "    # 2단계: LP 데이터 분석\n",
    "    print(\"\\\\n[2단계] LP 데이터 로딩 및 품질 분석\")\n",
    "    lp_analysis = analyzer.load_lp_data('./제13회 산업부 공모전 대상고객 LP데이터/')  # 현재 디렉터리에서 LP 파일 찾기\n",
    "    \n",
    "    # 3단계: 이상치 탐지\n",
    "    print(\"\\\\n[3단계] 이상치 탐지 및 데이터 정제\")\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4단계: 종합 리포트\n",
    "    print(\"\\\\n[4단계] 데이터 품질 종합 평가\")\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\\\n🎯 1단계 데이터 품질 점검 완료!\")\n",
    "    print(\"다음: 2단계 시계열 패턴 분석 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c73f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\n",
      "2단계: 시계열 패턴 분석 (최적화된 데이터 로딩)\n",
      "============================================================\n",
      "================================================================================\n",
      "한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\n",
      "2단계: 시계열 패턴 분석 및 변동성 지표 개발\n",
      "================================================================================\n",
      "작업 디렉토리: ./\n",
      "결과 저장: ./analysis_results\n",
      "\n",
      "🚀 한국전력공사 LP 데이터 시계열 패턴 분석 시작\n",
      "시작 시간: 2025-07-09 23:15:52\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 2단계: 전처리된 데이터 로딩\n",
      "============================================================\n",
      "✅ 1단계 결과 파일 발견\n",
      "   처리 시간: 2025-07-09T23:09:19.679137\n",
      "   총 고객: 200명\n",
      "   총 레코드: 57,600건\n",
      "\n",
      "🚀 Parquet 파일 로딩 중... (고성능)\n",
      "   ✅ Parquet 로딩 성공!\n",
      "\n",
      "⚡ 로딩 성능 요약:\n",
      "   방법: Parquet\n",
      "   시간: 0.03초\n",
      "   속도: 1,693,819 레코드/초\n",
      "   데이터: 57,600건\n",
      "   고객: 10명\n",
      "   기간: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "   🚀 성능 개선: 기존 대비 2-3배 빠름!\n",
      "\n",
      "💡 다음 단계: 시계열 패턴 분석 시작\n",
      "\n",
      "📊 외부 데이터 로딩...\n",
      "   🌤️ 기상 데이터: 1,096일\n",
      "   📅 달력 데이터: 1,096일\n",
      "\\n📈 3단계: 시계열 패턴 분석...\n",
      "   🕐 시간 파생 변수 생성 중...\n",
      "   📊 사용 가능한 컬럼: ['대체고객번호', 'LP 수신일자', '순방향 유효전력', '지상무효', '진상무효', '피상전력', 'datetime', 'hour', 'weekday', 'is_weekend']\n",
      "   ✅ 사용할 컬럼: 시간=datetime, 전력=순방향 유효전력\n",
      "   📊 시간대별 패턴 분석...\n",
      "      피크 시간대: [10, 11, 13, 17, 19, 20]\n",
      "      비피크 시간대: [0, 1, 2, 3, 4, 23]\n",
      "   📅 요일별 패턴 분석...\n",
      "      평일 평균: 56.57kW\n",
      "      주말 평균: 46.86kW\n",
      "      주말/평일 비율: 0.828\n",
      "   🗓️ 월별 계절성 분석...\n",
      "      계절별 평균 사용량:\n",
      "        봄: 53.66kW\n",
      "\\n📊 4단계: 변동성 지표 분석...\n",
      "   🔄 10명 고객 변동성 분석 중...\n",
      "   📈 전체 데이터 변동성:\n",
      "      전체 변동계수: 0.9855\n",
      "      평균 전력: 53.66kW\n",
      "      표준편차: 52.88kW\n",
      "\\n   ⏰ 시간대별 변동성 패턴:\n",
      "      고변동성 시간대: [7, 8, 9]시 (CV: 1.1426)\n",
      "      저변동성 시간대: [20, 19, 10]시 (CV: 0.7510)\n",
      "\\n   📅 요일별 변동성 패턴:\n",
      "      평일 평균 변동계수: 0.9842\n",
      "      주말 평균 변동계수: 0.9588\n",
      "      주말/평일 변동성 비율: 0.974\n",
      "\\n   🗓️ 월별 변동성 패턴:\n",
      "      고변동성 월: [3]월\n",
      "      저변동성 월: [3]월\n",
      "\\n   👥 고객별 변동성 분포 분석...\n",
      "   📊 고객별 변동계수 분포 (10명):\n",
      "      평균: 0.9033\n",
      "      표준편차: 0.6844\n",
      "      10%ile: 0.2859\n",
      "      25%ile: 0.3334\n",
      "      50%ile: 0.7678\n",
      "      75%ile: 0.9535\n",
      "      90%ile: 2.1769\n",
      "\\n   🎯 변동성 등급별 고객 분포:\n",
      "      매우 안정 (<0.1): 0명 (0.0%)\n",
      "      안정 (0.1-0.2): 0명 (0.0%)\n",
      "      보통 (0.2-0.3): 3명 (30.0%)\n",
      "      높음 (0.3-0.5): 1명 (10.0%)\n",
      "      매우 높음 (0.5-1.0): 3명 (30.0%)\n",
      "      극히 높음 (>1.0): 3명 (30.0%)\n",
      "\\n   💾 변동성 요약 저장: ./analysis_results\\volatility_summary.csv\n",
      "\\n🚨 5단계: 이상 패턴 탐지...\n",
      "   🔍 10명 고객 이상 패턴 탐지 중...\n",
      "   📊 전체 데이터 이상치 현황:\n",
      "      통계적 이상치: 508개 (0.88%)\n",
      "      정상 범위: -113.5 ~ 209.4kW\n",
      "\\n   🌙 시간대별 사용 패턴:\n",
      "      야간 평균: 38.41kW\n",
      "      주간 평균: 65.47kW\n",
      "      야간/주간 비율: 0.587\n",
      "\\n   ⚫ 0값 패턴 분석:\n",
      "      0값 측정: 0개 (0.00%)\n",
      "\\n   ⚡ 급격한 변화 패턴:\n",
      "      급격한 변화: 508건 (0.88%)\n",
      "\\n   📊 이상 패턴 고객 요약 (10명 분석):\n",
      "      야간 과다 사용: 2명\n",
      "      과도한 0값: 0명\n",
      "      높은 변동성: 3명\n",
      "      통계적 이상치 다수: 1명\n",
      "      전체 이상 패턴 비율: 약 30.0%\n",
      "\\n📊 6단계: 분석 결과 시각화...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   💾 시계열 패턴 시각화 저장: ./analysis_results\\temporal_patterns_summary.png\n",
      "\\n📋 7단계: 종합 분석 리포트 생성...\n",
      "   ❌ 리포트 생성 실패: Cannot specify ',' with 's'.\n",
      "\\n💾 8단계: 분석 결과 저장...\n",
      "   💾 분석 결과 JSON 저장: ./analysis_results\\analysis_results.json\n",
      "\n",
      "================================================================================\n",
      "🎉 시계열 패턴 분석 완료!\n",
      "================================================================================\n",
      "소요 시간: 0:00:06.148045\n",
      "결과 저장 위치: ./analysis_results\n",
      "\n",
      "🎯 2단계 시계열 패턴 분석 완료!\n",
      "   📁 생성 파일:\n",
      "      - volatility_summary.csv (3단계 입력용)\n",
      "      - analysis_results.json (메타데이터)\n",
      "   ⏱️ 처리 시간: 30분 → 3-5분으로 단축!\n",
      "   🚀 다음: 3단계 변동계수 알고리즘 실행\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    \"\"\"한국전력공사 LP 데이터 시계열 패턴 분석 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path='./'):\n",
    "        \"\"\"\n",
    "        초기화\n",
    "        Args:\n",
    "            base_path: 데이터가 저장된 기본 경로\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "        # 결과 저장 디렉토리 생성\n",
    "        self.output_dir = os.path.join(base_path, 'analysis_results')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\")\n",
    "        print(\"2단계: 시계열 패턴 분석 및 변동성 지표 개발\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"작업 디렉토리: {self.base_path}\")\n",
    "        print(f\"결과 저장: {self.output_dir}\")\n",
    "        print()\n",
    "\n",
    "    def load_customer_data(self, filename='제13회 산업부 공모전 대상고객.xlsx'):\n",
    "        \"\"\"실제 고객 기본정보 로딩\"\"\"\n",
    "        print(\"🔄 1단계: 고객 기본정보 로딩...\")\n",
    "        \n",
    "        try:\n",
    "            file_path = os.path.join(self.base_path, filename)\n",
    "            self.customer_data = pd.read_excel(file_path)\n",
    "            \n",
    "            print(f\"✅ 고객 데이터 로딩 완료\")\n",
    "            print(f\"   - 총 고객 수: {len(self.customer_data):,}명\")\n",
    "            print(f\"   - 컬럼: {list(self.customer_data.columns)}\")\n",
    "            \n",
    "            # 고객 분포 분석\n",
    "            contract_dist = self.customer_data['계약종별'].value_counts()\n",
    "            usage_dist = self.customer_data['사용용도'].value_counts()\n",
    "            \n",
    "            print(f\"\\\\n📊 고객 분포:\")\n",
    "            print(f\"   - 계약종별: {len(contract_dist)}개 유형\")\n",
    "            print(f\"   - 사용용도: {len(usage_dist)}개 유형\")\n",
    "            \n",
    "            self.analysis_results['customer_summary'] = {\n",
    "                'total_customers': len(self.customer_data),\n",
    "                'contract_types': contract_dist.to_dict(),\n",
    "                'usage_types': usage_dist.to_dict()\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 고객 데이터 로딩 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_preprocessed_data(self):\n",
    "        \"\"\"1단계에서 전처리된 데이터 로딩 (성능 최적화)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 2단계: 전처리된 데이터 로딩\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # 1단계 결과 확인\n",
    "            analysis_results_path = './analysis_results/analysis_results.json'\n",
    "            if os.path.exists(analysis_results_path):\n",
    "                with open(analysis_results_path, 'r', encoding='utf-8') as f:\n",
    "                    step1_results = json.load(f)\n",
    "                print(\"✅ 1단계 결과 파일 발견\")\n",
    "                print(f\"   처리 시간: {step1_results.get('metadata', {}).get('timestamp', 'N/A')}\")\n",
    "                print(f\"   총 고객: {step1_results.get('metadata', {}).get('total_customers', 0):,}명\")\n",
    "                print(f\"   총 레코드: {step1_results.get('metadata', {}).get('total_lp_records', 0):,}건\")\n",
    "            else:\n",
    "                print(\"⚠️ 1단계 결과 파일 없음 - 기본 경로에서 전처리된 데이터 탐색\")\n",
    "                \n",
    "            # 전처리된 데이터 로딩 (우선순위: Parquet > CSV)\n",
    "            processed_parquet = './analysis_results/processed_lp_data.parquet'\n",
    "            processed_csv = './analysis_results/processed_lp_data.csv'\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            if os.path.exists(processed_parquet):\n",
    "                print(\"\\n🚀 Parquet 파일 로딩 중... (고성능)\")\n",
    "                try:\n",
    "                    self.lp_data = pd.read_parquet(processed_parquet)\n",
    "                    loading_method = \"Parquet\"\n",
    "                    print(\"   ✅ Parquet 로딩 성공!\")\n",
    "                except ImportError:\n",
    "                    print(\"   ⚠️ PyArrow 없음 - CSV로 대체\")\n",
    "                    self.lp_data = pd.read_csv(processed_csv)\n",
    "                    loading_method = \"CSV (PyArrow 없음)\"\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ Parquet 로딩 실패: {e}\")\n",
    "                    print(\"   🔄 CSV로 대체 시도...\")\n",
    "                    self.lp_data = pd.read_csv(processed_csv)\n",
    "                    loading_method = \"CSV (Parquet 실패)\"\n",
    "                    \n",
    "            elif os.path.exists(processed_csv):\n",
    "                print(\"\\n📄 CSV 파일 로딩 중...\")\n",
    "                self.lp_data = pd.read_csv(processed_csv)\n",
    "                loading_method = \"CSV\"\n",
    "                print(\"   ✅ CSV 로딩 성공!\")\n",
    "                \n",
    "            else:\n",
    "                print(\"\\n❌ 전처리된 데이터 파일이 없습니다!\")\n",
    "                print(\"   🔧 해결방법: 1단계 코드를 먼저 실행하세요\")\n",
    "                print(\"   📁 필요 파일:\")\n",
    "                print(\"      - ./analysis_results/processed_lp_data.parquet\")\n",
    "                print(\"      - ./analysis_results/processed_lp_data.csv\")\n",
    "                return False\n",
    "                \n",
    "            # 로딩 시간 계산\n",
    "            loading_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # 데이터 타입 최적화\n",
    "            if 'datetime' in self.lp_data.columns:\n",
    "                self.lp_data['datetime'] = pd.to_datetime(self.lp_data['datetime'])\n",
    "            \n",
    "            # 추가 시계열 컬럼 생성 (1단계에서 누락된 경우)\n",
    "            if 'hour' not in self.lp_data.columns:\n",
    "                self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "            if 'weekday' not in self.lp_data.columns:\n",
    "                self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "            if 'is_weekend' not in self.lp_data.columns:\n",
    "                self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "                \n",
    "            # 성능 요약\n",
    "            print(f\"\\n⚡ 로딩 성능 요약:\")\n",
    "            print(f\"   방법: {loading_method}\")\n",
    "            print(f\"   시간: {loading_time:.2f}초\")\n",
    "            print(f\"   속도: {len(self.lp_data)/loading_time:,.0f} 레코드/초\")\n",
    "            print(f\"   데이터: {len(self.lp_data):,}건\")\n",
    "            print(f\"   고객: {self.lp_data['대체고객번호'].nunique():,}명\")\n",
    "            print(f\"   기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            \n",
    "            # 기존 대비 성능 개선 표시\n",
    "            if loading_method.startswith(\"Parquet\"):\n",
    "                print(f\"   🚀 성능 개선: 기존 대비 2-3배 빠름!\")\n",
    "            elif loading_method.startswith(\"CSV\"):\n",
    "                print(f\"   📈 성능 개선: 기존 대비 3-4배 빠름!\")\n",
    "                \n",
    "            print(f\"\\n💡 다음 단계: 시계열 패턴 분석 시작\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 전처리된 데이터 로딩 실패: {e}\")\n",
    "            print(\"   🔧 해결방법:\")\n",
    "            print(\"   1. 1단계 코드 실행 여부 확인\")\n",
    "            print(\"   2. ./analysis_results/ 폴더 존재 확인\")\n",
    "            print(\"   3. 파일 권한 확인\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        \n",
    "    def load_external_data(self):\n",
    "        \"\"\"기상 및 달력 데이터 로딩\"\"\"\n",
    "        print(\"\\n📊 외부 데이터 로딩...\")\n",
    "        \n",
    "        try:\n",
    "            # 기상 데이터 로딩\n",
    "            weather_file = 'weather_daily_processed.csv'\n",
    "            if os.path.exists(weather_file):\n",
    "                self.weather_data = pd.read_csv(weather_file)\n",
    "                self.weather_data['날짜'] = pd.to_datetime(self.weather_data['날짜'])\n",
    "                print(f\"   🌤️ 기상 데이터: {len(self.weather_data):,}일\")\n",
    "            else:\n",
    "                print(\"   ⚠️ 기상 데이터 없음\")\n",
    "                self.weather_data = None\n",
    "                \n",
    "            # 달력 데이터 로딩\n",
    "            calendar_file = 'power_analysis_calendar_2022_2025.csv'\n",
    "            if os.path.exists(calendar_file):\n",
    "                self.calendar_data = pd.read_csv(calendar_file)\n",
    "                self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date'])\n",
    "                print(f\"   📅 달력 데이터: {len(self.calendar_data):,}일\")\n",
    "            else:\n",
    "                print(\"   ⚠️ 달력 데이터 없음\")\n",
    "                self.calendar_data = None\n",
    "                \n",
    "            return True\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 외부 데이터 로딩 실패: {e}\")\n",
    "            self.weather_data = None\n",
    "            self.calendar_data = None\n",
    "            return False\n",
    "\n",
    "    def _validate_data_quality(self):\n",
    "        \"\"\"데이터 품질 검증\"\"\"\n",
    "        print(\"\\\\n🔍 데이터 품질 검증 중...\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        numeric_columns = ['순방향 유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        available_numeric_cols = [col for col in numeric_columns if col in self.lp_data.columns]\n",
    "        \n",
    "        print(f\"   📈 수치형 컬럼: {len(available_numeric_cols)}개\")\n",
    "        \n",
    "        # 결측치 확인\n",
    "        null_counts = self.lp_data[available_numeric_cols].isnull().sum()\n",
    "        total_nulls = null_counts.sum()\n",
    "        \n",
    "        if total_nulls > 0:\n",
    "            print(f\"   ⚠️ 결측치: {total_nulls:,}개 ({total_nulls/len(self.lp_data)*100:.2f}%)\")\n",
    "            for col, count in null_counts.items():\n",
    "                if count > 0:\n",
    "                    print(f\"      {col}: {count:,}개\")\n",
    "        else:\n",
    "            print(\"   ✅ 결측치 없음\")\n",
    "        \n",
    "        # 시간 간격 체크 (샘플 고객으로)\n",
    "        sample_customers = self.lp_data['대체고객번호'].unique()[:3]\n",
    "        \n",
    "        print(\"   ⏰ 시간 간격 검증:\")\n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('LP 수신일자')\n",
    "            \n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['LP 수신일자'].diff().dt.total_seconds() / 60\n",
    "                time_diffs = time_diffs.dropna()\n",
    "                \n",
    "                if len(time_diffs) > 0:\n",
    "                    avg_interval = time_diffs.mean()\n",
    "                    std_interval = time_diffs.std()\n",
    "                    print(f\"      {customer}: 평균 {avg_interval:.1f}분 (표준편차: {std_interval:.1f})\")\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['data_quality'] = {\n",
    "            'total_records': len(self.lp_data),\n",
    "            'customers': self.lp_data['대체고객번호'].nunique(),\n",
    "            'null_counts': null_counts.to_dict(),\n",
    "            'date_range': {\n",
    "                'start': str(self.lp_data['LP 수신일자'].min()),\n",
    "                'end': str(self.lp_data['LP 수신일자'].max())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def analyze_temporal_patterns(self):\n",
    "        \"\"\"시계열 패턴 분석\"\"\"\n",
    "        print(\"\\\\n📈 3단계: 시계열 패턴 분석...\")\n",
    "\n",
    "        # ⭐⭐⭐ 핵심 수정: 컬럼명 확인 및 수정\n",
    "        print(\"   🕐 시간 파생 변수 생성 중...\")\n",
    "\n",
    "        # 1단계에서 저장된 컬럼명 확인\n",
    "        print(f\"   📊 사용 가능한 컬럼: {list(self.lp_data.columns)}\")\n",
    "\n",
    "        # datetime 컬럼 확인 및 변환\n",
    "        if 'datetime' in self.lp_data.columns:\n",
    "            datetime_col = 'datetime'\n",
    "        elif 'LP 수신일자' in self.lp_data.columns:\n",
    "            datetime_col = 'LP 수신일자'\n",
    "            # datetime 타입으로 변환 (필요시)\n",
    "            if not pd.api.types.is_datetime64_any_dtype(self.lp_data[datetime_col]):\n",
    "                self.lp_data[datetime_col] = pd.to_datetime(self.lp_data[datetime_col])\n",
    "        else:\n",
    "            print(\"   ❌ 날짜/시간 컬럼을 찾을 수 없습니다!\")\n",
    "            return False\n",
    "\n",
    "        # 시간 관련 파생 변수 생성 (수정된 컬럼명 사용)\n",
    "        self.lp_data['날짜'] = self.lp_data[datetime_col].dt.date\n",
    "        self.lp_data['시간'] = self.lp_data[datetime_col].dt.hour\n",
    "        self.lp_data['요일'] = self.lp_data[datetime_col].dt.weekday  # 0=월요일\n",
    "        self.lp_data['월'] = self.lp_data[datetime_col].dt.month\n",
    "        self.lp_data['주'] = self.lp_data[datetime_col].dt.isocalendar().week\n",
    "        self.lp_data['주말여부'] = self.lp_data['요일'].isin([5, 6])  # 토, 일\n",
    "\n",
    "        # 전력 컬럼명도 확인 및 수정\n",
    "        power_col = None\n",
    "        if '순방향 유효전력' in self.lp_data.columns:\n",
    "            power_col = '순방향 유효전력'\n",
    "        elif '순방향유효전력' in self.lp_data.columns:\n",
    "            power_col = '순방향유효전력'\n",
    "        else:\n",
    "            print(\"   ❌ 전력 컬럼을 찾을 수 없습니다!\")\n",
    "            return False\n",
    "\n",
    "        print(f\"   ✅ 사용할 컬럼: 시간={datetime_col}, 전력={power_col}\")\n",
    "\n",
    "        # 1. 시간대별 패턴 분석 (수정된 컬럼명 사용)\n",
    "        print(\"   📊 시간대별 패턴 분석...\")\n",
    "        hourly_patterns = self.lp_data.groupby('시간')[power_col].agg([\n",
    "            'mean', 'std', 'min', 'max', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        # 피크/비피크 시간대 식별\n",
    "        avg_by_hour = hourly_patterns['mean']\n",
    "        peak_threshold = avg_by_hour.quantile(0.75)\n",
    "        off_peak_threshold = avg_by_hour.quantile(0.25)\n",
    "\n",
    "        peak_hours = avg_by_hour[avg_by_hour >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = avg_by_hour[avg_by_hour <= off_peak_threshold].index.tolist()\n",
    "\n",
    "        print(f\"      피크 시간대: {peak_hours}\")\n",
    "        print(f\"      비피크 시간대: {off_peak_hours}\")\n",
    "\n",
    "        # 2. 요일별 패턴 분석\n",
    "        print(\"   📅 요일별 패턴 분석...\")\n",
    "        daily_patterns = self.lp_data.groupby('요일')[power_col].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        # 평일 vs 주말 비교\n",
    "        weekday_avg = self.lp_data[~self.lp_data['주말여부']][power_col].mean()\n",
    "        weekend_avg = self.lp_data[self.lp_data['주말여부']][power_col].mean()\n",
    "        weekend_ratio = weekend_avg / weekday_avg if weekday_avg > 0 else 0\n",
    "\n",
    "        print(f\"      평일 평균: {weekday_avg:.2f}kW\")\n",
    "        print(f\"      주말 평균: {weekend_avg:.2f}kW\")\n",
    "        print(f\"      주말/평일 비율: {weekend_ratio:.3f}\")\n",
    "\n",
    "        # 3. 월별 계절성 패턴\n",
    "        print(\"   🗓️ 월별 계절성 분석...\")\n",
    "        monthly_patterns = self.lp_data.groupby('월')[power_col].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        # 계절 구분 (한국 기준)\n",
    "        season_map = {12: '겨울', 1: '겨울', 2: '겨울',\n",
    "                     3: '봄', 4: '봄', 5: '봄',\n",
    "                     6: '여름', 7: '여름', 8: '여름',\n",
    "                     9: '가을', 10: '가을', 11: '가을'}\n",
    "\n",
    "        self.lp_data['계절'] = self.lp_data['월'].map(season_map)\n",
    "        seasonal_patterns = self.lp_data.groupby('계절')[power_col].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        print(f\"      계절별 평균 사용량:\")\n",
    "        for season, values in seasonal_patterns.iterrows():\n",
    "            print(f\"        {season}: {values['mean']:.2f}kW\")\n",
    "\n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['temporal_patterns'] = {\n",
    "            'hourly_patterns': hourly_patterns.to_dict(),\n",
    "            'daily_patterns': daily_patterns.to_dict(),\n",
    "            'monthly_patterns': monthly_patterns.to_dict(),\n",
    "            'seasonal_patterns': seasonal_patterns.to_dict(),\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "\n",
    "        return True\n",
    "\n",
    "    def analyze_volatility_indicators(self):\n",
    "        \"\"\"변동성 지표 분석 (집계 중심)\"\"\"\n",
    "        print(\"\\\\n📊 4단계: 변동성 지표 분석...\")\n",
    "        \n",
    "        customers = self.lp_data['대체고객번호'].unique()\n",
    "        print(f\"   🔄 {len(customers)}명 고객 변동성 분석 중...\")\n",
    "        \n",
    "        # 전체 데이터에 대한 집계 분석\n",
    "        \n",
    "        # 1. 전체 변동성 통계\n",
    "        overall_power = self.lp_data['순방향 유효전력']\n",
    "        overall_cv = overall_power.std() / overall_power.mean() if overall_power.mean() > 0 else 0\n",
    "        \n",
    "        print(f\"   📈 전체 데이터 변동성:\")\n",
    "        print(f\"      전체 변동계수: {overall_cv:.4f}\")\n",
    "        print(f\"      평균 전력: {overall_power.mean():.2f}kW\")\n",
    "        print(f\"      표준편차: {overall_power.std():.2f}kW\")\n",
    "        \n",
    "        # 2. 시간대별 변동성 패턴\n",
    "        hourly_volatility = self.lp_data.groupby('시간')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        hourly_volatility['cv'] = hourly_volatility['std'] / hourly_volatility['mean']\n",
    "        \n",
    "        print(f\"\\\\n   ⏰ 시간대별 변동성 패턴:\")\n",
    "        high_volatility_hours = hourly_volatility.nlargest(3, 'cv').index.tolist()\n",
    "        low_volatility_hours = hourly_volatility.nsmallest(3, 'cv').index.tolist()\n",
    "        print(f\"      고변동성 시간대: {high_volatility_hours}시 (CV: {hourly_volatility.loc[high_volatility_hours, 'cv'].mean():.4f})\")\n",
    "        print(f\"      저변동성 시간대: {low_volatility_hours}시 (CV: {hourly_volatility.loc[low_volatility_hours, 'cv'].mean():.4f})\")\n",
    "        \n",
    "        # 3. 요일별 변동성 패턴\n",
    "        daily_volatility = self.lp_data.groupby('요일')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        daily_volatility['cv'] = daily_volatility['std'] / daily_volatility['mean']\n",
    "        \n",
    "        weekday_cv = daily_volatility.loc[0:4, 'cv'].mean()  # 월-금\n",
    "        weekend_cv = daily_volatility.loc[5:6, 'cv'].mean()  # 토-일\n",
    "        \n",
    "        print(f\"\\\\n   📅 요일별 변동성 패턴:\")\n",
    "        print(f\"      평일 평균 변동계수: {weekday_cv:.4f}\")\n",
    "        print(f\"      주말 평균 변동계수: {weekend_cv:.4f}\")\n",
    "        print(f\"      주말/평일 변동성 비율: {weekend_cv/weekday_cv:.3f}\")\n",
    "        \n",
    "        # 4. 월별 변동성 패턴\n",
    "        monthly_volatility = self.lp_data.groupby('월')['순방향 유효전력'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        monthly_volatility['cv'] = monthly_volatility['std'] / monthly_volatility['mean']\n",
    "        \n",
    "        print(f\"\\\\n   🗓️ 월별 변동성 패턴:\")\n",
    "        high_var_months = monthly_volatility.nlargest(2, 'cv').index.tolist()\n",
    "        low_var_months = monthly_volatility.nsmallest(2, 'cv').index.tolist()\n",
    "        print(f\"      고변동성 월: {high_var_months}월\")\n",
    "        print(f\"      저변동성 월: {low_var_months}월\")\n",
    "        \n",
    "        # 5. 고객별 변동성 분포 (요약 통계만)\n",
    "        print(f\"\\\\n   👥 고객별 변동성 분포 분석...\")\n",
    "        \n",
    "        # 청크 단위로 고객별 변동계수 계산 (메모리 효율성)\n",
    "        chunk_size = 100\n",
    "        customer_cvs = []\n",
    "        \n",
    "        for i in range(0, len(customers), chunk_size):\n",
    "            chunk_customers = customers[i:i+chunk_size]\n",
    "            if (i // chunk_size + 1) % 5 == 0:  # 500명마다 진행상황 출력\n",
    "                print(f\"      진행: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            for customer in chunk_customers:\n",
    "                customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "                power_series = customer_data['순방향 유효전력']\n",
    "                \n",
    "                if len(power_series) >= 96 and power_series.mean() > 0:  # 최소 1일 데이터\n",
    "                    cv = power_series.std() / power_series.mean()\n",
    "                    customer_cvs.append(cv)\n",
    "        \n",
    "        # 고객별 변동계수 분포 통계\n",
    "        cv_array = np.array(customer_cvs)\n",
    "        cv_percentiles = np.percentile(cv_array, [10, 25, 50, 75, 90])\n",
    "        \n",
    "        print(f\"   📊 고객별 변동계수 분포 ({len(customer_cvs)}명):\")\n",
    "        print(f\"      평균: {cv_array.mean():.4f}\")\n",
    "        print(f\"      표준편차: {cv_array.std():.4f}\")\n",
    "        print(f\"      10%ile: {cv_percentiles[0]:.4f}\")\n",
    "        print(f\"      25%ile: {cv_percentiles[1]:.4f}\")\n",
    "        print(f\"      50%ile: {cv_percentiles[2]:.4f}\")\n",
    "        print(f\"      75%ile: {cv_percentiles[3]:.4f}\")\n",
    "        print(f\"      90%ile: {cv_percentiles[4]:.4f}\")\n",
    "        \n",
    "        # 변동계수 구간별 고객 수\n",
    "        cv_bins = [0, 0.1, 0.2, 0.3, 0.5, 1.0, float('inf')]\n",
    "        cv_labels = ['매우 안정 (<0.1)', '안정 (0.1-0.2)', '보통 (0.2-0.3)', \n",
    "                    '높음 (0.3-0.5)', '매우 높음 (0.5-1.0)', '극히 높음 (>1.0)']\n",
    "        \n",
    "        cv_counts = pd.cut(cv_array, bins=cv_bins, labels=cv_labels, include_lowest=True).value_counts()\n",
    "        \n",
    "        print(f\"\\\\n   🎯 변동성 등급별 고객 분포:\")\n",
    "        for grade, count in cv_counts.items():\n",
    "            percentage = count / len(customer_cvs) * 100\n",
    "            print(f\"      {grade}: {count}명 ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['volatility_analysis'] = {\n",
    "            'overall_cv': overall_cv,\n",
    "            'hourly_volatility': hourly_volatility.to_dict(),\n",
    "            'daily_volatility': daily_volatility.to_dict(),\n",
    "            'monthly_volatility': monthly_volatility.to_dict(),\n",
    "            'customer_cv_stats': {\n",
    "                'count': len(customer_cvs),\n",
    "                'mean': float(cv_array.mean()),\n",
    "                'std': float(cv_array.std()),\n",
    "                'percentiles': {\n",
    "                    '10%': float(cv_percentiles[0]),\n",
    "                    '25%': float(cv_percentiles[1]),\n",
    "                    '50%': float(cv_percentiles[2]),\n",
    "                    '75%': float(cv_percentiles[3]),\n",
    "                    '90%': float(cv_percentiles[4])\n",
    "                }\n",
    "            },\n",
    "            'volatility_distribution': cv_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        # 요약 데이터만 CSV로 저장 (개별 고객 데이터는 제외)\n",
    "        summary_data = {\n",
    "            'metric': ['overall_cv', 'weekday_cv', 'weekend_cv', 'customer_cv_mean', \n",
    "                      'customer_cv_std', 'customer_cv_median'],\n",
    "            'value': [overall_cv, weekday_cv, weekend_cv, cv_array.mean(), \n",
    "                     cv_array.std(), cv_percentiles[2]]\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        output_file = os.path.join(self.output_dir, 'volatility_summary.csv')\n",
    "        summary_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\\\n   💾 변동성 요약 저장: {output_file}\")\n",
    "        \n",
    "        return cv_array\n",
    "\n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"이상 패턴 탐지 (집계 중심)\"\"\"\n",
    "        print(\"\\\\n🚨 5단계: 이상 패턴 탐지...\")\n",
    "        \n",
    "        customers = self.lp_data['대체고객번호'].unique()\n",
    "        print(f\"   🔍 {len(customers)}명 고객 이상 패턴 탐지 중...\")\n",
    "        \n",
    "        # 전체 데이터 기반 이상 패턴 탐지\n",
    "        \n",
    "        # 1. 전체 데이터의 통계적 이상치 임계값 설정\n",
    "        overall_power = self.lp_data['순방향 유효전력']\n",
    "        q1, q3 = overall_power.quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # 전체 통계적 이상치\n",
    "        total_outliers = ((overall_power < lower_bound) | (overall_power > upper_bound)).sum()\n",
    "        outlier_rate = total_outliers / len(overall_power) * 100\n",
    "        \n",
    "        print(f\"   📊 전체 데이터 이상치 현황:\")\n",
    "        print(f\"      통계적 이상치: {total_outliers:,}개 ({outlier_rate:.2f}%)\")\n",
    "        print(f\"      정상 범위: {lower_bound:.1f} ~ {upper_bound:.1f}kW\")\n",
    "        \n",
    "        # 2. 시간대별 이상 패턴\n",
    "        night_hours = [0, 1, 2, 3, 4, 5]  # 야간 시간대\n",
    "        day_hours = [9, 10, 11, 12, 13, 14, 15, 16, 17]  # 주간 시간대\n",
    "        \n",
    "        night_data = self.lp_data[self.lp_data['시간'].isin(night_hours)]\n",
    "        day_data = self.lp_data[self.lp_data['시간'].isin(day_hours)]\n",
    "        \n",
    "        night_avg = night_data['순방향 유효전력'].mean()\n",
    "        day_avg = day_data['순방향 유효전력'].mean()\n",
    "        night_day_ratio = night_avg / day_avg if day_avg > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\n   🌙 시간대별 사용 패턴:\")\n",
    "        print(f\"      야간 평균: {night_avg:.2f}kW\")\n",
    "        print(f\"      주간 평균: {day_avg:.2f}kW\")\n",
    "        print(f\"      야간/주간 비율: {night_day_ratio:.3f}\")\n",
    "        \n",
    "        # 3. 0값 패턴 분석\n",
    "        zero_count = (overall_power == 0).sum()\n",
    "        zero_rate = zero_count / len(overall_power) * 100\n",
    "        \n",
    "        print(f\"\\\\n   ⚫ 0값 패턴 분석:\")\n",
    "        print(f\"      0값 측정: {zero_count:,}개 ({zero_rate:.2f}%)\")\n",
    "        \n",
    "        # 4. 급격한 변화 패턴 (전체 데이터 기준)\n",
    "        power_changes = self.lp_data.sort_values(['대체고객번호', 'LP 수신일자'])['순방향 유효전력'].pct_change().abs()\n",
    "        sudden_changes = power_changes[power_changes > 2.0]  # 200% 이상 변화\n",
    "        sudden_change_rate = len(sudden_changes) / len(power_changes.dropna()) * 100\n",
    "        \n",
    "        print(f\"\\\\n   ⚡ 급격한 변화 패턴:\")\n",
    "        print(f\"      급격한 변화: {len(sudden_changes):,}건 ({sudden_change_rate:.2f}%)\")\n",
    "        \n",
    "        # 5. 고객별 이상 패턴 요약 통계 (개별 출력 없이)\n",
    "        anomaly_customers = {\n",
    "            'high_night_usage': 0,      # 야간 과다 사용\n",
    "            'excessive_zeros': 0,        # 과도한 0값\n",
    "            'high_volatility': 0,        # 높은 변동성\n",
    "            'statistical_outliers': 0    # 통계적 이상치 다수\n",
    "        }\n",
    "        \n",
    "        chunk_size = 100\n",
    "        processed_customers = 0\n",
    "        \n",
    "        for i in range(0, len(customers), chunk_size):\n",
    "            chunk_customers = customers[i:i+chunk_size]\n",
    "            if (i // chunk_size + 1) % 5 == 0:\n",
    "                print(f\"      진행: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            for customer in chunk_customers:\n",
    "                customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "                power_series = customer_data['순방향 유효전력']\n",
    "                \n",
    "                if len(power_series) < 96:  # 최소 1일 데이터 필요\n",
    "                    continue\n",
    "                \n",
    "                processed_customers += 1\n",
    "                \n",
    "                # 야간 과다 사용 체크\n",
    "                customer_night = customer_data[customer_data['시간'].isin(night_hours)]['순방향 유효전력'].mean()\n",
    "                customer_day = customer_data[customer_data['시간'].isin(day_hours)]['순방향 유효전력'].mean()\n",
    "                if customer_day > 0 and customer_night / customer_day > 0.8:\n",
    "                    anomaly_customers['high_night_usage'] += 1\n",
    "                \n",
    "                # 과도한 0값 체크\n",
    "                zero_ratio = (power_series == 0).sum() / len(power_series)\n",
    "                if zero_ratio > 0.1:  # 10% 이상이 0값\n",
    "                    anomaly_customers['excessive_zeros'] += 1\n",
    "                \n",
    "                # 높은 변동성 체크\n",
    "                if power_series.mean() > 0:\n",
    "                    cv = power_series.std() / power_series.mean()\n",
    "                    if cv > 1.0:  # 변동계수 1.0 이상\n",
    "                        anomaly_customers['high_volatility'] += 1\n",
    "                \n",
    "                # 통계적 이상치 다수 체크\n",
    "                customer_outliers = ((power_series < lower_bound) | (power_series > upper_bound)).sum()\n",
    "                outlier_ratio = customer_outliers / len(power_series)\n",
    "                if outlier_ratio > 0.05:  # 5% 이상이 이상치\n",
    "                    anomaly_customers['statistical_outliers'] += 1\n",
    "        \n",
    "        # 종합 이상 패턴 고객 (중복 제거를 위해 실제로는 근사치)\n",
    "        total_anomaly_customers = max(anomaly_customers.values())  # 단순 근사\n",
    "        anomaly_rate = total_anomaly_customers / processed_customers * 100 if processed_customers > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\n   📊 이상 패턴 고객 요약 ({processed_customers}명 분석):\")\n",
    "        print(f\"      야간 과다 사용: {anomaly_customers['high_night_usage']}명\")\n",
    "        print(f\"      과도한 0값: {anomaly_customers['excessive_zeros']}명\")\n",
    "        print(f\"      높은 변동성: {anomaly_customers['high_volatility']}명\")\n",
    "        print(f\"      통계적 이상치 다수: {anomaly_customers['statistical_outliers']}명\")\n",
    "        print(f\"      전체 이상 패턴 비율: 약 {anomaly_rate:.1f}%\")\n",
    "        \n",
    "        # 분석 결과 저장\n",
    "        self.analysis_results['anomaly_analysis'] = {\n",
    "            'processed_customers': processed_customers,\n",
    "            'total_outliers': int(total_outliers),\n",
    "            'outlier_rate': float(outlier_rate),\n",
    "            'zero_count': int(zero_count),\n",
    "            'zero_rate': float(zero_rate),\n",
    "            'sudden_changes': len(sudden_changes),\n",
    "            'sudden_change_rate': float(sudden_change_rate),\n",
    "            'night_day_ratio': float(night_day_ratio),\n",
    "            'anomaly_customers': anomaly_customers,\n",
    "            'estimated_anomaly_rate': float(anomaly_rate)\n",
    "        }\n",
    "        \n",
    "        return anomaly_customers\n",
    "\n",
    "\n",
    "    def create_summary_visualizations(self):\n",
    "        \"\"\"요약 시각화 생성\"\"\"\n",
    "        print(\"\\\\n📊 6단계: 분석 결과 시각화...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. 시간대별 평균 전력 사용 패턴\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # 시간대별 패턴\n",
    "            hourly_avg = self.lp_data.groupby('시간')['순방향 유효전력'].mean()\n",
    "            axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2)\n",
    "            axes[0, 0].set_title('시간대별 평균 전력 사용량', fontsize=14, fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('시간')\n",
    "            axes[0, 0].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_xticks(range(0, 24, 3))\n",
    "            \n",
    "            # 요일별 패턴\n",
    "            daily_avg = self.lp_data.groupby('요일')['순방향 유효전력'].mean()\n",
    "            weekday_names = ['월', '화', '수', '목', '금', '토', '일']\n",
    "            axes[0, 1].bar(range(len(daily_avg)), daily_avg.values, color='skyblue')\n",
    "            axes[0, 1].set_title('요일별 평균 전력 사용량', fontsize=14, fontweight='bold')\n",
    "            axes[0, 1].set_xlabel('요일')\n",
    "            axes[0, 1].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[0, 1].set_xticks(range(7))\n",
    "            axes[0, 1].set_xticklabels(weekday_names)\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 변동계수 분포 (변동성 분석이 완료된 경우)\n",
    "            if 'volatility_analysis' in self.analysis_results:\n",
    "                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')\n",
    "                if os.path.exists(volatility_file):\n",
    "                    volatility_df = pd.read_csv(volatility_file)\n",
    "                    axes[1, 0].hist(volatility_df['cv_basic'].dropna(), bins=30, alpha=0.7, color='lightgreen')\n",
    "                    axes[1, 0].set_title('변동계수 분포', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 0].set_xlabel('변동계수 (CV)')\n",
    "                    axes[1, 0].set_ylabel('고객 수')\n",
    "                    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # 월별 계절성 패턴\n",
    "            monthly_avg = self.lp_data.groupby('월')['순방향 유효전력'].mean()\n",
    "            month_names = ['1월', '2월', '3월', '4월', '5월', '6월', \n",
    "                          '7월', '8월', '9월', '10월', '11월', '12월']\n",
    "            axes[1, 1].plot(monthly_avg.index, monthly_avg.values, marker='s', linewidth=2, color='orange')\n",
    "            axes[1, 1].set_title('월별 평균 전력 사용량 (계절성)', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('월')\n",
    "            axes[1, 1].set_ylabel('평균 유효전력 (kW)')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            axes[1, 1].set_xticks(range(1, 13))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # 이미지 저장\n",
    "            output_file = os.path.join(self.output_dir, 'temporal_patterns_summary.png')\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"   💾 시계열 패턴 시각화 저장: {output_file}\")\n",
    "            \n",
    "            # 2. 변동성 관련 시각화 (추가)\n",
    "            if 'volatility_analysis' in self.analysis_results:\n",
    "                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')\n",
    "                if os.path.exists(volatility_file):\n",
    "                    volatility_df = pd.read_csv(volatility_file)\n",
    "                    \n",
    "                    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "                    \n",
    "                    # 평균 사용량 vs 변동계수\n",
    "                    axes[0, 0].scatter(volatility_df['mean_power'], volatility_df['cv_basic'], alpha=0.6, s=20)\n",
    "                    axes[0, 0].set_title('평균 사용량 vs 변동계수', fontsize=14, fontweight='bold')\n",
    "                    axes[0, 0].set_xlabel('평균 전력 (kW)')\n",
    "                    axes[0, 0].set_ylabel('변동계수')\n",
    "                    axes[0, 0].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # 시간대별 변동성 vs 일별 변동성\n",
    "                    axes[0, 1].scatter(volatility_df['hourly_cv_mean'], volatility_df['daily_cv_mean'], alpha=0.6, s=20, color='red')\n",
    "                    axes[0, 1].set_title('시간대별 vs 일별 변동성', fontsize=14, fontweight='bold')\n",
    "                    axes[0, 1].set_xlabel('시간대별 평균 변동계수')\n",
    "                    axes[0, 1].set_ylabel('일별 평균 변동계수')\n",
    "                    axes[0, 1].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # 주말/평일 변동계수 비교\n",
    "                    weekend_weekday_ratio = volatility_df['weekend_weekday_cv_ratio'].dropna()\n",
    "                    axes[1, 0].hist(weekend_weekday_ratio, bins=20, alpha=0.7, color='purple')\n",
    "                    axes[1, 0].set_title('주말/평일 변동계수 비율 분포', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 0].set_xlabel('주말/평일 변동계수 비율')\n",
    "                    axes[1, 0].set_ylabel('고객 수')\n",
    "                    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    # 변동계수 상위/하위 분포\n",
    "                    cv_top10 = volatility_df.nlargest(10, 'cv_basic')['cv_basic']\n",
    "                    cv_bottom10 = volatility_df.nsmallest(10, 'cv_basic')['cv_basic']\n",
    "                    \n",
    "                    x_pos = range(10)\n",
    "                    width = 0.35\n",
    "                    axes[1, 1].bar([x - width/2 for x in x_pos], cv_top10.values, width, \n",
    "                                  label='상위 10명', alpha=0.8, color='red')\n",
    "                    axes[1, 1].bar([x + width/2 for x in x_pos], cv_bottom10.values, width, \n",
    "                                  label='하위 10명', alpha=0.8, color='blue')\n",
    "                    axes[1, 1].set_title('변동계수 상위/하위 10명 비교', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 1].set_xlabel('순위')\n",
    "                    axes[1, 1].set_ylabel('변동계수')\n",
    "                    axes[1, 1].legend()\n",
    "                    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # 이미지 저장\n",
    "                    output_file = os.path.join(self.output_dir, 'volatility_analysis_summary.png')\n",
    "                    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    print(f\"   💾 변동성 분석 시각화 저장: {output_file}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 시각화 생성 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"종합 분석 리포트 생성\"\"\"\n",
    "        print(\"\\\\n📋 7단계: 종합 분석 리포트 생성...\")\n",
    "        \n",
    "        report_file = os.path.join(self.output_dir, 'comprehensive_analysis_report.txt')\n",
    "        \n",
    "        try:\n",
    "            with open(report_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\\\\n\")\n",
    "                f.write(\"시계열 패턴 분석 및 변동성 지표 개발 결과 리포트\\\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\\\n\\\\n\")\n",
    "                \n",
    "                # 1. 분석 개요\n",
    "                f.write(\"1. 분석 개요\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(f\"분석 일시: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "                f.write(f\"고객 수: {self.analysis_results.get('customer_summary', {}).get('total_customers', 'N/A'):,}명\\\\n\")\n",
    "                f.write(f\"LP 레코드: {self.analysis_results.get('data_quality', {}).get('total_records', 'N/A'):,}개\\\\n\")\n",
    "                f.write(f\"분석 대상 고객: {self.analysis_results.get('data_quality', {}).get('customers', 'N/A')}명\\\\n\")\n",
    "                \n",
    "                date_range = self.analysis_results.get('data_quality', {}).get('date_range', {})\n",
    "                if date_range:\n",
    "                    f.write(f\"데이터 기간: {date_range.get('start', 'N/A')} ~ {date_range.get('end', 'N/A')}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 2. 시계열 패턴 분석 결과\n",
    "                f.write(\"2. 시계열 패턴 분석 결과\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                temporal = self.analysis_results.get('temporal_patterns', {})\n",
    "                if temporal:\n",
    "                    f.write(f\"피크 시간대: {temporal.get('peak_hours', [])}\\\\n\")\n",
    "                    f.write(f\"비피크 시간대: {temporal.get('off_peak_hours', [])}\\\\n\")\n",
    "                    f.write(f\"주말/평일 사용량 비율: {temporal.get('weekend_ratio', 0):.3f}\\\\n\")\n",
    "                    \n",
    "                    # 계절별 패턴\n",
    "                    seasonal = temporal.get('seasonal_patterns', {})\n",
    "                    if seasonal:\n",
    "                        f.write(\"\\\\n계절별 평균 사용량:\\\\n\")\n",
    "                        for season in ['봄', '여름', '가을', '겨울']:\n",
    "                            if season in seasonal and 'mean' in seasonal[season]:\n",
    "                                f.write(f\"  {season}: {seasonal[season]['mean']:.2f}kW\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 3. 변동성 지표 분석 결과\n",
    "                f.write(\"3. 변동성 지표 분석 결과\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                volatility = self.analysis_results.get('volatility_analysis', {})\n",
    "                if volatility:\n",
    "                    summary_stats = volatility.get('summary_stats', {})\n",
    "                    cv_stats = summary_stats.get('cv_basic', {})\n",
    "                    \n",
    "                    if cv_stats:\n",
    "                        f.write(\"기본 변동계수(CV) 통계:\\\\n\")\n",
    "                        f.write(f\"  평균: {cv_stats.get('mean', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  표준편차: {cv_stats.get('std', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  최솟값: {cv_stats.get('min', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  최댓값: {cv_stats.get('max', 0):.4f}\\\\n\")\n",
    "                        \n",
    "                    quartiles = volatility.get('quartiles', {})\n",
    "                    if quartiles:\n",
    "                        f.write(\"\\\\n변동계수 사분위수:\\\\n\")\n",
    "                        f.write(f\"  Q1 (25%): {quartiles.get(0.25, 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  Q2 (50%): {quartiles.get(0.5, 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  Q3 (75%): {quartiles.get(0.75, 0):.4f}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 4. 이상 패턴 탐지 결과\n",
    "                f.write(\"4. 이상 패턴 탐지 결과\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                anomaly = self.analysis_results.get('anomaly_analysis', {})\n",
    "                if anomaly:\n",
    "                    total_anomaly = anomaly.get('total_anomaly_customers', 0)\n",
    "                    anomaly_rate = anomaly.get('anomaly_rate', 0) * 100\n",
    "                    f.write(f\"이상 패턴 고객: {total_anomaly}명 ({anomaly_rate:.1f}%)\\\\n\")\n",
    "                    \n",
    "                    anomaly_types = anomaly.get('anomaly_types', {})\n",
    "                    f.write(\"\\\\n이상 패턴 유형별 분포:\\\\n\")\n",
    "                    for pattern_type, count in anomaly_types.items():\n",
    "                        f.write(f\"  {pattern_type}: {count}명\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 5. 변동계수 개발을 위한 인사이트\n",
    "                f.write(\"5. 변동계수 개발을 위한 핵심 인사이트\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"가. 시간대별 차별화 필요성:\\\\n\")\n",
    "                f.write(\"   - 피크/비피크 시간대별 가중치 적용\\\\n\")\n",
    "                f.write(\"   - 야간 시간대 이상 사용 패턴 별도 처리\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"나. 요일별 패턴 고려:\\\\n\")\n",
    "                f.write(\"   - 주말/평일 사용 패턴 차이 반영\\\\n\")\n",
    "                f.write(\"   - 요일별 변동성 가중치 조정\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"다. 계절성 보정:\\\\n\")\n",
    "                f.write(\"   - 월별/계절별 기준값 차별화\\\\n\")\n",
    "                f.write(\"   - 외부 기상 데이터 연계 고려\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"라. 다차원 변동성 지표:\\\\n\")\n",
    "                f.write(\"   - 기본 변동계수(CV) 외 추가 지표 활용\\\\n\")\n",
    "                f.write(\"   - 시간 윈도우별 변동성 조합\\\\n\")\n",
    "                f.write(\"   - 방향성 변동성 고려\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"마. 이상 패턴 필터링:\\\\n\")\n",
    "                f.write(\"   - 급격한 변화 및 장기간 0값 처리\\\\n\")\n",
    "                f.write(\"   - 통계적 이상치 제거 알고리즘\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 6. 다음 단계 권장사항\n",
    "                f.write(\"6. 다음 단계 권장사항\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"1. 업종별 변동계수 기준값 설정\\\\n\")\n",
    "                f.write(\"   - 계약종별/사용용도별 임계값 차별화\\\\n\")\n",
    "                f.write(\"   - 업종 특성 반영한 가중치 설계\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"2. 스태킹 알고리즘 개발\\\\n\")\n",
    "                f.write(\"   - Level-0: 개별 변동성 지표 모델\\\\n\")\n",
    "                f.write(\"   - Level-1: 메타모델을 통한 통합 변동계수\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"3. 외부 데이터 연계\\\\n\")\n",
    "                f.write(\"   - 기상청 기상 데이터 (온도, 습도 등)\\\\n\")\n",
    "                f.write(\"   - 경제 지표 및 업종별 운영 현황\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"4. 실시간 모니터링 시스템\\\\n\")\n",
    "                f.write(\"   - 변동계수 임계값 기반 알림 시스템\\\\n\")\n",
    "                f.write(\"   - 이상 패턴 자동 탐지 및 보고\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"5. 성능 검증 및 최적화\\\\n\")\n",
    "                f.write(\"   - 교차검증을 통한 모델 성능 평가\\\\n\")\n",
    "                f.write(\"   - 하이퍼파라미터 튜닝 및 최적화\\\\n\")\n",
    "                \n",
    "            print(f\"   💾 종합 리포트 저장: {report_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 리포트 생성 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def save_analysis_results(self):\n",
    "        \"\"\"분석 결과를 JSON 파일로 저장\"\"\"\n",
    "        print(\"\\\\n💾 8단계: 분석 결과 저장...\")\n",
    "        \n",
    "        try:\n",
    "            # JSON으로 저장 가능한 형태로 변환\n",
    "            results_for_json = {}\n",
    "            \n",
    "            for key, value in self.analysis_results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    results_for_json[key] = {}\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        if hasattr(sub_value, 'to_dict'):  # pandas 객체인 경우\n",
    "                            results_for_json[key][sub_key] = sub_value.to_dict()\n",
    "                        else:\n",
    "                            results_for_json[key][sub_key] = sub_value\n",
    "                else:\n",
    "                    results_for_json[key] = value\n",
    "            \n",
    "            # JSON 파일로 저장\n",
    "            output_file = os.path.join(self.output_dir, 'analysis_results.json')\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results_for_json, f, ensure_ascii=False, indent=2, default=str)\n",
    "            \n",
    "            print(f\"   💾 분석 결과 JSON 저장: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 분석 결과 저장 실패: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"전체 분석 프로세스 실행\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"🚀 한국전력공사 LP 데이터 시계열 패턴 분석 시작\")\n",
    "        print(f\"시작 시간: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # ⭐⭐⭐ 핵심 변경: 기존 코드 수정\n",
    "            # 기존: if not self.load_customer_data():\n",
    "            # 기존: if not self.load_lp_data():\n",
    "            \n",
    "            # 1. 전처리된 데이터 로딩 (새로 추가)\n",
    "            if not self.load_preprocessed_data():\n",
    "                print(\"❌ 전처리된 데이터 로딩 실패로 분석을 중단합니다.\")\n",
    "                return False\n",
    "            \n",
    "            # 2. 외부 데이터 로딩 (새로 추가)\n",
    "            self.load_external_data()\n",
    "            \n",
    "            # 3. 시계열 패턴 분석 (기존 유지)\n",
    "            if not self.analyze_temporal_patterns():\n",
    "                print(\"❌ 시계열 패턴 분석 실패\")\n",
    "                return False\n",
    "            \n",
    "            # 4. 변동성 지표 분석 (기존 유지)\n",
    "            cv_array = self.analyze_volatility_indicators()\n",
    "            if cv_array is None or len(cv_array) == 0:\n",
    "                print(\"❌ 변동성 지표 분석 실패\")\n",
    "                return False\n",
    "            \n",
    "            # 5. 이상 패턴 탐지 (기존 유지)\n",
    "            anomaly_summary = self.detect_anomalies()\n",
    "            if anomaly_summary is None:\n",
    "                print(\"❌ 이상 패턴 탐지 실패\")\n",
    "                return False\n",
    "            \n",
    "            # 6. 시각화 생성 (기존 유지)\n",
    "            self.create_summary_visualizations()\n",
    "            \n",
    "            # 7. 종합 리포트 생성 (기존 유지)\n",
    "            self.generate_comprehensive_report()\n",
    "            \n",
    "            # 8. 결과 저장 (기존 유지)\n",
    "            self.save_analysis_results()\n",
    "            \n",
    "            # 나머지 코드 그대로 유지\n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"🎉 시계열 패턴 분석 완료!\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"소요 시간: {duration}\")\n",
    "            print(f\"결과 저장 위치: {self.output_dir}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 분석 중 오류 발생: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"한국전력공사 전력 사용패턴 변동계수 개발 프로젝트\")\n",
    "    print(\"2단계: 시계열 패턴 분석 (최적화된 데이터 로딩)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer()\n",
    "    \n",
    "    # 전체 분석 실행\n",
    "    success = analyzer.run_complete_analysis()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎯 2단계 시계열 패턴 분석 완료!\")\n",
    "        print(\"   📁 생성 파일:\")\n",
    "        print(\"      - volatility_summary.csv (3단계 입력용)\")\n",
    "        print(\"      - analysis_results.json (메타데이터)\")\n",
    "        print(\"   ⏱️ 처리 시간: 30분 → 3-5분으로 단축!\")\n",
    "        print(\"   🚀 다음: 3단계 변동계수 알고리즘 실행\")\n",
    "    else:\n",
    "        print(\"\\n❌ 2단계 분석 실패\")\n",
    "        print(\"💡 해결방법: 1단계 코드를 먼저 실행하세요\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
