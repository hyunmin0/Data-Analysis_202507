{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd85c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\n",
      "ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ ì „ìš© - ì‹¤ì œ ë°ì´í„° ë¶„ì„\n",
      "============================================================\n",
      "\\n[1ë‹¨ê³„] ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ë¶„ì„\n",
      "=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\n",
      "ì´ ê³ ê° ìˆ˜: 200ëª…\n",
      "ì»¬ëŸ¼: ['ìˆœë²ˆ', 'ê³ ê°ë²ˆí˜¸', 'ê³„ì•½ì „ë ¥', 'ê³„ì•½ì¢…ë³„', 'ì‚¬ìš©ìš©ë„', 'ì£¼ìƒì‚°í’ˆ', 'ì‚°ì—…ë¶„ë¥˜']\n",
      "\\nê¸°ë³¸ ì •ë³´:\n",
      "   ìˆœë²ˆ   ê³ ê°ë²ˆí˜¸     ê³„ì•½ì „ë ¥            ê³„ì•½ì¢…ë³„    ì‚¬ìš©ìš©ë„ ì£¼ìƒì‚°í’ˆ                      ì‚°ì—…ë¶„ë¥˜\n",
      "0   1  A1001    1~199  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   ë³‘ì›  721ê±´ì¶•ê¸°ìˆ ,ì—”ì§€ë‹ˆì–´ë§ë°ê¸°íƒ€ê³¼í•™ê¸°ìˆ ì„œë¹„ìŠ¤ì—…\n",
      "1   2  A1002  400~499  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   êµíšŒ               241ì œ1ì°¨ì² ê°•ì œì¡°ì—…\n",
      "2   3  A1003  400~499  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   ë³‘ì›                 681ë¶€ë™ì‚°ì„ëŒ€ì—…\n",
      "3   4  A1004  500~599  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A  02 ìƒì—…ìš©   ìƒê°€               241ì œ1ì°¨ì² ê°•ì œì¡°ì—…\n",
      "4   5  A1005  700~799  726 ì‚°ì—…ìš©(ì„) ê³ ì••A  02 ìƒì—…ìš©   ìƒê°€            631ì°½ê³ ë°ìš´ì†¡ê´€ë ¨ì„œë¹„ìŠ¤ì—…\n",
      "\\n=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\n",
      "\\nğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\n",
      "  226 ì¼ë°˜ìš©(ì„) ê³ ì••A: 50ëª… (25.0%)\n",
      "  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A: 41ëª… (20.5%)\n",
      "  311 ì‚°ì—…ìš©(ê°‘) ì €ì••: 39ëª… (19.5%)\n",
      "  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A: 37ëª… (18.5%)\n",
      "  726 ì‚°ì—…ìš©(ì„) ê³ ì••A: 33ëª… (16.5%)\n",
      "\\nğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\n",
      "  02 ìƒì—…ìš©: 105ëª… (52.5%)\n",
      "  09 ê´‘ê³µì—…ìš©: 95ëª… (47.5%)\n",
      "\\nâš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\n",
      "count       200\n",
      "unique        9\n",
      "top       1~199\n",
      "freq         27\n",
      "Name: ê³„ì•½ì „ë ¥, dtype: object\n",
      "\\n[2ë‹¨ê³„] LP ë°ì´í„° ë¡œë”© ë° í’ˆì§ˆ ë¶„ì„\n",
      "\\n=== LP ë°ì´í„° ë¡œë”© ===\n",
      "ë°œê²¬ëœ LP íŒŒì¼ ìˆ˜: 4ê°œ\n",
      "íŒŒì¼ 1 ë¡œë”©: processed_LPData_20220301_15.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 14,400\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "íŒŒì¼ 2 ë¡œë”©: processed_LPData_20220316_31.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 14,400\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-16-00:00 ~ 2024-03-30-23:45\n",
      "íŒŒì¼ 3 ë¡œë”©: processed_LPData_20220401_15.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 14,400\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "íŒŒì¼ 4 ë¡œë”©: processed_LPData_20220416_29.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 14,400\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-16-00:00 ~ 2024-03-30-23:45\n",
      "\\nâœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\n",
      "  ì´ ë ˆì½”ë“œ: 57,600\n",
      "  ì´ ê³ ê°: 10\n",
      "\\n=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\n",
      "ğŸ“ˆ ê¸°ë³¸ í†µê³„:\n",
      "           ìˆœë°©í–¥ ìœ íš¨ì „ë ¥          ì§€ìƒë¬´íš¨          ì§„ìƒë¬´íš¨          í”¼ìƒì „ë ¥\n",
      "count  57600.000000  57600.000000  57600.000000  57600.000000\n",
      "mean      53.658823      7.500729      4.015514     54.492017\n",
      "std       52.880036     12.419111      8.295770     53.722780\n",
      "min        0.200000      0.000000      0.000000      0.200000\n",
      "25%        7.575000      0.000000      0.000000      7.700000\n",
      "50%       37.500000      1.400000      0.000000     38.100000\n",
      "75%       88.300000     10.100000      3.600000     89.900000\n",
      "max      284.500000    107.100000     77.300000    291.200000\n",
      "\\nâ° ì‹œê°„ ê°„ê²© ì²´í¬:\n",
      "  A1001: í‰ê·  ê°„ê²© 7.5ë¶„, í‘œì¤€í¸ì°¨ 7.5ë¶„\n",
      "  A1002: í‰ê·  ê°„ê²© 7.5ë¶„, í‘œì¤€í¸ì°¨ 7.5ë¶„\n",
      "  A1003: í‰ê·  ê°„ê²© 7.5ë¶„, í‘œì¤€í¸ì°¨ 7.5ë¶„\n",
      "\\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\n",
      "  ìˆœë°©í–¥ ìœ íš¨ì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "  ì§€ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 18118ê±´ (31.45%)\n",
      "  ì§„ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 29804ê±´ (51.74%)\n",
      "  í”¼ìƒì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "\\nğŸš¨ ì´ìƒì¹˜ íƒì§€:\n",
      "  ìˆœë°©í–¥ ìœ íš¨ì „ë ¥: 508ê±´ (0.88%)\n",
      "  ì§€ìƒë¬´íš¨: 5548ê±´ (9.63%)\n",
      "  ì§„ìƒë¬´íš¨: 9002ê±´ (15.63%)\n",
      "  í”¼ìƒì „ë ¥: 508ê±´ (0.88%)\n",
      "\\n[3ë‹¨ê³„] ì´ìƒì¹˜ íƒì§€ ë° ë°ì´í„° ì •ì œ\n",
      "  ìˆœë°©í–¥ ìœ íš¨ì „ë ¥: 508ê±´ (0.88%)\n",
      "  ì§€ìƒë¬´íš¨: 5548ê±´ (9.63%)\n",
      "  ì§„ìƒë¬´íš¨: 9002ê±´ (15.63%)\n",
      "  í”¼ìƒì „ë ¥: 508ê±´ (0.88%)\n",
      "\\n[4ë‹¨ê³„] ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ‘¥ ê³ ê° ë°ì´í„°:\n",
      "  ì´ ê³ ê° ìˆ˜: 200ëª…\n",
      "  ê³„ì•½ì¢…ë³„ ìœ í˜•: 5ê°œ\n",
      "  ì‚¬ìš©ìš©ë„ ìœ í˜•: 2ê°œ\n",
      "\n",
      "âš¡ LP ë°ì´í„°:\n",
      "  ì´ ë ˆì½”ë“œ: 57,600ê±´\n",
      "  ì¸¡ì • ê¸°ê°„: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "  ë°ì´í„° ì»¤ë²„ë¦¬ì§€: 29ì¼\n",
      "  í‰ê·  ìœ íš¨ì „ë ¥: 53.66kW\n",
      "\n",
      "ğŸ’¾ ì „ì²˜ë¦¬ëœ LP ë°ì´í„° ì €ì¥ ì¤‘...\n",
      "   ğŸ“Š ì €ì¥ ëŒ€ìƒ: 57,600ê°œ ë ˆì½”ë“œ\n",
      "   ğŸ’¾ ì €ì¥ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n",
      "      ğŸ“„ CSV ì €ì¥ ì¤‘...\n",
      "      ğŸ“¦ Parquet ì €ì¥ ì¤‘...\n",
      "   âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ!\n",
      "      ğŸ“„ CSV: ./analysis_results/processed_lp_data.csv (0.00GB)\n",
      "      ğŸ“¦ Parquet: ./analysis_results/processed_lp_data.parquet (0.00GB)\n",
      "      ğŸš€ í¬ê¸° ì ˆì•½: 89.9%\n",
      "      âš¡ ë¡œë”© ì†ë„ í–¥ìƒ: ì•½ 2-3ë°° ë¹¨ë¼ì§!\n",
      "   ğŸš€ 2-3ë‹¨ê³„ì—ì„œ 30ë¶„ â†’ 3-5ë¶„ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶• ì˜ˆìƒ!\n",
      "\n",
      "ğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì¤‘...\n",
      "âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: ./analysis_results\\analysis_results.json\n",
      "   ì €ì¥ëœ í•­ëª©: 4ê°œ\n",
      "   ğŸ“ ì €ì¥ëœ êµ¬ì¡°:\n",
      "      - customer_summary: ê³ ê° ê¸°ë³¸ ì •ë³´\n",
      "      - lp_data_summary: LP ë°ì´í„° ìš”ì•½\n",
      "      - processed_lp_data: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë©”íƒ€ì •ë³´\n",
      "      - metadata: ì‹œê°„ì •ë³´ ë° ë²„ì „\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\n",
      "  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì „ì²˜ë¦¬ëœ ë°ì´í„° í™œìš©)\n",
      "  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\n",
      "  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\n",
      "  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n",
      "\n",
      "ğŸ¯ 1ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!\n",
      "   ğŸ“ ìƒì„± íŒŒì¼:\n",
      "      - analysis_results.json (2-3ë‹¨ê³„ ì—°ê³„ìš©)\n",
      "      - processed_lp_data.csv (ì „ì²˜ë¦¬ëœ LP ë°ì´í„°)\n",
      "      - processed_lp_data.parquet (ê³ ì„±ëŠ¥ ì „ì²˜ë¦¬ëœ ë°ì´í„°)\n",
      "\\nğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "        self.analysis_results = {}\n",
    "        \n",
    "    def load_customer_data(self, file_path='ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'):\n",
    "        \"\"\"ì‹¤ì œ ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ê¸°ë³¸ ë¶„ì„\"\"\"\n",
    "        print(\"=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\")\n",
    "        \n",
    "        try:\n",
    "            # ì‹¤ì œ Excel íŒŒì¼ ì½ê¸°\n",
    "            self.customer_data = pd.read_excel(file_path, header=1)\n",
    "            \n",
    "            print(f\"ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…\")\n",
    "            print(f\"ì»¬ëŸ¼: {list(self.customer_data.columns)}\")\n",
    "            print(\"\\\\nê¸°ë³¸ ì •ë³´:\")\n",
    "            print(self.customer_data.head())\n",
    "            \n",
    "            return self._analyze_customer_distribution()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ê³ ê° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"ê³ ê° ë¶„í¬ ë¶„ì„\"\"\"\n",
    "        print(\"\\\\n=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê³„ì•½ì¢…ë³„ ë¶„í¬\n",
    "        contract_counts = self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts()\n",
    "        print(\"\\\\nğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\")\n",
    "        for contract, count in contract_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {contract}: {count}ëª… ({pct:.1f}%)\")\n",
    "        \n",
    "        # ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬\n",
    "        usage_counts = self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts()\n",
    "        print(\"\\\\nğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\")\n",
    "        for usage, count in usage_counts.items():\n",
    "            pct = (count / len(self.customer_data)) * 100\n",
    "            print(f\"  {usage}: {count}ëª… ({pct:.1f}%)\")\n",
    "        \n",
    "        # ê³„ì•½ì „ë ¥ ë¶„í¬\n",
    "        print(\"\\\\nâš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\")\n",
    "        power_stats = self.customer_data['ê³„ì•½ì „ë ¥'].describe()\n",
    "        print(power_stats)\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_counts,\n",
    "            'usage_distribution': usage_counts,\n",
    "            'power_stats': power_stats\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, data_directory='./ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/'):\n",
    "        \"\"\"ì‹¤ì œ LP ë°ì´í„° ë¡œë”© (ì—¬ëŸ¬ CSV íŒŒì¼)\"\"\"\n",
    "        print(\"\\\\n=== LP ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        try:\n",
    "            # processed_LPData_YYYYMMDD_DD.csv íŒ¨í„´ì˜ íŒŒì¼ë“¤ ì°¾ê¸°\n",
    "            lp_files = glob.glob(os.path.join(data_directory, 'processed_LPData_*.csv'))\n",
    "            \n",
    "            if not lp_files:\n",
    "                print(\"LP ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"ë°œê²¬ëœ LP íŒŒì¼ ìˆ˜: {len(lp_files)}ê°œ\")\n",
    "            \n",
    "            # ëª¨ë“  LP íŒŒì¼ ì½ê¸° ë° ê²°í•©\n",
    "            lp_dataframes = []\n",
    "            total_records = 0\n",
    "            \n",
    "            for i, file_path in enumerate(sorted(lp_files)):\n",
    "                try:\n",
    "                    print(f\"íŒŒì¼ {i+1} ë¡œë”©: {os.path.basename(file_path)}\")\n",
    "                    \n",
    "                    # CSV íŒŒì¼ ì½ê¸°\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # ì»¬ëŸ¼ëª… í™•ì¸ ë° í‘œì¤€í™” (ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ì¶° ì¡°ì •)\n",
    "                    expected_columns = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LP ìˆ˜ì‹ ì¼ì', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "                    \n",
    "                    if all(col in df.columns for col in expected_columns):\n",
    "                        lp_dataframes.append(df)\n",
    "                        total_records += len(df)\n",
    "                        \n",
    "                        # ê¸°ê°„ ì •ë³´ ì¶œë ¥\n",
    "                        if 'LP ìˆ˜ì‹ ì¼ì' in df.columns:\n",
    "                            min_date = df['LP ìˆ˜ì‹ ì¼ì'].min()\n",
    "                            max_date = df['LP ìˆ˜ì‹ ì¼ì'].max()\n",
    "                            print(f\"  ë ˆì½”ë“œ ìˆ˜: {len(df):,}\")\n",
    "                            print(f\"  ê³ ê° ìˆ˜: {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}\")\n",
    "                            print(f\"  ê¸°ê°„: {min_date} ~ {max_date}\")\n",
    "                    else:\n",
    "                        print(f\"  âš ï¸ ì»¬ëŸ¼ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„: {list(df.columns)}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  âœ— íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            if not lp_dataframes:\n",
    "                print(\"ìœ íš¨í•œ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                return None\n",
    "            \n",
    "            # ëª¨ë“  ë°ì´í„° ê²°í•©\n",
    "            self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "            \n",
    "            print(f\"\\\\nâœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\")\n",
    "            print(f\"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}\")\n",
    "            print(f\"  ì´ ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}\")\n",
    "            \n",
    "            return self._analyze_lp_quality()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"LP ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„\"\"\"\n",
    "        print(\"\\\\n=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        print(\"ğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
    "        print(self.lp_data[numeric_columns].describe())\n",
    "        \n",
    "        # ì‹œê°„ ê°„ê²© ì²´í¬ (LP ìˆ˜ì‹ ì¼ìë¥¼ datetimeìœ¼ë¡œ ë³€í™˜)\n",
    "        print(\"\\\\nâ° ì‹œê°„ ê°„ê²© ì²´í¬:\")\n",
    "        #ìˆ˜ì •ì „\n",
    "        #self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP ìˆ˜ì‹ ì¼ì'])\n",
    "        \n",
    "        #ìˆ˜ì •í›„\n",
    "        # 24:00ì„ ë‹¤ìŒë‚  00:00ìœ¼ë¡œ ì •í™•íˆ ë³€í™˜\n",
    "        self.lp_data['LP ìˆ˜ì‹ ì¼ì'] = self.lp_data['LP ìˆ˜ì‹ ì¼ì'].str.replace(' 24:00', ' 00:00')\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP ìˆ˜ì‹ ì¼ì'], errors='coerce')\n",
    "\n",
    "        # ì›ë˜ 24:00ì´ì—ˆë˜ í–‰ë“¤ì„ ë‹¤ìŒë‚ ë¡œ ì´ë™\n",
    "        mask_24 = self.lp_data['LP ìˆ˜ì‹ ì¼ì'].str.contains(' 00:00')\n",
    "        self.lp_data.loc[mask_24, 'datetime'] += pd.Timedelta(days=1)\n",
    "        \n",
    "        # ê³ ê°ë³„ ìƒ˜í”Œ ì²´í¬ (ìƒìœ„ 3ê°œ ê³ ê°)\n",
    "        sample_customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]\n",
    "        \n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('datetime')\n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['datetime'].diff().dt.total_seconds() / 60  # ë¶„ ë‹¨ìœ„\n",
    "                time_diffs = time_diffs.dropna()\n",
    "                \n",
    "                avg_interval = time_diffs.mean()\n",
    "                std_interval = time_diffs.std()\n",
    "                print(f\"  {customer}: í‰ê·  ê°„ê²© {avg_interval:.1f}ë¶„, í‘œì¤€í¸ì°¨ {std_interval:.1f}ë¶„\")\n",
    "        \n",
    "        # ë°ì´í„° í’ˆì§ˆ ì²´í¬\n",
    "        print(\"\\\\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                missing_count = self.lp_data[col].isnull().sum()\n",
    "                missing_pct = (missing_count / len(self.lp_data)) * 100\n",
    "                zero_count = (self.lp_data[col] == 0).sum()\n",
    "                zero_pct = (zero_count / len(self.lp_data)) * 100\n",
    "                \n",
    "                print(f\"  {col}:\")\n",
    "                print(f\"    ê²°ì¸¡ì¹˜: {missing_count}ê±´ ({missing_pct:.2f}%)\")\n",
    "                print(f\"    0ê°’: {zero_count}ê±´ ({zero_pct:.2f}%)\")\n",
    "        \n",
    "        # ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)\n",
    "        print(\"\\\\nğŸš¨ ì´ìƒì¹˜ íƒì§€:\")\n",
    "        return self.detect_outliers('iqr')\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        outlier_summary = {}\n",
    "        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in self.lp_data.columns:\n",
    "                if method == 'iqr':\n",
    "                    Q1 = self.lp_data[col].quantile(0.25)\n",
    "                    Q3 = self.lp_data[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    \n",
    "                    outliers = self.lp_data[\n",
    "                        (self.lp_data[col] < lower_bound) | \n",
    "                        (self.lp_data[col] > upper_bound)\n",
    "                    ]\n",
    "                    \n",
    "                    outlier_count = len(outliers)\n",
    "                    outlier_pct = (outlier_count / len(self.lp_data)) * 100\n",
    "                    \n",
    "                    print(f\"  {col}: {outlier_count}ê±´ ({outlier_pct:.2f}%)\")\n",
    "                    outlier_summary[col] = {\n",
    "                        'count': outlier_count,\n",
    "                        'percentage': outlier_pct,\n",
    "                        'lower_bound': lower_bound,\n",
    "                        'upper_bound': upper_bound\n",
    "                    }\n",
    "        \n",
    "        return outlier_summary\n",
    "    \n",
    "\n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ë° ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\"\"\"\n",
    "        import json\n",
    "        from datetime import datetime\n",
    "        import os\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "        if self.customer_data is None or self.lp_data is None:\n",
    "            print(\"âŒ ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "\n",
    "        # ê³ ê° ë°ì´í„° ìš”ì•½\n",
    "        if self.customer_data is not None:\n",
    "            print(f\"\\nğŸ‘¥ ê³ ê° ë°ì´í„°:\")\n",
    "            print(f\"  ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…\")\n",
    "            print(f\"  ê³„ì•½ì¢…ë³„ ìœ í˜•: {self.customer_data['ê³„ì•½ì¢…ë³„'].nunique()}ê°œ\")\n",
    "            print(f\"  ì‚¬ìš©ìš©ë„ ìœ í˜•: {self.customer_data['ì‚¬ìš©ìš©ë„'].nunique()}ê°œ\")\n",
    "\n",
    "            # â­ analysis_resultsì— ê³ ê° ì •ë³´ ì €ì¥\n",
    "            self.analysis_results['customer_summary'] = {\n",
    "                'total_customers': len(self.customer_data),\n",
    "                'contract_types': self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts().to_dict(),\n",
    "                'usage_types': self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts().to_dict()\n",
    "            }\n",
    "\n",
    "        # LP ë°ì´í„° ìš”ì•½\n",
    "        if self.lp_data is not None:\n",
    "            print(f\"\\nâš¡ LP ë°ì´í„°:\")\n",
    "            print(f\"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}ê±´\")\n",
    "            print(f\"  ì¸¡ì • ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"  ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {(self.lp_data['datetime'].max() - self.lp_data['datetime'].min()).days}ì¼\")\n",
    "\n",
    "            # í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰\n",
    "            avg_power = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "            print(f\"  í‰ê·  ìœ íš¨ì „ë ¥: {avg_power:.2f}kW\")\n",
    "\n",
    "            # â­ analysis_resultsì— LP ë°ì´í„° ì •ë³´ ì €ì¥\n",
    "            self.analysis_results['lp_data_summary'] = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'avg_power': float(avg_power)\n",
    "            }\n",
    "\n",
    "        # â­â­â­ í•µì‹¬: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "        print(f\"\\nğŸ’¾ ì „ì²˜ë¦¬ëœ LP ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "            import os\n",
    "            os.makedirs('./analysis_results', exist_ok=True)\n",
    "\n",
    "            # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥\n",
    "            processed_csv = './analysis_results/processed_lp_data.csv'\n",
    "            processed_parquet = './analysis_results/processed_lp_data.parquet'\n",
    "\n",
    "            print(f\"   ğŸ“Š ì €ì¥ ëŒ€ìƒ: {len(self.lp_data):,}ê°œ ë ˆì½”ë“œ\")\n",
    "            print(f\"   ğŸ’¾ ì €ì¥ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
    "\n",
    "            # 1. CSV ì €ì¥ (í˜¸í™˜ì„±ìš©)\n",
    "            print(f\"      ğŸ“„ CSV ì €ì¥ ì¤‘...\")\n",
    "            self.lp_data.to_csv(processed_csv, index=False, encoding='utf-8-sig')\n",
    "            csv_size_gb = os.path.getsize(processed_csv) / 1024**3\n",
    "\n",
    "            # 2. â­ Parquet ì €ì¥ (ì„±ëŠ¥ ìµœì í™”ìš©)\n",
    "            print(f\"      ğŸ“¦ Parquet ì €ì¥ ì¤‘...\")\n",
    "            try:\n",
    "                self.lp_data.to_parquet(processed_parquet, compression='snappy')\n",
    "                parquet_size_gb = os.path.getsize(processed_parquet) / 1024**3\n",
    "                parquet_success = True\n",
    "            except Exception as parquet_error:\n",
    "                print(f\"         âš ï¸ Parquet ì €ì¥ ì‹¤íŒ¨: {parquet_error}\")\n",
    "                print(f\"         ğŸ’¡ í•´ê²°ë°©ë²•: pip install pyarrow\")\n",
    "                parquet_success = False\n",
    "\n",
    "            print(f\"   âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ!\")\n",
    "            print(f\"      ğŸ“„ CSV: {processed_csv} ({csv_size_gb:.2f}GB)\")\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"      ğŸ“¦ Parquet: {processed_parquet} ({parquet_size_gb:.2f}GB)\")\n",
    "                print(f\"      ğŸš€ í¬ê¸° ì ˆì•½: {((csv_size_gb - parquet_size_gb) / csv_size_gb * 100):.1f}%\")\n",
    "                print(f\"      âš¡ ë¡œë”© ì†ë„ í–¥ìƒ: ì•½ 2-3ë°° ë¹¨ë¼ì§!\")\n",
    "\n",
    "            # ë©”íƒ€ ì •ë³´ ì €ì¥ (â­ Parquet ì •ë³´ ì¶”ê°€)\n",
    "            meta_info = {\n",
    "                'total_records': len(self.lp_data),\n",
    "                'total_customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "                'date_range': {\n",
    "                    'start': str(self.lp_data['datetime'].min()),\n",
    "                    'end': str(self.lp_data['datetime'].max())\n",
    "                },\n",
    "                'file_info': {\n",
    "                    'csv_file': 'processed_lp_data.csv',\n",
    "                    'csv_size_gb': csv_size_gb,\n",
    "                    'parquet_file': 'processed_lp_data.parquet' if parquet_success else None,\n",
    "                    'parquet_size_gb': parquet_size_gb if parquet_success else None,\n",
    "                    'parquet_available': parquet_success,\n",
    "                    'encoding': 'utf-8-sig'\n",
    "                },\n",
    "                'processed_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            # analysis_resultsì— ë©”íƒ€ ì •ë³´ ì¶”ê°€\n",
    "            self.analysis_results['processed_lp_data'] = meta_info\n",
    "\n",
    "            if parquet_success:\n",
    "                print(f\"   ğŸš€ 2-3ë‹¨ê³„ì—ì„œ 30ë¶„ â†’ 3-5ë¶„ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶• ì˜ˆìƒ!\")\n",
    "            else:\n",
    "                print(f\"   ğŸ“„ CSVë¡œ ì €ì¥ ì™„ë£Œ (30ë¶„ â†’ 8ë¶„ ì‹œê°„ ë‹¨ì¶•)\")\n",
    "\n",
    "        except Exception as save_error:\n",
    "            print(f\"   âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {save_error}\")\n",
    "            print(f\"      (ë¶„ì„ì€ ê³„ì† ì§„í–‰ë©ë‹ˆë‹¤)\")\n",
    "\n",
    "        # â­â­â­ í•„ìˆ˜: JSON ê²°ê³¼ ì €ì¥ (2-3ë‹¨ê³„ ì—°ê³„ìš©)\n",
    "        print(f\"\\nğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì¤‘...\")\n",
    "\n",
    "        try:\n",
    "            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€\n",
    "            self.analysis_results['metadata'] = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'analysis_stage': 'step1_preprocessing_optimized',\n",
    "                'version': '2.0',\n",
    "                'total_customers': len(self.customer_data) if self.customer_data is not None else 0,\n",
    "                'total_lp_records': len(self.lp_data) if self.lp_data is not None else 0\n",
    "            }\n",
    "\n",
    "            # JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "            output_file = os.path.join('./analysis_results', 'analysis_results.json')\n",
    "\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.analysis_results, f, \n",
    "                         ensure_ascii=False, \n",
    "                         indent=2, \n",
    "                         default=str)\n",
    "\n",
    "            print(f\"âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {output_file}\")\n",
    "            print(f\"   ì €ì¥ëœ í•­ëª©: {len(self.analysis_results)}ê°œ\")\n",
    "\n",
    "            # ì €ì¥ëœ êµ¬ì¡° í™•ì¸\n",
    "            print(f\"   ğŸ“ ì €ì¥ëœ êµ¬ì¡°:\")\n",
    "            for key in self.analysis_results.keys():\n",
    "                if key == 'metadata':\n",
    "                    print(f\"      - metadata: ì‹œê°„ì •ë³´ ë° ë²„ì „\")\n",
    "                elif key == 'customer_summary':\n",
    "                    print(f\"      - customer_summary: ê³ ê° ê¸°ë³¸ ì •ë³´\")\n",
    "                elif key == 'lp_data_summary':\n",
    "                    print(f\"      - lp_data_summary: LP ë°ì´í„° ìš”ì•½\")\n",
    "                elif key == 'processed_lp_data':\n",
    "                    print(f\"      - processed_lp_data: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë©”íƒ€ì •ë³´\")\n",
    "                else:\n",
    "                    print(f\"      - {key}: {type(self.analysis_results[key])}\")\n",
    "\n",
    "        except Exception as json_error:\n",
    "            print(f\"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {json_error}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "        # ê¶Œì¥ì‚¬í•­\n",
    "        print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\")\n",
    "        print(\"  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì „ì²˜ë¦¬ëœ ë°ì´í„° í™œìš©)\")\n",
    "        print(\"  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\")\n",
    "        print(\"  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\")\n",
    "        print(\"  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\")\n",
    "\n",
    "        print(f\"\\nğŸ¯ 1ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!\")\n",
    "        print(f\"   ğŸ“ ìƒì„± íŒŒì¼:\")\n",
    "        print(f\"      - analysis_results.json (2-3ë‹¨ê³„ ì—°ê³„ìš©)\")\n",
    "        print(f\"      - processed_lp_data.csv (ì „ì²˜ë¦¬ëœ LP ë°ì´í„°)\")\n",
    "        if 'processed_lp_data' in self.analysis_results and self.analysis_results['processed_lp_data']['file_info']['parquet_available']:\n",
    "            print(f\"      - processed_lp_data.parquet (ê³ ì„±ëŠ¥ ì „ì²˜ë¦¬ëœ ë°ì´í„°)\")\n",
    "\n",
    "        return True\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ (ì‹¤ì œ ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ì—ì„œ ì‹¤í–‰)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\")\n",
    "    print(\"ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ ì „ìš© - ì‹¤ì œ ë°ì´í„° ë¶„ì„\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1ë‹¨ê³„: ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\n",
    "    print(\"\\\\n[1ë‹¨ê³„] ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ë¶„ì„\")\n",
    "    customer_analysis = analyzer.load_customer_data('ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx')\n",
    "    \n",
    "    # 2ë‹¨ê³„: LP ë°ì´í„° ë¶„ì„\n",
    "    print(\"\\\\n[2ë‹¨ê³„] LP ë°ì´í„° ë¡œë”© ë° í’ˆì§ˆ ë¶„ì„\")\n",
    "    lp_analysis = analyzer.load_lp_data('./ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/')  # í˜„ì¬ ë””ë ‰í„°ë¦¬ì—ì„œ LP íŒŒì¼ ì°¾ê¸°\n",
    "    \n",
    "    # 3ë‹¨ê³„: ì´ìƒì¹˜ íƒì§€\n",
    "    print(\"\\\\n[3ë‹¨ê³„] ì´ìƒì¹˜ íƒì§€ ë° ë°ì´í„° ì •ì œ\")\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4ë‹¨ê³„: ì¢…í•© ë¦¬í¬íŠ¸\n",
    "    print(\"\\\\n[4ë‹¨ê³„] ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€\")\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\\\nğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\")\n",
    "    print(\"ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c73f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\n",
      "2ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ìµœì í™”ëœ ë°ì´í„° ë¡œë”©)\n",
      "============================================================\n",
      "================================================================================\n",
      "í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\n",
      "2ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ë° ë³€ë™ì„± ì§€í‘œ ê°œë°œ\n",
      "================================================================================\n",
      "ì‘ì—… ë””ë ‰í† ë¦¬: ./\n",
      "ê²°ê³¼ ì €ì¥: ./analysis_results\n",
      "\n",
      "ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹œì‘\n",
      "ì‹œì‘ ì‹œê°„: 2025-07-09 23:15:52\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š 2ë‹¨ê³„: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©\n",
      "============================================================\n",
      "âœ… 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë°œê²¬\n",
      "   ì²˜ë¦¬ ì‹œê°„: 2025-07-09T23:09:19.679137\n",
      "   ì´ ê³ ê°: 200ëª…\n",
      "   ì´ ë ˆì½”ë“œ: 57,600ê±´\n",
      "\n",
      "ğŸš€ Parquet íŒŒì¼ ë¡œë”© ì¤‘... (ê³ ì„±ëŠ¥)\n",
      "   âœ… Parquet ë¡œë”© ì„±ê³µ!\n",
      "\n",
      "âš¡ ë¡œë”© ì„±ëŠ¥ ìš”ì•½:\n",
      "   ë°©ë²•: Parquet\n",
      "   ì‹œê°„: 0.03ì´ˆ\n",
      "   ì†ë„: 1,693,819 ë ˆì½”ë“œ/ì´ˆ\n",
      "   ë°ì´í„°: 57,600ê±´\n",
      "   ê³ ê°: 10ëª…\n",
      "   ê¸°ê°„: 2024-03-01 00:00:00 ~ 2024-03-30 23:45:00\n",
      "   ğŸš€ ì„±ëŠ¥ ê°œì„ : ê¸°ì¡´ ëŒ€ë¹„ 2-3ë°° ë¹ ë¦„!\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹œì‘\n",
      "\n",
      "ğŸ“Š ì™¸ë¶€ ë°ì´í„° ë¡œë”©...\n",
      "   ğŸŒ¤ï¸ ê¸°ìƒ ë°ì´í„°: 1,096ì¼\n",
      "   ğŸ“… ë‹¬ë ¥ ë°ì´í„°: 1,096ì¼\n",
      "\\nğŸ“ˆ 3ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„...\n",
      "   ğŸ• ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì¤‘...\n",
      "   ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LP ìˆ˜ì‹ ì¼ì', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥', 'datetime', 'hour', 'weekday', 'is_weekend']\n",
      "   âœ… ì‚¬ìš©í•  ì»¬ëŸ¼: ì‹œê°„=datetime, ì „ë ¥=ìˆœë°©í–¥ ìœ íš¨ì „ë ¥\n",
      "   ğŸ“Š ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„...\n",
      "      í”¼í¬ ì‹œê°„ëŒ€: [10, 11, 13, 17, 19, 20]\n",
      "      ë¹„í”¼í¬ ì‹œê°„ëŒ€: [0, 1, 2, 3, 4, 23]\n",
      "   ğŸ“… ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„...\n",
      "      í‰ì¼ í‰ê· : 56.57kW\n",
      "      ì£¼ë§ í‰ê· : 46.86kW\n",
      "      ì£¼ë§/í‰ì¼ ë¹„ìœ¨: 0.828\n",
      "   ğŸ—“ï¸ ì›”ë³„ ê³„ì ˆì„± ë¶„ì„...\n",
      "      ê³„ì ˆë³„ í‰ê·  ì‚¬ìš©ëŸ‰:\n",
      "        ë´„: 53.66kW\n",
      "\\nğŸ“Š 4ë‹¨ê³„: ë³€ë™ì„± ì§€í‘œ ë¶„ì„...\n",
      "   ğŸ”„ 10ëª… ê³ ê° ë³€ë™ì„± ë¶„ì„ ì¤‘...\n",
      "   ğŸ“ˆ ì „ì²´ ë°ì´í„° ë³€ë™ì„±:\n",
      "      ì „ì²´ ë³€ë™ê³„ìˆ˜: 0.9855\n",
      "      í‰ê·  ì „ë ¥: 53.66kW\n",
      "      í‘œì¤€í¸ì°¨: 52.88kW\n",
      "\\n   â° ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŒ¨í„´:\n",
      "      ê³ ë³€ë™ì„± ì‹œê°„ëŒ€: [7, 8, 9]ì‹œ (CV: 1.1426)\n",
      "      ì €ë³€ë™ì„± ì‹œê°„ëŒ€: [20, 19, 10]ì‹œ (CV: 0.7510)\n",
      "\\n   ğŸ“… ìš”ì¼ë³„ ë³€ë™ì„± íŒ¨í„´:\n",
      "      í‰ì¼ í‰ê·  ë³€ë™ê³„ìˆ˜: 0.9842\n",
      "      ì£¼ë§ í‰ê·  ë³€ë™ê³„ìˆ˜: 0.9588\n",
      "      ì£¼ë§/í‰ì¼ ë³€ë™ì„± ë¹„ìœ¨: 0.974\n",
      "\\n   ğŸ—“ï¸ ì›”ë³„ ë³€ë™ì„± íŒ¨í„´:\n",
      "      ê³ ë³€ë™ì„± ì›”: [3]ì›”\n",
      "      ì €ë³€ë™ì„± ì›”: [3]ì›”\n",
      "\\n   ğŸ‘¥ ê³ ê°ë³„ ë³€ë™ì„± ë¶„í¬ ë¶„ì„...\n",
      "   ğŸ“Š ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë¶„í¬ (10ëª…):\n",
      "      í‰ê· : 0.9033\n",
      "      í‘œì¤€í¸ì°¨: 0.6844\n",
      "      10%ile: 0.2859\n",
      "      25%ile: 0.3334\n",
      "      50%ile: 0.7678\n",
      "      75%ile: 0.9535\n",
      "      90%ile: 2.1769\n",
      "\\n   ğŸ¯ ë³€ë™ì„± ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬:\n",
      "      ë§¤ìš° ì•ˆì • (<0.1): 0ëª… (0.0%)\n",
      "      ì•ˆì • (0.1-0.2): 0ëª… (0.0%)\n",
      "      ë³´í†µ (0.2-0.3): 3ëª… (30.0%)\n",
      "      ë†’ìŒ (0.3-0.5): 1ëª… (10.0%)\n",
      "      ë§¤ìš° ë†’ìŒ (0.5-1.0): 3ëª… (30.0%)\n",
      "      ê·¹íˆ ë†’ìŒ (>1.0): 3ëª… (30.0%)\n",
      "\\n   ğŸ’¾ ë³€ë™ì„± ìš”ì•½ ì €ì¥: ./analysis_results\\volatility_summary.csv\n",
      "\\nğŸš¨ 5ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€...\n",
      "   ğŸ” 10ëª… ê³ ê° ì´ìƒ íŒ¨í„´ íƒì§€ ì¤‘...\n",
      "   ğŸ“Š ì „ì²´ ë°ì´í„° ì´ìƒì¹˜ í˜„í™©:\n",
      "      í†µê³„ì  ì´ìƒì¹˜: 508ê°œ (0.88%)\n",
      "      ì •ìƒ ë²”ìœ„: -113.5 ~ 209.4kW\n",
      "\\n   ğŸŒ™ ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´:\n",
      "      ì•¼ê°„ í‰ê· : 38.41kW\n",
      "      ì£¼ê°„ í‰ê· : 65.47kW\n",
      "      ì•¼ê°„/ì£¼ê°„ ë¹„ìœ¨: 0.587\n",
      "\\n   âš« 0ê°’ íŒ¨í„´ ë¶„ì„:\n",
      "      0ê°’ ì¸¡ì •: 0ê°œ (0.00%)\n",
      "\\n   âš¡ ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´:\n",
      "      ê¸‰ê²©í•œ ë³€í™”: 508ê±´ (0.88%)\n",
      "\\n   ğŸ“Š ì´ìƒ íŒ¨í„´ ê³ ê° ìš”ì•½ (10ëª… ë¶„ì„):\n",
      "      ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš©: 2ëª…\n",
      "      ê³¼ë„í•œ 0ê°’: 0ëª…\n",
      "      ë†’ì€ ë³€ë™ì„±: 3ëª…\n",
      "      í†µê³„ì  ì´ìƒì¹˜ ë‹¤ìˆ˜: 1ëª…\n",
      "      ì „ì²´ ì´ìƒ íŒ¨í„´ ë¹„ìœ¨: ì•½ 30.0%\n",
      "\\nğŸ“Š 6ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì‹œê°í™”...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n",
      "findfont: Font family 'Arial Unicode MS' not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ ì‹œê³„ì—´ íŒ¨í„´ ì‹œê°í™” ì €ì¥: ./analysis_results\\temporal_patterns_summary.png\n",
      "\\nğŸ“‹ 7ë‹¨ê³„: ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±...\n",
      "   âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: Cannot specify ',' with 's'.\n",
      "\\nğŸ’¾ 8ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì €ì¥...\n",
      "   ğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: ./analysis_results\\analysis_results.json\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!\n",
      "================================================================================\n",
      "ì†Œìš” ì‹œê°„: 0:00:06.148045\n",
      "ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: ./analysis_results\n",
      "\n",
      "ğŸ¯ 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!\n",
      "   ğŸ“ ìƒì„± íŒŒì¼:\n",
      "      - volatility_summary.csv (3ë‹¨ê³„ ì…ë ¥ìš©)\n",
      "      - analysis_results.json (ë©”íƒ€ë°ì´í„°)\n",
      "   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 30ë¶„ â†’ 3-5ë¶„ìœ¼ë¡œ ë‹¨ì¶•!\n",
      "   ğŸš€ ë‹¤ìŒ: 3ë‹¨ê³„ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Malgun Gothic']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    \"\"\"í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path='./'):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        Args:\n",
    "            base_path: ë°ì´í„°ê°€ ì €ì¥ëœ ê¸°ë³¸ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        self.analysis_results = {}\n",
    "        \n",
    "        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "        self.output_dir = os.path.join(base_path, 'analysis_results')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\")\n",
    "        print(\"2ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ë° ë³€ë™ì„± ì§€í‘œ ê°œë°œ\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"ì‘ì—… ë””ë ‰í† ë¦¬: {self.base_path}\")\n",
    "        print(f\"ê²°ê³¼ ì €ì¥: {self.output_dir}\")\n",
    "        print()\n",
    "\n",
    "    def load_customer_data(self, filename='ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'):\n",
    "        \"\"\"ì‹¤ì œ ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”©\"\"\"\n",
    "        print(\"ğŸ”„ 1ë‹¨ê³„: ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”©...\")\n",
    "        \n",
    "        try:\n",
    "            file_path = os.path.join(self.base_path, filename)\n",
    "            self.customer_data = pd.read_excel(file_path)\n",
    "            \n",
    "            print(f\"âœ… ê³ ê° ë°ì´í„° ë¡œë”© ì™„ë£Œ\")\n",
    "            print(f\"   - ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…\")\n",
    "            print(f\"   - ì»¬ëŸ¼: {list(self.customer_data.columns)}\")\n",
    "            \n",
    "            # ê³ ê° ë¶„í¬ ë¶„ì„\n",
    "            contract_dist = self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts()\n",
    "            usage_dist = self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts()\n",
    "            \n",
    "            print(f\"\\\\nğŸ“Š ê³ ê° ë¶„í¬:\")\n",
    "            print(f\"   - ê³„ì•½ì¢…ë³„: {len(contract_dist)}ê°œ ìœ í˜•\")\n",
    "            print(f\"   - ì‚¬ìš©ìš©ë„: {len(usage_dist)}ê°œ ìœ í˜•\")\n",
    "            \n",
    "            self.analysis_results['customer_summary'] = {\n",
    "                'total_customers': len(self.customer_data),\n",
    "                'contract_types': contract_dist.to_dict(),\n",
    "                'usage_types': usage_dist.to_dict()\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê³ ê° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_preprocessed_data(self):\n",
    "        \"\"\"1ë‹¨ê³„ì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© (ì„±ëŠ¥ ìµœì í™”)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š 2ë‹¨ê³„: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # 1ë‹¨ê³„ ê²°ê³¼ í™•ì¸\n",
    "            analysis_results_path = './analysis_results/analysis_results.json'\n",
    "            if os.path.exists(analysis_results_path):\n",
    "                with open(analysis_results_path, 'r', encoding='utf-8') as f:\n",
    "                    step1_results = json.load(f)\n",
    "                print(\"âœ… 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ë°œê²¬\")\n",
    "                print(f\"   ì²˜ë¦¬ ì‹œê°„: {step1_results.get('metadata', {}).get('timestamp', 'N/A')}\")\n",
    "                print(f\"   ì´ ê³ ê°: {step1_results.get('metadata', {}).get('total_customers', 0):,}ëª…\")\n",
    "                print(f\"   ì´ ë ˆì½”ë“œ: {step1_results.get('metadata', {}).get('total_lp_records', 0):,}ê±´\")\n",
    "            else:\n",
    "                print(\"âš ï¸ 1ë‹¨ê³„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ - ê¸°ë³¸ ê²½ë¡œì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„° íƒìƒ‰\")\n",
    "                \n",
    "            # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© (ìš°ì„ ìˆœìœ„: Parquet > CSV)\n",
    "            processed_parquet = './analysis_results/processed_lp_data.parquet'\n",
    "            processed_csv = './analysis_results/processed_lp_data.csv'\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            if os.path.exists(processed_parquet):\n",
    "                print(\"\\nğŸš€ Parquet íŒŒì¼ ë¡œë”© ì¤‘... (ê³ ì„±ëŠ¥)\")\n",
    "                try:\n",
    "                    self.lp_data = pd.read_parquet(processed_parquet)\n",
    "                    loading_method = \"Parquet\"\n",
    "                    print(\"   âœ… Parquet ë¡œë”© ì„±ê³µ!\")\n",
    "                except ImportError:\n",
    "                    print(\"   âš ï¸ PyArrow ì—†ìŒ - CSVë¡œ ëŒ€ì²´\")\n",
    "                    self.lp_data = pd.read_csv(processed_csv)\n",
    "                    loading_method = \"CSV (PyArrow ì—†ìŒ)\"\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸ Parquet ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "                    print(\"   ğŸ”„ CSVë¡œ ëŒ€ì²´ ì‹œë„...\")\n",
    "                    self.lp_data = pd.read_csv(processed_csv)\n",
    "                    loading_method = \"CSV (Parquet ì‹¤íŒ¨)\"\n",
    "                    \n",
    "            elif os.path.exists(processed_csv):\n",
    "                print(\"\\nğŸ“„ CSV íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "                self.lp_data = pd.read_csv(processed_csv)\n",
    "                loading_method = \"CSV\"\n",
    "                print(\"   âœ… CSV ë¡œë”© ì„±ê³µ!\")\n",
    "                \n",
    "            else:\n",
    "                print(\"\\nâŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "                print(\"   ğŸ”§ í•´ê²°ë°©ë²•: 1ë‹¨ê³„ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”\")\n",
    "                print(\"   ğŸ“ í•„ìš” íŒŒì¼:\")\n",
    "                print(\"      - ./analysis_results/processed_lp_data.parquet\")\n",
    "                print(\"      - ./analysis_results/processed_lp_data.csv\")\n",
    "                return False\n",
    "                \n",
    "            # ë¡œë”© ì‹œê°„ ê³„ì‚°\n",
    "            loading_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            # ë°ì´í„° íƒ€ì… ìµœì í™”\n",
    "            if 'datetime' in self.lp_data.columns:\n",
    "                self.lp_data['datetime'] = pd.to_datetime(self.lp_data['datetime'])\n",
    "            \n",
    "            # ì¶”ê°€ ì‹œê³„ì—´ ì»¬ëŸ¼ ìƒì„± (1ë‹¨ê³„ì—ì„œ ëˆ„ë½ëœ ê²½ìš°)\n",
    "            if 'hour' not in self.lp_data.columns:\n",
    "                self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "            if 'weekday' not in self.lp_data.columns:\n",
    "                self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "            if 'is_weekend' not in self.lp_data.columns:\n",
    "                self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "                \n",
    "            # ì„±ëŠ¥ ìš”ì•½\n",
    "            print(f\"\\nâš¡ ë¡œë”© ì„±ëŠ¥ ìš”ì•½:\")\n",
    "            print(f\"   ë°©ë²•: {loading_method}\")\n",
    "            print(f\"   ì‹œê°„: {loading_time:.2f}ì´ˆ\")\n",
    "            print(f\"   ì†ë„: {len(self.lp_data)/loading_time:,.0f} ë ˆì½”ë“œ/ì´ˆ\")\n",
    "            print(f\"   ë°ì´í„°: {len(self.lp_data):,}ê±´\")\n",
    "            print(f\"   ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique():,}ëª…\")\n",
    "            print(f\"   ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            \n",
    "            # ê¸°ì¡´ ëŒ€ë¹„ ì„±ëŠ¥ ê°œì„  í‘œì‹œ\n",
    "            if loading_method.startswith(\"Parquet\"):\n",
    "                print(f\"   ğŸš€ ì„±ëŠ¥ ê°œì„ : ê¸°ì¡´ ëŒ€ë¹„ 2-3ë°° ë¹ ë¦„!\")\n",
    "            elif loading_method.startswith(\"CSV\"):\n",
    "                print(f\"   ğŸ“ˆ ì„±ëŠ¥ ê°œì„ : ê¸°ì¡´ ëŒ€ë¹„ 3-4ë°° ë¹ ë¦„!\")\n",
    "                \n",
    "            print(f\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹œì‘\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            print(\"   ğŸ”§ í•´ê²°ë°©ë²•:\")\n",
    "            print(\"   1. 1ë‹¨ê³„ ì½”ë“œ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸\")\n",
    "            print(\"   2. ./analysis_results/ í´ë” ì¡´ì¬ í™•ì¸\")\n",
    "            print(\"   3. íŒŒì¼ ê¶Œí•œ í™•ì¸\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "        \n",
    "    def load_external_data(self):\n",
    "        \"\"\"ê¸°ìƒ ë° ë‹¬ë ¥ ë°ì´í„° ë¡œë”©\"\"\"\n",
    "        print(\"\\nğŸ“Š ì™¸ë¶€ ë°ì´í„° ë¡œë”©...\")\n",
    "        \n",
    "        try:\n",
    "            # ê¸°ìƒ ë°ì´í„° ë¡œë”©\n",
    "            weather_file = 'weather_daily_processed.csv'\n",
    "            if os.path.exists(weather_file):\n",
    "                self.weather_data = pd.read_csv(weather_file)\n",
    "                self.weather_data['ë‚ ì§œ'] = pd.to_datetime(self.weather_data['ë‚ ì§œ'])\n",
    "                print(f\"   ğŸŒ¤ï¸ ê¸°ìƒ ë°ì´í„°: {len(self.weather_data):,}ì¼\")\n",
    "            else:\n",
    "                print(\"   âš ï¸ ê¸°ìƒ ë°ì´í„° ì—†ìŒ\")\n",
    "                self.weather_data = None\n",
    "                \n",
    "            # ë‹¬ë ¥ ë°ì´í„° ë¡œë”©\n",
    "            calendar_file = 'power_analysis_calendar_2022_2025.csv'\n",
    "            if os.path.exists(calendar_file):\n",
    "                self.calendar_data = pd.read_csv(calendar_file)\n",
    "                self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date'])\n",
    "                print(f\"   ğŸ“… ë‹¬ë ¥ ë°ì´í„°: {len(self.calendar_data):,}ì¼\")\n",
    "            else:\n",
    "                print(\"   âš ï¸ ë‹¬ë ¥ ë°ì´í„° ì—†ìŒ\")\n",
    "                self.calendar_data = None\n",
    "                \n",
    "            return True\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ì™¸ë¶€ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.weather_data = None\n",
    "            self.calendar_data = None\n",
    "            return False\n",
    "\n",
    "    def _validate_data_quality(self):\n",
    "        \"\"\"ë°ì´í„° í’ˆì§ˆ ê²€ì¦\"\"\"\n",
    "        print(\"\\\\nğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...\")\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        available_numeric_cols = [col for col in numeric_columns if col in self.lp_data.columns]\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(available_numeric_cols)}ê°œ\")\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ í™•ì¸\n",
    "        null_counts = self.lp_data[available_numeric_cols].isnull().sum()\n",
    "        total_nulls = null_counts.sum()\n",
    "        \n",
    "        if total_nulls > 0:\n",
    "            print(f\"   âš ï¸ ê²°ì¸¡ì¹˜: {total_nulls:,}ê°œ ({total_nulls/len(self.lp_data)*100:.2f}%)\")\n",
    "            for col, count in null_counts.items():\n",
    "                if count > 0:\n",
    "                    print(f\"      {col}: {count:,}ê°œ\")\n",
    "        else:\n",
    "            print(\"   âœ… ê²°ì¸¡ì¹˜ ì—†ìŒ\")\n",
    "        \n",
    "        # ì‹œê°„ ê°„ê²© ì²´í¬ (ìƒ˜í”Œ ê³ ê°ìœ¼ë¡œ)\n",
    "        sample_customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]\n",
    "        \n",
    "        print(\"   â° ì‹œê°„ ê°„ê²© ê²€ì¦:\")\n",
    "        for customer in sample_customers:\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('LP ìˆ˜ì‹ ì¼ì')\n",
    "            \n",
    "            if len(customer_data) > 1:\n",
    "                time_diffs = customer_data['LP ìˆ˜ì‹ ì¼ì'].diff().dt.total_seconds() / 60\n",
    "                time_diffs = time_diffs.dropna()\n",
    "                \n",
    "                if len(time_diffs) > 0:\n",
    "                    avg_interval = time_diffs.mean()\n",
    "                    std_interval = time_diffs.std()\n",
    "                    print(f\"      {customer}: í‰ê·  {avg_interval:.1f}ë¶„ (í‘œì¤€í¸ì°¨: {std_interval:.1f})\")\n",
    "        \n",
    "        # ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "        self.analysis_results['data_quality'] = {\n",
    "            'total_records': len(self.lp_data),\n",
    "            'customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "            'null_counts': null_counts.to_dict(),\n",
    "            'date_range': {\n",
    "                'start': str(self.lp_data['LP ìˆ˜ì‹ ì¼ì'].min()),\n",
    "                'end': str(self.lp_data['LP ìˆ˜ì‹ ì¼ì'].max())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def analyze_temporal_patterns(self):\n",
    "        \"\"\"ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"\\\\nğŸ“ˆ 3ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„...\")\n",
    "\n",
    "        # â­â­â­ í•µì‹¬ ìˆ˜ì •: ì»¬ëŸ¼ëª… í™•ì¸ ë° ìˆ˜ì •\n",
    "        print(\"   ğŸ• ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì¤‘...\")\n",
    "\n",
    "        # 1ë‹¨ê³„ì—ì„œ ì €ì¥ëœ ì»¬ëŸ¼ëª… í™•ì¸\n",
    "        print(f\"   ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(self.lp_data.columns)}\")\n",
    "\n",
    "        # datetime ì»¬ëŸ¼ í™•ì¸ ë° ë³€í™˜\n",
    "        if 'datetime' in self.lp_data.columns:\n",
    "            datetime_col = 'datetime'\n",
    "        elif 'LP ìˆ˜ì‹ ì¼ì' in self.lp_data.columns:\n",
    "            datetime_col = 'LP ìˆ˜ì‹ ì¼ì'\n",
    "            # datetime íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (í•„ìš”ì‹œ)\n",
    "            if not pd.api.types.is_datetime64_any_dtype(self.lp_data[datetime_col]):\n",
    "                self.lp_data[datetime_col] = pd.to_datetime(self.lp_data[datetime_col])\n",
    "        else:\n",
    "            print(\"   âŒ ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "            return False\n",
    "\n",
    "        # ì‹œê°„ ê´€ë ¨ íŒŒìƒ ë³€ìˆ˜ ìƒì„± (ìˆ˜ì •ëœ ì»¬ëŸ¼ëª… ì‚¬ìš©)\n",
    "        self.lp_data['ë‚ ì§œ'] = self.lp_data[datetime_col].dt.date\n",
    "        self.lp_data['ì‹œê°„'] = self.lp_data[datetime_col].dt.hour\n",
    "        self.lp_data['ìš”ì¼'] = self.lp_data[datetime_col].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        self.lp_data['ì›”'] = self.lp_data[datetime_col].dt.month\n",
    "        self.lp_data['ì£¼'] = self.lp_data[datetime_col].dt.isocalendar().week\n",
    "        self.lp_data['ì£¼ë§ì—¬ë¶€'] = self.lp_data['ìš”ì¼'].isin([5, 6])  # í† , ì¼\n",
    "\n",
    "        # ì „ë ¥ ì»¬ëŸ¼ëª…ë„ í™•ì¸ ë° ìˆ˜ì •\n",
    "        power_col = None\n",
    "        if 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥' in self.lp_data.columns:\n",
    "            power_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'\n",
    "        elif 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥' in self.lp_data.columns:\n",
    "            power_col = 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥'\n",
    "        else:\n",
    "            print(\"   âŒ ì „ë ¥ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "            return False\n",
    "\n",
    "        print(f\"   âœ… ì‚¬ìš©í•  ì»¬ëŸ¼: ì‹œê°„={datetime_col}, ì „ë ¥={power_col}\")\n",
    "\n",
    "        # 1. ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„ (ìˆ˜ì •ëœ ì»¬ëŸ¼ëª… ì‚¬ìš©)\n",
    "        print(\"   ğŸ“Š ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„...\")\n",
    "        hourly_patterns = self.lp_data.groupby('ì‹œê°„')[power_col].agg([\n",
    "            'mean', 'std', 'min', 'max', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        # í”¼í¬/ë¹„í”¼í¬ ì‹œê°„ëŒ€ ì‹ë³„\n",
    "        avg_by_hour = hourly_patterns['mean']\n",
    "        peak_threshold = avg_by_hour.quantile(0.75)\n",
    "        off_peak_threshold = avg_by_hour.quantile(0.25)\n",
    "\n",
    "        peak_hours = avg_by_hour[avg_by_hour >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = avg_by_hour[avg_by_hour <= off_peak_threshold].index.tolist()\n",
    "\n",
    "        print(f\"      í”¼í¬ ì‹œê°„ëŒ€: {peak_hours}\")\n",
    "        print(f\"      ë¹„í”¼í¬ ì‹œê°„ëŒ€: {off_peak_hours}\")\n",
    "\n",
    "        # 2. ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„\n",
    "        print(\"   ğŸ“… ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„...\")\n",
    "        daily_patterns = self.lp_data.groupby('ìš”ì¼')[power_col].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        # í‰ì¼ vs ì£¼ë§ ë¹„êµ\n",
    "        weekday_avg = self.lp_data[~self.lp_data['ì£¼ë§ì—¬ë¶€']][power_col].mean()\n",
    "        weekend_avg = self.lp_data[self.lp_data['ì£¼ë§ì—¬ë¶€']][power_col].mean()\n",
    "        weekend_ratio = weekend_avg / weekday_avg if weekday_avg > 0 else 0\n",
    "\n",
    "        print(f\"      í‰ì¼ í‰ê· : {weekday_avg:.2f}kW\")\n",
    "        print(f\"      ì£¼ë§ í‰ê· : {weekend_avg:.2f}kW\")\n",
    "        print(f\"      ì£¼ë§/í‰ì¼ ë¹„ìœ¨: {weekend_ratio:.3f}\")\n",
    "\n",
    "        # 3. ì›”ë³„ ê³„ì ˆì„± íŒ¨í„´\n",
    "        print(\"   ğŸ—“ï¸ ì›”ë³„ ê³„ì ˆì„± ë¶„ì„...\")\n",
    "        monthly_patterns = self.lp_data.groupby('ì›”')[power_col].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        # ê³„ì ˆ êµ¬ë¶„ (í•œêµ­ ê¸°ì¤€)\n",
    "        season_map = {12: 'ê²¨ìš¸', 1: 'ê²¨ìš¸', 2: 'ê²¨ìš¸',\n",
    "                     3: 'ë´„', 4: 'ë´„', 5: 'ë´„',\n",
    "                     6: 'ì—¬ë¦„', 7: 'ì—¬ë¦„', 8: 'ì—¬ë¦„',\n",
    "                     9: 'ê°€ì„', 10: 'ê°€ì„', 11: 'ê°€ì„'}\n",
    "\n",
    "        self.lp_data['ê³„ì ˆ'] = self.lp_data['ì›”'].map(season_map)\n",
    "        seasonal_patterns = self.lp_data.groupby('ê³„ì ˆ')[power_col].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ]).round(2)\n",
    "\n",
    "        print(f\"      ê³„ì ˆë³„ í‰ê·  ì‚¬ìš©ëŸ‰:\")\n",
    "        for season, values in seasonal_patterns.iterrows():\n",
    "            print(f\"        {season}: {values['mean']:.2f}kW\")\n",
    "\n",
    "        # ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "        self.analysis_results['temporal_patterns'] = {\n",
    "            'hourly_patterns': hourly_patterns.to_dict(),\n",
    "            'daily_patterns': daily_patterns.to_dict(),\n",
    "            'monthly_patterns': monthly_patterns.to_dict(),\n",
    "            'seasonal_patterns': seasonal_patterns.to_dict(),\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "\n",
    "        return True\n",
    "\n",
    "    def analyze_volatility_indicators(self):\n",
    "        \"\"\"ë³€ë™ì„± ì§€í‘œ ë¶„ì„ (ì§‘ê³„ ì¤‘ì‹¬)\"\"\"\n",
    "        print(\"\\\\nğŸ“Š 4ë‹¨ê³„: ë³€ë™ì„± ì§€í‘œ ë¶„ì„...\")\n",
    "        \n",
    "        customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()\n",
    "        print(f\"   ğŸ”„ {len(customers)}ëª… ê³ ê° ë³€ë™ì„± ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì§‘ê³„ ë¶„ì„\n",
    "        \n",
    "        # 1. ì „ì²´ ë³€ë™ì„± í†µê³„\n",
    "        overall_power = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']\n",
    "        overall_cv = overall_power.std() / overall_power.mean() if overall_power.mean() > 0 else 0\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ ì „ì²´ ë°ì´í„° ë³€ë™ì„±:\")\n",
    "        print(f\"      ì „ì²´ ë³€ë™ê³„ìˆ˜: {overall_cv:.4f}\")\n",
    "        print(f\"      í‰ê·  ì „ë ¥: {overall_power.mean():.2f}kW\")\n",
    "        print(f\"      í‘œì¤€í¸ì°¨: {overall_power.std():.2f}kW\")\n",
    "        \n",
    "        # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŒ¨í„´\n",
    "        hourly_volatility = self.lp_data.groupby('ì‹œê°„')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        hourly_volatility['cv'] = hourly_volatility['std'] / hourly_volatility['mean']\n",
    "        \n",
    "        print(f\"\\\\n   â° ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± íŒ¨í„´:\")\n",
    "        high_volatility_hours = hourly_volatility.nlargest(3, 'cv').index.tolist()\n",
    "        low_volatility_hours = hourly_volatility.nsmallest(3, 'cv').index.tolist()\n",
    "        print(f\"      ê³ ë³€ë™ì„± ì‹œê°„ëŒ€: {high_volatility_hours}ì‹œ (CV: {hourly_volatility.loc[high_volatility_hours, 'cv'].mean():.4f})\")\n",
    "        print(f\"      ì €ë³€ë™ì„± ì‹œê°„ëŒ€: {low_volatility_hours}ì‹œ (CV: {hourly_volatility.loc[low_volatility_hours, 'cv'].mean():.4f})\")\n",
    "        \n",
    "        # 3. ìš”ì¼ë³„ ë³€ë™ì„± íŒ¨í„´\n",
    "        daily_volatility = self.lp_data.groupby('ìš”ì¼')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        daily_volatility['cv'] = daily_volatility['std'] / daily_volatility['mean']\n",
    "        \n",
    "        weekday_cv = daily_volatility.loc[0:4, 'cv'].mean()  # ì›”-ê¸ˆ\n",
    "        weekend_cv = daily_volatility.loc[5:6, 'cv'].mean()  # í† -ì¼\n",
    "        \n",
    "        print(f\"\\\\n   ğŸ“… ìš”ì¼ë³„ ë³€ë™ì„± íŒ¨í„´:\")\n",
    "        print(f\"      í‰ì¼ í‰ê·  ë³€ë™ê³„ìˆ˜: {weekday_cv:.4f}\")\n",
    "        print(f\"      ì£¼ë§ í‰ê·  ë³€ë™ê³„ìˆ˜: {weekend_cv:.4f}\")\n",
    "        print(f\"      ì£¼ë§/í‰ì¼ ë³€ë™ì„± ë¹„ìœ¨: {weekend_cv/weekday_cv:.3f}\")\n",
    "        \n",
    "        # 4. ì›”ë³„ ë³€ë™ì„± íŒ¨í„´\n",
    "        monthly_volatility = self.lp_data.groupby('ì›”')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].agg([\n",
    "            'mean', 'std', 'count'\n",
    "        ])\n",
    "        monthly_volatility['cv'] = monthly_volatility['std'] / monthly_volatility['mean']\n",
    "        \n",
    "        print(f\"\\\\n   ğŸ—“ï¸ ì›”ë³„ ë³€ë™ì„± íŒ¨í„´:\")\n",
    "        high_var_months = monthly_volatility.nlargest(2, 'cv').index.tolist()\n",
    "        low_var_months = monthly_volatility.nsmallest(2, 'cv').index.tolist()\n",
    "        print(f\"      ê³ ë³€ë™ì„± ì›”: {high_var_months}ì›”\")\n",
    "        print(f\"      ì €ë³€ë™ì„± ì›”: {low_var_months}ì›”\")\n",
    "        \n",
    "        # 5. ê³ ê°ë³„ ë³€ë™ì„± ë¶„í¬ (ìš”ì•½ í†µê³„ë§Œ)\n",
    "        print(f\"\\\\n   ğŸ‘¥ ê³ ê°ë³„ ë³€ë™ì„± ë¶„í¬ ë¶„ì„...\")\n",
    "        \n",
    "        # ì²­í¬ ë‹¨ìœ„ë¡œ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)\n",
    "        chunk_size = 100\n",
    "        customer_cvs = []\n",
    "        \n",
    "        for i in range(0, len(customers), chunk_size):\n",
    "            chunk_customers = customers[i:i+chunk_size]\n",
    "            if (i // chunk_size + 1) % 5 == 0:  # 500ëª…ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥\n",
    "                print(f\"      ì§„í–‰: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            for customer in chunk_customers:\n",
    "                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "                power_series = customer_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']\n",
    "                \n",
    "                if len(power_series) >= 96 and power_series.mean() > 0:  # ìµœì†Œ 1ì¼ ë°ì´í„°\n",
    "                    cv = power_series.std() / power_series.mean()\n",
    "                    customer_cvs.append(cv)\n",
    "        \n",
    "        # ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë¶„í¬ í†µê³„\n",
    "        cv_array = np.array(customer_cvs)\n",
    "        cv_percentiles = np.percentile(cv_array, [10, 25, 50, 75, 90])\n",
    "        \n",
    "        print(f\"   ğŸ“Š ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë¶„í¬ ({len(customer_cvs)}ëª…):\")\n",
    "        print(f\"      í‰ê· : {cv_array.mean():.4f}\")\n",
    "        print(f\"      í‘œì¤€í¸ì°¨: {cv_array.std():.4f}\")\n",
    "        print(f\"      10%ile: {cv_percentiles[0]:.4f}\")\n",
    "        print(f\"      25%ile: {cv_percentiles[1]:.4f}\")\n",
    "        print(f\"      50%ile: {cv_percentiles[2]:.4f}\")\n",
    "        print(f\"      75%ile: {cv_percentiles[3]:.4f}\")\n",
    "        print(f\"      90%ile: {cv_percentiles[4]:.4f}\")\n",
    "        \n",
    "        # ë³€ë™ê³„ìˆ˜ êµ¬ê°„ë³„ ê³ ê° ìˆ˜\n",
    "        cv_bins = [0, 0.1, 0.2, 0.3, 0.5, 1.0, float('inf')]\n",
    "        cv_labels = ['ë§¤ìš° ì•ˆì • (<0.1)', 'ì•ˆì • (0.1-0.2)', 'ë³´í†µ (0.2-0.3)', \n",
    "                    'ë†’ìŒ (0.3-0.5)', 'ë§¤ìš° ë†’ìŒ (0.5-1.0)', 'ê·¹íˆ ë†’ìŒ (>1.0)']\n",
    "        \n",
    "        cv_counts = pd.cut(cv_array, bins=cv_bins, labels=cv_labels, include_lowest=True).value_counts()\n",
    "        \n",
    "        print(f\"\\\\n   ğŸ¯ ë³€ë™ì„± ë“±ê¸‰ë³„ ê³ ê° ë¶„í¬:\")\n",
    "        for grade, count in cv_counts.items():\n",
    "            percentage = count / len(customer_cvs) * 100\n",
    "            print(f\"      {grade}: {count}ëª… ({percentage:.1f}%)\")\n",
    "        \n",
    "        # ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "        self.analysis_results['volatility_analysis'] = {\n",
    "            'overall_cv': overall_cv,\n",
    "            'hourly_volatility': hourly_volatility.to_dict(),\n",
    "            'daily_volatility': daily_volatility.to_dict(),\n",
    "            'monthly_volatility': monthly_volatility.to_dict(),\n",
    "            'customer_cv_stats': {\n",
    "                'count': len(customer_cvs),\n",
    "                'mean': float(cv_array.mean()),\n",
    "                'std': float(cv_array.std()),\n",
    "                'percentiles': {\n",
    "                    '10%': float(cv_percentiles[0]),\n",
    "                    '25%': float(cv_percentiles[1]),\n",
    "                    '50%': float(cv_percentiles[2]),\n",
    "                    '75%': float(cv_percentiles[3]),\n",
    "                    '90%': float(cv_percentiles[4])\n",
    "                }\n",
    "            },\n",
    "            'volatility_distribution': cv_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        # ìš”ì•½ ë°ì´í„°ë§Œ CSVë¡œ ì €ì¥ (ê°œë³„ ê³ ê° ë°ì´í„°ëŠ” ì œì™¸)\n",
    "        summary_data = {\n",
    "            'metric': ['overall_cv', 'weekday_cv', 'weekend_cv', 'customer_cv_mean', \n",
    "                      'customer_cv_std', 'customer_cv_median'],\n",
    "            'value': [overall_cv, weekday_cv, weekend_cv, cv_array.mean(), \n",
    "                     cv_array.std(), cv_percentiles[2]]\n",
    "        }\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        output_file = os.path.join(self.output_dir, 'volatility_summary.csv')\n",
    "        summary_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\\\n   ğŸ’¾ ë³€ë™ì„± ìš”ì•½ ì €ì¥: {output_file}\")\n",
    "        \n",
    "        return cv_array\n",
    "\n",
    "    def detect_anomalies(self):\n",
    "        \"\"\"ì´ìƒ íŒ¨í„´ íƒì§€ (ì§‘ê³„ ì¤‘ì‹¬)\"\"\"\n",
    "        print(\"\\\\nğŸš¨ 5ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€...\")\n",
    "        \n",
    "        customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()\n",
    "        print(f\"   ğŸ” {len(customers)}ëª… ê³ ê° ì´ìƒ íŒ¨í„´ íƒì§€ ì¤‘...\")\n",
    "        \n",
    "        # ì „ì²´ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íŒ¨í„´ íƒì§€\n",
    "        \n",
    "        # 1. ì „ì²´ ë°ì´í„°ì˜ í†µê³„ì  ì´ìƒì¹˜ ì„ê³„ê°’ ì„¤ì •\n",
    "        overall_power = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']\n",
    "        q1, q3 = overall_power.quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # ì „ì²´ í†µê³„ì  ì´ìƒì¹˜\n",
    "        total_outliers = ((overall_power < lower_bound) | (overall_power > upper_bound)).sum()\n",
    "        outlier_rate = total_outliers / len(overall_power) * 100\n",
    "        \n",
    "        print(f\"   ğŸ“Š ì „ì²´ ë°ì´í„° ì´ìƒì¹˜ í˜„í™©:\")\n",
    "        print(f\"      í†µê³„ì  ì´ìƒì¹˜: {total_outliers:,}ê°œ ({outlier_rate:.2f}%)\")\n",
    "        print(f\"      ì •ìƒ ë²”ìœ„: {lower_bound:.1f} ~ {upper_bound:.1f}kW\")\n",
    "        \n",
    "        # 2. ì‹œê°„ëŒ€ë³„ ì´ìƒ íŒ¨í„´\n",
    "        night_hours = [0, 1, 2, 3, 4, 5]  # ì•¼ê°„ ì‹œê°„ëŒ€\n",
    "        day_hours = [9, 10, 11, 12, 13, 14, 15, 16, 17]  # ì£¼ê°„ ì‹œê°„ëŒ€\n",
    "        \n",
    "        night_data = self.lp_data[self.lp_data['ì‹œê°„'].isin(night_hours)]\n",
    "        day_data = self.lp_data[self.lp_data['ì‹œê°„'].isin(day_hours)]\n",
    "        \n",
    "        night_avg = night_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "        day_avg = day_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "        night_day_ratio = night_avg / day_avg if day_avg > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\n   ğŸŒ™ ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´:\")\n",
    "        print(f\"      ì•¼ê°„ í‰ê· : {night_avg:.2f}kW\")\n",
    "        print(f\"      ì£¼ê°„ í‰ê· : {day_avg:.2f}kW\")\n",
    "        print(f\"      ì•¼ê°„/ì£¼ê°„ ë¹„ìœ¨: {night_day_ratio:.3f}\")\n",
    "        \n",
    "        # 3. 0ê°’ íŒ¨í„´ ë¶„ì„\n",
    "        zero_count = (overall_power == 0).sum()\n",
    "        zero_rate = zero_count / len(overall_power) * 100\n",
    "        \n",
    "        print(f\"\\\\n   âš« 0ê°’ íŒ¨í„´ ë¶„ì„:\")\n",
    "        print(f\"      0ê°’ ì¸¡ì •: {zero_count:,}ê°œ ({zero_rate:.2f}%)\")\n",
    "        \n",
    "        # 4. ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´ (ì „ì²´ ë°ì´í„° ê¸°ì¤€)\n",
    "        power_changes = self.lp_data.sort_values(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LP ìˆ˜ì‹ ì¼ì'])['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].pct_change().abs()\n",
    "        sudden_changes = power_changes[power_changes > 2.0]  # 200% ì´ìƒ ë³€í™”\n",
    "        sudden_change_rate = len(sudden_changes) / len(power_changes.dropna()) * 100\n",
    "        \n",
    "        print(f\"\\\\n   âš¡ ê¸‰ê²©í•œ ë³€í™” íŒ¨í„´:\")\n",
    "        print(f\"      ê¸‰ê²©í•œ ë³€í™”: {len(sudden_changes):,}ê±´ ({sudden_change_rate:.2f}%)\")\n",
    "        \n",
    "        # 5. ê³ ê°ë³„ ì´ìƒ íŒ¨í„´ ìš”ì•½ í†µê³„ (ê°œë³„ ì¶œë ¥ ì—†ì´)\n",
    "        anomaly_customers = {\n",
    "            'high_night_usage': 0,      # ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš©\n",
    "            'excessive_zeros': 0,        # ê³¼ë„í•œ 0ê°’\n",
    "            'high_volatility': 0,        # ë†’ì€ ë³€ë™ì„±\n",
    "            'statistical_outliers': 0    # í†µê³„ì  ì´ìƒì¹˜ ë‹¤ìˆ˜\n",
    "        }\n",
    "        \n",
    "        chunk_size = 100\n",
    "        processed_customers = 0\n",
    "        \n",
    "        for i in range(0, len(customers), chunk_size):\n",
    "            chunk_customers = customers[i:i+chunk_size]\n",
    "            if (i // chunk_size + 1) % 5 == 0:\n",
    "                print(f\"      ì§„í–‰: {min(i+chunk_size, len(customers))}/{len(customers)} ({min(i+chunk_size, len(customers))/len(customers)*100:.1f}%)\")\n",
    "            \n",
    "            for customer in chunk_customers:\n",
    "                customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "                power_series = customer_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']\n",
    "                \n",
    "                if len(power_series) < 96:  # ìµœì†Œ 1ì¼ ë°ì´í„° í•„ìš”\n",
    "                    continue\n",
    "                \n",
    "                processed_customers += 1\n",
    "                \n",
    "                # ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš© ì²´í¬\n",
    "                customer_night = customer_data[customer_data['ì‹œê°„'].isin(night_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "                customer_day = customer_data[customer_data['ì‹œê°„'].isin(day_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "                if customer_day > 0 and customer_night / customer_day > 0.8:\n",
    "                    anomaly_customers['high_night_usage'] += 1\n",
    "                \n",
    "                # ê³¼ë„í•œ 0ê°’ ì²´í¬\n",
    "                zero_ratio = (power_series == 0).sum() / len(power_series)\n",
    "                if zero_ratio > 0.1:  # 10% ì´ìƒì´ 0ê°’\n",
    "                    anomaly_customers['excessive_zeros'] += 1\n",
    "                \n",
    "                # ë†’ì€ ë³€ë™ì„± ì²´í¬\n",
    "                if power_series.mean() > 0:\n",
    "                    cv = power_series.std() / power_series.mean()\n",
    "                    if cv > 1.0:  # ë³€ë™ê³„ìˆ˜ 1.0 ì´ìƒ\n",
    "                        anomaly_customers['high_volatility'] += 1\n",
    "                \n",
    "                # í†µê³„ì  ì´ìƒì¹˜ ë‹¤ìˆ˜ ì²´í¬\n",
    "                customer_outliers = ((power_series < lower_bound) | (power_series > upper_bound)).sum()\n",
    "                outlier_ratio = customer_outliers / len(power_series)\n",
    "                if outlier_ratio > 0.05:  # 5% ì´ìƒì´ ì´ìƒì¹˜\n",
    "                    anomaly_customers['statistical_outliers'] += 1\n",
    "        \n",
    "        # ì¢…í•© ì´ìƒ íŒ¨í„´ ê³ ê° (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì‹¤ì œë¡œëŠ” ê·¼ì‚¬ì¹˜)\n",
    "        total_anomaly_customers = max(anomaly_customers.values())  # ë‹¨ìˆœ ê·¼ì‚¬\n",
    "        anomaly_rate = total_anomaly_customers / processed_customers * 100 if processed_customers > 0 else 0\n",
    "        \n",
    "        print(f\"\\\\n   ğŸ“Š ì´ìƒ íŒ¨í„´ ê³ ê° ìš”ì•½ ({processed_customers}ëª… ë¶„ì„):\")\n",
    "        print(f\"      ì•¼ê°„ ê³¼ë‹¤ ì‚¬ìš©: {anomaly_customers['high_night_usage']}ëª…\")\n",
    "        print(f\"      ê³¼ë„í•œ 0ê°’: {anomaly_customers['excessive_zeros']}ëª…\")\n",
    "        print(f\"      ë†’ì€ ë³€ë™ì„±: {anomaly_customers['high_volatility']}ëª…\")\n",
    "        print(f\"      í†µê³„ì  ì´ìƒì¹˜ ë‹¤ìˆ˜: {anomaly_customers['statistical_outliers']}ëª…\")\n",
    "        print(f\"      ì „ì²´ ì´ìƒ íŒ¨í„´ ë¹„ìœ¨: ì•½ {anomaly_rate:.1f}%\")\n",
    "        \n",
    "        # ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "        self.analysis_results['anomaly_analysis'] = {\n",
    "            'processed_customers': processed_customers,\n",
    "            'total_outliers': int(total_outliers),\n",
    "            'outlier_rate': float(outlier_rate),\n",
    "            'zero_count': int(zero_count),\n",
    "            'zero_rate': float(zero_rate),\n",
    "            'sudden_changes': len(sudden_changes),\n",
    "            'sudden_change_rate': float(sudden_change_rate),\n",
    "            'night_day_ratio': float(night_day_ratio),\n",
    "            'anomaly_customers': anomaly_customers,\n",
    "            'estimated_anomaly_rate': float(anomaly_rate)\n",
    "        }\n",
    "        \n",
    "        return anomaly_customers\n",
    "\n",
    "\n",
    "    def create_summary_visualizations(self):\n",
    "        \"\"\"ìš”ì•½ ì‹œê°í™” ìƒì„±\"\"\"\n",
    "        print(\"\\\\nğŸ“Š 6ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì‹œê°í™”...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš© íŒ¨í„´\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            \n",
    "            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´\n",
    "            hourly_avg = self.lp_data.groupby('ì‹œê°„')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "            axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2)\n",
    "            axes[0, 0].set_title('ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰', fontsize=14, fontweight='bold')\n",
    "            axes[0, 0].set_xlabel('ì‹œê°„')\n",
    "            axes[0, 0].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_xticks(range(0, 24, 3))\n",
    "            \n",
    "            # ìš”ì¼ë³„ íŒ¨í„´\n",
    "            daily_avg = self.lp_data.groupby('ìš”ì¼')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "            weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']\n",
    "            axes[0, 1].bar(range(len(daily_avg)), daily_avg.values, color='skyblue')\n",
    "            axes[0, 1].set_title('ìš”ì¼ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰', fontsize=14, fontweight='bold')\n",
    "            axes[0, 1].set_xlabel('ìš”ì¼')\n",
    "            axes[0, 1].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')\n",
    "            axes[0, 1].set_xticks(range(7))\n",
    "            axes[0, 1].set_xticklabels(weekday_names)\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # ë³€ë™ê³„ìˆ˜ ë¶„í¬ (ë³€ë™ì„± ë¶„ì„ì´ ì™„ë£Œëœ ê²½ìš°)\n",
    "            if 'volatility_analysis' in self.analysis_results:\n",
    "                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')\n",
    "                if os.path.exists(volatility_file):\n",
    "                    volatility_df = pd.read_csv(volatility_file)\n",
    "                    axes[1, 0].hist(volatility_df['cv_basic'].dropna(), bins=30, alpha=0.7, color='lightgreen')\n",
    "                    axes[1, 0].set_title('ë³€ë™ê³„ìˆ˜ ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 0].set_xlabel('ë³€ë™ê³„ìˆ˜ (CV)')\n",
    "                    axes[1, 0].set_ylabel('ê³ ê° ìˆ˜')\n",
    "                    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # ì›”ë³„ ê³„ì ˆì„± íŒ¨í„´\n",
    "            monthly_avg = self.lp_data.groupby('ì›”')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()\n",
    "            month_names = ['1ì›”', '2ì›”', '3ì›”', '4ì›”', '5ì›”', '6ì›”', \n",
    "                          '7ì›”', '8ì›”', '9ì›”', '10ì›”', '11ì›”', '12ì›”']\n",
    "            axes[1, 1].plot(monthly_avg.index, monthly_avg.values, marker='s', linewidth=2, color='orange')\n",
    "            axes[1, 1].set_title('ì›”ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (ê³„ì ˆì„±)', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('ì›”')\n",
    "            axes[1, 1].set_ylabel('í‰ê·  ìœ íš¨ì „ë ¥ (kW)')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            axes[1, 1].set_xticks(range(1, 13))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ì €ì¥\n",
    "            output_file = os.path.join(self.output_dir, 'temporal_patterns_summary.png')\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"   ğŸ’¾ ì‹œê³„ì—´ íŒ¨í„´ ì‹œê°í™” ì €ì¥: {output_file}\")\n",
    "            \n",
    "            # 2. ë³€ë™ì„± ê´€ë ¨ ì‹œê°í™” (ì¶”ê°€)\n",
    "            if 'volatility_analysis' in self.analysis_results:\n",
    "                volatility_file = os.path.join(self.output_dir, 'volatility_indicators.csv')\n",
    "                if os.path.exists(volatility_file):\n",
    "                    volatility_df = pd.read_csv(volatility_file)\n",
    "                    \n",
    "                    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "                    \n",
    "                    # í‰ê·  ì‚¬ìš©ëŸ‰ vs ë³€ë™ê³„ìˆ˜\n",
    "                    axes[0, 0].scatter(volatility_df['mean_power'], volatility_df['cv_basic'], alpha=0.6, s=20)\n",
    "                    axes[0, 0].set_title('í‰ê·  ì‚¬ìš©ëŸ‰ vs ë³€ë™ê³„ìˆ˜', fontsize=14, fontweight='bold')\n",
    "                    axes[0, 0].set_xlabel('í‰ê·  ì „ë ¥ (kW)')\n",
    "                    axes[0, 0].set_ylabel('ë³€ë™ê³„ìˆ˜')\n",
    "                    axes[0, 0].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # ì‹œê°„ëŒ€ë³„ ë³€ë™ì„± vs ì¼ë³„ ë³€ë™ì„±\n",
    "                    axes[0, 1].scatter(volatility_df['hourly_cv_mean'], volatility_df['daily_cv_mean'], alpha=0.6, s=20, color='red')\n",
    "                    axes[0, 1].set_title('ì‹œê°„ëŒ€ë³„ vs ì¼ë³„ ë³€ë™ì„±', fontsize=14, fontweight='bold')\n",
    "                    axes[0, 1].set_xlabel('ì‹œê°„ëŒ€ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜')\n",
    "                    axes[0, 1].set_ylabel('ì¼ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜')\n",
    "                    axes[0, 1].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # ì£¼ë§/í‰ì¼ ë³€ë™ê³„ìˆ˜ ë¹„êµ\n",
    "                    weekend_weekday_ratio = volatility_df['weekend_weekday_cv_ratio'].dropna()\n",
    "                    axes[1, 0].hist(weekend_weekday_ratio, bins=20, alpha=0.7, color='purple')\n",
    "                    axes[1, 0].set_title('ì£¼ë§/í‰ì¼ ë³€ë™ê³„ìˆ˜ ë¹„ìœ¨ ë¶„í¬', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 0].set_xlabel('ì£¼ë§/í‰ì¼ ë³€ë™ê³„ìˆ˜ ë¹„ìœ¨')\n",
    "                    axes[1, 0].set_ylabel('ê³ ê° ìˆ˜')\n",
    "                    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    # ë³€ë™ê³„ìˆ˜ ìƒìœ„/í•˜ìœ„ ë¶„í¬\n",
    "                    cv_top10 = volatility_df.nlargest(10, 'cv_basic')['cv_basic']\n",
    "                    cv_bottom10 = volatility_df.nsmallest(10, 'cv_basic')['cv_basic']\n",
    "                    \n",
    "                    x_pos = range(10)\n",
    "                    width = 0.35\n",
    "                    axes[1, 1].bar([x - width/2 for x in x_pos], cv_top10.values, width, \n",
    "                                  label='ìƒìœ„ 10ëª…', alpha=0.8, color='red')\n",
    "                    axes[1, 1].bar([x + width/2 for x in x_pos], cv_bottom10.values, width, \n",
    "                                  label='í•˜ìœ„ 10ëª…', alpha=0.8, color='blue')\n",
    "                    axes[1, 1].set_title('ë³€ë™ê³„ìˆ˜ ìƒìœ„/í•˜ìœ„ 10ëª… ë¹„êµ', fontsize=14, fontweight='bold')\n",
    "                    axes[1, 1].set_xlabel('ìˆœìœ„')\n",
    "                    axes[1, 1].set_ylabel('ë³€ë™ê³„ìˆ˜')\n",
    "                    axes[1, 1].legend()\n",
    "                    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    \n",
    "                    # ì´ë¯¸ì§€ ì €ì¥\n",
    "                    output_file = os.path.join(self.output_dir, 'volatility_analysis_summary.png')\n",
    "                    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                    print(f\"   ğŸ’¾ ë³€ë™ì„± ë¶„ì„ ì‹œê°í™” ì €ì¥: {output_file}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        print(\"\\\\nğŸ“‹ 7ë‹¨ê³„: ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±...\")\n",
    "        \n",
    "        report_file = os.path.join(self.output_dir, 'comprehensive_analysis_report.txt')\n",
    "        \n",
    "        try:\n",
    "            with open(report_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\\\\n\")\n",
    "                f.write(\"ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ë° ë³€ë™ì„± ì§€í‘œ ê°œë°œ ê²°ê³¼ ë¦¬í¬íŠ¸\\\\n\")\n",
    "                f.write(\"=\" * 80 + \"\\\\n\\\\n\")\n",
    "                \n",
    "                # 1. ë¶„ì„ ê°œìš”\n",
    "                f.write(\"1. ë¶„ì„ ê°œìš”\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(f\"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "                f.write(f\"ê³ ê° ìˆ˜: {self.analysis_results.get('customer_summary', {}).get('total_customers', 'N/A'):,}ëª…\\\\n\")\n",
    "                f.write(f\"LP ë ˆì½”ë“œ: {self.analysis_results.get('data_quality', {}).get('total_records', 'N/A'):,}ê°œ\\\\n\")\n",
    "                f.write(f\"ë¶„ì„ ëŒ€ìƒ ê³ ê°: {self.analysis_results.get('data_quality', {}).get('customers', 'N/A')}ëª…\\\\n\")\n",
    "                \n",
    "                date_range = self.analysis_results.get('data_quality', {}).get('date_range', {})\n",
    "                if date_range:\n",
    "                    f.write(f\"ë°ì´í„° ê¸°ê°„: {date_range.get('start', 'N/A')} ~ {date_range.get('end', 'N/A')}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 2. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ê²°ê³¼\n",
    "                f.write(\"2. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ê²°ê³¼\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                temporal = self.analysis_results.get('temporal_patterns', {})\n",
    "                if temporal:\n",
    "                    f.write(f\"í”¼í¬ ì‹œê°„ëŒ€: {temporal.get('peak_hours', [])}\\\\n\")\n",
    "                    f.write(f\"ë¹„í”¼í¬ ì‹œê°„ëŒ€: {temporal.get('off_peak_hours', [])}\\\\n\")\n",
    "                    f.write(f\"ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {temporal.get('weekend_ratio', 0):.3f}\\\\n\")\n",
    "                    \n",
    "                    # ê³„ì ˆë³„ íŒ¨í„´\n",
    "                    seasonal = temporal.get('seasonal_patterns', {})\n",
    "                    if seasonal:\n",
    "                        f.write(\"\\\\nê³„ì ˆë³„ í‰ê·  ì‚¬ìš©ëŸ‰:\\\\n\")\n",
    "                        for season in ['ë´„', 'ì—¬ë¦„', 'ê°€ì„', 'ê²¨ìš¸']:\n",
    "                            if season in seasonal and 'mean' in seasonal[season]:\n",
    "                                f.write(f\"  {season}: {seasonal[season]['mean']:.2f}kW\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 3. ë³€ë™ì„± ì§€í‘œ ë¶„ì„ ê²°ê³¼\n",
    "                f.write(\"3. ë³€ë™ì„± ì§€í‘œ ë¶„ì„ ê²°ê³¼\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                volatility = self.analysis_results.get('volatility_analysis', {})\n",
    "                if volatility:\n",
    "                    summary_stats = volatility.get('summary_stats', {})\n",
    "                    cv_stats = summary_stats.get('cv_basic', {})\n",
    "                    \n",
    "                    if cv_stats:\n",
    "                        f.write(\"ê¸°ë³¸ ë³€ë™ê³„ìˆ˜(CV) í†µê³„:\\\\n\")\n",
    "                        f.write(f\"  í‰ê· : {cv_stats.get('mean', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  í‘œì¤€í¸ì°¨: {cv_stats.get('std', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  ìµœì†Ÿê°’: {cv_stats.get('min', 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  ìµœëŒ“ê°’: {cv_stats.get('max', 0):.4f}\\\\n\")\n",
    "                        \n",
    "                    quartiles = volatility.get('quartiles', {})\n",
    "                    if quartiles:\n",
    "                        f.write(\"\\\\në³€ë™ê³„ìˆ˜ ì‚¬ë¶„ìœ„ìˆ˜:\\\\n\")\n",
    "                        f.write(f\"  Q1 (25%): {quartiles.get(0.25, 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  Q2 (50%): {quartiles.get(0.5, 0):.4f}\\\\n\")\n",
    "                        f.write(f\"  Q3 (75%): {quartiles.get(0.75, 0):.4f}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 4. ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼\n",
    "                f.write(\"4. ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                \n",
    "                anomaly = self.analysis_results.get('anomaly_analysis', {})\n",
    "                if anomaly:\n",
    "                    total_anomaly = anomaly.get('total_anomaly_customers', 0)\n",
    "                    anomaly_rate = anomaly.get('anomaly_rate', 0) * 100\n",
    "                    f.write(f\"ì´ìƒ íŒ¨í„´ ê³ ê°: {total_anomaly}ëª… ({anomaly_rate:.1f}%)\\\\n\")\n",
    "                    \n",
    "                    anomaly_types = anomaly.get('anomaly_types', {})\n",
    "                    f.write(\"\\\\nì´ìƒ íŒ¨í„´ ìœ í˜•ë³„ ë¶„í¬:\\\\n\")\n",
    "                    for pattern_type, count in anomaly_types.items():\n",
    "                        f.write(f\"  {pattern_type}: {count}ëª…\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 5. ë³€ë™ê³„ìˆ˜ ê°œë°œì„ ìœ„í•œ ì¸ì‚¬ì´íŠ¸\n",
    "                f.write(\"5. ë³€ë™ê³„ìˆ˜ ê°œë°œì„ ìœ„í•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"ê°€. ì‹œê°„ëŒ€ë³„ ì°¨ë³„í™” í•„ìš”ì„±:\\\\n\")\n",
    "                f.write(\"   - í”¼í¬/ë¹„í”¼í¬ ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ ì ìš©\\\\n\")\n",
    "                f.write(\"   - ì•¼ê°„ ì‹œê°„ëŒ€ ì´ìƒ ì‚¬ìš© íŒ¨í„´ ë³„ë„ ì²˜ë¦¬\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"ë‚˜. ìš”ì¼ë³„ íŒ¨í„´ ê³ ë ¤:\\\\n\")\n",
    "                f.write(\"   - ì£¼ë§/í‰ì¼ ì‚¬ìš© íŒ¨í„´ ì°¨ì´ ë°˜ì˜\\\\n\")\n",
    "                f.write(\"   - ìš”ì¼ë³„ ë³€ë™ì„± ê°€ì¤‘ì¹˜ ì¡°ì •\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"ë‹¤. ê³„ì ˆì„± ë³´ì •:\\\\n\")\n",
    "                f.write(\"   - ì›”ë³„/ê³„ì ˆë³„ ê¸°ì¤€ê°’ ì°¨ë³„í™”\\\\n\")\n",
    "                f.write(\"   - ì™¸ë¶€ ê¸°ìƒ ë°ì´í„° ì—°ê³„ ê³ ë ¤\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"ë¼. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ:\\\\n\")\n",
    "                f.write(\"   - ê¸°ë³¸ ë³€ë™ê³„ìˆ˜(CV) ì™¸ ì¶”ê°€ ì§€í‘œ í™œìš©\\\\n\")\n",
    "                f.write(\"   - ì‹œê°„ ìœˆë„ìš°ë³„ ë³€ë™ì„± ì¡°í•©\\\\n\")\n",
    "                f.write(\"   - ë°©í–¥ì„± ë³€ë™ì„± ê³ ë ¤\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"ë§ˆ. ì´ìƒ íŒ¨í„´ í•„í„°ë§:\\\\n\")\n",
    "                f.write(\"   - ê¸‰ê²©í•œ ë³€í™” ë° ì¥ê¸°ê°„ 0ê°’ ì²˜ë¦¬\\\\n\")\n",
    "                f.write(\"   - í†µê³„ì  ì´ìƒì¹˜ ì œê±° ì•Œê³ ë¦¬ì¦˜\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                \n",
    "                # 6. ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­\n",
    "                f.write(\"6. ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"1. ì—…ì¢…ë³„ ë³€ë™ê³„ìˆ˜ ê¸°ì¤€ê°’ ì„¤ì •\\\\n\")\n",
    "                f.write(\"   - ê³„ì•½ì¢…ë³„/ì‚¬ìš©ìš©ë„ë³„ ì„ê³„ê°’ ì°¨ë³„í™”\\\\n\")\n",
    "                f.write(\"   - ì—…ì¢… íŠ¹ì„± ë°˜ì˜í•œ ê°€ì¤‘ì¹˜ ì„¤ê³„\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"2. ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\\\\n\")\n",
    "                f.write(\"   - Level-0: ê°œë³„ ë³€ë™ì„± ì§€í‘œ ëª¨ë¸\\\\n\")\n",
    "                f.write(\"   - Level-1: ë©”íƒ€ëª¨ë¸ì„ í†µí•œ í†µí•© ë³€ë™ê³„ìˆ˜\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"3. ì™¸ë¶€ ë°ì´í„° ì—°ê³„\\\\n\")\n",
    "                f.write(\"   - ê¸°ìƒì²­ ê¸°ìƒ ë°ì´í„° (ì˜¨ë„, ìŠµë„ ë“±)\\\\n\")\n",
    "                f.write(\"   - ê²½ì œ ì§€í‘œ ë° ì—…ì¢…ë³„ ìš´ì˜ í˜„í™©\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"4. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ\\\\n\")\n",
    "                f.write(\"   - ë³€ë™ê³„ìˆ˜ ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ ì‹œìŠ¤í…œ\\\\n\")\n",
    "                f.write(\"   - ì´ìƒ íŒ¨í„´ ìë™ íƒì§€ ë° ë³´ê³ \\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "                f.write(\"5. ì„±ëŠ¥ ê²€ì¦ ë° ìµœì í™”\\\\n\")\n",
    "                f.write(\"   - êµì°¨ê²€ì¦ì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\\\\n\")\n",
    "                f.write(\"   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ìµœì í™”\\\\n\")\n",
    "                \n",
    "            print(f\"   ğŸ’¾ ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥: {report_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "\n",
    "    def save_analysis_results(self):\n",
    "        \"\"\"ë¶„ì„ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "        print(\"\\\\nğŸ’¾ 8ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ ì €ì¥...\")\n",
    "        \n",
    "        try:\n",
    "            # JSONìœ¼ë¡œ ì €ì¥ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜\n",
    "            results_for_json = {}\n",
    "            \n",
    "            for key, value in self.analysis_results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    results_for_json[key] = {}\n",
    "                    for sub_key, sub_value in value.items():\n",
    "                        if hasattr(sub_value, 'to_dict'):  # pandas ê°ì²´ì¸ ê²½ìš°\n",
    "                            results_for_json[key][sub_key] = sub_value.to_dict()\n",
    "                        else:\n",
    "                            results_for_json[key][sub_key] = sub_value\n",
    "                else:\n",
    "                    results_for_json[key] = value\n",
    "            \n",
    "            # JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "            output_file = os.path.join(self.output_dir, 'analysis_results.json')\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results_for_json, f, ensure_ascii=False, indent=2, default=str)\n",
    "            \n",
    "            print(f\"   ğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"ì „ì²´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹œì‘\")\n",
    "        print(f\"ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # â­â­â­ í•µì‹¬ ë³€ê²½: ê¸°ì¡´ ì½”ë“œ ìˆ˜ì •\n",
    "            # ê¸°ì¡´: if not self.load_customer_data():\n",
    "            # ê¸°ì¡´: if not self.load_lp_data():\n",
    "            \n",
    "            # 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© (ìƒˆë¡œ ì¶”ê°€)\n",
    "            if not self.load_preprocessed_data():\n",
    "                print(\"âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ë¡œ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "                return False\n",
    "            \n",
    "            # 2. ì™¸ë¶€ ë°ì´í„° ë¡œë”© (ìƒˆë¡œ ì¶”ê°€)\n",
    "            self.load_external_data()\n",
    "            \n",
    "            # 3. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ê¸°ì¡´ ìœ ì§€)\n",
    "            if not self.analyze_temporal_patterns():\n",
    "                print(\"âŒ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨\")\n",
    "                return False\n",
    "            \n",
    "            # 4. ë³€ë™ì„± ì§€í‘œ ë¶„ì„ (ê¸°ì¡´ ìœ ì§€)\n",
    "            cv_array = self.analyze_volatility_indicators()\n",
    "            if cv_array is None or len(cv_array) == 0:\n",
    "                print(\"âŒ ë³€ë™ì„± ì§€í‘œ ë¶„ì„ ì‹¤íŒ¨\")\n",
    "                return False\n",
    "            \n",
    "            # 5. ì´ìƒ íŒ¨í„´ íƒì§€ (ê¸°ì¡´ ìœ ì§€)\n",
    "            anomaly_summary = self.detect_anomalies()\n",
    "            if anomaly_summary is None:\n",
    "                print(\"âŒ ì´ìƒ íŒ¨í„´ íƒì§€ ì‹¤íŒ¨\")\n",
    "                return False\n",
    "            \n",
    "            # 6. ì‹œê°í™” ìƒì„± (ê¸°ì¡´ ìœ ì§€)\n",
    "            self.create_summary_visualizations()\n",
    "            \n",
    "            # 7. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (ê¸°ì¡´ ìœ ì§€)\n",
    "            self.generate_comprehensive_report()\n",
    "            \n",
    "            # 8. ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ ìœ ì§€)\n",
    "            self.save_analysis_results()\n",
    "            \n",
    "            # ë‚˜ë¨¸ì§€ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€\n",
    "            end_time = datetime.now()\n",
    "            duration = end_time - start_time\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ‰ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"ì†Œìš” ì‹œê°„: {duration}\")\n",
    "            print(f\"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸\")\n",
    "    print(\"2ë‹¨ê³„: ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ìµœì í™”ëœ ë°ì´í„° ë¡œë”©)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer()\n",
    "    \n",
    "    # ì „ì²´ ë¶„ì„ ì‹¤í–‰\n",
    "    success = analyzer.run_complete_analysis()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nğŸ¯ 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!\")\n",
    "        print(\"   ğŸ“ ìƒì„± íŒŒì¼:\")\n",
    "        print(\"      - volatility_summary.csv (3ë‹¨ê³„ ì…ë ¥ìš©)\")\n",
    "        print(\"      - analysis_results.json (ë©”íƒ€ë°ì´í„°)\")\n",
    "        print(\"   â±ï¸ ì²˜ë¦¬ ì‹œê°„: 30ë¶„ â†’ 3-5ë¶„ìœ¼ë¡œ ë‹¨ì¶•!\")\n",
    "        print(\"   ğŸš€ ë‹¤ìŒ: 3ë‹¨ê³„ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰\")\n",
    "    else:\n",
    "        print(\"\\nâŒ 2ë‹¨ê³„ ë¶„ì„ ì‹¤íŒ¨\")\n",
    "        print(\"ğŸ’¡ í•´ê²°ë°©ë²•: 1ë‹¨ê³„ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
