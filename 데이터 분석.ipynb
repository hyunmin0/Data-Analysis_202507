{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í°íŠ¸ ì„¤ì • (ì—ëŸ¬ ë°©ì§€)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KepcoDataPreprocessor:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì „ì²˜ë¦¬ ë° íƒìƒ‰ì  ë¶„ì„\n",
    "    \n",
    "    ë‹¨ê³„ë³„ ë¶„ì„ ê³„íš:\n",
    "    1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„)\n",
    "    2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) \n",
    "    3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„)\n",
    "    4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„)\n",
    "    5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_quality_report = {}\n",
    "        self.pattern_analysis = {}\n",
    "        self.variability_analysis = {}\n",
    "    \n",
    "    # ============ ë°ì´í„° ë¡œë”© ë° ê²°í•© ============\n",
    "    \n",
    "    def load_and_combine_lp_data(self, lp_files):\n",
    "        \"\"\"\n",
    "        LP ë°ì´í„° íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ê²°í•©\n",
    "        íŒŒì¼ë“¤ì€ í•œ ë‹¬ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì œê³µë¨ (ì˜ˆ: LPë°ì´í„°1.csv + LPë°ì´í„°2.csv)\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“‚ LP ë°ì´í„° íŒŒì¼ë“¤ ë¡œë”© ë° ê²°í•© ì¤‘...\")\n",
    "        \n",
    "        combined_data = []\n",
    "        \n",
    "        for i, file_path in enumerate(lp_files):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"  âœ… {file_path}: {len(df):,}ê±´ ë¡œë”©\")\n",
    "                \n",
    "                # ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "                if 'LPìˆ˜ì‹ ì¼ì' in df.columns:\n",
    "                    dates = pd.to_datetime(df['LPìˆ˜ì‹ ì¼ì'])\n",
    "                    print(f\"     ê¸°ê°„: {dates.min()} ~ {dates.max()}\")\n",
    "                    print(f\"     ê³ ê°ìˆ˜: {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "                \n",
    "                combined_data.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        if not combined_data:\n",
    "            raise ValueError(\"ë¡œë”©ëœ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ë°ì´í„° ê²°í•©\n",
    "        final_data = pd.concat(combined_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"  ğŸ”— ê²°í•© ì™„ë£Œ: ì´ {len(final_data):,}ê±´\")\n",
    "        print(f\"     ì „ì²´ ê¸°ê°„: {pd.to_datetime(final_data['LPìˆ˜ì‹ ì¼ì']).min()} ~ {pd.to_datetime(final_data['LPìˆ˜ì‹ ì¼ì']).max()}\")\n",
    "        print(f\"     ì´ ê³ ê°ìˆ˜: {final_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        \n",
    "        return final_data\n",
    "    \n",
    "    # ============ 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„) ============\n",
    "    \n",
    "    def check_data_quality(self, lp_data, customer_data):\n",
    "        \"\"\"\n",
    "        ë°ì´í„° í’ˆì§ˆ ì ê²€ ë° ê¸°ë³¸ ì •ë³´ ë¶„ì„\n",
    "        \"\"\"\n",
    "        print(\"ğŸ” 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ ì‹œì‘...\")\n",
    "        \n",
    "        # ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\n",
    "        customer_info = self._analyze_customer_info(customer_data)\n",
    "        \n",
    "        # LP ë°ì´í„° í’ˆì§ˆ ì ê²€\n",
    "        lp_quality = self._check_lp_data_quality(lp_data)\n",
    "        \n",
    "        self.data_quality_report = {\n",
    "            'customer_info': customer_info,\n",
    "            'lp_quality': lp_quality,\n",
    "            'data_completeness': self._calculate_completeness(lp_data),\n",
    "            'anomaly_detection': self._detect_data_anomalies(lp_data)\n",
    "        }\n",
    "        \n",
    "        self._print_quality_summary()\n",
    "        return self.data_quality_report\n",
    "    \n",
    "    def _analyze_customer_info(self, customer_data):\n",
    "        \"\"\"ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\"\"\"\n",
    "        if customer_data is None:\n",
    "            return {\"message\": \"ê³ ê° ë°ì´í„° ì—†ìŒ\"}\n",
    "        \n",
    "        print(f\"  ğŸ“‹ ê³ ê° ë°ì´í„° ì»¬ëŸ¼: {list(customer_data.columns)}\")\n",
    "        \n",
    "        # ê°€ëŠ¥í•œ ì»¬ëŸ¼ëª…ë“¤ ë§¤í•‘\n",
    "        column_mapping = {\n",
    "            'ê³„ì•½ì¢…ë³„': ['ê³„ì•½ì¢…ë³„', 'contract_type', 'Contract_Type'],\n",
    "            'ì‚¬ìš©ìš©ë„': ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose'], \n",
    "            'ì‚°ì—…ë¶„ë¥˜': ['ì‚°ì—…ë¶„ë¥˜', 'industry', 'Industry']\n",
    "        }\n",
    "        \n",
    "        info = {'total_customers': len(customer_data)}\n",
    "        \n",
    "        for key, possible_cols in column_mapping.items():\n",
    "            found_col = None\n",
    "            for col in possible_cols:\n",
    "                if col in customer_data.columns:\n",
    "                    found_col = col\n",
    "                    break\n",
    "            \n",
    "            if found_col:\n",
    "                info[f'{key}_dist'] = customer_data[found_col].value_counts().to_dict()\n",
    "                print(f\"  âœ… {key} ë¶„í¬: {dict(list(info[f'{key}_dist'].items())[:3])}...\")  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥\n",
    "            else:\n",
    "                info[f'{key}_dist'] = {}\n",
    "                print(f\"  âš ï¸ {key} ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "        \n",
    "        print(f\"âœ… ê³ ê°ìˆ˜: {info['total_customers']:,}ëª…\")\n",
    "        return info\n",
    "    \n",
    "    def _check_lp_data_quality(self, lp_data):\n",
    "        \"\"\"LP ë°ì´í„° í’ˆì§ˆ ì ê²€\"\"\"\n",
    "        # ë°ì´í„° íƒ€ì… ë³€í™˜\n",
    "        lp_data['LPìˆ˜ì‹ ì¼ì'] = pd.to_datetime(lp_data['LPìˆ˜ì‹ ì¼ì'])\n",
    "        \n",
    "        quality = {\n",
    "            'total_records': len(lp_data),\n",
    "            'date_range': {\n",
    "                'start': lp_data['LPìˆ˜ì‹ ì¼ì'].min(),\n",
    "                'end': lp_data['LPìˆ˜ì‹ ì¼ì'].max()\n",
    "            },\n",
    "            'unique_customers': lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "            'missing_values': lp_data.isnull().sum().to_dict(),\n",
    "            'negative_values': (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] < 0).sum(),\n",
    "            'zero_values': (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] == 0).sum(),\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… ì´ ë ˆì½”ë“œ: {quality['total_records']:,}ê±´\")\n",
    "        print(f\"âœ… ê¸°ê°„: {quality['date_range']['start']} ~ {quality['date_range']['end']}\")\n",
    "        print(f\"âœ… ê³ ê°ìˆ˜: {quality['unique_customers']:,}ëª…\")\n",
    "        print(f\"âœ… ìŒìˆ˜ê°’: {quality['negative_values']:,}ê±´\")\n",
    "        print(f\"âœ… 0ê°’: {quality['zero_values']:,}ê±´\")\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    def _calculate_completeness(self, lp_data):\n",
    "        \"\"\"ë°ì´í„° ì™„ì •ì„± ê³„ì‚°\"\"\"\n",
    "        lp_data['date'] = lp_data['LPìˆ˜ì‹ ì¼ì'].dt.date\n",
    "        lp_data['hour'] = lp_data['LPìˆ˜ì‹ ì¼ì'].dt.hour\n",
    "        lp_data['quarter_hour'] = (lp_data['LPìˆ˜ì‹ ì¼ì'].dt.minute // 15) * 15\n",
    "        \n",
    "        # 15ë¶„ ê°„ê²© ì •í™•ì„± ì²´í¬\n",
    "        expected_intervals = pd.date_range(\n",
    "            start=lp_data['LPìˆ˜ì‹ ì¼ì'].min(),\n",
    "            end=lp_data['LPìˆ˜ì‹ ì¼ì'].max(),\n",
    "            freq='15min'\n",
    "        )\n",
    "        \n",
    "        completeness = {\n",
    "            'expected_records': len(expected_intervals) * lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "            'actual_records': len(lp_data),\n",
    "            'completeness_rate': len(lp_data) / (len(expected_intervals) * lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()) * 100\n",
    "        }\n",
    "        \n",
    "        # ê³ ê°ë³„ ì™„ì •ì„±\n",
    "        customer_completeness = lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸').size() / len(expected_intervals) * 100\n",
    "        completeness['customer_completeness'] = {\n",
    "            'mean': customer_completeness.mean(),\n",
    "            'min': customer_completeness.min(),\n",
    "            'max': customer_completeness.max(),\n",
    "            'std': customer_completeness.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ì™„ì •ì„±: {completeness['completeness_rate']:.2f}%\")\n",
    "        \n",
    "        return completeness\n",
    "    \n",
    "    def _detect_data_anomalies(self, lp_data):\n",
    "        \"\"\"ë°ì´í„° ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        # í†µê³„ì  ì´ìƒì¹˜ íƒì§€\n",
    "        Q1 = lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].quantile(0.25)\n",
    "        Q3 = lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        outliers_iqr = lp_data[\n",
    "            (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] < Q1 - 1.5 * IQR) | \n",
    "            (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        # Z-score ì´ìƒì¹˜\n",
    "        z_scores = np.abs((lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] - lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()) / lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].std())\n",
    "        outliers_zscore = lp_data[z_scores > 3]\n",
    "        \n",
    "        anomalies = {\n",
    "            'iqr_outliers': len(outliers_iqr),\n",
    "            'zscore_outliers': len(outliers_zscore),\n",
    "            'outlier_rate_iqr': len(outliers_iqr) / len(lp_data) * 100,\n",
    "            'outlier_rate_zscore': len(outliers_zscore) / len(lp_data) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… IQR ì´ìƒì¹˜: {anomalies['outlier_rate_iqr']:.3f}%\")\n",
    "        print(f\"âœ… Z-score ì´ìƒì¹˜: {anomalies['outlier_rate_zscore']:.3f}%\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def _print_quality_summary(self):\n",
    "        \"\"\"í’ˆì§ˆ ì ê²€ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # ============ 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) ============\n",
    "    \n",
    "    def analyze_basic_patterns(self, lp_data, customer_data=None, calendar_data=None):\n",
    "        \"\"\"\n",
    "        ê¸°ë³¸ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“Š 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ ì‹œì‘...\")\n",
    "        \n",
    "        # ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        processed_data = self._preprocess_for_pattern_analysis(lp_data, customer_data, calendar_data)\n",
    "        \n",
    "        # ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„\n",
    "        time_patterns = self._analyze_time_patterns(processed_data)\n",
    "        \n",
    "        # ê³ ê° ì„¸ë¶„í™” ê¸°ì´ˆ ë¶„ì„\n",
    "        customer_segmentation = self._analyze_customer_segmentation(processed_data)\n",
    "        \n",
    "        self.pattern_analysis = {\n",
    "            'time_patterns': time_patterns,\n",
    "            'customer_segmentation': customer_segmentation,\n",
    "            'processed_data': processed_data\n",
    "        }\n",
    "        \n",
    "        return self.pattern_analysis\n",
    "    \n",
    "    def _preprocess_for_pattern_analysis(self, lp_data, customer_data, calendar_data):\n",
    "        \"\"\"íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        print(\"  ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì‹œê°„ ë³€ìˆ˜ ìƒì„±\n",
    "        lp_data['datetime'] = pd.to_datetime(lp_data['LPìˆ˜ì‹ ì¼ì'])\n",
    "        lp_data['date'] = lp_data['datetime'].dt.date\n",
    "        lp_data['hour'] = lp_data['datetime'].dt.hour\n",
    "        lp_data['weekday'] = lp_data['datetime'].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        lp_data['month'] = lp_data['datetime'].dt.month\n",
    "        lp_data['quarter'] = lp_data['datetime'].dt.quarter\n",
    "        lp_data['is_weekend'] = lp_data['weekday'].isin([5, 6])  # í† , ì¼\n",
    "        \n",
    "        # ì¼ê°„ ì§‘ê³„ ë°ì´í„° ìƒì„±\n",
    "        daily_agg = lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date']).agg({\n",
    "            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': ['sum', 'mean', 'max', 'min', 'std']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        daily_agg.columns = ['customer_id', 'date', 'daily_sum', 'daily_mean', 'daily_max', 'daily_min', 'daily_std']\n",
    "        \n",
    "        # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ë‹¤ì‹œ ìƒì„± (ì¼ê°„ ì§‘ê³„ í›„)\n",
    "        daily_agg['date_dt'] = pd.to_datetime(daily_agg['date'])\n",
    "        daily_agg['weekday'] = daily_agg['date_dt'].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        daily_agg['month'] = daily_agg['date_dt'].dt.month\n",
    "        daily_agg['quarter'] = daily_agg['date_dt'].dt.quarter\n",
    "        daily_agg['is_weekend'] = daily_agg['weekday'].isin([5, 6])  # í† , ì¼\n",
    "        \n",
    "        print(f\"  âœ… ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ: weekday, month, quarter, is_weekend\")\n",
    "        \n",
    "        # ê³ ê° ì •ë³´ ë³‘í•©\n",
    "        if customer_data is not None:\n",
    "            # ê³ ê° ë°ì´í„°ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸\n",
    "            customer_key_col = None\n",
    "            possible_keys = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'ê³ ê°ë²ˆí˜¸', 'customer_id', 'Customer_ID']\n",
    "            \n",
    "            for col in possible_keys:\n",
    "                if col in customer_data.columns:\n",
    "                    customer_key_col = col\n",
    "                    break\n",
    "            \n",
    "            if customer_key_col:\n",
    "                daily_agg = daily_agg.merge(customer_data, left_on='customer_id', right_on=customer_key_col, how='left')\n",
    "                print(f\"  âœ… ê³ ê° ì •ë³´ ë³‘í•© ì™„ë£Œ (í‚¤: {customer_key_col})\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ ê³ ê° ë°ì´í„° ë³‘í•© ì‹¤íŒ¨ - í‚¤ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(customer_data.columns)}\")\n",
    "                print(f\"  â„¹ï¸ ê³ ê° ì •ë³´ ì—†ì´ ë¶„ì„ ê³„ì†...\")\n",
    "        \n",
    "        # ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•©\n",
    "        if calendar_data is not None:\n",
    "            # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ ë° ë³€í™˜\n",
    "            date_col = None\n",
    "            possible_date_cols = ['date', 'ë‚ ì§œ', 'Date', 'DATE']\n",
    "            \n",
    "            for col in possible_date_cols:\n",
    "                if col in calendar_data.columns:\n",
    "                    date_col = col\n",
    "                    break\n",
    "            \n",
    "            if date_col:\n",
    "                # ê¸°ìƒ ë°ì´í„°ê°€ weather_daily_processed.csvì¸ ê²½ìš° ë‚ ì§œ í˜•ì‹ í™•ì¸\n",
    "                if 'year' in calendar_data.columns and 'month' in calendar_data.columns and 'day' in calendar_data.columns:\n",
    "                    # year, month, day ì»¬ëŸ¼ìœ¼ë¡œ ë‚ ì§œ ìƒì„±\n",
    "                    calendar_data['date_parsed'] = pd.to_datetime(calendar_data[['year', 'month', 'day']])\n",
    "                    calendar_data['date_for_merge'] = calendar_data['date_parsed'].dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on='date_for_merge', how='left')\n",
    "                    print(f\"  âœ… ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•© ì™„ë£Œ (year-month-day ê¸°ì¤€)\")\n",
    "                else:\n",
    "                    # ì¼ë°˜ì ì¸ date ì»¬ëŸ¼ ì‚¬ìš©\n",
    "                    calendar_data[date_col] = pd.to_datetime(calendar_data[date_col]).dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on=date_col, how='left')\n",
    "                    print(f\"  âœ… ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•© ì™„ë£Œ (í‚¤: {date_col})\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ ê¸°ìƒ/ë‹¬ë ¥ ë°ì´í„° ë³‘í•© ì‹¤íŒ¨ - ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(calendar_data.columns)}\")\n",
    "                print(f\"  â„¹ï¸ ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ì—†ì´ ë¶„ì„ ê³„ì†...\")\n",
    "        \n",
    "        print(f\"  âœ… ì¼ê°„ ì§‘ê³„ ë°ì´í„°: {len(daily_agg):,}ê±´\")\n",
    "        return daily_agg\n",
    "    \n",
    "    def _analyze_time_patterns(self, data):\n",
    "        \"\"\"ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"  ğŸ“ˆ ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. ì‹œê°„ëŒ€ë³„ íŒ¨í„´ (ì¼ê°„ ì§‘ê³„ ë°ì´í„°ì—ì„œëŠ” ì˜ë¯¸ ì—†ìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°)\n",
    "        # hourly_pattern = data.groupby('hour')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "        # patterns['hourly'] = hourly_pattern\n",
    "        \n",
    "        # 2. ìš”ì¼ë³„ íŒ¨í„´\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_pattern = data.groupby('weekday')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekday'] = weekday_pattern\n",
    "        \n",
    "        # 3. ì›”ë³„ íŒ¨í„´ (ê³„ì ˆì„±)\n",
    "        if 'month' in data.columns:\n",
    "            monthly_pattern = data.groupby('month')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['monthly'] = monthly_pattern\n",
    "        \n",
    "        # 4. ì£¼ì¤‘/ì£¼ë§ íŒ¨í„´\n",
    "        if 'is_weekend' in data.columns:\n",
    "            weekend_pattern = data.groupby('is_weekend')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekend'] = weekend_pattern\n",
    "        \n",
    "        # 5. ì—…ì¢…ë³„ íŒ¨í„´ (ê³ ê° ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            industry_pattern = data.groupby(usage_purpose_col)['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['industry'] = industry_pattern\n",
    "            print(f\"  âœ… {usage_purpose_col} ê¸°ì¤€ ì—…ì¢…ë³„ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ\")\n",
    "        \n",
    "        print(f\"  âœ… ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({len(patterns)}ê°œ íŒ¨í„´)\")\n",
    "        return patterns\n",
    "    \n",
    "    def _analyze_customer_segmentation(self, data):\n",
    "        \"\"\"ê³ ê° ì„¸ë¶„í™” ê¸°ì´ˆ ë¶„ì„\"\"\"\n",
    "        print(\"  ğŸ‘¥ ê³ ê° ì„¸ë¶„í™” ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        # ê³ ê°ë³„ í‰ê·  ì‚¬ìš©ëŸ‰ ê³„ì‚°\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë¶„ë¥˜\n",
    "        segmentation = {\n",
    "            'large_users': customer_avg.quantile(0.9),  # ìƒìœ„ 10%\n",
    "            'medium_users': customer_avg.quantile(0.5),  # ì¤‘ê°„ 50%\n",
    "            'small_users': customer_avg.quantile(0.1),   # í•˜ìœ„ 10%\n",
    "        }\n",
    "        \n",
    "        # ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ ë¶„í¬\n",
    "        customer_stats = {\n",
    "            'customer_count': len(customer_avg),\n",
    "            'usage_distribution': {\n",
    "                'mean': customer_avg.mean(),\n",
    "                'std': customer_avg.std(),\n",
    "                'min': customer_avg.min(),\n",
    "                'max': customer_avg.max(),\n",
    "                'q25': customer_avg.quantile(0.25),\n",
    "                'q50': customer_avg.quantile(0.50),\n",
    "                'q75': customer_avg.quantile(0.75)\n",
    "            },\n",
    "            'segmentation_thresholds': segmentation\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… {customer_stats['customer_count']:,}ëª… ê³ ê° ì„¸ë¶„í™” ì™„ë£Œ\")\n",
    "        return customer_stats\n",
    "    \n",
    "    # ============ 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„) ============\n",
    "    \n",
    "    def analyze_variability(self, processed_data):\n",
    "        \"\"\"\n",
    "        ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ - ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ê¸°ì´ˆ ì‘ì—…\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“ˆ 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ ì‹œì‘...\")\n",
    "        \n",
    "        # ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\n",
    "        basic_variability = self._calculate_basic_variability(processed_data)\n",
    "        \n",
    "        # ë³€ë™ì„± íŒ¨í„´ ë¶„ì„\n",
    "        variability_patterns = self._analyze_variability_patterns(processed_data)\n",
    "        \n",
    "        self.variability_analysis = {\n",
    "            'basic_variability': basic_variability,\n",
    "            'variability_patterns': variability_patterns\n",
    "        }\n",
    "        \n",
    "        return self.variability_analysis\n",
    "    \n",
    "    def _calculate_basic_variability(self, data):\n",
    "        \"\"\"ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        print(\"  ğŸ“Š ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì¤‘...\")\n",
    "        \n",
    "        variability_metrics = {}\n",
    "        \n",
    "        # ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "        customer_cv = data.groupby('customer_id').apply(\n",
    "            lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "        )\n",
    "        \n",
    "        # 1. ì¼ê°„ ë³€ë™ê³„ìˆ˜\n",
    "        variability_metrics['daily_cv'] = {\n",
    "            'mean': customer_cv.mean(),\n",
    "            'std': customer_cv.std(),\n",
    "            'distribution': customer_cv.describe()\n",
    "        }\n",
    "        \n",
    "        # 2. ì£¼ê°„ ë³€ë™ê³„ìˆ˜ (ì£¼ë³„ íŒ¨í„´ì˜ ì¼ê´€ì„±)\n",
    "        try:\n",
    "            # ì£¼ ë²ˆí˜¸ ìƒì„±\n",
    "            data_with_week = data.copy()\n",
    "            data_with_week['week'] = pd.to_datetime(data_with_week['date']).dt.isocalendar().week\n",
    "            \n",
    "            weekly_cv = data_with_week.groupby(['customer_id', 'week']).agg({\n",
    "                'daily_mean': ['mean', 'std']\n",
    "            }).reset_index()\n",
    "            weekly_cv.columns = ['customer_id', 'week', 'weekly_mean', 'weekly_std']\n",
    "            weekly_cv['weekly_cv'] = weekly_cv['weekly_std'] / weekly_cv['weekly_mean']\n",
    "            \n",
    "            customer_weekly_cv = weekly_cv.groupby('customer_id')['weekly_cv'].mean()\n",
    "            variability_metrics['weekly_cv'] = {\n",
    "                'mean': customer_weekly_cv.mean(),\n",
    "                'std': customer_weekly_cv.std(),\n",
    "                'distribution': customer_weekly_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ ì£¼ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "            variability_metrics['weekly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 3. ì›”ê°„ ë³€ë™ê³„ìˆ˜\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                monthly_cv = data.groupby(['customer_id', 'month']).agg({\n",
    "                    'daily_mean': ['mean', 'std']\n",
    "                }).reset_index()\n",
    "                monthly_cv.columns = ['customer_id', 'month', 'monthly_mean', 'monthly_std']\n",
    "                monthly_cv['monthly_cv'] = monthly_cv['monthly_std'] / monthly_cv['monthly_mean']\n",
    "                \n",
    "                customer_monthly_cv = monthly_cv.groupby('customer_id')['monthly_cv'].mean()\n",
    "                variability_metrics['monthly_cv'] = {\n",
    "                    'mean': customer_monthly_cv.mean(),\n",
    "                    'std': customer_monthly_cv.std(),\n",
    "                    'distribution': customer_monthly_cv.describe()\n",
    "                }\n",
    "            else:\n",
    "                print(\"    âš ï¸ month ì»¬ëŸ¼ì´ ì—†ì–´ ì›”ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ê±´ë„ˆë›°ê¸°\")\n",
    "                variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ ì›”ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "            variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 4. ì¶”ê°€ ë³€ë™ì„± ì§€í‘œ\n",
    "        # ë²”ìœ„ ê¸°ë°˜ ë³€ë™ì„±\n",
    "        try:\n",
    "            customer_range_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: (x['daily_mean'].max() - x['daily_mean'].min()) / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            variability_metrics['range_based_cv'] = {\n",
    "                'mean': customer_range_cv.mean(),\n",
    "                'std': customer_range_cv.std(),\n",
    "                'distribution': customer_range_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ ë²”ìœ„ ê¸°ë°˜ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "            variability_metrics['range_based_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        print(f\"  âœ… ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ\")\n",
    "        return variability_metrics\n",
    "    \n",
    "    def _analyze_variability_patterns(self, data):\n",
    "        \"\"\"ë³€ë™ì„± íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"  ğŸ” ë³€ë™ì„± íŒ¨í„´ ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. ì—…ì¢…ë³„ ë³€ë™ì„± ë¹„êµ\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            try:\n",
    "                industry_variability = data.groupby([usage_purpose_col, 'customer_id']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                industry_variability.columns = [usage_purpose_col, 'customer_id', 'cv']\n",
    "                \n",
    "                industry_cv_summary = industry_variability.groupby(usage_purpose_col)['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['industry_variability'] = industry_cv_summary\n",
    "                print(f\"  âœ… {usage_purpose_col} ê¸°ì¤€ ì—…ì¢…ë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ ì—…ì¢…ë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        # 2. ê³„ì ˆë³„ ë³€ë™ì„± ì°¨ì´\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                seasonal_variability = data.groupby(['customer_id', 'month']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                seasonal_variability.columns = ['customer_id', 'month', 'cv']\n",
    "                \n",
    "                seasonal_cv_summary = seasonal_variability.groupby('month')['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['seasonal_variability'] = seasonal_cv_summary\n",
    "                print(f\"  âœ… ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ\")\n",
    "            else:\n",
    "                print(\"  âš ï¸ month ì»¬ëŸ¼ì´ ì—†ì–´ ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ê±´ë„ˆë›°ê¸°\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        # 3. ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„±\n",
    "        try:\n",
    "            customer_avg_usage = data.groupby('customer_id')['daily_mean'].mean()\n",
    "            customer_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ê·¸ë£¹í•‘\n",
    "            usage_quantiles = customer_avg_usage.quantile([0.33, 0.67])\n",
    "            def categorize_usage(usage):\n",
    "                if usage <= usage_quantiles.iloc[0]:\n",
    "                    return 'Low'\n",
    "                elif usage <= usage_quantiles.iloc[1]:\n",
    "                    return 'Medium'\n",
    "                else:\n",
    "                    return 'High'\n",
    "            \n",
    "            customer_usage_category = customer_avg_usage.apply(categorize_usage)\n",
    "            usage_cv_df = pd.DataFrame({\n",
    "                'usage_category': customer_usage_category,\n",
    "                'cv': customer_cv\n",
    "            })\n",
    "            \n",
    "            usage_cv_summary = usage_cv_df.groupby('usage_category')['cv'].agg(['mean', 'std', 'count'])\n",
    "            patterns['usage_level_variability'] = usage_cv_summary\n",
    "            print(f\"  âœ… ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        print(f\"  âœ… ë³€ë™ì„± íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({len(patterns)}ê°œ íŒ¨í„´)\")\n",
    "        return patterns\n",
    "    \n",
    "    # ============ 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„) ============\n",
    "    \n",
    "    def detect_anomalous_patterns(self, processed_data):\n",
    "        \"\"\"\n",
    "        ì´ìƒ íŒ¨í„´ íƒì§€\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ¯ 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ ì‹œì‘...\")\n",
    "        \n",
    "        # í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„\n",
    "        statistical_outliers = self._identify_statistical_outliers(processed_data)\n",
    "        \n",
    "        # ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\n",
    "        temporal_anomalies = self._detect_temporal_anomalies(processed_data)\n",
    "        \n",
    "        # ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜\n",
    "        abnormal_patterns = self._define_abnormal_patterns(processed_data)\n",
    "        \n",
    "        anomaly_results = {\n",
    "            'statistical_outliers': statistical_outliers,\n",
    "            'temporal_anomalies': temporal_anomalies,\n",
    "            'abnormal_patterns': abnormal_patterns\n",
    "        }\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def _identify_statistical_outliers(self, data):\n",
    "        \"\"\"í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„\"\"\"\n",
    "        print(\"  ğŸ” í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„ ì¤‘...\")\n",
    "        \n",
    "        outliers = {}\n",
    "        \n",
    "        # IQR ë°©ë²•\n",
    "        Q1 = data['daily_mean'].quantile(0.25)\n",
    "        Q3 = data['daily_mean'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        iqr_outliers = data[\n",
    "            (data['daily_mean'] < Q1 - 1.5 * IQR) | \n",
    "            (data['daily_mean'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        outliers['iqr_outliers'] = {\n",
    "            'count': len(iqr_outliers),\n",
    "            'rate': len(iqr_outliers) / len(data) * 100,\n",
    "            'customer_count': iqr_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        # Z-score ë°©ë²•\n",
    "        z_scores = np.abs((data['daily_mean'] - data['daily_mean'].mean()) / data['daily_mean'].std())\n",
    "        zscore_outliers = data[z_scores > 3]\n",
    "        \n",
    "        outliers['zscore_outliers'] = {\n",
    "            'count': len(zscore_outliers),\n",
    "            'rate': len(zscore_outliers) / len(data) * 100,\n",
    "            'customer_count': zscore_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… IQR ì´ìƒì¹˜: {outliers['iqr_outliers']['count']:,}ê±´ ({outliers['iqr_outliers']['rate']:.2f}%)\")\n",
    "        print(f\"  âœ… Z-score ì´ìƒì¹˜: {outliers['zscore_outliers']['count']:,}ê±´ ({outliers['zscore_outliers']['rate']:.2f}%)\")\n",
    "        \n",
    "        return outliers\n",
    "    \n",
    "    def _detect_temporal_anomalies(self, data):\n",
    "        \"\"\"ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        print(\"  â° ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€ ì¤‘...\")\n",
    "        \n",
    "        temporal_anomalies = {}\n",
    "        \n",
    "        # ê³ ê°ë³„ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\n",
    "        for customer_id in data['customer_id'].unique()[:100]:  # ìƒ˜í”Œë¡œ 100ëª…ë§Œ\n",
    "            customer_data = data[data['customer_id'] == customer_id].sort_values('date')\n",
    "            \n",
    "            if len(customer_data) < 30:  # ìµœì†Œ 30ì¼ ë°ì´í„° í•„ìš”\n",
    "                continue\n",
    "            \n",
    "            # ê¸‰ê²©í•œ ì¦ê°€/ê°ì†Œ íƒì§€ (>200% ë³€í™”)\n",
    "            customer_data['pct_change'] = customer_data['daily_mean'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['pct_change']) > 2.0]  # 200% ë³€í™”\n",
    "            \n",
    "            # ì—°ì†ì ì¸ 0ê°’ íƒì§€\n",
    "            zero_streaks = customer_data[customer_data['daily_mean'] == 0]\n",
    "            \n",
    "            if len(sudden_changes) > 0 or len(zero_streaks) > 5:  # 5ì¼ ì´ìƒ ì—°ì† 0ê°’\n",
    "                temporal_anomalies[customer_id] = {\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'zero_streaks': len(zero_streaks)\n",
    "                }\n",
    "        \n",
    "        print(f\"  âœ… {len(temporal_anomalies):,}ëª… ê³ ê°ì—ì„œ ì‹œê³„ì—´ ì´ìƒ íƒì§€\")\n",
    "        \n",
    "        return temporal_anomalies\n",
    "    \n",
    "    def _define_abnormal_patterns(self, data):\n",
    "        \"\"\"ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜\"\"\"\n",
    "        print(\"  ğŸ“‹ ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜ ì¤‘...\")\n",
    "        \n",
    "        abnormal_patterns = {\n",
    "            'pattern_definitions': {\n",
    "                1: 'ì „ë ¥ ì‚¬ìš© ê¸‰ì¦/ê¸‰ê° (ì‚¬ì—… í™•ì¥/ì¶•ì†Œ)',\n",
    "                2: 'ì‚¬ìš© íŒ¨í„´ ë³€í™” (ìš´ì˜ì‹œê°„ ë³€ê²½)', \n",
    "                3: 'íš¨ìœ¨ì„± ê¸‰ë³€ (ì„¤ë¹„ êµì²´/ê³ ì¥)',\n",
    "                4: 'ê³„ì ˆì„± ì´íƒˆ (ì‚¬ì—… ëª¨ë¸ ë³€í™”)'\n",
    "            },\n",
    "            'detection_criteria': {\n",
    "                'usage_spike': 'daily_mean > mean + 3*std',\n",
    "                'usage_drop': 'daily_mean < mean - 3*std',\n",
    "                'pattern_shift': 'monthly pattern change > 50%',\n",
    "                'efficiency_change': 'weekly efficiency variance > threshold'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… {len(abnormal_patterns['pattern_definitions'])}ê°€ì§€ ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜ ì™„ë£Œ\")\n",
    "        \n",
    "        return abnormal_patterns\n",
    "    \n",
    "    # ============ 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„) ============\n",
    "    \n",
    "    def decide_preprocessing_strategy(self, data_quality_report, pattern_analysis, variability_analysis):\n",
    "        \"\"\"\n",
    "        ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì •\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ”§ 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì •...\")\n",
    "        \n",
    "        preprocessing_strategy = {\n",
    "            'missing_data_handling': self._decide_missing_data_strategy(data_quality_report),\n",
    "            'outlier_handling': self._decide_outlier_strategy(data_quality_report),\n",
    "            'normalization_method': self._decide_normalization_strategy(pattern_analysis),\n",
    "            'feature_engineering': self._decide_feature_engineering(pattern_analysis, variability_analysis)\n",
    "        }\n",
    "        \n",
    "        self._print_preprocessing_summary(preprocessing_strategy)\n",
    "        \n",
    "        return preprocessing_strategy\n",
    "    \n",
    "    def _decide_missing_data_strategy(self, quality_report):\n",
    "        \"\"\"ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ ê²°ì •\"\"\"\n",
    "        completeness_rate = quality_report['data_completeness']['completeness_rate']\n",
    "        \n",
    "        if completeness_rate > 95:\n",
    "            strategy = \"ì„ í˜•ë³´ê°„ ë˜ëŠ” forward fill\"\n",
    "        elif completeness_rate > 80:\n",
    "            strategy = \"ê³„ì ˆì„± ê³ ë ¤ ë³´ê°„\"\n",
    "        else:\n",
    "            strategy = \"ì¥ê¸° ê²°ì¸¡ ê¸°ê°„ ë¶„ì„ ì œì™¸\"\n",
    "        \n",
    "        return {\n",
    "            'completeness_rate': completeness_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_outlier_strategy(self, quality_report):\n",
    "        \"\"\"ì´ìƒì¹˜ ì²˜ë¦¬ ì „ëµ ê²°ì •\"\"\"\n",
    "        outlier_rate = quality_report['anomaly_detection']['outlier_rate_iqr']\n",
    "        \n",
    "        if outlier_rate < 1:\n",
    "            strategy = \"ì´ìƒì¹˜ ìœ ì§€ (ì •ìƒ ë²”ìœ„)\"\n",
    "        elif outlier_rate < 5:\n",
    "            strategy = \"extreme outlierë§Œ ì œê±°\"\n",
    "        else:\n",
    "            strategy = \"robust í†µê³„ëŸ‰ ì‚¬ìš©\"\n",
    "        \n",
    "        return {\n",
    "            'outlier_rate': outlier_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_normalization_strategy(self, pattern_analysis):\n",
    "        \"\"\"ì •ê·œí™” ë°©ë²• ê²°ì •\"\"\"\n",
    "        customer_stats = pattern_analysis['customer_segmentation']\n",
    "        usage_std = customer_stats['usage_distribution']['std']\n",
    "        usage_mean = customer_stats['usage_distribution']['mean']\n",
    "        cv = usage_std / usage_mean if usage_mean > 0 else 0\n",
    "        \n",
    "        if cv > 1.0:\n",
    "            strategy = \"ê³ ê°ë³„ í‘œì¤€í™” + ë¡œê·¸ ë³€í™˜\"\n",
    "        elif cv > 0.5:\n",
    "            strategy = \"ê³ ê°ë³„ í‘œì¤€í™”\"\n",
    "        else:\n",
    "            strategy = \"ì „ì²´ Min-Max ì •ê·œí™”\"\n",
    "        \n",
    "        return {\n",
    "            'coefficient_of_variation': cv,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_feature_engineering(self, pattern_analysis, variability_analysis):\n",
    "        \"\"\"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ ê²°ì •\"\"\"\n",
    "        features_to_create = []\n",
    "        \n",
    "        # ì‹œê°„ ê¸°ë°˜ í”¼ì²˜\n",
    "        time_patterns = pattern_analysis['time_patterns']\n",
    "        if 'monthly' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'month_sin', 'month_cos',  # ê³„ì ˆ ìˆœí™˜ í”¼ì²˜\n",
    "                'is_summer', 'is_winter',  # ê³„ì ˆ ë”ë¯¸ ë³€ìˆ˜\n",
    "            ])\n",
    "        \n",
    "        # ìš”ì¼ ê¸°ë°˜ í”¼ì²˜\n",
    "        if 'weekday' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'weekday_sin', 'weekday_cos',  # ìš”ì¼ ìˆœí™˜ í”¼ì²˜\n",
    "                'is_weekend'                   # ì£¼ë§ ì—¬ë¶€\n",
    "            ])\n",
    "        \n",
    "        # ë³€ë™ì„± ê¸°ë°˜ í”¼ì²˜\n",
    "        if variability_analysis:\n",
    "            features_to_create.extend([\n",
    "                'rolling_mean_7d',       # 7ì¼ ì´ë™í‰ê· \n",
    "                'rolling_std_7d',        # 7ì¼ ì´ë™í‘œì¤€í¸ì°¨\n",
    "                'usage_volatility',      # ë³€ë™ì„± ì§€ìˆ˜\n",
    "            ])\n",
    "        \n",
    "        # ê³ ê° ê¸°ë°˜ í”¼ì²˜\n",
    "        features_to_create.extend([\n",
    "            'customer_avg_usage',      # ê³ ê° í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "            'customer_usage_rank',     # ê³ ê° ì‚¬ìš©ëŸ‰ ìˆœìœ„\n",
    "            'deviation_from_avg'       # í‰ê·  ëŒ€ë¹„ í¸ì°¨\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'features_to_create': features_to_create,\n",
    "            'total_features': len(features_to_create)\n",
    "        }\n",
    "    \n",
    "    def _print_preprocessing_summary(self, strategy):\n",
    "        \"\"\"ì „ì²˜ë¦¬ ì „ëµ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ”§ ì „ì²˜ë¦¬ ì „ëµ ê²°ì • ì™„ë£Œ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"ğŸ“‹ ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {strategy['missing_data_handling']['recommended_strategy']}\")\n",
    "        print(f\"ğŸ¯ ì´ìƒì¹˜ ì²˜ë¦¬: {strategy['outlier_handling']['recommended_strategy']}\")\n",
    "        print(f\"ğŸ“Š ì •ê·œí™” ë°©ë²•: {strategy['normalization_method']['recommended_strategy']}\")\n",
    "        print(f\"ğŸ› ï¸ ìƒì„±í•  í”¼ì²˜: {strategy['feature_engineering']['total_features']}ê°œ\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"- ì–´ë–¤ ë³€ë™ì„± ì§€í‘œê°€ ì‹¤ì œ ì‚¬ì—… ë³€í™”ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ”ê°€?\")\n",
    "        print(\"- ì—…ì¢…ë³„ë¡œ ë‹¤ë¥¸ ì„ê³„ê°’ì´ í•„ìš”í•œê°€?\")\n",
    "        print(\"- ì‹œê°„ ìœˆë„ìš°ëŠ” ì–¼ë§ˆë‚˜ ì„¤ì •í•´ì•¼ í•˜ëŠ”ê°€?\")\n",
    "        print(\"- ê³„ì ˆì„± ë³´ì •ì´ í•„ìš”í•œê°€?\")\n",
    "    \n",
    "    # ============ ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„± ============\n",
    "    \n",
    "    def create_eda_visualizations(self, processed_data):\n",
    "        \"\"\"íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì‹œê°í™”\"\"\"\n",
    "        print(\"\\nğŸ“ˆ EDA ì‹œê°í™” ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # 1. ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´ (ì‹œê°„ëŒ€ë³„ ëŒ€ì‹ )\n",
    "        self._plot_weekday_patterns_daily(processed_data)\n",
    "        \n",
    "        # 2. ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´ (ë°” ì°¨íŠ¸)\n",
    "        self._plot_weekday_patterns(processed_data)\n",
    "        \n",
    "        # 3. ì›”ë³„ ì‚¬ìš©ëŸ‰ ë°•ìŠ¤í”Œë¡¯\n",
    "        self._plot_monthly_boxplot(processed_data)\n",
    "        \n",
    "        # 4. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ ë¶„í¬\n",
    "        self._plot_customer_distribution(processed_data)\n",
    "        \n",
    "        print(\"âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    def _plot_weekday_patterns_daily(self, data):\n",
    "        \"\"\"ìš”ì¼ë³„ í‰ê·  ì‚¬ìš© íŒ¨í„´ (ë¼ì¸ ì°¨íŠ¸)\"\"\"\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(7), weekday_avg.values, marker='o', linewidth=2, markersize=8)\n",
    "            plt.title('Daily Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Weekday')\n",
    "            plt.ylabel('Average Usage (kWh)')\n",
    "            plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"  âš ï¸ ìš”ì¼ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - weekday ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "    \n",
    "    def _plot_weekday_patterns(self, data):\n",
    "        \"\"\"ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´\"\"\"\n",
    "        if 'weekday' not in data.columns:\n",
    "            print(\"  âš ï¸ ìš”ì¼ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - weekday ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "            return\n",
    "            \n",
    "        weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(range(7), weekday_avg.values, color=['skyblue' if i < 5 else 'lightcoral' for i in range(7)])\n",
    "        plt.title('Average Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.ylabel('Average Usage (kWh)')\n",
    "        plt.xticks(range(7), weekday_names)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ì£¼ì¤‘/ì£¼ë§ êµ¬ë¶„ í‘œì‹œ\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i >= 5:  # ì£¼ë§\n",
    "                bar.set_label('Weekend' if i == 5 else '')\n",
    "            else:  # ì£¼ì¤‘\n",
    "                bar.set_label('Weekday' if i == 0 else '')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_monthly_boxplot(self, data):\n",
    "        \"\"\"ì›”ë³„ ì‚¬ìš©ëŸ‰ ë°•ìŠ¤í”Œë¡¯\"\"\"\n",
    "        if 'month' not in data.columns:\n",
    "            print(\"  âš ï¸ ì›”ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - month ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # ì›”ë³„ ë°ì´í„° ì¤€ë¹„\n",
    "        monthly_data = [data[data['month'] == m]['daily_mean'].values for m in range(1, 13)]\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        \n",
    "        # ë°ì´í„°ê°€ ìˆëŠ” ì›”ë§Œ í‘œì‹œ\n",
    "        valid_months = []\n",
    "        valid_data = []\n",
    "        valid_names = []\n",
    "        \n",
    "        for i, month_data in enumerate(monthly_data):\n",
    "            if len(month_data) > 0:\n",
    "                valid_months.append(i + 1)\n",
    "                valid_data.append(month_data)\n",
    "                valid_names.append(month_names[i])\n",
    "        \n",
    "        if not valid_data:\n",
    "            print(\"  âš ï¸ ì›”ë³„ ë°ì´í„° ì—†ìŒ\")\n",
    "            return\n",
    "        \n",
    "        box_plot = plt.boxplot(valid_data, labels=valid_names, patch_artist=True)\n",
    "        \n",
    "        # ê³„ì ˆë³„ ìƒ‰ìƒ êµ¬ë¶„\n",
    "        colors = []\n",
    "        for month in valid_months:\n",
    "            if month in [12, 1, 2]:  # ê²¨ìš¸\n",
    "                colors.append('lightblue')\n",
    "            elif month in [3, 4, 5]:  # ë´„\n",
    "                colors.append('lightgreen')\n",
    "            elif month in [6, 7, 8]:  # ì—¬ë¦„\n",
    "                colors.append('lightcoral')\n",
    "            else:  # ê°€ì„\n",
    "                colors.append('orange')\n",
    "        \n",
    "        for patch, color in zip(box_plot['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        plt.title('Monthly Power Usage Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Daily Average Usage (kWh)')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_customer_distribution(self, data):\n",
    "        \"\"\"ê³ ê°ë³„ í‰ê·  ì‚¬ìš©ëŸ‰ ë¶„í¬\"\"\"\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # íˆìŠ¤í† ê·¸ë¨\n",
    "        ax1.hist(customer_avg.values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax1.set_title('Customer Average Usage Distribution', fontweight='bold')\n",
    "        ax1.set_xlabel('Average Usage (kWh)')\n",
    "        ax1.set_ylabel('Number of Customers')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ë°•ìŠ¤í”Œë¡¯\n",
    "        ax2.boxplot(customer_avg.values, vert=True)\n",
    "        ax2.set_title('Customer Average Usage Boxplot', fontweight='bold')\n",
    "        ax2.set_ylabel('Average Usage (kWh)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_eda_report(self):\n",
    "        \"\"\"EDA ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KEPCO LP Data Preprocessing and EDA Report\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if hasattr(self, 'data_quality_report'):\n",
    "            print(\"\\nğŸ” Step 1: Data Quality Check Results\")\n",
    "            print(\"-\" * 40)\n",
    "            quality = self.data_quality_report\n",
    "            print(f\"Total Records: {quality['lp_quality']['total_records']:,}\")\n",
    "            print(f\"Customers: {quality['lp_quality']['unique_customers']:,}\")\n",
    "            print(f\"Data Completeness: {quality['data_completeness']['completeness_rate']:.2f}%\")\n",
    "            print(f\"Outlier Rate: {quality['anomaly_detection']['outlier_rate_iqr']:.3f}%\")\n",
    "        \n",
    "        if hasattr(self, 'pattern_analysis'):\n",
    "            print(\"\\nğŸ“Š Step 2: Basic Pattern Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            pattern = self.pattern_analysis\n",
    "            if 'customer_segmentation' in pattern:\n",
    "                seg = pattern['customer_segmentation']\n",
    "                print(f\"Analyzed Customers: {seg['customer_count']:,}\")\n",
    "                print(f\"Average Usage: {seg['usage_distribution']['mean']:.2f} kWh\")\n",
    "                print(f\"Usage Std Dev: {seg['usage_distribution']['std']:.2f} kWh\")\n",
    "        \n",
    "        if hasattr(self, 'variability_analysis'):\n",
    "            print(\"\\nğŸ“ˆ Step 3: Variability Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            var = self.variability_analysis\n",
    "            if 'basic_variability' in var:\n",
    "                basic = var['basic_variability']\n",
    "                print(f\"Average Daily CV: {basic['daily_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['weekly_cv']['mean']):\n",
    "                    print(f\"Average Weekly CV: {basic['weekly_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['monthly_cv']['mean']):\n",
    "                    print(f\"Average Monthly CV: {basic['monthly_cv']['mean']:.4f}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ Next Steps Recommendations:\")\n",
    "        print(\"- Define and design variability coefficient\")\n",
    "        print(\"- Implement stacking ensemble model\") \n",
    "        print(\"- Apply overfitting prevention techniques\")\n",
    "        print(\"- Develop business activity change prediction algorithm\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ - LP ë°ì´í„° íŒŒì¼ë“¤ì„ ê²°í•©í•˜ì—¬ ë¶„ì„\n",
    "\n",
    "# ë°ì´í„° ë¡œë”© (ì—¬ëŸ¬ LP íŒŒì¼ë“¤ ê²°í•©)\n",
    "lp_files = ['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv']  # í•œ ë‹¬ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆˆ íŒŒì¼ë“¤\n",
    "customer_data = pd.read_excel('ê³ ê°ë²ˆí˜¸.xlsx')\n",
    "weather_data = pd.read_csv('weather_daily_processed.csv') \n",
    "calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° EDA ì‹¤í–‰\n",
    "preprocessor = KepcoDataPreprocessor()\n",
    "\n",
    "# LP ë°ì´í„° ê²°í•©\n",
    "combined_lp_data = preprocessor.load_and_combine_lp_data(lp_files)\n",
    "\n",
    "# 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„)\n",
    "quality_report = preprocessor.check_data_quality(combined_lp_data, customer_data)\n",
    "\n",
    "# 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) - ê¸°ìƒ ë°ì´í„°ë„ í•¨ê»˜ ë³‘í•©!\n",
    "pattern_analysis = preprocessor.analyze_basic_patterns(combined_lp_data, customer_data, weather_data)\n",
    "\n",
    "# 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„)\n",
    "variability_analysis = preprocessor.analyze_variability(pattern_analysis['processed_data'])\n",
    "\n",
    "# 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„)\n",
    "anomaly_results = preprocessor.detect_anomalous_patterns(pattern_analysis['processed_data'])\n",
    "\n",
    "# 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„)\n",
    "preprocessing_strategy = preprocessor.decide_preprocessing_strategy(\n",
    "    quality_report, pattern_analysis, variability_analysis\n",
    ")\n",
    "\n",
    "# ì‹œê°í™” ìƒì„± (í°íŠ¸ ì˜¤ë¥˜ ì—†ì´)\n",
    "preprocessor.create_eda_visualizations(pattern_analysis['processed_data'])\n",
    "\n",
    "# ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\n",
    "preprocessor.generate_eda_report()\n",
    "\n",
    "print(\"âœ… Complete LP data preprocessing and EDA finished!\")\n",
    "print(\"ğŸ“¤ Next: Define variability coefficient and implement stacking model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
