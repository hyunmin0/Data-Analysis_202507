{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í°íŠ¸ ì„¤ì • (ì—ëŸ¬ ë°©ì§€)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KepcoDataPreprocessor:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì „ì²˜ë¦¬ ë° íƒìƒ‰ì  ë¶„ì„\n",
    "    \n",
    "    ë‹¨ê³„ë³„ ë¶„ì„ ê³„íš:\n",
    "    1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„)\n",
    "    2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) \n",
    "    3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„)\n",
    "    4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„)\n",
    "    5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_quality_report = {}\n",
    "        self.pattern_analysis = {}\n",
    "        self.variability_analysis = {}\n",
    "    \n",
    "    # ============ ë°ì´í„° ë¡œë”© ë° ê²°í•© ============\n",
    "    \n",
    "    def load_and_combine_lp_data(self, lp_files):\n",
    "        \"\"\"\n",
    "        LP ë°ì´í„° íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ê²°í•©\n",
    "        íŒŒì¼ë“¤ì€ í•œ ë‹¬ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì œê³µë¨ (ì˜ˆ: LPë°ì´í„°1.csv + LPë°ì´í„°2.csv)\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“‚ LP ë°ì´í„° íŒŒì¼ë“¤ ë¡œë”© ë° ê²°í•© ì¤‘...\")\n",
    "        \n",
    "        combined_data = []\n",
    "        \n",
    "        for i, file_path in enumerate(lp_files):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"  âœ… {file_path}: {len(df):,}ê±´ ë¡œë”©\")\n",
    "                \n",
    "                # ê¸°ë³¸ ì •ë³´ ì¶œë ¥\n",
    "                if 'LPìˆ˜ì‹ ì¼ì' in df.columns:\n",
    "                    dates = pd.to_datetime(df['LPìˆ˜ì‹ ì¼ì'])\n",
    "                    print(f\"     ê¸°ê°„: {dates.min()} ~ {dates.max()}\")\n",
    "                    print(f\"     ê³ ê°ìˆ˜: {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "                \n",
    "                combined_data.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        if not combined_data:\n",
    "            raise ValueError(\"ë¡œë”©ëœ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ë°ì´í„° ê²°í•©\n",
    "        final_data = pd.concat(combined_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"  ğŸ”— ê²°í•© ì™„ë£Œ: ì´ {len(final_data):,}ê±´\")\n",
    "        print(f\"     ì „ì²´ ê¸°ê°„: {pd.to_datetime(final_data['LPìˆ˜ì‹ ì¼ì']).min()} ~ {pd.to_datetime(final_data['LPìˆ˜ì‹ ì¼ì']).max()}\")\n",
    "        print(f\"     ì´ ê³ ê°ìˆ˜: {final_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        \n",
    "        return final_data\n",
    "    \n",
    "    # ============ 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„) ============\n",
    "    \n",
    "    def check_data_quality(self, lp_data, customer_data):\n",
    "        \"\"\"\n",
    "        ë°ì´í„° í’ˆì§ˆ ì ê²€ ë° ê¸°ë³¸ ì •ë³´ ë¶„ì„\n",
    "        \"\"\"\n",
    "        print(\"ğŸ” 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ ì‹œì‘...\")\n",
    "        \n",
    "        # ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\n",
    "        customer_info = self._analyze_customer_info(customer_data)\n",
    "        \n",
    "        # LP ë°ì´í„° í’ˆì§ˆ ì ê²€\n",
    "        lp_quality = self._check_lp_data_quality(lp_data)\n",
    "        \n",
    "        self.data_quality_report = {\n",
    "            'customer_info': customer_info,\n",
    "            'lp_quality': lp_quality,\n",
    "            'data_completeness': self._calculate_completeness(lp_data),\n",
    "            'anomaly_detection': self._detect_data_anomalies(lp_data)\n",
    "        }\n",
    "        \n",
    "        self._print_quality_summary()\n",
    "        return self.data_quality_report\n",
    "    \n",
    "    def _analyze_customer_info(self, customer_data):\n",
    "        \"\"\"ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\"\"\"\n",
    "        if customer_data is None:\n",
    "            return {\"message\": \"ê³ ê° ë°ì´í„° ì—†ìŒ\"}\n",
    "        \n",
    "        print(f\"  ğŸ“‹ ê³ ê° ë°ì´í„° ì»¬ëŸ¼: {list(customer_data.columns)}\")\n",
    "        \n",
    "        # ê°€ëŠ¥í•œ ì»¬ëŸ¼ëª…ë“¤ ë§¤í•‘\n",
    "        column_mapping = {\n",
    "            'ê³„ì•½ì¢…ë³„': ['ê³„ì•½ì¢…ë³„', 'contract_type', 'Contract_Type'],\n",
    "            'ì‚¬ìš©ìš©ë„': ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose'], \n",
    "            'ì‚°ì—…ë¶„ë¥˜': ['ì‚°ì—…ë¶„ë¥˜', 'industry', 'Industry']\n",
    "        }\n",
    "        \n",
    "        info = {'total_customers': len(customer_data)}\n",
    "        \n",
    "        for key, possible_cols in column_mapping.items():\n",
    "            found_col = None\n",
    "            for col in possible_cols:\n",
    "                if col in customer_data.columns:\n",
    "                    found_col = col\n",
    "                    break\n",
    "            \n",
    "            if found_col:\n",
    "                info[f'{key}_dist'] = customer_data[found_col].value_counts().to_dict()\n",
    "                print(f\"  âœ… {key} ë¶„í¬: {dict(list(info[f'{key}_dist'].items())[:3])}...\")  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥\n",
    "            else:\n",
    "                info[f'{key}_dist'] = {}\n",
    "                print(f\"  âš ï¸ {key} ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "        \n",
    "        print(f\"âœ… ê³ ê°ìˆ˜: {info['total_customers']:,}ëª…\")\n",
    "        return info\n",
    "    \n",
    "    def _check_lp_data_quality(self, lp_data):\n",
    "        \"\"\"LP ë°ì´í„° í’ˆì§ˆ ì ê²€\"\"\"\n",
    "        # ë°ì´í„° íƒ€ì… ë³€í™˜\n",
    "        lp_data['LPìˆ˜ì‹ ì¼ì'] = pd.to_datetime(lp_data['LPìˆ˜ì‹ ì¼ì'])\n",
    "        \n",
    "        quality = {\n",
    "            'total_records': len(lp_data),\n",
    "            'date_range': {\n",
    "                'start': lp_data['LPìˆ˜ì‹ ì¼ì'].min(),\n",
    "                'end': lp_data['LPìˆ˜ì‹ ì¼ì'].max()\n",
    "            },\n",
    "            'unique_customers': lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "            'missing_values': lp_data.isnull().sum().to_dict(),\n",
    "            'negative_values': (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] < 0).sum(),\n",
    "            'zero_values': (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] == 0).sum(),\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… ì´ ë ˆì½”ë“œ: {quality['total_records']:,}ê±´\")\n",
    "        print(f\"âœ… ê¸°ê°„: {quality['date_range']['start']} ~ {quality['date_range']['end']}\")\n",
    "        print(f\"âœ… ê³ ê°ìˆ˜: {quality['unique_customers']:,}ëª…\")\n",
    "        print(f\"âœ… ìŒìˆ˜ê°’: {quality['negative_values']:,}ê±´\")\n",
    "        print(f\"âœ… 0ê°’: {quality['zero_values']:,}ê±´\")\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    def _calculate_completeness(self, lp_data):\n",
    "        \"\"\"ë°ì´í„° ì™„ì •ì„± ê³„ì‚°\"\"\"\n",
    "        lp_data['date'] = lp_data['LPìˆ˜ì‹ ì¼ì'].dt.date\n",
    "        lp_data['hour'] = lp_data['LPìˆ˜ì‹ ì¼ì'].dt.hour\n",
    "        lp_data['quarter_hour'] = (lp_data['LPìˆ˜ì‹ ì¼ì'].dt.minute // 15) * 15\n",
    "        \n",
    "        # 15ë¶„ ê°„ê²© ì •í™•ì„± ì²´í¬\n",
    "        expected_intervals = pd.date_range(\n",
    "            start=lp_data['LPìˆ˜ì‹ ì¼ì'].min(),\n",
    "            end=lp_data['LPìˆ˜ì‹ ì¼ì'].max(),\n",
    "            freq='15min'\n",
    "        )\n",
    "        \n",
    "        completeness = {\n",
    "            'expected_records': len(expected_intervals) * lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "            'actual_records': len(lp_data),\n",
    "            'completeness_rate': len(lp_data) / (len(expected_intervals) * lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()) * 100\n",
    "        }\n",
    "        \n",
    "        # ê³ ê°ë³„ ì™„ì •ì„±\n",
    "        customer_completeness = lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸').size() / len(expected_intervals) * 100\n",
    "        completeness['customer_completeness'] = {\n",
    "            'mean': customer_completeness.mean(),\n",
    "            'min': customer_completeness.min(),\n",
    "            'max': customer_completeness.max(),\n",
    "            'std': customer_completeness.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ì™„ì •ì„±: {completeness['completeness_rate']:.2f}%\")\n",
    "        \n",
    "        return completeness\n",
    "    \n",
    "    def _detect_data_anomalies(self, lp_data):\n",
    "        \"\"\"ë°ì´í„° ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        # í†µê³„ì  ì´ìƒì¹˜ íƒì§€\n",
    "        Q1 = lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].quantile(0.25)\n",
    "        Q3 = lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        outliers_iqr = lp_data[\n",
    "            (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] < Q1 - 1.5 * IQR) | \n",
    "            (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        # Z-score ì´ìƒì¹˜\n",
    "        z_scores = np.abs((lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] - lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()) / lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].std())\n",
    "        outliers_zscore = lp_data[z_scores > 3]\n",
    "        \n",
    "        anomalies = {\n",
    "            'iqr_outliers': len(outliers_iqr),\n",
    "            'zscore_outliers': len(outliers_zscore),\n",
    "            'outlier_rate_iqr': len(outliers_iqr) / len(lp_data) * 100,\n",
    "            'outlier_rate_zscore': len(outliers_zscore) / len(lp_data) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… IQR ì´ìƒì¹˜: {anomalies['outlier_rate_iqr']:.3f}%\")\n",
    "        print(f\"âœ… Z-score ì´ìƒì¹˜: {anomalies['outlier_rate_zscore']:.3f}%\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def _print_quality_summary(self):\n",
    "        \"\"\"í’ˆì§ˆ ì ê²€ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # ============ 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) ============\n",
    "    \n",
    "    def analyze_basic_patterns(self, lp_data, customer_data=None, calendar_data=None):\n",
    "        \"\"\"\n",
    "        ê¸°ë³¸ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“Š 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ ì‹œì‘...\")\n",
    "        \n",
    "        # ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        processed_data = self._preprocess_for_pattern_analysis(lp_data, customer_data, calendar_data)\n",
    "        \n",
    "        # ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„\n",
    "        time_patterns = self._analyze_time_patterns(processed_data)\n",
    "        \n",
    "        # ê³ ê° ì„¸ë¶„í™” ê¸°ì´ˆ ë¶„ì„\n",
    "        customer_segmentation = self._analyze_customer_segmentation(processed_data)\n",
    "        \n",
    "        self.pattern_analysis = {\n",
    "            'time_patterns': time_patterns,\n",
    "            'customer_segmentation': customer_segmentation,\n",
    "            'processed_data': processed_data\n",
    "        }\n",
    "        \n",
    "        return self.pattern_analysis\n",
    "    \n",
    "    def _preprocess_for_pattern_analysis(self, lp_data, customer_data, calendar_data):\n",
    "        \"\"\"íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        print(\"  ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì‹œê°„ ë³€ìˆ˜ ìƒì„±\n",
    "        lp_data['datetime'] = pd.to_datetime(lp_data['LPìˆ˜ì‹ ì¼ì'])\n",
    "        lp_data['date'] = lp_data['datetime'].dt.date\n",
    "        lp_data['hour'] = lp_data['datetime'].dt.hour\n",
    "        lp_data['weekday'] = lp_data['datetime'].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        lp_data['month'] = lp_data['datetime'].dt.month\n",
    "        lp_data['quarter'] = lp_data['datetime'].dt.quarter\n",
    "        lp_data['is_weekend'] = lp_data['weekday'].isin([5, 6])  # í† , ì¼\n",
    "        \n",
    "        # ì¼ê°„ ì§‘ê³„ ë°ì´í„° ìƒì„±\n",
    "        daily_agg = lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date']).agg({\n",
    "            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': ['sum', 'mean', 'max', 'min', 'std']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        daily_agg.columns = ['customer_id', 'date', 'daily_sum', 'daily_mean', 'daily_max', 'daily_min', 'daily_std']\n",
    "        \n",
    "        # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ë‹¤ì‹œ ìƒì„± (ì¼ê°„ ì§‘ê³„ í›„)\n",
    "        daily_agg['date_dt'] = pd.to_datetime(daily_agg['date'])\n",
    "        daily_agg['weekday'] = daily_agg['date_dt'].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        daily_agg['month'] = daily_agg['date_dt'].dt.month\n",
    "        daily_agg['quarter'] = daily_agg['date_dt'].dt.quarter\n",
    "        daily_agg['is_weekend'] = daily_agg['weekday'].isin([5, 6])  # í† , ì¼\n",
    "        \n",
    "        print(f\"  âœ… ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ: weekday, month, quarter, is_weekend\")\n",
    "        \n",
    "        # ê³ ê° ì •ë³´ ë³‘í•©\n",
    "        if customer_data is not None:\n",
    "            # ê³ ê° ë°ì´í„°ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸\n",
    "            customer_key_col = None\n",
    "            possible_keys = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'ê³ ê°ë²ˆí˜¸', 'customer_id', 'Customer_ID']\n",
    "            \n",
    "            for col in possible_keys:\n",
    "                if col in customer_data.columns:\n",
    "                    customer_key_col = col\n",
    "                    break\n",
    "            \n",
    "            if customer_key_col:\n",
    "                daily_agg = daily_agg.merge(customer_data, left_on='customer_id', right_on=customer_key_col, how='left')\n",
    "                print(f\"  âœ… ê³ ê° ì •ë³´ ë³‘í•© ì™„ë£Œ (í‚¤: {customer_key_col})\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ ê³ ê° ë°ì´í„° ë³‘í•© ì‹¤íŒ¨ - í‚¤ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(customer_data.columns)}\")\n",
    "                print(f\"  â„¹ï¸ ê³ ê° ì •ë³´ ì—†ì´ ë¶„ì„ ê³„ì†...\")\n",
    "        \n",
    "        # ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•©\n",
    "        if calendar_data is not None:\n",
    "            # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ ë° ë³€í™˜\n",
    "            date_col = None\n",
    "            possible_date_cols = ['date', 'ë‚ ì§œ', 'Date', 'DATE']\n",
    "            \n",
    "            for col in possible_date_cols:\n",
    "                if col in calendar_data.columns:\n",
    "                    date_col = col\n",
    "                    break\n",
    "            \n",
    "            if date_col:\n",
    "                # ê¸°ìƒ ë°ì´í„°ê°€ weather_daily_processed.csvì¸ ê²½ìš° ë‚ ì§œ í˜•ì‹ í™•ì¸\n",
    "                if 'year' in calendar_data.columns and 'month' in calendar_data.columns and 'day' in calendar_data.columns:\n",
    "                    # year, month, day ì»¬ëŸ¼ìœ¼ë¡œ ë‚ ì§œ ìƒì„±\n",
    "                    calendar_data['date_parsed'] = pd.to_datetime(calendar_data[['year', 'month', 'day']])\n",
    "                    calendar_data['date_for_merge'] = calendar_data['date_parsed'].dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on='date_for_merge', how='left')\n",
    "                    print(f\"  âœ… ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•© ì™„ë£Œ (year-month-day ê¸°ì¤€)\")\n",
    "                else:\n",
    "                    # ì¼ë°˜ì ì¸ date ì»¬ëŸ¼ ì‚¬ìš©\n",
    "                    calendar_data[date_col] = pd.to_datetime(calendar_data[date_col]).dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on=date_col, how='left')\n",
    "                    print(f\"  âœ… ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•© ì™„ë£Œ (í‚¤: {date_col})\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ ê¸°ìƒ/ë‹¬ë ¥ ë°ì´í„° ë³‘í•© ì‹¤íŒ¨ - ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(calendar_data.columns)}\")\n",
    "                print(f\"  â„¹ï¸ ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ì—†ì´ ë¶„ì„ ê³„ì†...\")\n",
    "        \n",
    "        print(f\"  âœ… ì¼ê°„ ì§‘ê³„ ë°ì´í„°: {len(daily_agg):,}ê±´\")\n",
    "        return daily_agg\n",
    "    \n",
    "    def _analyze_time_patterns(self, data):\n",
    "        \"\"\"ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"  ğŸ“ˆ ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. ì‹œê°„ëŒ€ë³„ íŒ¨í„´ (ì¼ê°„ ì§‘ê³„ ë°ì´í„°ì—ì„œëŠ” ì˜ë¯¸ ì—†ìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°)\n",
    "        # hourly_pattern = data.groupby('hour')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "        # patterns['hourly'] = hourly_pattern\n",
    "        \n",
    "        # 2. ìš”ì¼ë³„ íŒ¨í„´\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_pattern = data.groupby('weekday')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekday'] = weekday_pattern\n",
    "        \n",
    "        # 3. ì›”ë³„ íŒ¨í„´ (ê³„ì ˆì„±)\n",
    "        if 'month' in data.columns:\n",
    "            monthly_pattern = data.groupby('month')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['monthly'] = monthly_pattern\n",
    "        \n",
    "        # 4. ì£¼ì¤‘/ì£¼ë§ íŒ¨í„´\n",
    "        if 'is_weekend' in data.columns:\n",
    "            weekend_pattern = data.groupby('is_weekend')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekend'] = weekend_pattern\n",
    "        \n",
    "        # 5. ì—…ì¢…ë³„ íŒ¨í„´ (ê³ ê° ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            industry_pattern = data.groupby(usage_purpose_col)['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['industry'] = industry_pattern\n",
    "            print(f\"  âœ… {usage_purpose_col} ê¸°ì¤€ ì—…ì¢…ë³„ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ\")\n",
    "        \n",
    "        print(f\"  âœ… ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({len(patterns)}ê°œ íŒ¨í„´)\")\n",
    "        return patterns\n",
    "    \n",
    "    def _analyze_customer_segmentation(self, data):\n",
    "        \"\"\"ê³ ê° ì„¸ë¶„í™” ê¸°ì´ˆ ë¶„ì„\"\"\"\n",
    "        print(\"  ğŸ‘¥ ê³ ê° ì„¸ë¶„í™” ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        # ê³ ê°ë³„ í‰ê·  ì‚¬ìš©ëŸ‰ ê³„ì‚°\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë¶„ë¥˜\n",
    "        segmentation = {\n",
    "            'large_users': customer_avg.quantile(0.9),  # ìƒìœ„ 10%\n",
    "            'medium_users': customer_avg.quantile(0.5),  # ì¤‘ê°„ 50%\n",
    "            'small_users': customer_avg.quantile(0.1),   # í•˜ìœ„ 10%\n",
    "        }\n",
    "        \n",
    "        # ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ ë¶„í¬\n",
    "        customer_stats = {\n",
    "            'customer_count': len(customer_avg),\n",
    "            'usage_distribution': {\n",
    "                'mean': customer_avg.mean(),\n",
    "                'std': customer_avg.std(),\n",
    "                'min': customer_avg.min(),\n",
    "                'max': customer_avg.max(),\n",
    "                'q25': customer_avg.quantile(0.25),\n",
    "                'q50': customer_avg.quantile(0.50),\n",
    "                'q75': customer_avg.quantile(0.75)\n",
    "            },\n",
    "            'segmentation_thresholds': segmentation\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… {customer_stats['customer_count']:,}ëª… ê³ ê° ì„¸ë¶„í™” ì™„ë£Œ\")\n",
    "        return customer_stats\n",
    "    \n",
    "    # ============ 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„) ============\n",
    "    \n",
    "    def analyze_variability(self, processed_data):\n",
    "        \"\"\"\n",
    "        ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ - ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ê¸°ì´ˆ ì‘ì—…\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ“ˆ 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ ì‹œì‘...\")\n",
    "        \n",
    "        # ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\n",
    "        basic_variability = self._calculate_basic_variability(processed_data)\n",
    "        \n",
    "        # ë³€ë™ì„± íŒ¨í„´ ë¶„ì„\n",
    "        variability_patterns = self._analyze_variability_patterns(processed_data)\n",
    "        \n",
    "        self.variability_analysis = {\n",
    "            'basic_variability': basic_variability,\n",
    "            'variability_patterns': variability_patterns\n",
    "        }\n",
    "        \n",
    "        return self.variability_analysis\n",
    "    \n",
    "    def _calculate_basic_variability(self, data):\n",
    "        \"\"\"ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        print(\"  ğŸ“Š ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì¤‘...\")\n",
    "        \n",
    "        variability_metrics = {}\n",
    "        \n",
    "        # ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "        customer_cv = data.groupby('customer_id').apply(\n",
    "            lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "        )\n",
    "        \n",
    "        # 1. ì¼ê°„ ë³€ë™ê³„ìˆ˜\n",
    "        variability_metrics['daily_cv'] = {\n",
    "            'mean': customer_cv.mean(),\n",
    "            'std': customer_cv.std(),\n",
    "            'distribution': customer_cv.describe()\n",
    "        }\n",
    "        \n",
    "        # 2. ì£¼ê°„ ë³€ë™ê³„ìˆ˜ (ì£¼ë³„ íŒ¨í„´ì˜ ì¼ê´€ì„±)\n",
    "        try:\n",
    "            # ì£¼ ë²ˆí˜¸ ìƒì„±\n",
    "            data_with_week = data.copy()\n",
    "            data_with_week['week'] = pd.to_datetime(data_with_week['date']).dt.isocalendar().week\n",
    "            \n",
    "            weekly_cv = data_with_week.groupby(['customer_id', 'week']).agg({\n",
    "                'daily_mean': ['mean', 'std']\n",
    "            }).reset_index()\n",
    "            weekly_cv.columns = ['customer_id', 'week', 'weekly_mean', 'weekly_std']\n",
    "            weekly_cv['weekly_cv'] = weekly_cv['weekly_std'] / weekly_cv['weekly_mean']\n",
    "            \n",
    "            customer_weekly_cv = weekly_cv.groupby('customer_id')['weekly_cv'].mean()\n",
    "            variability_metrics['weekly_cv'] = {\n",
    "                'mean': customer_weekly_cv.mean(),\n",
    "                'std': customer_weekly_cv.std(),\n",
    "                'distribution': customer_weekly_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ ì£¼ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "            variability_metrics['weekly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 3. ì›”ê°„ ë³€ë™ê³„ìˆ˜\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                monthly_cv = data.groupby(['customer_id', 'month']).agg({\n",
    "                    'daily_mean': ['mean', 'std']\n",
    "                }).reset_index()\n",
    "                monthly_cv.columns = ['customer_id', 'month', 'monthly_mean', 'monthly_std']\n",
    "                monthly_cv['monthly_cv'] = monthly_cv['monthly_std'] / monthly_cv['monthly_mean']\n",
    "                \n",
    "                customer_monthly_cv = monthly_cv.groupby('customer_id')['monthly_cv'].mean()\n",
    "                variability_metrics['monthly_cv'] = {\n",
    "                    'mean': customer_monthly_cv.mean(),\n",
    "                    'std': customer_monthly_cv.std(),\n",
    "                    'distribution': customer_monthly_cv.describe()\n",
    "                }\n",
    "            else:\n",
    "                print(\"    âš ï¸ month ì»¬ëŸ¼ì´ ì—†ì–´ ì›”ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ê±´ë„ˆë›°ê¸°\")\n",
    "                variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ ì›”ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "            variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 4. ì¶”ê°€ ë³€ë™ì„± ì§€í‘œ\n",
    "        # ë²”ìœ„ ê¸°ë°˜ ë³€ë™ì„±\n",
    "        try:\n",
    "            customer_range_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: (x['daily_mean'].max() - x['daily_mean'].min()) / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            variability_metrics['range_based_cv'] = {\n",
    "                'mean': customer_range_cv.mean(),\n",
    "                'std': customer_range_cv.std(),\n",
    "                'distribution': customer_range_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸ ë²”ìœ„ ê¸°ë°˜ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}\")\n",
    "            variability_metrics['range_based_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        print(f\"  âœ… ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ\")\n",
    "        return variability_metrics\n",
    "    \n",
    "    def _analyze_variability_patterns(self, data):\n",
    "        \"\"\"ë³€ë™ì„± íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"  ğŸ” ë³€ë™ì„± íŒ¨í„´ ë¶„ì„ ì¤‘...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. ì—…ì¢…ë³„ ë³€ë™ì„± ë¹„êµ\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            try:\n",
    "                industry_variability = data.groupby([usage_purpose_col, 'customer_id']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                industry_variability.columns = [usage_purpose_col, 'customer_id', 'cv']\n",
    "                \n",
    "                industry_cv_summary = industry_variability.groupby(usage_purpose_col)['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['industry_variability'] = industry_cv_summary\n",
    "                print(f\"  âœ… {usage_purpose_col} ê¸°ì¤€ ì—…ì¢…ë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ ì—…ì¢…ë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        # 2. ê³„ì ˆë³„ ë³€ë™ì„± ì°¨ì´\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                seasonal_variability = data.groupby(['customer_id', 'month']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                seasonal_variability.columns = ['customer_id', 'month', 'cv']\n",
    "                \n",
    "                seasonal_cv_summary = seasonal_variability.groupby('month')['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['seasonal_variability'] = seasonal_cv_summary\n",
    "                print(f\"  âœ… ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ\")\n",
    "            else:\n",
    "                print(\"  âš ï¸ month ì»¬ëŸ¼ì´ ì—†ì–´ ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ê±´ë„ˆë›°ê¸°\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        # 3. ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„±\n",
    "        try:\n",
    "            customer_avg_usage = data.groupby('customer_id')['daily_mean'].mean()\n",
    "            customer_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ê·¸ë£¹í•‘\n",
    "            usage_quantiles = customer_avg_usage.quantile([0.33, 0.67])\n",
    "            def categorize_usage(usage):\n",
    "                if usage <= usage_quantiles.iloc[0]:\n",
    "                    return 'Low'\n",
    "                elif usage <= usage_quantiles.iloc[1]:\n",
    "                    return 'Medium'\n",
    "                else:\n",
    "                    return 'High'\n",
    "            \n",
    "            customer_usage_category = customer_avg_usage.apply(categorize_usage)\n",
    "            usage_cv_df = pd.DataFrame({\n",
    "                'usage_category': customer_usage_category,\n",
    "                'cv': customer_cv\n",
    "            })\n",
    "            \n",
    "            usage_cv_summary = usage_cv_df.groupby('usage_category')['cv'].agg(['mean', 'std', 'count'])\n",
    "            patterns['usage_level_variability'] = usage_cv_summary\n",
    "            print(f\"  âœ… ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        print(f\"  âœ… ë³€ë™ì„± íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({len(patterns)}ê°œ íŒ¨í„´)\")\n",
    "        return patterns\n",
    "    \n",
    "    # ============ 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„) ============\n",
    "    \n",
    "    def detect_anomalous_patterns(self, processed_data):\n",
    "        \"\"\"\n",
    "        ì´ìƒ íŒ¨í„´ íƒì§€\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ¯ 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ ì‹œì‘...\")\n",
    "        \n",
    "        # í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„\n",
    "        statistical_outliers = self._identify_statistical_outliers(processed_data)\n",
    "        \n",
    "        # ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\n",
    "        temporal_anomalies = self._detect_temporal_anomalies(processed_data)\n",
    "        \n",
    "        # ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜\n",
    "        abnormal_patterns = self._define_abnormal_patterns(processed_data)\n",
    "        \n",
    "        anomaly_results = {\n",
    "            'statistical_outliers': statistical_outliers,\n",
    "            'temporal_anomalies': temporal_anomalies,\n",
    "            'abnormal_patterns': abnormal_patterns\n",
    "        }\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def _identify_statistical_outliers(self, data):\n",
    "        \"\"\"í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„\"\"\"\n",
    "        print(\"  ğŸ” í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„ ì¤‘...\")\n",
    "        \n",
    "        outliers = {}\n",
    "        \n",
    "        # IQR ë°©ë²•\n",
    "        Q1 = data['daily_mean'].quantile(0.25)\n",
    "        Q3 = data['daily_mean'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        iqr_outliers = data[\n",
    "            (data['daily_mean'] < Q1 - 1.5 * IQR) | \n",
    "            (data['daily_mean'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        outliers['iqr_outliers'] = {\n",
    "            'count': len(iqr_outliers),\n",
    "            'rate': len(iqr_outliers) / len(data) * 100,\n",
    "            'customer_count': iqr_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        # Z-score ë°©ë²•\n",
    "        z_scores = np.abs((data['daily_mean'] - data['daily_mean'].mean()) / data['daily_mean'].std())\n",
    "        zscore_outliers = data[z_scores > 3]\n",
    "        \n",
    "        outliers['zscore_outliers'] = {\n",
    "            'count': len(zscore_outliers),\n",
    "            'rate': len(zscore_outliers) / len(data) * 100,\n",
    "            'customer_count': zscore_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… IQR ì´ìƒì¹˜: {outliers['iqr_outliers']['count']:,}ê±´ ({outliers['iqr_outliers']['rate']:.2f}%)\")\n",
    "        print(f\"  âœ… Z-score ì´ìƒì¹˜: {outliers['zscore_outliers']['count']:,}ê±´ ({outliers['zscore_outliers']['rate']:.2f}%)\")\n",
    "        \n",
    "        return outliers\n",
    "    \n",
    "    def _detect_temporal_anomalies(self, data):\n",
    "        \"\"\"ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        print(\"  â° ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€ ì¤‘...\")\n",
    "        \n",
    "        temporal_anomalies = {}\n",
    "        \n",
    "        # ê³ ê°ë³„ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\n",
    "        for customer_id in data['customer_id'].unique()[:100]:  # ìƒ˜í”Œë¡œ 100ëª…ë§Œ\n",
    "            customer_data = data[data['customer_id'] == customer_id].sort_values('date')\n",
    "            \n",
    "            if len(customer_data) < 30:  # ìµœì†Œ 30ì¼ ë°ì´í„° í•„ìš”\n",
    "                continue\n",
    "            \n",
    "            # ê¸‰ê²©í•œ ì¦ê°€/ê°ì†Œ íƒì§€ (>200% ë³€í™”)\n",
    "            customer_data['pct_change'] = customer_data['daily_mean'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['pct_change']) > 2.0]  # 200% ë³€í™”\n",
    "            \n",
    "            # ì—°ì†ì ì¸ 0ê°’ íƒì§€\n",
    "            zero_streaks = customer_data[customer_data['daily_mean'] == 0]\n",
    "            \n",
    "            if len(sudden_changes) > 0 or len(zero_streaks) > 5:  # 5ì¼ ì´ìƒ ì—°ì† 0ê°’\n",
    "                temporal_anomalies[customer_id] = {\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'zero_streaks': len(zero_streaks)\n",
    "                }\n",
    "        \n",
    "        print(f\"  âœ… {len(temporal_anomalies):,}ëª… ê³ ê°ì—ì„œ ì‹œê³„ì—´ ì´ìƒ íƒì§€\")\n",
    "        \n",
    "        return temporal_anomalies\n",
    "    \n",
    "    def _define_abnormal_patterns(self, data):\n",
    "        \"\"\"ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜\"\"\"\n",
    "        print(\"  ğŸ“‹ ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜ ì¤‘...\")\n",
    "        \n",
    "        abnormal_patterns = {\n",
    "            'pattern_definitions': {\n",
    "                1: 'ì „ë ¥ ì‚¬ìš© ê¸‰ì¦/ê¸‰ê° (ì‚¬ì—… í™•ì¥/ì¶•ì†Œ)',\n",
    "                2: 'ì‚¬ìš© íŒ¨í„´ ë³€í™” (ìš´ì˜ì‹œê°„ ë³€ê²½)', \n",
    "                3: 'íš¨ìœ¨ì„± ê¸‰ë³€ (ì„¤ë¹„ êµì²´/ê³ ì¥)',\n",
    "                4: 'ê³„ì ˆì„± ì´íƒˆ (ì‚¬ì—… ëª¨ë¸ ë³€í™”)'\n",
    "            },\n",
    "            'detection_criteria': {\n",
    "                'usage_spike': 'daily_mean > mean + 3*std',\n",
    "                'usage_drop': 'daily_mean < mean - 3*std',\n",
    "                'pattern_shift': 'monthly pattern change > 50%',\n",
    "                'efficiency_change': 'weekly efficiency variance > threshold'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ… {len(abnormal_patterns['pattern_definitions'])}ê°€ì§€ ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜ ì™„ë£Œ\")\n",
    "        \n",
    "        return abnormal_patterns\n",
    "    \n",
    "    # ============ 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„) ============\n",
    "    \n",
    "    def decide_preprocessing_strategy(self, data_quality_report, pattern_analysis, variability_analysis):\n",
    "        \"\"\"\n",
    "        ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì •\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ”§ 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì •...\")\n",
    "        \n",
    "        preprocessing_strategy = {\n",
    "            'missing_data_handling': self._decide_missing_data_strategy(data_quality_report),\n",
    "            'outlier_handling': self._decide_outlier_strategy(data_quality_report),\n",
    "            'normalization_method': self._decide_normalization_strategy(pattern_analysis),\n",
    "            'feature_engineering': self._decide_feature_engineering(pattern_analysis, variability_analysis)\n",
    "        }\n",
    "        \n",
    "        self._print_preprocessing_summary(preprocessing_strategy)\n",
    "        \n",
    "        return preprocessing_strategy\n",
    "    \n",
    "    def _decide_missing_data_strategy(self, quality_report):\n",
    "        \"\"\"ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ ê²°ì •\"\"\"\n",
    "        completeness_rate = quality_report['data_completeness']['completeness_rate']\n",
    "        \n",
    "        if completeness_rate > 95:\n",
    "            strategy = \"ì„ í˜•ë³´ê°„ ë˜ëŠ” forward fill\"\n",
    "        elif completeness_rate > 80:\n",
    "            strategy = \"ê³„ì ˆì„± ê³ ë ¤ ë³´ê°„\"\n",
    "        else:\n",
    "            strategy = \"ì¥ê¸° ê²°ì¸¡ ê¸°ê°„ ë¶„ì„ ì œì™¸\"\n",
    "        \n",
    "        return {\n",
    "            'completeness_rate': completeness_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_outlier_strategy(self, quality_report):\n",
    "        \"\"\"ì´ìƒì¹˜ ì²˜ë¦¬ ì „ëµ ê²°ì •\"\"\"\n",
    "        outlier_rate = quality_report['anomaly_detection']['outlier_rate_iqr']\n",
    "        \n",
    "        if outlier_rate < 1:\n",
    "            strategy = \"ì´ìƒì¹˜ ìœ ì§€ (ì •ìƒ ë²”ìœ„)\"\n",
    "        elif outlier_rate < 5:\n",
    "            strategy = \"extreme outlierë§Œ ì œê±°\"\n",
    "        else:\n",
    "            strategy = \"robust í†µê³„ëŸ‰ ì‚¬ìš©\"\n",
    "        \n",
    "        return {\n",
    "            'outlier_rate': outlier_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_normalization_strategy(self, pattern_analysis):\n",
    "        \"\"\"ì •ê·œí™” ë°©ë²• ê²°ì •\"\"\"\n",
    "        customer_stats = pattern_analysis['customer_segmentation']\n",
    "        usage_std = customer_stats['usage_distribution']['std']\n",
    "        usage_mean = customer_stats['usage_distribution']['mean']\n",
    "        cv = usage_std / usage_mean if usage_mean > 0 else 0\n",
    "        \n",
    "        if cv > 1.0:\n",
    "            strategy = \"ê³ ê°ë³„ í‘œì¤€í™” + ë¡œê·¸ ë³€í™˜\"\n",
    "        elif cv > 0.5:\n",
    "            strategy = \"ê³ ê°ë³„ í‘œì¤€í™”\"\n",
    "        else:\n",
    "            strategy = \"ì „ì²´ Min-Max ì •ê·œí™”\"\n",
    "        \n",
    "        return {\n",
    "            'coefficient_of_variation': cv,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_feature_engineering(self, pattern_analysis, variability_analysis):\n",
    "        \"\"\"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ ê²°ì •\"\"\"\n",
    "        features_to_create = []\n",
    "        \n",
    "        # ì‹œê°„ ê¸°ë°˜ í”¼ì²˜\n",
    "        time_patterns = pattern_analysis['time_patterns']\n",
    "        if 'monthly' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'month_sin', 'month_cos',  # ê³„ì ˆ ìˆœí™˜ í”¼ì²˜\n",
    "                'is_summer', 'is_winter',  # ê³„ì ˆ ë”ë¯¸ ë³€ìˆ˜\n",
    "            ])\n",
    "        \n",
    "        # ìš”ì¼ ê¸°ë°˜ í”¼ì²˜\n",
    "        if 'weekday' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'weekday_sin', 'weekday_cos',  # ìš”ì¼ ìˆœí™˜ í”¼ì²˜\n",
    "                'is_weekend'                   # ì£¼ë§ ì—¬ë¶€\n",
    "            ])\n",
    "        \n",
    "        # ë³€ë™ì„± ê¸°ë°˜ í”¼ì²˜\n",
    "        if variability_analysis:\n",
    "            features_to_create.extend([\n",
    "                'rolling_mean_7d',       # 7ì¼ ì´ë™í‰ê· \n",
    "                'rolling_std_7d',        # 7ì¼ ì´ë™í‘œì¤€í¸ì°¨\n",
    "                'usage_volatility',      # ë³€ë™ì„± ì§€ìˆ˜\n",
    "            ])\n",
    "        \n",
    "        # ê³ ê° ê¸°ë°˜ í”¼ì²˜\n",
    "        features_to_create.extend([\n",
    "            'customer_avg_usage',      # ê³ ê° í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "            'customer_usage_rank',     # ê³ ê° ì‚¬ìš©ëŸ‰ ìˆœìœ„\n",
    "            'deviation_from_avg'       # í‰ê·  ëŒ€ë¹„ í¸ì°¨\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'features_to_create': features_to_create,\n",
    "            'total_features': len(features_to_create)\n",
    "        }\n",
    "    \n",
    "    def _print_preprocessing_summary(self, strategy):\n",
    "        \"\"\"ì „ì²˜ë¦¬ ì „ëµ ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ”§ ì „ì²˜ë¦¬ ì „ëµ ê²°ì • ì™„ë£Œ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"ğŸ“‹ ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {strategy['missing_data_handling']['recommended_strategy']}\")\n",
    "        print(f\"ğŸ¯ ì´ìƒì¹˜ ì²˜ë¦¬: {strategy['outlier_handling']['recommended_strategy']}\")\n",
    "        print(f\"ğŸ“Š ì •ê·œí™” ë°©ë²•: {strategy['normalization_method']['recommended_strategy']}\")\n",
    "        print(f\"ğŸ› ï¸ ìƒì„±í•  í”¼ì²˜: {strategy['feature_engineering']['total_features']}ê°œ\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"- ì–´ë–¤ ë³€ë™ì„± ì§€í‘œê°€ ì‹¤ì œ ì‚¬ì—… ë³€í™”ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ”ê°€?\")\n",
    "        print(\"- ì—…ì¢…ë³„ë¡œ ë‹¤ë¥¸ ì„ê³„ê°’ì´ í•„ìš”í•œê°€?\")\n",
    "        print(\"- ì‹œê°„ ìœˆë„ìš°ëŠ” ì–¼ë§ˆë‚˜ ì„¤ì •í•´ì•¼ í•˜ëŠ”ê°€?\")\n",
    "        print(\"- ê³„ì ˆì„± ë³´ì •ì´ í•„ìš”í•œê°€?\")\n",
    "    \n",
    "    # ============ ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„± ============\n",
    "    \n",
    "    def create_eda_visualizations(self, processed_data):\n",
    "        \"\"\"íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì‹œê°í™”\"\"\"\n",
    "        print(\"\\nğŸ“ˆ EDA ì‹œê°í™” ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # 1. ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´ (ì‹œê°„ëŒ€ë³„ ëŒ€ì‹ )\n",
    "        self._plot_weekday_patterns_daily(processed_data)\n",
    "        \n",
    "        # 2. ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´ (ë°” ì°¨íŠ¸)\n",
    "        self._plot_weekday_patterns(processed_data)\n",
    "        \n",
    "        # 3. ì›”ë³„ ì‚¬ìš©ëŸ‰ ë°•ìŠ¤í”Œë¡¯\n",
    "        self._plot_monthly_boxplot(processed_data)\n",
    "        \n",
    "        # 4. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ ë¶„í¬\n",
    "        self._plot_customer_distribution(processed_data)\n",
    "        \n",
    "        print(\"âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    def _plot_weekday_patterns_daily(self, data):\n",
    "        \"\"\"ìš”ì¼ë³„ í‰ê·  ì‚¬ìš© íŒ¨í„´ (ë¼ì¸ ì°¨íŠ¸)\"\"\"\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(7), weekday_avg.values, marker='o', linewidth=2, markersize=8)\n",
    "            plt.title('Daily Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Weekday')\n",
    "            plt.ylabel('Average Usage (kWh)')\n",
    "            plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"  âš ï¸ ìš”ì¼ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - weekday ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "    \n",
    "    def _plot_weekday_patterns(self, data):\n",
    "        \"\"\"ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´\"\"\"\n",
    "        if 'weekday' not in data.columns:\n",
    "            print(\"  âš ï¸ ìš”ì¼ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - weekday ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "            return\n",
    "            \n",
    "        weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(range(7), weekday_avg.values, color=['skyblue' if i < 5 else 'lightcoral' for i in range(7)])\n",
    "        plt.title('Average Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.ylabel('Average Usage (kWh)')\n",
    "        plt.xticks(range(7), weekday_names)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ì£¼ì¤‘/ì£¼ë§ êµ¬ë¶„ í‘œì‹œ\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i >= 5:  # ì£¼ë§\n",
    "                bar.set_label('Weekend' if i == 5 else '')\n",
    "            else:  # ì£¼ì¤‘\n",
    "                bar.set_label('Weekday' if i == 0 else '')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_monthly_boxplot(self, data):\n",
    "        \"\"\"ì›”ë³„ ì‚¬ìš©ëŸ‰ ë°•ìŠ¤í”Œë¡¯\"\"\"\n",
    "        if 'month' not in data.columns:\n",
    "            print(\"  âš ï¸ ì›”ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - month ì»¬ëŸ¼ ì—†ìŒ\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # ì›”ë³„ ë°ì´í„° ì¤€ë¹„\n",
    "        monthly_data = [data[data['month'] == m]['daily_mean'].values for m in range(1, 13)]\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        \n",
    "        # ë°ì´í„°ê°€ ìˆëŠ” ì›”ë§Œ í‘œì‹œ\n",
    "        valid_months = []\n",
    "        valid_data = []\n",
    "        valid_names = []\n",
    "        \n",
    "        for i, month_data in enumerate(monthly_data):\n",
    "            if len(month_data) > 0:\n",
    "                valid_months.append(i + 1)\n",
    "                valid_data.append(month_data)\n",
    "                valid_names.append(month_names[i])\n",
    "        \n",
    "        if not valid_data:\n",
    "            print(\"  âš ï¸ ì›”ë³„ ë°ì´í„° ì—†ìŒ\")\n",
    "            return\n",
    "        \n",
    "        box_plot = plt.boxplot(valid_data, labels=valid_names, patch_artist=True)\n",
    "        \n",
    "        # ê³„ì ˆë³„ ìƒ‰ìƒ êµ¬ë¶„\n",
    "        colors = []\n",
    "        for month in valid_months:\n",
    "            if month in [12, 1, 2]:  # ê²¨ìš¸\n",
    "                colors.append('lightblue')\n",
    "            elif month in [3, 4, 5]:  # ë´„\n",
    "                colors.append('lightgreen')\n",
    "            elif month in [6, 7, 8]:  # ì—¬ë¦„\n",
    "                colors.append('lightcoral')\n",
    "            else:  # ê°€ì„\n",
    "                colors.append('orange')\n",
    "        \n",
    "        for patch, color in zip(box_plot['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        plt.title('Monthly Power Usage Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Daily Average Usage (kWh)')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_customer_distribution(self, data):\n",
    "        \"\"\"ê³ ê°ë³„ í‰ê·  ì‚¬ìš©ëŸ‰ ë¶„í¬\"\"\"\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # íˆìŠ¤í† ê·¸ë¨\n",
    "        ax1.hist(customer_avg.values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax1.set_title('Customer Average Usage Distribution', fontweight='bold')\n",
    "        ax1.set_xlabel('Average Usage (kWh)')\n",
    "        ax1.set_ylabel('Number of Customers')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ë°•ìŠ¤í”Œë¡¯\n",
    "        ax2.boxplot(customer_avg.values, vert=True)\n",
    "        ax2.set_title('Customer Average Usage Boxplot', fontweight='bold')\n",
    "        ax2.set_ylabel('Average Usage (kWh)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_eda_report(self):\n",
    "        \"\"\"EDA ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KEPCO LP Data Preprocessing and EDA Report\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if hasattr(self, 'data_quality_report'):\n",
    "            print(\"\\nğŸ” Step 1: Data Quality Check Results\")\n",
    "            print(\"-\" * 40)\n",
    "            quality = self.data_quality_report\n",
    "            print(f\"Total Records: {quality['lp_quality']['total_records']:,}\")\n",
    "            print(f\"Customers: {quality['lp_quality']['unique_customers']:,}\")\n",
    "            print(f\"Data Completeness: {quality['data_completeness']['completeness_rate']:.2f}%\")\n",
    "            print(f\"Outlier Rate: {quality['anomaly_detection']['outlier_rate_iqr']:.3f}%\")\n",
    "        \n",
    "        if hasattr(self, 'pattern_analysis'):\n",
    "            print(\"\\nğŸ“Š Step 2: Basic Pattern Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            pattern = self.pattern_analysis\n",
    "            if 'customer_segmentation' in pattern:\n",
    "                seg = pattern['customer_segmentation']\n",
    "                print(f\"Analyzed Customers: {seg['customer_count']:,}\")\n",
    "                print(f\"Average Usage: {seg['usage_distribution']['mean']:.2f} kWh\")\n",
    "                print(f\"Usage Std Dev: {seg['usage_distribution']['std']:.2f} kWh\")\n",
    "        \n",
    "        if hasattr(self, 'variability_analysis'):\n",
    "            print(\"\\nğŸ“ˆ Step 3: Variability Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            var = self.variability_analysis\n",
    "            if 'basic_variability' in var:\n",
    "                basic = var['basic_variability']\n",
    "                print(f\"Average Daily CV: {basic['daily_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['weekly_cv']['mean']):\n",
    "                    print(f\"Average Weekly CV: {basic['weekly_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['monthly_cv']['mean']):\n",
    "                    print(f\"Average Monthly CV: {basic['monthly_cv']['mean']:.4f}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ Next Steps Recommendations:\")\n",
    "        print(\"- Define and design variability coefficient\")\n",
    "        print(\"- Implement stacking ensemble model\") \n",
    "        print(\"- Apply overfitting prevention techniques\")\n",
    "        print(\"- Develop business activity change prediction algorithm\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ - LP ë°ì´í„° íŒŒì¼ë“¤ì„ ê²°í•©í•˜ì—¬ ë¶„ì„\n",
    "\n",
    "# ë°ì´í„° ë¡œë”© (ì—¬ëŸ¬ LP íŒŒì¼ë“¤ ê²°í•©)\n",
    "lp_files = ['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv']  # í•œ ë‹¬ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆˆ íŒŒì¼ë“¤\n",
    "customer_data = pd.read_excel('ê³ ê°ë²ˆí˜¸.xlsx')\n",
    "weather_data = pd.read_csv('weather_daily_processed.csv') \n",
    "calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° EDA ì‹¤í–‰\n",
    "preprocessor = KepcoDataPreprocessor()\n",
    "\n",
    "# LP ë°ì´í„° ê²°í•©\n",
    "combined_lp_data = preprocessor.load_and_combine_lp_data(lp_files)\n",
    "\n",
    "# 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„)\n",
    "quality_report = preprocessor.check_data_quality(combined_lp_data, customer_data)\n",
    "\n",
    "# 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) - ê¸°ìƒ ë°ì´í„°ë„ í•¨ê»˜ ë³‘í•©!\n",
    "pattern_analysis = preprocessor.analyze_basic_patterns(combined_lp_data, customer_data, weather_data)\n",
    "\n",
    "# 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„)\n",
    "variability_analysis = preprocessor.analyze_variability(pattern_analysis['processed_data'])\n",
    "\n",
    "# 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„)\n",
    "anomaly_results = preprocessor.detect_anomalous_patterns(pattern_analysis['processed_data'])\n",
    "\n",
    "# 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„)\n",
    "preprocessing_strategy = preprocessor.decide_preprocessing_strategy(\n",
    "    quality_report, pattern_analysis, variability_analysis\n",
    ")\n",
    "\n",
    "# ì‹œê°í™” ìƒì„± (í°íŠ¸ ì˜¤ë¥˜ ì—†ì´)\n",
    "preprocessor.create_eda_visualizations(pattern_analysis['processed_data'])\n",
    "\n",
    "# ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\n",
    "preprocessor.generate_eda_report()\n",
    "\n",
    "print(\"âœ… Complete LP data preprocessing and EDA finished!\")\n",
    "print(\"ğŸ“¤ Next: Define variability coefficient and implement stacking model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452490b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¤ë¥˜ ìˆ˜ì •ëœ ì‹¤í–‰ ì½”ë“œ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.weather_data = None\n",
    "        self.calendar_data = None\n",
    "        self.customer_data = None\n",
    "        self.combined_data = None\n",
    "        self.volatility_metrics = {}\n",
    "        \n",
    "    def load_real_data(self, lp_files=None, weather_file='weather_daily_processed.csv', \n",
    "                      calendar_file='power_analysis_calendar_2022_2025.csv', \n",
    "                      customer_file='ê³ ê°ë²ˆí˜¸.xlsx'):\n",
    "        \"\"\"ì‹¤ì œ íŒŒì¼ë“¤ì„ ì‚¬ìš©í•œ ë°ì´í„° ë¡œë”©\"\"\"\n",
    "        print(\"=== ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë¡œë”© ë° ì „ì²˜ë¦¬ ===\")\n",
    "        \n",
    "        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "        current_dir = os.getcwd()\n",
    "        print(f\"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}\")\n",
    "        \n",
    "        # 1. ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”©\n",
    "        try:\n",
    "            if os.path.exists(customer_file):\n",
    "                self.customer_data = pd.read_excel(customer_file)\n",
    "                print(f\"âœ… ê³ ê° ë°ì´í„° ë¡œë”©: {len(self.customer_data):,}ëª…\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ {customer_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "                self.customer_data = self._create_sample_customer_data()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê³ ê° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.customer_data = self._create_sample_customer_data()\n",
    "            \n",
    "        # 2. LP ë°ì´í„° ë¡œë”© (ì—¬ëŸ¬ íŒŒì¼ í†µí•©)\n",
    "        if lp_files:\n",
    "            lp_dataframes = []\n",
    "            for file in lp_files:\n",
    "                try:\n",
    "                    if os.path.exists(file):\n",
    "                        df = pd.read_csv(file)\n",
    "                        lp_dataframes.append(df)\n",
    "                        print(f\"âœ… LP ë°ì´í„° ë¡œë”©: {file} ({len(df):,}ê±´)\")\n",
    "                    else:\n",
    "                        print(f\"âš ï¸ {file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ LP ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {file} - {e}\")\n",
    "            \n",
    "            if lp_dataframes:\n",
    "                self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "                print(f\"âœ… ì „ì²´ LP ë°ì´í„°: {len(self.lp_data):,}ê±´\")\n",
    "            else:\n",
    "                print(\"âš ï¸ LP íŒŒì¼ ë¡œë”© ì‹¤íŒ¨, ìƒ˜í”Œ ë°ì´í„° ìƒì„±\")\n",
    "                self.lp_data = self._create_test_lp_data()\n",
    "        else:\n",
    "            print(\"âš ï¸ LP íŒŒì¼ ë¯¸ì§€ì •, ìƒ˜í”Œ ë°ì´í„° ìƒì„±\")\n",
    "            self.lp_data = self._create_test_lp_data()\n",
    "            \n",
    "        # 3. ë‚ ì”¨ ë°ì´í„° ë¡œë”©\n",
    "        try:\n",
    "            if os.path.exists(weather_file):\n",
    "                self.weather_data = pd.read_csv(weather_file, encoding='utf-8')\n",
    "                print(f\"âœ… ë‚ ì”¨ ë°ì´í„° ë¡œë”©: {len(self.weather_data):,}ì¼\")\n",
    "                print(f\"   ì»¬ëŸ¼: {list(self.weather_data.columns)[:5]}...\")  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ\n",
    "            else:\n",
    "                print(f\"âš ï¸ {weather_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "                self.weather_data = self._create_weather_test_data()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë‚ ì”¨ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.weather_data = self._create_weather_test_data()\n",
    "            \n",
    "        # 4. ë‹¬ë ¥ ë°ì´í„° ë¡œë”©\n",
    "        try:\n",
    "            if os.path.exists(calendar_file):\n",
    "                self.calendar_data = pd.read_csv(calendar_file, encoding='utf-8')\n",
    "                print(f\"âœ… ë‹¬ë ¥ ë°ì´í„° ë¡œë”©: {len(self.calendar_data):,}ì¼\")\n",
    "                print(f\"   ì»¬ëŸ¼: {list(self.calendar_data.columns)[:5]}...\")  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ\n",
    "            else:\n",
    "                print(f\"âš ï¸ {calendar_file} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "                self.calendar_data = self._create_calendar_test_data()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë‹¬ë ¥ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.calendar_data = self._create_calendar_test_data()\n",
    "            \n",
    "        # 5. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•©\n",
    "        self._preprocess_data()\n",
    "        self.combined_data = self._merge_all_data()\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ìµœì¢… í†µí•© ë°ì´í„°ì…‹: {len(self.combined_data):,}ê±´\")\n",
    "        return self.combined_data\n",
    "    \n",
    "    def _create_sample_customer_data(self):\n",
    "        \"\"\"ìƒ˜í”Œ ê³ ê° ë°ì´í„° ìƒì„±\"\"\"\n",
    "        customers = []\n",
    "        for i in range(10):  # 10ëª… ìƒ˜í”Œ\n",
    "            customers.append({\n",
    "                'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': f'A{1001+i}',\n",
    "                'ê³„ì•½ì „ë ¥': np.random.choice([100, 200, 300, 500, 700]),\n",
    "                'ê³„ì•½ì¢…ë³„': np.random.choice([222, 226, 311, 322, 726]),\n",
    "                'ì‚¬ìš©ìš©ë„': np.random.choice(['02', '09']),\n",
    "                'ì‚°ì—…ë¶„ë¥˜': f'ì—…ì¢…{np.random.randint(1, 6)}'\n",
    "            })\n",
    "        return pd.DataFrame(customers)\n",
    "    \n",
    "    def _create_test_lp_data(self):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© LP ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        for customer in customers:\n",
    "            # ê³ ê°ë³„ ê³ ìœ  íŠ¹ì„± (ë³€ë™ì„± íŒ¨í„´)\n",
    "            base_usage = np.random.uniform(30, 150)\n",
    "            volatility_level = np.random.uniform(0.1, 0.4)\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                \n",
    "                for hour in range(0, 24, 6):  # 6ì‹œê°„ ê°„ê²©\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´\n",
    "                        if 8 <= hour <= 18:  # ì—…ë¬´ì‹œê°„\n",
    "                            hour_factor = 1.2\n",
    "                        elif 19 <= hour <= 22:  # ì €ë…ì‹œê°„\n",
    "                            hour_factor = 0.8\n",
    "                        else:  # ì‹¬ì•¼ì‹œê°„\n",
    "                            hour_factor = 0.4\n",
    "                        \n",
    "                        # ìš”ì¼ë³„ íŒ¨í„´\n",
    "                        weekday = current_date.weekday()\n",
    "                        if weekday >= 5:  # ì£¼ë§\n",
    "                            weekday_factor = 0.6\n",
    "                        else:  # í‰ì¼\n",
    "                            weekday_factor = 1.0\n",
    "                        \n",
    "                        # ìµœì¢… ì „ë ¥ëŸ‰ ê³„ì‚°\n",
    "                        power = base_usage * hour_factor * weekday_factor\n",
    "                        power += np.random.normal(0, base_usage * volatility_level)\n",
    "                        power = max(5, power)  # ìµœì†Œê°’ ë³´ì¥\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(power * 0.2, 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(power * 0.1, 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(power * 1.05, 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _create_weather_test_data(self):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ë‚ ì”¨ ë°ì´í„° ìƒì„±\"\"\"\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        weather_data = []\n",
    "        for day in range(days):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            # 3ì›” ê¸°í›„ íŠ¹ì„±\n",
    "            base_temp = 12 + np.random.normal(0, 4)\n",
    "            temp_range = np.random.uniform(8, 15)\n",
    "            \n",
    "            ìµœê³ ê¸°ì˜¨ = base_temp + temp_range/2\n",
    "            ìµœì €ê¸°ì˜¨ = base_temp - temp_range/2\n",
    "            í‰ê· ê¸°ì˜¨ = (ìµœê³ ê¸°ì˜¨ + ìµœì €ê¸°ì˜¨) / 2\n",
    "            \n",
    "            # ìŠµë„ ê³„ì‚°\n",
    "            í‰ê· ìŠµë„ = max(30, min(90, 70 - (í‰ê· ê¸°ì˜¨ - 12) * 2 + np.random.normal(0, 10)))\n",
    "            \n",
    "            # ê°•ìˆ˜ëŸ‰\n",
    "            ê°•ìˆ˜ì—¬ë¶€ = 1 if np.random.random() < 0.1 else 0\n",
    "            ì´ê°•ìˆ˜ëŸ‰ = np.random.exponential(5) if ê°•ìˆ˜ì—¬ë¶€ else 0\n",
    "            \n",
    "            # ëƒ‰ë‚œë°©ë„ì¼\n",
    "            ëƒ‰ë°©ë„ì¼ = max(0, í‰ê· ê¸°ì˜¨ - 18)\n",
    "            ë‚œë°©ë„ì¼ = max(0, 18 - í‰ê· ê¸°ì˜¨)\n",
    "            \n",
    "            weather_data.append({\n",
    "                'ë‚ ì§œ': current_date.strftime('%Y-%m-%d'),\n",
    "                'í‰ê· ê¸°ì˜¨': round(í‰ê· ê¸°ì˜¨, 1),\n",
    "                'í‰ê· ìŠµë„': round(í‰ê· ìŠµë„),\n",
    "                'ì´ê°•ìˆ˜ëŸ‰': round(ì´ê°•ìˆ˜ëŸ‰, 1),\n",
    "                'ëƒ‰ë°©ë„ì¼': round(ëƒ‰ë°©ë„ì¼, 1),\n",
    "                'ë‚œë°©ë„ì¼': round(ë‚œë°©ë„ì¼, 1),\n",
    "                'ê°•ìˆ˜ì—¬ë¶€': ê°•ìˆ˜ì—¬ë¶€\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(weather_data)\n",
    "    \n",
    "    def _create_calendar_test_data(self):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ë‹¬ë ¥ ë°ì´í„° ìƒì„±\"\"\"\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        calendar_data = []\n",
    "        for day in range(days):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            weekday = current_date.weekday()\n",
    "            is_weekend = weekday >= 5\n",
    "            is_workday = not is_weekend\n",
    "            is_holiday = (current_date.month == 3 and current_date.day == 1)  # 3.1ì ˆ\n",
    "            \n",
    "            calendar_data.append({\n",
    "                'date': current_date.strftime('%Y-%m-%d'),\n",
    "                'year': current_date.year,\n",
    "                'month': current_date.month,\n",
    "                'day': current_date.day,\n",
    "                'weekday': weekday,\n",
    "                'is_workday': is_workday,\n",
    "                'is_weekend': is_weekend,\n",
    "                'is_holiday': is_holiday,\n",
    "                'workday_indicator': 1 if is_workday and not is_holiday else 0,\n",
    "                'is_month_start': current_date.day == 1,\n",
    "                'is_month_end': current_date.day >= 29\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(calendar_data)\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        # LP ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        if self.lp_data is not None:\n",
    "            self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "            self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "            self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "            self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "        \n",
    "        # ë‚ ì”¨ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        if self.weather_data is not None:\n",
    "            self.weather_data['ë‚ ì§œ'] = pd.to_datetime(self.weather_data['ë‚ ì§œ']).dt.date\n",
    "        \n",
    "        # ë‹¬ë ¥ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        if self.calendar_data is not None:\n",
    "            self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date']).dt.date\n",
    "    \n",
    "    def _merge_all_data(self):\n",
    "        \"\"\"ëª¨ë“  ë°ì´í„° ë³‘í•©\"\"\"\n",
    "        # ì¼ë³„ë¡œ LP ë°ì´í„° ì§‘ê³„\n",
    "        daily_lp = self.lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date']).agg({\n",
    "            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': ['mean', 'std', 'max', 'min', 'sum', 'count']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        daily_lp.columns = ['customer_id', 'date', 'power_mean', 'power_std', \n",
    "                           'power_max', 'power_min', 'power_sum', 'power_count']\n",
    "        \n",
    "        # ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "        daily_lp['daily_cv'] = daily_lp['power_std'] / daily_lp['power_mean']\n",
    "        daily_lp['daily_cv'] = daily_lp['daily_cv'].fillna(0)\n",
    "        \n",
    "        # ë‚ ì”¨ ë°ì´í„°ì™€ ë³‘í•©\n",
    "        merged = daily_lp.merge(self.weather_data, left_on='date', right_on='ë‚ ì§œ', how='left')\n",
    "        \n",
    "        # ë‹¬ë ¥ ë°ì´í„°ì™€ ë³‘í•©\n",
    "        merged = merged.merge(self.calendar_data, on='date', how='left')\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def calculate_comprehensive_volatility_score(self):\n",
    "        \"\"\"ì¢…í•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ì¢…í•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ê³„ì‚° ===\")\n",
    "        \n",
    "        if self.combined_data is None:\n",
    "            print(\"âŒ ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë”©í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        comprehensive_scores = {}\n",
    "        \n",
    "        for customer in self.combined_data['customer_id'].unique():\n",
    "            customer_data = self.combined_data[self.combined_data['customer_id'] == customer]\n",
    "            \n",
    "            if len(customer_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 1. ê¸°ë³¸ ë³€ë™ê³„ìˆ˜\n",
    "            basic_cv = customer_data['daily_cv'].std()\n",
    "            \n",
    "            # 2. ë‚ ì”¨ ë³´ì • ë³€ë™ê³„ìˆ˜\n",
    "            try:\n",
    "                weather_features = customer_data[['í‰ê· ê¸°ì˜¨', 'í‰ê· ìŠµë„', 'ëƒ‰ë°©ë„ì¼', 'ë‚œë°©ë„ì¼']].fillna(0)\n",
    "                power_target = customer_data['power_mean'].fillna(customer_data['power_mean'].mean())\n",
    "                \n",
    "                if len(weather_features) > 1 and len(weather_features) == len(power_target):\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(weather_features, power_target)\n",
    "                    predicted = model.predict(weather_features)\n",
    "                    residuals = power_target - predicted\n",
    "                    weather_adjusted_cv = residuals.std() / residuals.mean() if residuals.mean() != 0 else basic_cv\n",
    "                else:\n",
    "                    weather_adjusted_cv = basic_cv\n",
    "            except:\n",
    "                weather_adjusted_cv = basic_cv\n",
    "            \n",
    "            # 3. ë‹¬ë ¥ ë³´ì • ë³€ë™ê³„ìˆ˜\n",
    "            try:\n",
    "                calendar_features = customer_data[['workday_indicator', 'is_month_start', 'is_month_end']].fillna(0)\n",
    "                \n",
    "                if len(calendar_features) > 1 and len(calendar_features) == len(power_target):\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(calendar_features, power_target)\n",
    "                    predicted = model.predict(calendar_features)\n",
    "                    residuals = power_target - predicted\n",
    "                    calendar_adjusted_cv = residuals.std() / residuals.mean() if residuals.mean() != 0 else basic_cv\n",
    "                else:\n",
    "                    calendar_adjusted_cv = basic_cv\n",
    "            except:\n",
    "                calendar_adjusted_cv = basic_cv\n",
    "            \n",
    "            # 4. ì™¸ë¶€ ìš”ì¸ ë¯¼ê°ë„\n",
    "            temp_correlation = abs(customer_data['í‰ê· ê¸°ì˜¨'].corr(customer_data['power_mean'])) or 0\n",
    "            \n",
    "            workday_data = customer_data[customer_data['is_workday'] == True]['power_mean']\n",
    "            weekend_data = customer_data[customer_data['is_weekend'] == True]['power_mean']\n",
    "            \n",
    "            if len(workday_data) > 0 and len(weekend_data) > 0:\n",
    "                weekday_effect = abs(workday_data.mean() - weekend_data.mean())\n",
    "                weekday_effect = weekday_effect / customer_data['power_mean'].mean() if customer_data['power_mean'].mean() > 0 else 0\n",
    "            else:\n",
    "                weekday_effect = 0\n",
    "            \n",
    "            # 5. ìµœì¢… ìˆœìˆ˜ ë³€ë™ì„± (ë‚ ì”¨ + ë‹¬ë ¥ ë³´ì •)\n",
    "            pure_volatility = (weather_adjusted_cv + calendar_adjusted_cv) / 2\n",
    "            \n",
    "            # 6. ì¢…í•© ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "            weights = {\n",
    "                'pure_volatility': 0.5,\n",
    "                'basic_cv': 0.3,\n",
    "                'temp_sensitivity': 0.1,\n",
    "                'weekday_effect': 0.1\n",
    "            }\n",
    "            \n",
    "            final_score = (\n",
    "                weights['pure_volatility'] * pure_volatility +\n",
    "                weights['basic_cv'] * basic_cv +\n",
    "                weights['temp_sensitivity'] * temp_correlation +\n",
    "                weights['weekday_effect'] * weekday_effect\n",
    "            )\n",
    "            \n",
    "            # 7. ì˜ì—…í™œë™ ë³€í™” ìœ„í—˜ë„ í‰ê°€\n",
    "            if pure_volatility > 0.3:\n",
    "                risk_level = \"ë†’ìŒ\"\n",
    "                recommendation = \"ì˜ì—…í™œë™ ë³€í™” ì˜ì‹¬ - ì •ë°€ ë¶„ì„ í•„ìš”\"\n",
    "            elif pure_volatility > 0.15:\n",
    "                risk_level = \"ë³´í†µ\"\n",
    "                recommendation = \"ì£¼ì˜ ê´€ì°° - ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§\"\n",
    "            else:\n",
    "                risk_level = \"ë‚®ìŒ\"\n",
    "                recommendation = \"ì •ìƒ ìš´ì˜ ì¶”ì • - ì•ˆì •ì \"\n",
    "            \n",
    "            comprehensive_scores[customer] = {\n",
    "                'basic_cv': round(basic_cv, 4),\n",
    "                'weather_adjusted_cv': round(weather_adjusted_cv, 4),\n",
    "                'calendar_adjusted_cv': round(calendar_adjusted_cv, 4),\n",
    "                'pure_volatility': round(pure_volatility, 4),\n",
    "                'temp_sensitivity': round(temp_correlation, 4),\n",
    "                'weekday_effect': round(weekday_effect, 4),\n",
    "                'final_score': round(final_score, 4),\n",
    "                'risk_level': risk_level,\n",
    "                'recommendation': recommendation,\n",
    "                'mean_power': round(customer_data['power_mean'].mean(), 1),\n",
    "                'data_points': len(customer_data)\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tê¸°ë³¸CV\\të‚ ì”¨ë³´ì •CV\\të‹¬ë ¥ë³´ì •CV\\tìˆœìˆ˜ë³€ë™ì„±\\tìµœì¢…ì ìˆ˜\\tìœ„í—˜ë„\")\n",
    "        for customer, scores in comprehensive_scores.items():\n",
    "            print(f\"{customer}\\t{scores['basic_cv']}\\t{scores['weather_adjusted_cv']}\\t{scores['calendar_adjusted_cv']}\\t{scores['pure_volatility']}\\t{scores['final_score']}\\t{scores['risk_level']}\")\n",
    "        \n",
    "        self.volatility_metrics['comprehensive'] = comprehensive_scores\n",
    "        return comprehensive_scores\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ¢ í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ - ìµœì¢… ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if not hasattr(self, 'volatility_metrics') or 'comprehensive' not in self.volatility_metrics:\n",
    "            comprehensive_scores = self.calculate_comprehensive_volatility_score()\n",
    "        else:\n",
    "            comprehensive_scores = self.volatility_metrics['comprehensive']\n",
    "        \n",
    "        if not comprehensive_scores:\n",
    "            print(\"âŒ ë³€ë™ì„± ìŠ¤ì½”ì–´ë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\nğŸ“Š ë¶„ì„ ê°œìš”:\")\n",
    "        print(f\"  â€¢ ë¶„ì„ ëŒ€ìƒ: {len(comprehensive_scores)}ëª… ê³ ê°\")\n",
    "        print(f\"  â€¢ ë°ì´í„° ê¸°ê°„: {self.combined_data['date'].min()} ~ {self.combined_data['date'].max()}\")\n",
    "        print(f\"  â€¢ ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(self.combined_data):,}ê±´\")\n",
    "        \n",
    "        # ë³€ë™ì„± ë¶„í¬ ë¶„ì„\n",
    "        pure_volatilities = [s['pure_volatility'] for s in comprehensive_scores.values()]\n",
    "        final_scores = [s['final_score'] for s in comprehensive_scores.values()]\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ìˆœìˆ˜ ë³€ë™ì„± í†µê³„:\")\n",
    "        print(f\"  â€¢ í‰ê· : {np.mean(pure_volatilities):.4f}\")\n",
    "        print(f\"  â€¢ í‘œì¤€í¸ì°¨: {np.std(pure_volatilities):.4f}\")\n",
    "        print(f\"  â€¢ ìµœì†Œê°’: {np.min(pure_volatilities):.4f}\")\n",
    "        print(f\"  â€¢ ìµœëŒ€ê°’: {np.max(pure_volatilities):.4f}\")\n",
    "        \n",
    "        # ìœ„í—˜ë„ë³„ ë¶„ë¥˜\n",
    "        risk_distribution = {}\n",
    "        for scores in comprehensive_scores.values():\n",
    "            risk = scores['risk_level']\n",
    "            risk_distribution[risk] = risk_distribution.get(risk, 0) + 1\n",
    "        \n",
    "        print(f\"\\nğŸš¨ ì˜ì—…í™œë™ ë³€í™” ìœ„í—˜ë„ ë¶„í¬:\")\n",
    "        for risk, count in risk_distribution.items():\n",
    "            percentage = count / len(comprehensive_scores) * 100\n",
    "            print(f\"  â€¢ {risk}: {count}ëª… ({percentage:.1f}%)\")\n",
    "        \n",
    "        # ìƒìœ„ ìœ„í—˜ ê³ ê° ì‹ë³„\n",
    "        high_risk_customers = {k: v for k, v in comprehensive_scores.items() \n",
    "                             if v['risk_level'] == 'ë†’ìŒ'}\n",
    "        \n",
    "        if high_risk_customers:\n",
    "            print(f\"\\nâš ï¸ ê³ ìœ„í—˜ ê³ ê° ìƒì„¸ ë¶„ì„:\")\n",
    "            print(\"ê³ ê°ë²ˆí˜¸\\tìˆœìˆ˜ë³€ë™ì„±\\tí‰ê· ì „ë ¥\\tê¶Œì¥ì‚¬í•­\")\n",
    "            for customer, scores in sorted(high_risk_customers.items(), \n",
    "                                         key=lambda x: x[1]['pure_volatility'], reverse=True):\n",
    "                print(f\"{customer}\\t{scores['pure_volatility']:.4f}\\t{scores['mean_power']}kW\\t{scores['recommendation']}\")\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ ì•Œê³ ë¦¬ì¦˜ íŠ¹ì§•:\")\n",
    "        print(\"  âœ… ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ: ê¸°ë³¸/ë‚ ì”¨ë³´ì •/ë‹¬ë ¥ë³´ì •/ìˆœìˆ˜ë³€ë™ì„±\")\n",
    "        print(\"  âœ… ì™¸ë¶€ìš”ì¸ ì œê±°: ë‚ ì”¨ ë° ë‹¬ë ¥ íš¨ê³¼ ìë™ ë³´ì •\")\n",
    "        print(\"  âœ… ìœ„í—˜ë„ ìë™ë¶„ë¥˜: 3ë‹¨ê³„ ìœ„í—˜ë„ + ë§ì¶¤ ê¶Œì¥ì‚¬í•­\")\n",
    "        print(\"  âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§: ì˜ì—…í™œë™ ë³€í™” ì¡°ê¸° ê°ì§€\")\n",
    "        print(\"  âœ… í™•ì¥ ê°€ëŠ¥ì„±: ì¶”ê°€ ì™¸ë¶€ìš”ì¸ ì‰½ê²Œ í†µí•© ê°€ëŠ¥\")\n",
    "        \n",
    "        return {\n",
    "            'comprehensive_scores': comprehensive_scores,\n",
    "            'statistics': {\n",
    "                'mean_pure_volatility': np.mean(pure_volatilities),\n",
    "                'std_pure_volatility': np.std(pure_volatilities),\n",
    "                'risk_distribution': risk_distribution\n",
    "            },\n",
    "            'high_risk_customers': high_risk_customers\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „\"\"\"\n",
    "    print(\"ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì‹¤í–‰\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCOVolatilityAnalyzer()\n",
    "    \n",
    "    # ì‹¤ì œ íŒŒì¼ë“¤ë¡œ ë°ì´í„° ë¡œë”©\n",
    "    try:\n",
    "        print(\"ğŸ“ ì‹¤ì œ ë°ì´í„° íŒŒì¼ ë¡œë”© ì‹œë„...\")\n",
    "        \n",
    "        # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "        lp_files = ['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv']\n",
    "        weather_file = 'weather_daily_processed.csv'\n",
    "        calendar_file = 'power_analysis_calendar_2022_2025.csv'\n",
    "        customer_file = 'ê³ ê°ë²ˆí˜¸.xlsx'\n",
    "        \n",
    "        # ë°ì´í„° ë¡œë”©\n",
    "        combined_data = analyzer.load_real_data(\n",
    "            lp_files=lp_files,\n",
    "            weather_file=weather_file,\n",
    "            calendar_file=calendar_file,\n",
    "            customer_file=customer_file\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(combined_data):,}ê±´\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì‹¤ì œ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "        print(\"ğŸ“Š ìƒ˜í”Œ ë°ì´í„°ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´\n",
    "        combined_data = analyzer.load_real_data()\n",
    "    \n",
    "    # ì¢…í•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "    print(\"\\nğŸ”„ ì¢…í•© ë³€ë™ì„± ë¶„ì„ ì‹œì‘...\")\n",
    "    volatility_scores = analyzer.calculate_comprehensive_volatility_score()\n",
    "    \n",
    "    # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    print(\"\\nğŸ“Š ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±...\")\n",
    "    final_report = analyzer.generate_final_report()\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥ (ì˜¤ë¥˜ ìˆ˜ì •)\n",
    "    if volatility_scores:\n",
    "        try:\n",
    "            # í˜„ì¬ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ì €ì¥ (ì˜ì–´ íŒŒì¼ëª… ì‚¬ìš©)\n",
    "            results_df = pd.DataFrame(volatility_scores).T\n",
    "            \n",
    "            # ì €ì¥ ê²½ë¡œ í™•ì¸\n",
    "            output_file = 'volatility_analysis_result.csv'\n",
    "            results_df.to_csv(output_file, encoding='utf-8-sig')\n",
    "            print(f\"ğŸ’¾ ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "            # ì¶”ê°€ ìš”ì•½ íŒŒì¼ë„ ì €ì¥\n",
    "            summary_file = 'analysis_summary.txt'\n",
    "            with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ìš”ì•½\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\")\n",
    "                f.write(f\"ë¶„ì„ ê³ ê° ìˆ˜: {len(volatility_scores)}ëª…\\n\")\n",
    "                f.write(f\"ë¶„ì„ ì™„ë£Œ ì‹œê°„: {datetime.now()}\\n\")\n",
    "                \n",
    "                # ìœ„í—˜ë„ë³„ ë¶„í¬\n",
    "                risk_dist = {}\n",
    "                for scores in volatility_scores.values():\n",
    "                    risk = scores['risk_level']\n",
    "                    risk_dist[risk] = risk_dist.get(risk, 0) + 1\n",
    "                    \n",
    "                f.write(\"\\nìœ„í—˜ë„ ë¶„í¬:\\n\")\n",
    "                for risk, count in risk_dist.items():\n",
    "                    f.write(f\"  {risk}: {count}ëª…\\n\")\n",
    "                    \n",
    "                # ê³ ìœ„í—˜ ê³ ê°\n",
    "                high_risk = {k: v for k, v in volatility_scores.items() if v['risk_level'] == 'ë†’ìŒ'}\n",
    "                if high_risk:\n",
    "                    f.write(f\"\\nê³ ìœ„í—˜ ê³ ê° ({len(high_risk)}ëª…):\\n\")\n",
    "                    for customer, scores in high_risk.items():\n",
    "                        f.write(f\"  {customer}: ìˆœìˆ˜ë³€ë™ì„± {scores['pure_volatility']:.4f}\\n\")\n",
    "            \n",
    "            print(f\"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸ê°€ '{summary_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "        except Exception as save_error:\n",
    "            print(f\"âš ï¸ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {save_error}\")\n",
    "            print(\"ğŸ“Š ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤:\")\n",
    "            \n",
    "            # ì½˜ì†”ì— ê²°ê³¼ ì¶œë ¥\n",
    "            print(\"\\n=== ë¶„ì„ ê²°ê³¼ ===\")\n",
    "            for customer, scores in volatility_scores.items():\n",
    "                print(f\"{customer}: ìœ„í—˜ë„ {scores['risk_level']}, ìˆœìˆ˜ë³€ë™ì„± {scores['pure_volatility']:.4f}\")\n",
    "        \n",
    "        # ìš”ì•½ í†µê³„\n",
    "        print(\"\\nğŸ“ˆ ë¶„ì„ ìš”ì•½:\")\n",
    "        print(f\"  â€¢ ë¶„ì„ ê³ ê° ìˆ˜: {len(volatility_scores)}ëª…\")\n",
    "        \n",
    "        # ìœ„í—˜ë„ë³„ ë¶„í¬\n",
    "        risk_dist = {}\n",
    "        for scores in volatility_scores.values():\n",
    "            risk = scores['risk_level']\n",
    "            risk_dist[risk] = risk_dist.get(risk, 0) + 1\n",
    "            \n",
    "        print(\"  â€¢ ìœ„í—˜ë„ ë¶„í¬:\")\n",
    "        for risk, count in risk_dist.items():\n",
    "            percentage = count / len(volatility_scores) * 100\n",
    "            print(f\"    - {risk}: {count}ëª… ({percentage:.1f}%)\")\n",
    "            \n",
    "        # ê³ ìœ„í—˜ ê³ ê° ì¶œë ¥\n",
    "        high_risk = {k: v for k, v in volatility_scores.items() if v['risk_level'] == 'ë†’ìŒ'}\n",
    "        if high_risk:\n",
    "            print(f\"\\nâš ï¸ ê³ ìœ„í—˜ ê³ ê° ({len(high_risk)}ëª…):\")\n",
    "            for customer, scores in sorted(high_risk.items(), \n",
    "                                         key=lambda x: x[1]['pure_volatility'], reverse=True):\n",
    "                print(f\"  {customer}: ìˆœìˆ˜ë³€ë™ì„± {scores['pure_volatility']:.4f} - {scores['recommendation']}\")\n",
    "        else:\n",
    "            print(\"\\nâœ… ê³ ìœ„í—˜ ê³ ê°ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ ë¶„ì„ ì™„ë£Œ!\")\n",
    "    return final_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ì‹¤í–‰\n",
    "    try:\n",
    "        report = main()\n",
    "        \n",
    "        # ì¶”ê°€ ë¶„ì„ ì•ˆë‚´\n",
    "        print(\"\\nğŸ’¡ ì¶”ê°€ ë¶„ì„ ì˜µì…˜:\")\n",
    "        print(\"1. ì‹œê°í™” ìƒì„±\")\n",
    "        print(\"2. ìƒì„¸ ê³ ê° ë¶„ì„\")\n",
    "        print(\"3. ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•\")\n",
    "        print(\"4. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •\")\n",
    "        \n",
    "    except Exception as main_error:\n",
    "        print(f\"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {main_error}\")\n",
    "        print(\"ğŸ”§ ì˜¤ë¥˜ í•´ê²° ë°©ë²•:\")\n",
    "        print(\"1. íŒŒì¼ ê²½ë¡œ í™•ì¸\")\n",
    "        print(\"2. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸\")\n",
    "        print(\"3. ë°ì´í„° íŒŒì¼ í˜•ì‹ í™•ì¸\")\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ë””ë²„ê¹… ì •ë³´\n",
    "        import traceback\n",
    "        print(\"\\nğŸ› ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:\")\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
