{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 폰트 설정 (에러 방지)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KepcoDataPreprocessor:\n",
    "    \"\"\"\n",
    "    한국전력공사 LP 데이터 전처리 및 탐색적 분석\n",
    "    \n",
    "    단계별 분석 계획:\n",
    "    1단계: 데이터 품질 점검 (30분)\n",
    "    2단계: 기본 패턴 탐색 (60분) \n",
    "    3단계: 변동성 기초 분석 (90분)\n",
    "    4단계: 이상 패턴 탐지 (60분)\n",
    "    5단계: 전처리 방향 결정 (30분)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_quality_report = {}\n",
    "        self.pattern_analysis = {}\n",
    "        self.variability_analysis = {}\n",
    "    \n",
    "    # ============ 데이터 로딩 및 결합 ============\n",
    "    \n",
    "    def load_and_combine_lp_data(self, lp_files):\n",
    "        \"\"\"\n",
    "        LP 데이터 파일들을 읽어서 결합\n",
    "        파일들은 한 달을 반으로 나눠서 제공됨 (예: LP데이터1.csv + LP데이터2.csv)\n",
    "        \"\"\"\n",
    "        print(\"📂 LP 데이터 파일들 로딩 및 결합 중...\")\n",
    "        \n",
    "        combined_data = []\n",
    "        \n",
    "        for i, file_path in enumerate(lp_files):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"  ✅ {file_path}: {len(df):,}건 로딩\")\n",
    "                \n",
    "                # 기본 정보 출력\n",
    "                if 'LP수신일자' in df.columns:\n",
    "                    dates = pd.to_datetime(df['LP수신일자'])\n",
    "                    print(f\"     기간: {dates.min()} ~ {dates.max()}\")\n",
    "                    print(f\"     고객수: {df['대체고객번호'].nunique()}명\")\n",
    "                \n",
    "                combined_data.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ {file_path} 로딩 실패: {e}\")\n",
    "        \n",
    "        if not combined_data:\n",
    "            raise ValueError(\"로딩된 LP 데이터가 없습니다.\")\n",
    "        \n",
    "        # 데이터 결합\n",
    "        final_data = pd.concat(combined_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"  🔗 결합 완료: 총 {len(final_data):,}건\")\n",
    "        print(f\"     전체 기간: {pd.to_datetime(final_data['LP수신일자']).min()} ~ {pd.to_datetime(final_data['LP수신일자']).max()}\")\n",
    "        print(f\"     총 고객수: {final_data['대체고객번호'].nunique()}명\")\n",
    "        \n",
    "        return final_data\n",
    "    \n",
    "    # ============ 1단계: 데이터 품질 점검 (30분) ============\n",
    "    \n",
    "    def check_data_quality(self, lp_data, customer_data):\n",
    "        \"\"\"\n",
    "        데이터 품질 점검 및 기본 정보 분석\n",
    "        \"\"\"\n",
    "        print(\"🔍 1단계: 데이터 품질 점검 시작...\")\n",
    "        \n",
    "        # 고객 기본정보 분석\n",
    "        customer_info = self._analyze_customer_info(customer_data)\n",
    "        \n",
    "        # LP 데이터 품질 점검\n",
    "        lp_quality = self._check_lp_data_quality(lp_data)\n",
    "        \n",
    "        self.data_quality_report = {\n",
    "            'customer_info': customer_info,\n",
    "            'lp_quality': lp_quality,\n",
    "            'data_completeness': self._calculate_completeness(lp_data),\n",
    "            'anomaly_detection': self._detect_data_anomalies(lp_data)\n",
    "        }\n",
    "        \n",
    "        self._print_quality_summary()\n",
    "        return self.data_quality_report\n",
    "    \n",
    "    def _analyze_customer_info(self, customer_data):\n",
    "        \"\"\"고객 기본정보 분석\"\"\"\n",
    "        if customer_data is None:\n",
    "            return {\"message\": \"고객 데이터 없음\"}\n",
    "        \n",
    "        print(f\"  📋 고객 데이터 컬럼: {list(customer_data.columns)}\")\n",
    "        \n",
    "        # 가능한 컬럼명들 매핑\n",
    "        column_mapping = {\n",
    "            '계약종별': ['계약종별', 'contract_type', 'Contract_Type'],\n",
    "            '사용용도': ['사용용도', 'usage_purpose', 'Usage_Purpose'], \n",
    "            '산업분류': ['산업분류', 'industry', 'Industry']\n",
    "        }\n",
    "        \n",
    "        info = {'total_customers': len(customer_data)}\n",
    "        \n",
    "        for key, possible_cols in column_mapping.items():\n",
    "            found_col = None\n",
    "            for col in possible_cols:\n",
    "                if col in customer_data.columns:\n",
    "                    found_col = col\n",
    "                    break\n",
    "            \n",
    "            if found_col:\n",
    "                info[f'{key}_dist'] = customer_data[found_col].value_counts().to_dict()\n",
    "                print(f\"  ✅ {key} 분포: {dict(list(info[f'{key}_dist'].items())[:3])}...\")  # 상위 3개만 출력\n",
    "            else:\n",
    "                info[f'{key}_dist'] = {}\n",
    "                print(f\"  ⚠️ {key} 컬럼 없음\")\n",
    "        \n",
    "        print(f\"✅ 고객수: {info['total_customers']:,}명\")\n",
    "        return info\n",
    "    \n",
    "    def _check_lp_data_quality(self, lp_data):\n",
    "        \"\"\"LP 데이터 품질 점검\"\"\"\n",
    "        # 데이터 타입 변환\n",
    "        lp_data['LP수신일자'] = pd.to_datetime(lp_data['LP수신일자'])\n",
    "        \n",
    "        quality = {\n",
    "            'total_records': len(lp_data),\n",
    "            'date_range': {\n",
    "                'start': lp_data['LP수신일자'].min(),\n",
    "                'end': lp_data['LP수신일자'].max()\n",
    "            },\n",
    "            'unique_customers': lp_data['대체고객번호'].nunique(),\n",
    "            'missing_values': lp_data.isnull().sum().to_dict(),\n",
    "            'negative_values': (lp_data['순방향유효전력'] < 0).sum(),\n",
    "            'zero_values': (lp_data['순방향유효전력'] == 0).sum(),\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 총 레코드: {quality['total_records']:,}건\")\n",
    "        print(f\"✅ 기간: {quality['date_range']['start']} ~ {quality['date_range']['end']}\")\n",
    "        print(f\"✅ 고객수: {quality['unique_customers']:,}명\")\n",
    "        print(f\"✅ 음수값: {quality['negative_values']:,}건\")\n",
    "        print(f\"✅ 0값: {quality['zero_values']:,}건\")\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    def _calculate_completeness(self, lp_data):\n",
    "        \"\"\"데이터 완정성 계산\"\"\"\n",
    "        lp_data['date'] = lp_data['LP수신일자'].dt.date\n",
    "        lp_data['hour'] = lp_data['LP수신일자'].dt.hour\n",
    "        lp_data['quarter_hour'] = (lp_data['LP수신일자'].dt.minute // 15) * 15\n",
    "        \n",
    "        # 15분 간격 정확성 체크\n",
    "        expected_intervals = pd.date_range(\n",
    "            start=lp_data['LP수신일자'].min(),\n",
    "            end=lp_data['LP수신일자'].max(),\n",
    "            freq='15min'\n",
    "        )\n",
    "        \n",
    "        completeness = {\n",
    "            'expected_records': len(expected_intervals) * lp_data['대체고객번호'].nunique(),\n",
    "            'actual_records': len(lp_data),\n",
    "            'completeness_rate': len(lp_data) / (len(expected_intervals) * lp_data['대체고객번호'].nunique()) * 100\n",
    "        }\n",
    "        \n",
    "        # 고객별 완정성\n",
    "        customer_completeness = lp_data.groupby('대체고객번호').size() / len(expected_intervals) * 100\n",
    "        completeness['customer_completeness'] = {\n",
    "            'mean': customer_completeness.mean(),\n",
    "            'min': customer_completeness.min(),\n",
    "            'max': customer_completeness.max(),\n",
    "            'std': customer_completeness.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 데이터 완정성: {completeness['completeness_rate']:.2f}%\")\n",
    "        \n",
    "        return completeness\n",
    "    \n",
    "    def _detect_data_anomalies(self, lp_data):\n",
    "        \"\"\"데이터 이상치 탐지\"\"\"\n",
    "        # 통계적 이상치 탐지\n",
    "        Q1 = lp_data['순방향유효전력'].quantile(0.25)\n",
    "        Q3 = lp_data['순방향유효전력'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        outliers_iqr = lp_data[\n",
    "            (lp_data['순방향유효전력'] < Q1 - 1.5 * IQR) | \n",
    "            (lp_data['순방향유효전력'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        # Z-score 이상치\n",
    "        z_scores = np.abs((lp_data['순방향유효전력'] - lp_data['순방향유효전력'].mean()) / lp_data['순방향유효전력'].std())\n",
    "        outliers_zscore = lp_data[z_scores > 3]\n",
    "        \n",
    "        anomalies = {\n",
    "            'iqr_outliers': len(outliers_iqr),\n",
    "            'zscore_outliers': len(outliers_zscore),\n",
    "            'outlier_rate_iqr': len(outliers_iqr) / len(lp_data) * 100,\n",
    "            'outlier_rate_zscore': len(outliers_zscore) / len(lp_data) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ IQR 이상치: {anomalies['outlier_rate_iqr']:.3f}%\")\n",
    "        print(f\"✅ Z-score 이상치: {anomalies['outlier_rate_zscore']:.3f}%\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def _print_quality_summary(self):\n",
    "        \"\"\"품질 점검 요약 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"📊 데이터 품질 점검 완료\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # ============ 2단계: 기본 패턴 탐색 (60분) ============\n",
    "    \n",
    "    def analyze_basic_patterns(self, lp_data, customer_data=None, calendar_data=None):\n",
    "        \"\"\"\n",
    "        기본 전력 사용 패턴 분석\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 2단계: 기본 패턴 탐색 시작...\")\n",
    "        \n",
    "        # 데이터 전처리\n",
    "        processed_data = self._preprocess_for_pattern_analysis(lp_data, customer_data, calendar_data)\n",
    "        \n",
    "        # 시간별 패턴 분석\n",
    "        time_patterns = self._analyze_time_patterns(processed_data)\n",
    "        \n",
    "        # 고객 세분화 기초 분석\n",
    "        customer_segmentation = self._analyze_customer_segmentation(processed_data)\n",
    "        \n",
    "        self.pattern_analysis = {\n",
    "            'time_patterns': time_patterns,\n",
    "            'customer_segmentation': customer_segmentation,\n",
    "            'processed_data': processed_data\n",
    "        }\n",
    "        \n",
    "        return self.pattern_analysis\n",
    "    \n",
    "    def _preprocess_for_pattern_analysis(self, lp_data, customer_data, calendar_data):\n",
    "        \"\"\"패턴 분석을 위한 데이터 전처리\"\"\"\n",
    "        print(\"  🔄 데이터 전처리 중...\")\n",
    "        \n",
    "        # 시간 변수 생성\n",
    "        lp_data['datetime'] = pd.to_datetime(lp_data['LP수신일자'])\n",
    "        lp_data['date'] = lp_data['datetime'].dt.date\n",
    "        lp_data['hour'] = lp_data['datetime'].dt.hour\n",
    "        lp_data['weekday'] = lp_data['datetime'].dt.weekday  # 0=월요일\n",
    "        lp_data['month'] = lp_data['datetime'].dt.month\n",
    "        lp_data['quarter'] = lp_data['datetime'].dt.quarter\n",
    "        lp_data['is_weekend'] = lp_data['weekday'].isin([5, 6])  # 토, 일\n",
    "        \n",
    "        # 일간 집계 데이터 생성\n",
    "        daily_agg = lp_data.groupby(['대체고객번호', 'date']).agg({\n",
    "            '순방향유효전력': ['sum', 'mean', 'max', 'min', 'std']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        daily_agg.columns = ['customer_id', 'date', 'daily_sum', 'daily_mean', 'daily_max', 'daily_min', 'daily_std']\n",
    "        \n",
    "        # 날짜 관련 피처 다시 생성 (일간 집계 후)\n",
    "        daily_agg['date_dt'] = pd.to_datetime(daily_agg['date'])\n",
    "        daily_agg['weekday'] = daily_agg['date_dt'].dt.weekday  # 0=월요일\n",
    "        daily_agg['month'] = daily_agg['date_dt'].dt.month\n",
    "        daily_agg['quarter'] = daily_agg['date_dt'].dt.quarter\n",
    "        daily_agg['is_weekend'] = daily_agg['weekday'].isin([5, 6])  # 토, 일\n",
    "        \n",
    "        print(f\"  ✅ 시간 피처 생성 완료: weekday, month, quarter, is_weekend\")\n",
    "        \n",
    "        # 고객 정보 병합\n",
    "        if customer_data is not None:\n",
    "            # 고객 데이터의 실제 컬럼명 확인\n",
    "            customer_key_col = None\n",
    "            possible_keys = ['대체고객번호', '고객번호', 'customer_id', 'Customer_ID']\n",
    "            \n",
    "            for col in possible_keys:\n",
    "                if col in customer_data.columns:\n",
    "                    customer_key_col = col\n",
    "                    break\n",
    "            \n",
    "            if customer_key_col:\n",
    "                daily_agg = daily_agg.merge(customer_data, left_on='customer_id', right_on=customer_key_col, how='left')\n",
    "                print(f\"  ✅ 고객 정보 병합 완료 (키: {customer_key_col})\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ 고객 데이터 병합 실패 - 키 컬럼을 찾을 수 없음. 사용 가능한 컬럼: {list(customer_data.columns)}\")\n",
    "                print(f\"  ℹ️ 고객 정보 없이 분석 계속...\")\n",
    "        \n",
    "        # 기상/달력 정보 병합\n",
    "        if calendar_data is not None:\n",
    "            # 날짜 컬럼 확인 및 변환\n",
    "            date_col = None\n",
    "            possible_date_cols = ['date', '날짜', 'Date', 'DATE']\n",
    "            \n",
    "            for col in possible_date_cols:\n",
    "                if col in calendar_data.columns:\n",
    "                    date_col = col\n",
    "                    break\n",
    "            \n",
    "            if date_col:\n",
    "                # 기상 데이터가 weather_daily_processed.csv인 경우 날짜 형식 확인\n",
    "                if 'year' in calendar_data.columns and 'month' in calendar_data.columns and 'day' in calendar_data.columns:\n",
    "                    # year, month, day 컬럼으로 날짜 생성\n",
    "                    calendar_data['date_parsed'] = pd.to_datetime(calendar_data[['year', 'month', 'day']])\n",
    "                    calendar_data['date_for_merge'] = calendar_data['date_parsed'].dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on='date_for_merge', how='left')\n",
    "                    print(f\"  ✅ 기상/달력 정보 병합 완료 (year-month-day 기준)\")\n",
    "                else:\n",
    "                    # 일반적인 date 컬럼 사용\n",
    "                    calendar_data[date_col] = pd.to_datetime(calendar_data[date_col]).dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on=date_col, how='left')\n",
    "                    print(f\"  ✅ 기상/달력 정보 병합 완료 (키: {date_col})\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ 기상/달력 데이터 병합 실패 - 날짜 컬럼을 찾을 수 없음. 사용 가능한 컬럼: {list(calendar_data.columns)}\")\n",
    "                print(f\"  ℹ️ 기상/달력 정보 없이 분석 계속...\")\n",
    "        \n",
    "        print(f\"  ✅ 일간 집계 데이터: {len(daily_agg):,}건\")\n",
    "        return daily_agg\n",
    "    \n",
    "    def _analyze_time_patterns(self, data):\n",
    "        \"\"\"시간별 패턴 분석\"\"\"\n",
    "        print(\"  📈 시간별 패턴 분석 중...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. 시간대별 패턴 (일간 집계 데이터에서는 의미 없으므로 건너뛰기)\n",
    "        # hourly_pattern = data.groupby('hour')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "        # patterns['hourly'] = hourly_pattern\n",
    "        \n",
    "        # 2. 요일별 패턴\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_pattern = data.groupby('weekday')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekday'] = weekday_pattern\n",
    "        \n",
    "        # 3. 월별 패턴 (계절성)\n",
    "        if 'month' in data.columns:\n",
    "            monthly_pattern = data.groupby('month')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['monthly'] = monthly_pattern\n",
    "        \n",
    "        # 4. 주중/주말 패턴\n",
    "        if 'is_weekend' in data.columns:\n",
    "            weekend_pattern = data.groupby('is_weekend')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekend'] = weekend_pattern\n",
    "        \n",
    "        # 5. 업종별 패턴 (고객 데이터가 있는 경우)\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['사용용도', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            industry_pattern = data.groupby(usage_purpose_col)['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['industry'] = industry_pattern\n",
    "            print(f\"  ✅ {usage_purpose_col} 기준 업종별 패턴 분석 완료\")\n",
    "        \n",
    "        print(f\"  ✅ 시간별 패턴 분석 완료 ({len(patterns)}개 패턴)\")\n",
    "        return patterns\n",
    "    \n",
    "    def _analyze_customer_segmentation(self, data):\n",
    "        \"\"\"고객 세분화 기초 분석\"\"\"\n",
    "        print(\"  👥 고객 세분화 분석 중...\")\n",
    "        \n",
    "        # 고객별 평균 사용량 계산\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        # 사용량 규모별 분류\n",
    "        segmentation = {\n",
    "            'large_users': customer_avg.quantile(0.9),  # 상위 10%\n",
    "            'medium_users': customer_avg.quantile(0.5),  # 중간 50%\n",
    "            'small_users': customer_avg.quantile(0.1),   # 하위 10%\n",
    "        }\n",
    "        \n",
    "        # 고객별 사용량 분포\n",
    "        customer_stats = {\n",
    "            'customer_count': len(customer_avg),\n",
    "            'usage_distribution': {\n",
    "                'mean': customer_avg.mean(),\n",
    "                'std': customer_avg.std(),\n",
    "                'min': customer_avg.min(),\n",
    "                'max': customer_avg.max(),\n",
    "                'q25': customer_avg.quantile(0.25),\n",
    "                'q50': customer_avg.quantile(0.50),\n",
    "                'q75': customer_avg.quantile(0.75)\n",
    "            },\n",
    "            'segmentation_thresholds': segmentation\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ {customer_stats['customer_count']:,}명 고객 세분화 완료\")\n",
    "        return customer_stats\n",
    "    \n",
    "    # ============ 3단계: 변동성 기초 분석 (90분) ============\n",
    "    \n",
    "    def analyze_variability(self, processed_data):\n",
    "        \"\"\"\n",
    "        변동성 기초 분석 - 변동계수 설계를 위한 기초 작업\n",
    "        \"\"\"\n",
    "        print(\"\\n📈 3단계: 변동성 기초 분석 시작...\")\n",
    "        \n",
    "        # 기본 변동성 지표 계산\n",
    "        basic_variability = self._calculate_basic_variability(processed_data)\n",
    "        \n",
    "        # 변동성 패턴 분석\n",
    "        variability_patterns = self._analyze_variability_patterns(processed_data)\n",
    "        \n",
    "        self.variability_analysis = {\n",
    "            'basic_variability': basic_variability,\n",
    "            'variability_patterns': variability_patterns\n",
    "        }\n",
    "        \n",
    "        return self.variability_analysis\n",
    "    \n",
    "    def _calculate_basic_variability(self, data):\n",
    "        \"\"\"기본 변동성 지표 계산\"\"\"\n",
    "        print(\"  📊 기본 변동성 지표 계산 중...\")\n",
    "        \n",
    "        variability_metrics = {}\n",
    "        \n",
    "        # 고객별 변동계수 계산\n",
    "        customer_cv = data.groupby('customer_id').apply(\n",
    "            lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "        )\n",
    "        \n",
    "        # 1. 일간 변동계수\n",
    "        variability_metrics['daily_cv'] = {\n",
    "            'mean': customer_cv.mean(),\n",
    "            'std': customer_cv.std(),\n",
    "            'distribution': customer_cv.describe()\n",
    "        }\n",
    "        \n",
    "        # 2. 주간 변동계수 (주별 패턴의 일관성)\n",
    "        try:\n",
    "            # 주 번호 생성\n",
    "            data_with_week = data.copy()\n",
    "            data_with_week['week'] = pd.to_datetime(data_with_week['date']).dt.isocalendar().week\n",
    "            \n",
    "            weekly_cv = data_with_week.groupby(['customer_id', 'week']).agg({\n",
    "                'daily_mean': ['mean', 'std']\n",
    "            }).reset_index()\n",
    "            weekly_cv.columns = ['customer_id', 'week', 'weekly_mean', 'weekly_std']\n",
    "            weekly_cv['weekly_cv'] = weekly_cv['weekly_std'] / weekly_cv['weekly_mean']\n",
    "            \n",
    "            customer_weekly_cv = weekly_cv.groupby('customer_id')['weekly_cv'].mean()\n",
    "            variability_metrics['weekly_cv'] = {\n",
    "                'mean': customer_weekly_cv.mean(),\n",
    "                'std': customer_weekly_cv.std(),\n",
    "                'distribution': customer_weekly_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ 주간 변동계수 계산 실패: {e}\")\n",
    "            variability_metrics['weekly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 3. 월간 변동계수\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                monthly_cv = data.groupby(['customer_id', 'month']).agg({\n",
    "                    'daily_mean': ['mean', 'std']\n",
    "                }).reset_index()\n",
    "                monthly_cv.columns = ['customer_id', 'month', 'monthly_mean', 'monthly_std']\n",
    "                monthly_cv['monthly_cv'] = monthly_cv['monthly_std'] / monthly_cv['monthly_mean']\n",
    "                \n",
    "                customer_monthly_cv = monthly_cv.groupby('customer_id')['monthly_cv'].mean()\n",
    "                variability_metrics['monthly_cv'] = {\n",
    "                    'mean': customer_monthly_cv.mean(),\n",
    "                    'std': customer_monthly_cv.std(),\n",
    "                    'distribution': customer_monthly_cv.describe()\n",
    "                }\n",
    "            else:\n",
    "                print(\"    ⚠️ month 컬럼이 없어 월간 변동계수 계산 건너뛰기\")\n",
    "                variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ 월간 변동계수 계산 실패: {e}\")\n",
    "            variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 4. 추가 변동성 지표\n",
    "        # 범위 기반 변동성\n",
    "        try:\n",
    "            customer_range_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: (x['daily_mean'].max() - x['daily_mean'].min()) / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            variability_metrics['range_based_cv'] = {\n",
    "                'mean': customer_range_cv.mean(),\n",
    "                'std': customer_range_cv.std(),\n",
    "                'distribution': customer_range_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ 범위 기반 변동계수 계산 실패: {e}\")\n",
    "            variability_metrics['range_based_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        print(f\"  ✅ 기본 변동성 지표 계산 완료\")\n",
    "        return variability_metrics\n",
    "    \n",
    "    def _analyze_variability_patterns(self, data):\n",
    "        \"\"\"변동성 패턴 분석\"\"\"\n",
    "        print(\"  🔍 변동성 패턴 분석 중...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. 업종별 변동성 비교\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['사용용도', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            try:\n",
    "                industry_variability = data.groupby([usage_purpose_col, 'customer_id']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                industry_variability.columns = [usage_purpose_col, 'customer_id', 'cv']\n",
    "                \n",
    "                industry_cv_summary = industry_variability.groupby(usage_purpose_col)['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['industry_variability'] = industry_cv_summary\n",
    "                print(f\"  ✅ {usage_purpose_col} 기준 업종별 변동성 분석 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 업종별 변동성 분석 실패: {e}\")\n",
    "        \n",
    "        # 2. 계절별 변동성 차이\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                seasonal_variability = data.groupby(['customer_id', 'month']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                seasonal_variability.columns = ['customer_id', 'month', 'cv']\n",
    "                \n",
    "                seasonal_cv_summary = seasonal_variability.groupby('month')['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['seasonal_variability'] = seasonal_cv_summary\n",
    "                print(f\"  ✅ 계절별 변동성 분석 완료\")\n",
    "            else:\n",
    "                print(\"  ⚠️ month 컬럼이 없어 계절별 변동성 분석 건너뛰기\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ 계절별 변동성 분석 실패: {e}\")\n",
    "        \n",
    "        # 3. 사용량 규모별 변동성\n",
    "        try:\n",
    "            customer_avg_usage = data.groupby('customer_id')['daily_mean'].mean()\n",
    "            customer_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # 사용량 규모별 그룹핑\n",
    "            usage_quantiles = customer_avg_usage.quantile([0.33, 0.67])\n",
    "            def categorize_usage(usage):\n",
    "                if usage <= usage_quantiles.iloc[0]:\n",
    "                    return 'Low'\n",
    "                elif usage <= usage_quantiles.iloc[1]:\n",
    "                    return 'Medium'\n",
    "                else:\n",
    "                    return 'High'\n",
    "            \n",
    "            customer_usage_category = customer_avg_usage.apply(categorize_usage)\n",
    "            usage_cv_df = pd.DataFrame({\n",
    "                'usage_category': customer_usage_category,\n",
    "                'cv': customer_cv\n",
    "            })\n",
    "            \n",
    "            usage_cv_summary = usage_cv_df.groupby('usage_category')['cv'].agg(['mean', 'std', 'count'])\n",
    "            patterns['usage_level_variability'] = usage_cv_summary\n",
    "            print(f\"  ✅ 사용량 규모별 변동성 분석 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ 사용량 규모별 변동성 분석 실패: {e}\")\n",
    "        \n",
    "        print(f\"  ✅ 변동성 패턴 분석 완료 ({len(patterns)}개 패턴)\")\n",
    "        return patterns\n",
    "    \n",
    "    # ============ 4단계: 이상 패턴 탐지 (60분) ============\n",
    "    \n",
    "    def detect_anomalous_patterns(self, processed_data):\n",
    "        \"\"\"\n",
    "        이상 패턴 탐지\n",
    "        \"\"\"\n",
    "        print(\"\\n🎯 4단계: 이상 패턴 탐지 시작...\")\n",
    "        \n",
    "        # 통계적 이상치 식별\n",
    "        statistical_outliers = self._identify_statistical_outliers(processed_data)\n",
    "        \n",
    "        # 시계열 이상치 탐지\n",
    "        temporal_anomalies = self._detect_temporal_anomalies(processed_data)\n",
    "        \n",
    "        # 비정상 패턴 정의\n",
    "        abnormal_patterns = self._define_abnormal_patterns(processed_data)\n",
    "        \n",
    "        anomaly_results = {\n",
    "            'statistical_outliers': statistical_outliers,\n",
    "            'temporal_anomalies': temporal_anomalies,\n",
    "            'abnormal_patterns': abnormal_patterns\n",
    "        }\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def _identify_statistical_outliers(self, data):\n",
    "        \"\"\"통계적 이상치 식별\"\"\"\n",
    "        print(\"  🔍 통계적 이상치 식별 중...\")\n",
    "        \n",
    "        outliers = {}\n",
    "        \n",
    "        # IQR 방법\n",
    "        Q1 = data['daily_mean'].quantile(0.25)\n",
    "        Q3 = data['daily_mean'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        iqr_outliers = data[\n",
    "            (data['daily_mean'] < Q1 - 1.5 * IQR) | \n",
    "            (data['daily_mean'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        outliers['iqr_outliers'] = {\n",
    "            'count': len(iqr_outliers),\n",
    "            'rate': len(iqr_outliers) / len(data) * 100,\n",
    "            'customer_count': iqr_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        # Z-score 방법\n",
    "        z_scores = np.abs((data['daily_mean'] - data['daily_mean'].mean()) / data['daily_mean'].std())\n",
    "        zscore_outliers = data[z_scores > 3]\n",
    "        \n",
    "        outliers['zscore_outliers'] = {\n",
    "            'count': len(zscore_outliers),\n",
    "            'rate': len(zscore_outliers) / len(data) * 100,\n",
    "            'customer_count': zscore_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ IQR 이상치: {outliers['iqr_outliers']['count']:,}건 ({outliers['iqr_outliers']['rate']:.2f}%)\")\n",
    "        print(f\"  ✅ Z-score 이상치: {outliers['zscore_outliers']['count']:,}건 ({outliers['zscore_outliers']['rate']:.2f}%)\")\n",
    "        \n",
    "        return outliers\n",
    "    \n",
    "    def _detect_temporal_anomalies(self, data):\n",
    "        \"\"\"시계열 이상치 탐지\"\"\"\n",
    "        print(\"  ⏰ 시계열 이상치 탐지 중...\")\n",
    "        \n",
    "        temporal_anomalies = {}\n",
    "        \n",
    "        # 고객별 시계열 이상치 탐지\n",
    "        for customer_id in data['customer_id'].unique()[:100]:  # 샘플로 100명만\n",
    "            customer_data = data[data['customer_id'] == customer_id].sort_values('date')\n",
    "            \n",
    "            if len(customer_data) < 30:  # 최소 30일 데이터 필요\n",
    "                continue\n",
    "            \n",
    "            # 급격한 증가/감소 탐지 (>200% 변화)\n",
    "            customer_data['pct_change'] = customer_data['daily_mean'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['pct_change']) > 2.0]  # 200% 변화\n",
    "            \n",
    "            # 연속적인 0값 탐지\n",
    "            zero_streaks = customer_data[customer_data['daily_mean'] == 0]\n",
    "            \n",
    "            if len(sudden_changes) > 0 or len(zero_streaks) > 5:  # 5일 이상 연속 0값\n",
    "                temporal_anomalies[customer_id] = {\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'zero_streaks': len(zero_streaks)\n",
    "                }\n",
    "        \n",
    "        print(f\"  ✅ {len(temporal_anomalies):,}명 고객에서 시계열 이상 탐지\")\n",
    "        \n",
    "        return temporal_anomalies\n",
    "    \n",
    "    def _define_abnormal_patterns(self, data):\n",
    "        \"\"\"비정상 패턴 정의\"\"\"\n",
    "        print(\"  📋 비정상 패턴 정의 중...\")\n",
    "        \n",
    "        abnormal_patterns = {\n",
    "            'pattern_definitions': {\n",
    "                1: '전력 사용 급증/급감 (사업 확장/축소)',\n",
    "                2: '사용 패턴 변화 (운영시간 변경)', \n",
    "                3: '효율성 급변 (설비 교체/고장)',\n",
    "                4: '계절성 이탈 (사업 모델 변화)'\n",
    "            },\n",
    "            'detection_criteria': {\n",
    "                'usage_spike': 'daily_mean > mean + 3*std',\n",
    "                'usage_drop': 'daily_mean < mean - 3*std',\n",
    "                'pattern_shift': 'monthly pattern change > 50%',\n",
    "                'efficiency_change': 'weekly efficiency variance > threshold'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ {len(abnormal_patterns['pattern_definitions'])}가지 비정상 패턴 정의 완료\")\n",
    "        \n",
    "        return abnormal_patterns\n",
    "    \n",
    "    # ============ 5단계: 전처리 방향 결정 (30분) ============\n",
    "    \n",
    "    def decide_preprocessing_strategy(self, data_quality_report, pattern_analysis, variability_analysis):\n",
    "        \"\"\"\n",
    "        전처리 방향 결정\n",
    "        \"\"\"\n",
    "        print(\"\\n🔧 5단계: 전처리 방향 결정...\")\n",
    "        \n",
    "        preprocessing_strategy = {\n",
    "            'missing_data_handling': self._decide_missing_data_strategy(data_quality_report),\n",
    "            'outlier_handling': self._decide_outlier_strategy(data_quality_report),\n",
    "            'normalization_method': self._decide_normalization_strategy(pattern_analysis),\n",
    "            'feature_engineering': self._decide_feature_engineering(pattern_analysis, variability_analysis)\n",
    "        }\n",
    "        \n",
    "        self._print_preprocessing_summary(preprocessing_strategy)\n",
    "        \n",
    "        return preprocessing_strategy\n",
    "    \n",
    "    def _decide_missing_data_strategy(self, quality_report):\n",
    "        \"\"\"결측치 처리 전략 결정\"\"\"\n",
    "        completeness_rate = quality_report['data_completeness']['completeness_rate']\n",
    "        \n",
    "        if completeness_rate > 95:\n",
    "            strategy = \"선형보간 또는 forward fill\"\n",
    "        elif completeness_rate > 80:\n",
    "            strategy = \"계절성 고려 보간\"\n",
    "        else:\n",
    "            strategy = \"장기 결측 기간 분석 제외\"\n",
    "        \n",
    "        return {\n",
    "            'completeness_rate': completeness_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_outlier_strategy(self, quality_report):\n",
    "        \"\"\"이상치 처리 전략 결정\"\"\"\n",
    "        outlier_rate = quality_report['anomaly_detection']['outlier_rate_iqr']\n",
    "        \n",
    "        if outlier_rate < 1:\n",
    "            strategy = \"이상치 유지 (정상 범위)\"\n",
    "        elif outlier_rate < 5:\n",
    "            strategy = \"extreme outlier만 제거\"\n",
    "        else:\n",
    "            strategy = \"robust 통계량 사용\"\n",
    "        \n",
    "        return {\n",
    "            'outlier_rate': outlier_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_normalization_strategy(self, pattern_analysis):\n",
    "        \"\"\"정규화 방법 결정\"\"\"\n",
    "        customer_stats = pattern_analysis['customer_segmentation']\n",
    "        usage_std = customer_stats['usage_distribution']['std']\n",
    "        usage_mean = customer_stats['usage_distribution']['mean']\n",
    "        cv = usage_std / usage_mean if usage_mean > 0 else 0\n",
    "        \n",
    "        if cv > 1.0:\n",
    "            strategy = \"고객별 표준화 + 로그 변환\"\n",
    "        elif cv > 0.5:\n",
    "            strategy = \"고객별 표준화\"\n",
    "        else:\n",
    "            strategy = \"전체 Min-Max 정규화\"\n",
    "        \n",
    "        return {\n",
    "            'coefficient_of_variation': cv,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_feature_engineering(self, pattern_analysis, variability_analysis):\n",
    "        \"\"\"피처 엔지니어링 전략 결정\"\"\"\n",
    "        features_to_create = []\n",
    "        \n",
    "        # 시간 기반 피처\n",
    "        time_patterns = pattern_analysis['time_patterns']\n",
    "        if 'monthly' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'month_sin', 'month_cos',  # 계절 순환 피처\n",
    "                'is_summer', 'is_winter',  # 계절 더미 변수\n",
    "            ])\n",
    "        \n",
    "        # 요일 기반 피처\n",
    "        if 'weekday' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'weekday_sin', 'weekday_cos',  # 요일 순환 피처\n",
    "                'is_weekend'                   # 주말 여부\n",
    "            ])\n",
    "        \n",
    "        # 변동성 기반 피처\n",
    "        if variability_analysis:\n",
    "            features_to_create.extend([\n",
    "                'rolling_mean_7d',       # 7일 이동평균\n",
    "                'rolling_std_7d',        # 7일 이동표준편차\n",
    "                'usage_volatility',      # 변동성 지수\n",
    "            ])\n",
    "        \n",
    "        # 고객 기반 피처\n",
    "        features_to_create.extend([\n",
    "            'customer_avg_usage',      # 고객 평균 사용량\n",
    "            'customer_usage_rank',     # 고객 사용량 순위\n",
    "            'deviation_from_avg'       # 평균 대비 편차\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'features_to_create': features_to_create,\n",
    "            'total_features': len(features_to_create)\n",
    "        }\n",
    "    \n",
    "    def _print_preprocessing_summary(self, strategy):\n",
    "        \"\"\"전처리 전략 요약 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🔧 전처리 전략 결정 완료\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"📋 결측치 처리: {strategy['missing_data_handling']['recommended_strategy']}\")\n",
    "        print(f\"🎯 이상치 처리: {strategy['outlier_handling']['recommended_strategy']}\")\n",
    "        print(f\"📊 정규화 방법: {strategy['normalization_method']['recommended_strategy']}\")\n",
    "        print(f\"🛠️ 생성할 피처: {strategy['feature_engineering']['total_features']}개\")\n",
    "        \n",
    "        print(\"\\n💡 변동계수 설계를 위한 인사이트:\")\n",
    "        print(\"- 어떤 변동성 지표가 실제 사업 변화를 잘 반영하는가?\")\n",
    "        print(\"- 업종별로 다른 임계값이 필요한가?\")\n",
    "        print(\"- 시간 윈도우는 얼마나 설정해야 하는가?\")\n",
    "        print(\"- 계절성 보정이 필요한가?\")\n",
    "    \n",
    "    # ============ 시각화 및 리포트 생성 ============\n",
    "    \n",
    "    def create_eda_visualizations(self, processed_data):\n",
    "        \"\"\"탐색적 데이터 분석 시각화\"\"\"\n",
    "        print(\"\\n📈 EDA 시각화 생성 중...\")\n",
    "        \n",
    "        # 1. 요일별 사용 패턴 (시간대별 대신)\n",
    "        self._plot_weekday_patterns_daily(processed_data)\n",
    "        \n",
    "        # 2. 요일별 사용 패턴 (바 차트)\n",
    "        self._plot_weekday_patterns(processed_data)\n",
    "        \n",
    "        # 3. 월별 사용량 박스플롯\n",
    "        self._plot_monthly_boxplot(processed_data)\n",
    "        \n",
    "        # 4. 고객별 사용량 분포\n",
    "        self._plot_customer_distribution(processed_data)\n",
    "        \n",
    "        print(\"✅ 시각화 생성 완료\")\n",
    "    \n",
    "    def _plot_weekday_patterns_daily(self, data):\n",
    "        \"\"\"요일별 평균 사용 패턴 (라인 차트)\"\"\"\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(7), weekday_avg.values, marker='o', linewidth=2, markersize=8)\n",
    "            plt.title('Daily Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Weekday')\n",
    "            plt.ylabel('Average Usage (kWh)')\n",
    "            plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"  ⚠️ 요일별 패턴 시각화 불가 - weekday 컬럼 없음\")\n",
    "    \n",
    "    def _plot_weekday_patterns(self, data):\n",
    "        \"\"\"요일별 사용 패턴\"\"\"\n",
    "        if 'weekday' not in data.columns:\n",
    "            print(\"  ⚠️ 요일별 패턴 시각화 불가 - weekday 컬럼 없음\")\n",
    "            return\n",
    "            \n",
    "        weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(range(7), weekday_avg.values, color=['skyblue' if i < 5 else 'lightcoral' for i in range(7)])\n",
    "        plt.title('Average Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.ylabel('Average Usage (kWh)')\n",
    "        plt.xticks(range(7), weekday_names)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 주중/주말 구분 표시\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i >= 5:  # 주말\n",
    "                bar.set_label('Weekend' if i == 5 else '')\n",
    "            else:  # 주중\n",
    "                bar.set_label('Weekday' if i == 0 else '')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_monthly_boxplot(self, data):\n",
    "        \"\"\"월별 사용량 박스플롯\"\"\"\n",
    "        if 'month' not in data.columns:\n",
    "            print(\"  ⚠️ 월별 패턴 시각화 불가 - month 컬럼 없음\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # 월별 데이터 준비\n",
    "        monthly_data = [data[data['month'] == m]['daily_mean'].values for m in range(1, 13)]\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        \n",
    "        # 데이터가 있는 월만 표시\n",
    "        valid_months = []\n",
    "        valid_data = []\n",
    "        valid_names = []\n",
    "        \n",
    "        for i, month_data in enumerate(monthly_data):\n",
    "            if len(month_data) > 0:\n",
    "                valid_months.append(i + 1)\n",
    "                valid_data.append(month_data)\n",
    "                valid_names.append(month_names[i])\n",
    "        \n",
    "        if not valid_data:\n",
    "            print(\"  ⚠️ 월별 데이터 없음\")\n",
    "            return\n",
    "        \n",
    "        box_plot = plt.boxplot(valid_data, labels=valid_names, patch_artist=True)\n",
    "        \n",
    "        # 계절별 색상 구분\n",
    "        colors = []\n",
    "        for month in valid_months:\n",
    "            if month in [12, 1, 2]:  # 겨울\n",
    "                colors.append('lightblue')\n",
    "            elif month in [3, 4, 5]:  # 봄\n",
    "                colors.append('lightgreen')\n",
    "            elif month in [6, 7, 8]:  # 여름\n",
    "                colors.append('lightcoral')\n",
    "            else:  # 가을\n",
    "                colors.append('orange')\n",
    "        \n",
    "        for patch, color in zip(box_plot['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        plt.title('Monthly Power Usage Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Daily Average Usage (kWh)')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_customer_distribution(self, data):\n",
    "        \"\"\"고객별 평균 사용량 분포\"\"\"\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 히스토그램\n",
    "        ax1.hist(customer_avg.values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax1.set_title('Customer Average Usage Distribution', fontweight='bold')\n",
    "        ax1.set_xlabel('Average Usage (kWh)')\n",
    "        ax1.set_ylabel('Number of Customers')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 박스플롯\n",
    "        ax2.boxplot(customer_avg.values, vert=True)\n",
    "        ax2.set_title('Customer Average Usage Boxplot', fontweight='bold')\n",
    "        ax2.set_ylabel('Average Usage (kWh)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_eda_report(self):\n",
    "        \"\"\"EDA 종합 리포트 생성\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KEPCO LP Data Preprocessing and EDA Report\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if hasattr(self, 'data_quality_report'):\n",
    "            print(\"\\n🔍 Step 1: Data Quality Check Results\")\n",
    "            print(\"-\" * 40)\n",
    "            quality = self.data_quality_report\n",
    "            print(f\"Total Records: {quality['lp_quality']['total_records']:,}\")\n",
    "            print(f\"Customers: {quality['lp_quality']['unique_customers']:,}\")\n",
    "            print(f\"Data Completeness: {quality['data_completeness']['completeness_rate']:.2f}%\")\n",
    "            print(f\"Outlier Rate: {quality['anomaly_detection']['outlier_rate_iqr']:.3f}%\")\n",
    "        \n",
    "        if hasattr(self, 'pattern_analysis'):\n",
    "            print(\"\\n📊 Step 2: Basic Pattern Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            pattern = self.pattern_analysis\n",
    "            if 'customer_segmentation' in pattern:\n",
    "                seg = pattern['customer_segmentation']\n",
    "                print(f\"Analyzed Customers: {seg['customer_count']:,}\")\n",
    "                print(f\"Average Usage: {seg['usage_distribution']['mean']:.2f} kWh\")\n",
    "                print(f\"Usage Std Dev: {seg['usage_distribution']['std']:.2f} kWh\")\n",
    "        \n",
    "        if hasattr(self, 'variability_analysis'):\n",
    "            print(\"\\n📈 Step 3: Variability Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            var = self.variability_analysis\n",
    "            if 'basic_variability' in var:\n",
    "                basic = var['basic_variability']\n",
    "                print(f\"Average Daily CV: {basic['daily_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['weekly_cv']['mean']):\n",
    "                    print(f\"Average Weekly CV: {basic['weekly_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['monthly_cv']['mean']):\n",
    "                    print(f\"Average Monthly CV: {basic['monthly_cv']['mean']:.4f}\")\n",
    "        \n",
    "        print(\"\\n💡 Next Steps Recommendations:\")\n",
    "        print(\"- Define and design variability coefficient\")\n",
    "        print(\"- Implement stacking ensemble model\") \n",
    "        print(\"- Apply overfitting prevention techniques\")\n",
    "        print(\"- Develop business activity change prediction algorithm\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 사용 예시 - LP 데이터 파일들을 결합하여 분석\n",
    "\n",
    "# 데이터 로딩 (여러 LP 파일들 결합)\n",
    "lp_files = ['LP데이터1.csv', 'LP데이터2.csv']  # 한 달을 반으로 나눈 파일들\n",
    "customer_data = pd.read_excel('고객번호.xlsx')\n",
    "weather_data = pd.read_csv('weather_daily_processed.csv') \n",
    "calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')\n",
    "\n",
    "# 전처리 및 EDA 실행\n",
    "preprocessor = KepcoDataPreprocessor()\n",
    "\n",
    "# LP 데이터 결합\n",
    "combined_lp_data = preprocessor.load_and_combine_lp_data(lp_files)\n",
    "\n",
    "# 1단계: 데이터 품질 점검 (30분)\n",
    "quality_report = preprocessor.check_data_quality(combined_lp_data, customer_data)\n",
    "\n",
    "# 2단계: 기본 패턴 탐색 (60분) - 기상 데이터도 함께 병합!\n",
    "pattern_analysis = preprocessor.analyze_basic_patterns(combined_lp_data, customer_data, weather_data)\n",
    "\n",
    "# 3단계: 변동성 기초 분석 (90분)\n",
    "variability_analysis = preprocessor.analyze_variability(pattern_analysis['processed_data'])\n",
    "\n",
    "# 4단계: 이상 패턴 탐지 (60분)\n",
    "anomaly_results = preprocessor.detect_anomalous_patterns(pattern_analysis['processed_data'])\n",
    "\n",
    "# 5단계: 전처리 방향 결정 (30분)\n",
    "preprocessing_strategy = preprocessor.decide_preprocessing_strategy(\n",
    "    quality_report, pattern_analysis, variability_analysis\n",
    ")\n",
    "\n",
    "# 시각화 생성 (폰트 오류 없이)\n",
    "preprocessor.create_eda_visualizations(pattern_analysis['processed_data'])\n",
    "\n",
    "# 종합 리포트 생성\n",
    "preprocessor.generate_eda_report()\n",
    "\n",
    "print(\"✅ Complete LP data preprocessing and EDA finished!\")\n",
    "print(\"📤 Next: Define variability coefficient and implement stacking model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
