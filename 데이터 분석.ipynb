{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 폰트 설정 (에러 방지)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KepcoDataPreprocessor:\n",
    "    \"\"\"\n",
    "    한국전력공사 LP 데이터 전처리 및 탐색적 분석\n",
    "    \n",
    "    단계별 분석 계획:\n",
    "    1단계: 데이터 품질 점검 (30분)\n",
    "    2단계: 기본 패턴 탐색 (60분) \n",
    "    3단계: 변동성 기초 분석 (90분)\n",
    "    4단계: 이상 패턴 탐지 (60분)\n",
    "    5단계: 전처리 방향 결정 (30분)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_quality_report = {}\n",
    "        self.pattern_analysis = {}\n",
    "        self.variability_analysis = {}\n",
    "    \n",
    "    # ============ 데이터 로딩 및 결합 ============\n",
    "    \n",
    "    def load_and_combine_lp_data(self, lp_files):\n",
    "        \"\"\"\n",
    "        LP 데이터 파일들을 읽어서 결합\n",
    "        파일들은 한 달을 반으로 나눠서 제공됨 (예: LP데이터1.csv + LP데이터2.csv)\n",
    "        \"\"\"\n",
    "        print(\"📂 LP 데이터 파일들 로딩 및 결합 중...\")\n",
    "        \n",
    "        combined_data = []\n",
    "        \n",
    "        for i, file_path in enumerate(lp_files):\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"  ✅ {file_path}: {len(df):,}건 로딩\")\n",
    "                \n",
    "                # 기본 정보 출력\n",
    "                if 'LP수신일자' in df.columns:\n",
    "                    dates = pd.to_datetime(df['LP수신일자'])\n",
    "                    print(f\"     기간: {dates.min()} ~ {dates.max()}\")\n",
    "                    print(f\"     고객수: {df['대체고객번호'].nunique()}명\")\n",
    "                \n",
    "                combined_data.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ {file_path} 로딩 실패: {e}\")\n",
    "        \n",
    "        if not combined_data:\n",
    "            raise ValueError(\"로딩된 LP 데이터가 없습니다.\")\n",
    "        \n",
    "        # 데이터 결합\n",
    "        final_data = pd.concat(combined_data, ignore_index=True)\n",
    "        \n",
    "        print(f\"  🔗 결합 완료: 총 {len(final_data):,}건\")\n",
    "        print(f\"     전체 기간: {pd.to_datetime(final_data['LP수신일자']).min()} ~ {pd.to_datetime(final_data['LP수신일자']).max()}\")\n",
    "        print(f\"     총 고객수: {final_data['대체고객번호'].nunique()}명\")\n",
    "        \n",
    "        return final_data\n",
    "    \n",
    "    # ============ 1단계: 데이터 품질 점검 (30분) ============\n",
    "    \n",
    "    def check_data_quality(self, lp_data, customer_data):\n",
    "        \"\"\"\n",
    "        데이터 품질 점검 및 기본 정보 분석\n",
    "        \"\"\"\n",
    "        print(\"🔍 1단계: 데이터 품질 점검 시작...\")\n",
    "        \n",
    "        # 고객 기본정보 분석\n",
    "        customer_info = self._analyze_customer_info(customer_data)\n",
    "        \n",
    "        # LP 데이터 품질 점검\n",
    "        lp_quality = self._check_lp_data_quality(lp_data)\n",
    "        \n",
    "        self.data_quality_report = {\n",
    "            'customer_info': customer_info,\n",
    "            'lp_quality': lp_quality,\n",
    "            'data_completeness': self._calculate_completeness(lp_data),\n",
    "            'anomaly_detection': self._detect_data_anomalies(lp_data)\n",
    "        }\n",
    "        \n",
    "        self._print_quality_summary()\n",
    "        return self.data_quality_report\n",
    "    \n",
    "    def _analyze_customer_info(self, customer_data):\n",
    "        \"\"\"고객 기본정보 분석\"\"\"\n",
    "        if customer_data is None:\n",
    "            return {\"message\": \"고객 데이터 없음\"}\n",
    "        \n",
    "        print(f\"  📋 고객 데이터 컬럼: {list(customer_data.columns)}\")\n",
    "        \n",
    "        # 가능한 컬럼명들 매핑\n",
    "        column_mapping = {\n",
    "            '계약종별': ['계약종별', 'contract_type', 'Contract_Type'],\n",
    "            '사용용도': ['사용용도', 'usage_purpose', 'Usage_Purpose'], \n",
    "            '산업분류': ['산업분류', 'industry', 'Industry']\n",
    "        }\n",
    "        \n",
    "        info = {'total_customers': len(customer_data)}\n",
    "        \n",
    "        for key, possible_cols in column_mapping.items():\n",
    "            found_col = None\n",
    "            for col in possible_cols:\n",
    "                if col in customer_data.columns:\n",
    "                    found_col = col\n",
    "                    break\n",
    "            \n",
    "            if found_col:\n",
    "                info[f'{key}_dist'] = customer_data[found_col].value_counts().to_dict()\n",
    "                print(f\"  ✅ {key} 분포: {dict(list(info[f'{key}_dist'].items())[:3])}...\")  # 상위 3개만 출력\n",
    "            else:\n",
    "                info[f'{key}_dist'] = {}\n",
    "                print(f\"  ⚠️ {key} 컬럼 없음\")\n",
    "        \n",
    "        print(f\"✅ 고객수: {info['total_customers']:,}명\")\n",
    "        return info\n",
    "    \n",
    "    def _check_lp_data_quality(self, lp_data):\n",
    "        \"\"\"LP 데이터 품질 점검\"\"\"\n",
    "        # 데이터 타입 변환\n",
    "        lp_data['LP수신일자'] = pd.to_datetime(lp_data['LP수신일자'])\n",
    "        \n",
    "        quality = {\n",
    "            'total_records': len(lp_data),\n",
    "            'date_range': {\n",
    "                'start': lp_data['LP수신일자'].min(),\n",
    "                'end': lp_data['LP수신일자'].max()\n",
    "            },\n",
    "            'unique_customers': lp_data['대체고객번호'].nunique(),\n",
    "            'missing_values': lp_data.isnull().sum().to_dict(),\n",
    "            'negative_values': (lp_data['순방향유효전력'] < 0).sum(),\n",
    "            'zero_values': (lp_data['순방향유효전력'] == 0).sum(),\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 총 레코드: {quality['total_records']:,}건\")\n",
    "        print(f\"✅ 기간: {quality['date_range']['start']} ~ {quality['date_range']['end']}\")\n",
    "        print(f\"✅ 고객수: {quality['unique_customers']:,}명\")\n",
    "        print(f\"✅ 음수값: {quality['negative_values']:,}건\")\n",
    "        print(f\"✅ 0값: {quality['zero_values']:,}건\")\n",
    "        \n",
    "        return quality\n",
    "    \n",
    "    def _calculate_completeness(self, lp_data):\n",
    "        \"\"\"데이터 완정성 계산\"\"\"\n",
    "        lp_data['date'] = lp_data['LP수신일자'].dt.date\n",
    "        lp_data['hour'] = lp_data['LP수신일자'].dt.hour\n",
    "        lp_data['quarter_hour'] = (lp_data['LP수신일자'].dt.minute // 15) * 15\n",
    "        \n",
    "        # 15분 간격 정확성 체크\n",
    "        expected_intervals = pd.date_range(\n",
    "            start=lp_data['LP수신일자'].min(),\n",
    "            end=lp_data['LP수신일자'].max(),\n",
    "            freq='15min'\n",
    "        )\n",
    "        \n",
    "        completeness = {\n",
    "            'expected_records': len(expected_intervals) * lp_data['대체고객번호'].nunique(),\n",
    "            'actual_records': len(lp_data),\n",
    "            'completeness_rate': len(lp_data) / (len(expected_intervals) * lp_data['대체고객번호'].nunique()) * 100\n",
    "        }\n",
    "        \n",
    "        # 고객별 완정성\n",
    "        customer_completeness = lp_data.groupby('대체고객번호').size() / len(expected_intervals) * 100\n",
    "        completeness['customer_completeness'] = {\n",
    "            'mean': customer_completeness.mean(),\n",
    "            'min': customer_completeness.min(),\n",
    "            'max': customer_completeness.max(),\n",
    "            'std': customer_completeness.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ 데이터 완정성: {completeness['completeness_rate']:.2f}%\")\n",
    "        \n",
    "        return completeness\n",
    "    \n",
    "    def _detect_data_anomalies(self, lp_data):\n",
    "        \"\"\"데이터 이상치 탐지\"\"\"\n",
    "        # 통계적 이상치 탐지\n",
    "        Q1 = lp_data['순방향유효전력'].quantile(0.25)\n",
    "        Q3 = lp_data['순방향유효전력'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        outliers_iqr = lp_data[\n",
    "            (lp_data['순방향유효전력'] < Q1 - 1.5 * IQR) | \n",
    "            (lp_data['순방향유효전력'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        # Z-score 이상치\n",
    "        z_scores = np.abs((lp_data['순방향유효전력'] - lp_data['순방향유효전력'].mean()) / lp_data['순방향유효전력'].std())\n",
    "        outliers_zscore = lp_data[z_scores > 3]\n",
    "        \n",
    "        anomalies = {\n",
    "            'iqr_outliers': len(outliers_iqr),\n",
    "            'zscore_outliers': len(outliers_zscore),\n",
    "            'outlier_rate_iqr': len(outliers_iqr) / len(lp_data) * 100,\n",
    "            'outlier_rate_zscore': len(outliers_zscore) / len(lp_data) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ IQR 이상치: {anomalies['outlier_rate_iqr']:.3f}%\")\n",
    "        print(f\"✅ Z-score 이상치: {anomalies['outlier_rate_zscore']:.3f}%\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def _print_quality_summary(self):\n",
    "        \"\"\"품질 점검 요약 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"📊 데이터 품질 점검 완료\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # ============ 2단계: 기본 패턴 탐색 (60분) ============\n",
    "    \n",
    "    def analyze_basic_patterns(self, lp_data, customer_data=None, calendar_data=None):\n",
    "        \"\"\"\n",
    "        기본 전력 사용 패턴 분석\n",
    "        \"\"\"\n",
    "        print(\"\\n📊 2단계: 기본 패턴 탐색 시작...\")\n",
    "        \n",
    "        # 데이터 전처리\n",
    "        processed_data = self._preprocess_for_pattern_analysis(lp_data, customer_data, calendar_data)\n",
    "        \n",
    "        # 시간별 패턴 분석\n",
    "        time_patterns = self._analyze_time_patterns(processed_data)\n",
    "        \n",
    "        # 고객 세분화 기초 분석\n",
    "        customer_segmentation = self._analyze_customer_segmentation(processed_data)\n",
    "        \n",
    "        self.pattern_analysis = {\n",
    "            'time_patterns': time_patterns,\n",
    "            'customer_segmentation': customer_segmentation,\n",
    "            'processed_data': processed_data\n",
    "        }\n",
    "        \n",
    "        return self.pattern_analysis\n",
    "    \n",
    "    def _preprocess_for_pattern_analysis(self, lp_data, customer_data, calendar_data):\n",
    "        \"\"\"패턴 분석을 위한 데이터 전처리\"\"\"\n",
    "        print(\"  🔄 데이터 전처리 중...\")\n",
    "        \n",
    "        # 시간 변수 생성\n",
    "        lp_data['datetime'] = pd.to_datetime(lp_data['LP수신일자'])\n",
    "        lp_data['date'] = lp_data['datetime'].dt.date\n",
    "        lp_data['hour'] = lp_data['datetime'].dt.hour\n",
    "        lp_data['weekday'] = lp_data['datetime'].dt.weekday  # 0=월요일\n",
    "        lp_data['month'] = lp_data['datetime'].dt.month\n",
    "        lp_data['quarter'] = lp_data['datetime'].dt.quarter\n",
    "        lp_data['is_weekend'] = lp_data['weekday'].isin([5, 6])  # 토, 일\n",
    "        \n",
    "        # 일간 집계 데이터 생성\n",
    "        daily_agg = lp_data.groupby(['대체고객번호', 'date']).agg({\n",
    "            '순방향유효전력': ['sum', 'mean', 'max', 'min', 'std']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        daily_agg.columns = ['customer_id', 'date', 'daily_sum', 'daily_mean', 'daily_max', 'daily_min', 'daily_std']\n",
    "        \n",
    "        # 날짜 관련 피처 다시 생성 (일간 집계 후)\n",
    "        daily_agg['date_dt'] = pd.to_datetime(daily_agg['date'])\n",
    "        daily_agg['weekday'] = daily_agg['date_dt'].dt.weekday  # 0=월요일\n",
    "        daily_agg['month'] = daily_agg['date_dt'].dt.month\n",
    "        daily_agg['quarter'] = daily_agg['date_dt'].dt.quarter\n",
    "        daily_agg['is_weekend'] = daily_agg['weekday'].isin([5, 6])  # 토, 일\n",
    "        \n",
    "        print(f\"  ✅ 시간 피처 생성 완료: weekday, month, quarter, is_weekend\")\n",
    "        \n",
    "        # 고객 정보 병합\n",
    "        if customer_data is not None:\n",
    "            # 고객 데이터의 실제 컬럼명 확인\n",
    "            customer_key_col = None\n",
    "            possible_keys = ['대체고객번호', '고객번호', 'customer_id', 'Customer_ID']\n",
    "            \n",
    "            for col in possible_keys:\n",
    "                if col in customer_data.columns:\n",
    "                    customer_key_col = col\n",
    "                    break\n",
    "            \n",
    "            if customer_key_col:\n",
    "                daily_agg = daily_agg.merge(customer_data, left_on='customer_id', right_on=customer_key_col, how='left')\n",
    "                print(f\"  ✅ 고객 정보 병합 완료 (키: {customer_key_col})\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ 고객 데이터 병합 실패 - 키 컬럼을 찾을 수 없음. 사용 가능한 컬럼: {list(customer_data.columns)}\")\n",
    "                print(f\"  ℹ️ 고객 정보 없이 분석 계속...\")\n",
    "        \n",
    "        # 기상/달력 정보 병합\n",
    "        if calendar_data is not None:\n",
    "            # 날짜 컬럼 확인 및 변환\n",
    "            date_col = None\n",
    "            possible_date_cols = ['date', '날짜', 'Date', 'DATE']\n",
    "            \n",
    "            for col in possible_date_cols:\n",
    "                if col in calendar_data.columns:\n",
    "                    date_col = col\n",
    "                    break\n",
    "            \n",
    "            if date_col:\n",
    "                # 기상 데이터가 weather_daily_processed.csv인 경우 날짜 형식 확인\n",
    "                if 'year' in calendar_data.columns and 'month' in calendar_data.columns and 'day' in calendar_data.columns:\n",
    "                    # year, month, day 컬럼으로 날짜 생성\n",
    "                    calendar_data['date_parsed'] = pd.to_datetime(calendar_data[['year', 'month', 'day']])\n",
    "                    calendar_data['date_for_merge'] = calendar_data['date_parsed'].dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on='date_for_merge', how='left')\n",
    "                    print(f\"  ✅ 기상/달력 정보 병합 완료 (year-month-day 기준)\")\n",
    "                else:\n",
    "                    # 일반적인 date 컬럼 사용\n",
    "                    calendar_data[date_col] = pd.to_datetime(calendar_data[date_col]).dt.date\n",
    "                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on=date_col, how='left')\n",
    "                    print(f\"  ✅ 기상/달력 정보 병합 완료 (키: {date_col})\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ 기상/달력 데이터 병합 실패 - 날짜 컬럼을 찾을 수 없음. 사용 가능한 컬럼: {list(calendar_data.columns)}\")\n",
    "                print(f\"  ℹ️ 기상/달력 정보 없이 분석 계속...\")\n",
    "        \n",
    "        print(f\"  ✅ 일간 집계 데이터: {len(daily_agg):,}건\")\n",
    "        return daily_agg\n",
    "    \n",
    "    def _analyze_time_patterns(self, data):\n",
    "        \"\"\"시간별 패턴 분석\"\"\"\n",
    "        print(\"  📈 시간별 패턴 분석 중...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. 시간대별 패턴 (일간 집계 데이터에서는 의미 없으므로 건너뛰기)\n",
    "        # hourly_pattern = data.groupby('hour')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "        # patterns['hourly'] = hourly_pattern\n",
    "        \n",
    "        # 2. 요일별 패턴\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_pattern = data.groupby('weekday')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekday'] = weekday_pattern\n",
    "        \n",
    "        # 3. 월별 패턴 (계절성)\n",
    "        if 'month' in data.columns:\n",
    "            monthly_pattern = data.groupby('month')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['monthly'] = monthly_pattern\n",
    "        \n",
    "        # 4. 주중/주말 패턴\n",
    "        if 'is_weekend' in data.columns:\n",
    "            weekend_pattern = data.groupby('is_weekend')['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['weekend'] = weekend_pattern\n",
    "        \n",
    "        # 5. 업종별 패턴 (고객 데이터가 있는 경우)\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['사용용도', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            industry_pattern = data.groupby(usage_purpose_col)['daily_mean'].agg(['mean', 'std', 'count'])\n",
    "            patterns['industry'] = industry_pattern\n",
    "            print(f\"  ✅ {usage_purpose_col} 기준 업종별 패턴 분석 완료\")\n",
    "        \n",
    "        print(f\"  ✅ 시간별 패턴 분석 완료 ({len(patterns)}개 패턴)\")\n",
    "        return patterns\n",
    "    \n",
    "    def _analyze_customer_segmentation(self, data):\n",
    "        \"\"\"고객 세분화 기초 분석\"\"\"\n",
    "        print(\"  👥 고객 세분화 분석 중...\")\n",
    "        \n",
    "        # 고객별 평균 사용량 계산\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        # 사용량 규모별 분류\n",
    "        segmentation = {\n",
    "            'large_users': customer_avg.quantile(0.9),  # 상위 10%\n",
    "            'medium_users': customer_avg.quantile(0.5),  # 중간 50%\n",
    "            'small_users': customer_avg.quantile(0.1),   # 하위 10%\n",
    "        }\n",
    "        \n",
    "        # 고객별 사용량 분포\n",
    "        customer_stats = {\n",
    "            'customer_count': len(customer_avg),\n",
    "            'usage_distribution': {\n",
    "                'mean': customer_avg.mean(),\n",
    "                'std': customer_avg.std(),\n",
    "                'min': customer_avg.min(),\n",
    "                'max': customer_avg.max(),\n",
    "                'q25': customer_avg.quantile(0.25),\n",
    "                'q50': customer_avg.quantile(0.50),\n",
    "                'q75': customer_avg.quantile(0.75)\n",
    "            },\n",
    "            'segmentation_thresholds': segmentation\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ {customer_stats['customer_count']:,}명 고객 세분화 완료\")\n",
    "        return customer_stats\n",
    "    \n",
    "    # ============ 3단계: 변동성 기초 분석 (90분) ============\n",
    "    \n",
    "    def analyze_variability(self, processed_data):\n",
    "        \"\"\"\n",
    "        변동성 기초 분석 - 변동계수 설계를 위한 기초 작업\n",
    "        \"\"\"\n",
    "        print(\"\\n📈 3단계: 변동성 기초 분석 시작...\")\n",
    "        \n",
    "        # 기본 변동성 지표 계산\n",
    "        basic_variability = self._calculate_basic_variability(processed_data)\n",
    "        \n",
    "        # 변동성 패턴 분석\n",
    "        variability_patterns = self._analyze_variability_patterns(processed_data)\n",
    "        \n",
    "        self.variability_analysis = {\n",
    "            'basic_variability': basic_variability,\n",
    "            'variability_patterns': variability_patterns\n",
    "        }\n",
    "        \n",
    "        return self.variability_analysis\n",
    "    \n",
    "    def _calculate_basic_variability(self, data):\n",
    "        \"\"\"기본 변동성 지표 계산\"\"\"\n",
    "        print(\"  📊 기본 변동성 지표 계산 중...\")\n",
    "        \n",
    "        variability_metrics = {}\n",
    "        \n",
    "        # 고객별 변동계수 계산\n",
    "        customer_cv = data.groupby('customer_id').apply(\n",
    "            lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "        )\n",
    "        \n",
    "        # 1. 일간 변동계수\n",
    "        variability_metrics['daily_cv'] = {\n",
    "            'mean': customer_cv.mean(),\n",
    "            'std': customer_cv.std(),\n",
    "            'distribution': customer_cv.describe()\n",
    "        }\n",
    "        \n",
    "        # 2. 주간 변동계수 (주별 패턴의 일관성)\n",
    "        try:\n",
    "            # 주 번호 생성\n",
    "            data_with_week = data.copy()\n",
    "            data_with_week['week'] = pd.to_datetime(data_with_week['date']).dt.isocalendar().week\n",
    "            \n",
    "            weekly_cv = data_with_week.groupby(['customer_id', 'week']).agg({\n",
    "                'daily_mean': ['mean', 'std']\n",
    "            }).reset_index()\n",
    "            weekly_cv.columns = ['customer_id', 'week', 'weekly_mean', 'weekly_std']\n",
    "            weekly_cv['weekly_cv'] = weekly_cv['weekly_std'] / weekly_cv['weekly_mean']\n",
    "            \n",
    "            customer_weekly_cv = weekly_cv.groupby('customer_id')['weekly_cv'].mean()\n",
    "            variability_metrics['weekly_cv'] = {\n",
    "                'mean': customer_weekly_cv.mean(),\n",
    "                'std': customer_weekly_cv.std(),\n",
    "                'distribution': customer_weekly_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ 주간 변동계수 계산 실패: {e}\")\n",
    "            variability_metrics['weekly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 3. 월간 변동계수\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                monthly_cv = data.groupby(['customer_id', 'month']).agg({\n",
    "                    'daily_mean': ['mean', 'std']\n",
    "                }).reset_index()\n",
    "                monthly_cv.columns = ['customer_id', 'month', 'monthly_mean', 'monthly_std']\n",
    "                monthly_cv['monthly_cv'] = monthly_cv['monthly_std'] / monthly_cv['monthly_mean']\n",
    "                \n",
    "                customer_monthly_cv = monthly_cv.groupby('customer_id')['monthly_cv'].mean()\n",
    "                variability_metrics['monthly_cv'] = {\n",
    "                    'mean': customer_monthly_cv.mean(),\n",
    "                    'std': customer_monthly_cv.std(),\n",
    "                    'distribution': customer_monthly_cv.describe()\n",
    "                }\n",
    "            else:\n",
    "                print(\"    ⚠️ month 컬럼이 없어 월간 변동계수 계산 건너뛰기\")\n",
    "                variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ 월간 변동계수 계산 실패: {e}\")\n",
    "            variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        # 4. 추가 변동성 지표\n",
    "        # 범위 기반 변동성\n",
    "        try:\n",
    "            customer_range_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: (x['daily_mean'].max() - x['daily_mean'].min()) / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            variability_metrics['range_based_cv'] = {\n",
    "                'mean': customer_range_cv.mean(),\n",
    "                'std': customer_range_cv.std(),\n",
    "                'distribution': customer_range_cv.describe()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"    ⚠️ 범위 기반 변동계수 계산 실패: {e}\")\n",
    "            variability_metrics['range_based_cv'] = {'mean': np.nan, 'std': np.nan}\n",
    "        \n",
    "        print(f\"  ✅ 기본 변동성 지표 계산 완료\")\n",
    "        return variability_metrics\n",
    "    \n",
    "    def _analyze_variability_patterns(self, data):\n",
    "        \"\"\"변동성 패턴 분석\"\"\"\n",
    "        print(\"  🔍 변동성 패턴 분석 중...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # 1. 업종별 변동성 비교\n",
    "        usage_purpose_col = None\n",
    "        possible_usage_cols = ['사용용도', 'usage_purpose', 'Usage_Purpose']\n",
    "        \n",
    "        for col in possible_usage_cols:\n",
    "            if col in data.columns:\n",
    "                usage_purpose_col = col\n",
    "                break\n",
    "        \n",
    "        if usage_purpose_col:\n",
    "            try:\n",
    "                industry_variability = data.groupby([usage_purpose_col, 'customer_id']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                industry_variability.columns = [usage_purpose_col, 'customer_id', 'cv']\n",
    "                \n",
    "                industry_cv_summary = industry_variability.groupby(usage_purpose_col)['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['industry_variability'] = industry_cv_summary\n",
    "                print(f\"  ✅ {usage_purpose_col} 기준 업종별 변동성 분석 완료\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 업종별 변동성 분석 실패: {e}\")\n",
    "        \n",
    "        # 2. 계절별 변동성 차이\n",
    "        try:\n",
    "            if 'month' in data.columns:\n",
    "                seasonal_variability = data.groupby(['customer_id', 'month']).apply(\n",
    "                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "                ).reset_index()\n",
    "                seasonal_variability.columns = ['customer_id', 'month', 'cv']\n",
    "                \n",
    "                seasonal_cv_summary = seasonal_variability.groupby('month')['cv'].agg(['mean', 'std', 'count'])\n",
    "                patterns['seasonal_variability'] = seasonal_cv_summary\n",
    "                print(f\"  ✅ 계절별 변동성 분석 완료\")\n",
    "            else:\n",
    "                print(\"  ⚠️ month 컬럼이 없어 계절별 변동성 분석 건너뛰기\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ 계절별 변동성 분석 실패: {e}\")\n",
    "        \n",
    "        # 3. 사용량 규모별 변동성\n",
    "        try:\n",
    "            customer_avg_usage = data.groupby('customer_id')['daily_mean'].mean()\n",
    "            customer_cv = data.groupby('customer_id').apply(\n",
    "                lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan\n",
    "            )\n",
    "            \n",
    "            # 사용량 규모별 그룹핑\n",
    "            usage_quantiles = customer_avg_usage.quantile([0.33, 0.67])\n",
    "            def categorize_usage(usage):\n",
    "                if usage <= usage_quantiles.iloc[0]:\n",
    "                    return 'Low'\n",
    "                elif usage <= usage_quantiles.iloc[1]:\n",
    "                    return 'Medium'\n",
    "                else:\n",
    "                    return 'High'\n",
    "            \n",
    "            customer_usage_category = customer_avg_usage.apply(categorize_usage)\n",
    "            usage_cv_df = pd.DataFrame({\n",
    "                'usage_category': customer_usage_category,\n",
    "                'cv': customer_cv\n",
    "            })\n",
    "            \n",
    "            usage_cv_summary = usage_cv_df.groupby('usage_category')['cv'].agg(['mean', 'std', 'count'])\n",
    "            patterns['usage_level_variability'] = usage_cv_summary\n",
    "            print(f\"  ✅ 사용량 규모별 변동성 분석 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ 사용량 규모별 변동성 분석 실패: {e}\")\n",
    "        \n",
    "        print(f\"  ✅ 변동성 패턴 분석 완료 ({len(patterns)}개 패턴)\")\n",
    "        return patterns\n",
    "    \n",
    "    # ============ 4단계: 이상 패턴 탐지 (60분) ============\n",
    "    \n",
    "    def detect_anomalous_patterns(self, processed_data):\n",
    "        \"\"\"\n",
    "        이상 패턴 탐지\n",
    "        \"\"\"\n",
    "        print(\"\\n🎯 4단계: 이상 패턴 탐지 시작...\")\n",
    "        \n",
    "        # 통계적 이상치 식별\n",
    "        statistical_outliers = self._identify_statistical_outliers(processed_data)\n",
    "        \n",
    "        # 시계열 이상치 탐지\n",
    "        temporal_anomalies = self._detect_temporal_anomalies(processed_data)\n",
    "        \n",
    "        # 비정상 패턴 정의\n",
    "        abnormal_patterns = self._define_abnormal_patterns(processed_data)\n",
    "        \n",
    "        anomaly_results = {\n",
    "            'statistical_outliers': statistical_outliers,\n",
    "            'temporal_anomalies': temporal_anomalies,\n",
    "            'abnormal_patterns': abnormal_patterns\n",
    "        }\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def _identify_statistical_outliers(self, data):\n",
    "        \"\"\"통계적 이상치 식별\"\"\"\n",
    "        print(\"  🔍 통계적 이상치 식별 중...\")\n",
    "        \n",
    "        outliers = {}\n",
    "        \n",
    "        # IQR 방법\n",
    "        Q1 = data['daily_mean'].quantile(0.25)\n",
    "        Q3 = data['daily_mean'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        iqr_outliers = data[\n",
    "            (data['daily_mean'] < Q1 - 1.5 * IQR) | \n",
    "            (data['daily_mean'] > Q3 + 1.5 * IQR)\n",
    "        ]\n",
    "        \n",
    "        outliers['iqr_outliers'] = {\n",
    "            'count': len(iqr_outliers),\n",
    "            'rate': len(iqr_outliers) / len(data) * 100,\n",
    "            'customer_count': iqr_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        # Z-score 방법\n",
    "        z_scores = np.abs((data['daily_mean'] - data['daily_mean'].mean()) / data['daily_mean'].std())\n",
    "        zscore_outliers = data[z_scores > 3]\n",
    "        \n",
    "        outliers['zscore_outliers'] = {\n",
    "            'count': len(zscore_outliers),\n",
    "            'rate': len(zscore_outliers) / len(data) * 100,\n",
    "            'customer_count': zscore_outliers['customer_id'].nunique()\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ IQR 이상치: {outliers['iqr_outliers']['count']:,}건 ({outliers['iqr_outliers']['rate']:.2f}%)\")\n",
    "        print(f\"  ✅ Z-score 이상치: {outliers['zscore_outliers']['count']:,}건 ({outliers['zscore_outliers']['rate']:.2f}%)\")\n",
    "        \n",
    "        return outliers\n",
    "    \n",
    "    def _detect_temporal_anomalies(self, data):\n",
    "        \"\"\"시계열 이상치 탐지\"\"\"\n",
    "        print(\"  ⏰ 시계열 이상치 탐지 중...\")\n",
    "        \n",
    "        temporal_anomalies = {}\n",
    "        \n",
    "        # 고객별 시계열 이상치 탐지\n",
    "        for customer_id in data['customer_id'].unique()[:100]:  # 샘플로 100명만\n",
    "            customer_data = data[data['customer_id'] == customer_id].sort_values('date')\n",
    "            \n",
    "            if len(customer_data) < 30:  # 최소 30일 데이터 필요\n",
    "                continue\n",
    "            \n",
    "            # 급격한 증가/감소 탐지 (>200% 변화)\n",
    "            customer_data['pct_change'] = customer_data['daily_mean'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['pct_change']) > 2.0]  # 200% 변화\n",
    "            \n",
    "            # 연속적인 0값 탐지\n",
    "            zero_streaks = customer_data[customer_data['daily_mean'] == 0]\n",
    "            \n",
    "            if len(sudden_changes) > 0 or len(zero_streaks) > 5:  # 5일 이상 연속 0값\n",
    "                temporal_anomalies[customer_id] = {\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'zero_streaks': len(zero_streaks)\n",
    "                }\n",
    "        \n",
    "        print(f\"  ✅ {len(temporal_anomalies):,}명 고객에서 시계열 이상 탐지\")\n",
    "        \n",
    "        return temporal_anomalies\n",
    "    \n",
    "    def _define_abnormal_patterns(self, data):\n",
    "        \"\"\"비정상 패턴 정의\"\"\"\n",
    "        print(\"  📋 비정상 패턴 정의 중...\")\n",
    "        \n",
    "        abnormal_patterns = {\n",
    "            'pattern_definitions': {\n",
    "                1: '전력 사용 급증/급감 (사업 확장/축소)',\n",
    "                2: '사용 패턴 변화 (운영시간 변경)', \n",
    "                3: '효율성 급변 (설비 교체/고장)',\n",
    "                4: '계절성 이탈 (사업 모델 변화)'\n",
    "            },\n",
    "            'detection_criteria': {\n",
    "                'usage_spike': 'daily_mean > mean + 3*std',\n",
    "                'usage_drop': 'daily_mean < mean - 3*std',\n",
    "                'pattern_shift': 'monthly pattern change > 50%',\n",
    "                'efficiency_change': 'weekly efficiency variance > threshold'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"  ✅ {len(abnormal_patterns['pattern_definitions'])}가지 비정상 패턴 정의 완료\")\n",
    "        \n",
    "        return abnormal_patterns\n",
    "    \n",
    "    # ============ 5단계: 전처리 방향 결정 (30분) ============\n",
    "    \n",
    "    def decide_preprocessing_strategy(self, data_quality_report, pattern_analysis, variability_analysis):\n",
    "        \"\"\"\n",
    "        전처리 방향 결정\n",
    "        \"\"\"\n",
    "        print(\"\\n🔧 5단계: 전처리 방향 결정...\")\n",
    "        \n",
    "        preprocessing_strategy = {\n",
    "            'missing_data_handling': self._decide_missing_data_strategy(data_quality_report),\n",
    "            'outlier_handling': self._decide_outlier_strategy(data_quality_report),\n",
    "            'normalization_method': self._decide_normalization_strategy(pattern_analysis),\n",
    "            'feature_engineering': self._decide_feature_engineering(pattern_analysis, variability_analysis)\n",
    "        }\n",
    "        \n",
    "        self._print_preprocessing_summary(preprocessing_strategy)\n",
    "        \n",
    "        return preprocessing_strategy\n",
    "    \n",
    "    def _decide_missing_data_strategy(self, quality_report):\n",
    "        \"\"\"결측치 처리 전략 결정\"\"\"\n",
    "        completeness_rate = quality_report['data_completeness']['completeness_rate']\n",
    "        \n",
    "        if completeness_rate > 95:\n",
    "            strategy = \"선형보간 또는 forward fill\"\n",
    "        elif completeness_rate > 80:\n",
    "            strategy = \"계절성 고려 보간\"\n",
    "        else:\n",
    "            strategy = \"장기 결측 기간 분석 제외\"\n",
    "        \n",
    "        return {\n",
    "            'completeness_rate': completeness_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_outlier_strategy(self, quality_report):\n",
    "        \"\"\"이상치 처리 전략 결정\"\"\"\n",
    "        outlier_rate = quality_report['anomaly_detection']['outlier_rate_iqr']\n",
    "        \n",
    "        if outlier_rate < 1:\n",
    "            strategy = \"이상치 유지 (정상 범위)\"\n",
    "        elif outlier_rate < 5:\n",
    "            strategy = \"extreme outlier만 제거\"\n",
    "        else:\n",
    "            strategy = \"robust 통계량 사용\"\n",
    "        \n",
    "        return {\n",
    "            'outlier_rate': outlier_rate,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_normalization_strategy(self, pattern_analysis):\n",
    "        \"\"\"정규화 방법 결정\"\"\"\n",
    "        customer_stats = pattern_analysis['customer_segmentation']\n",
    "        usage_std = customer_stats['usage_distribution']['std']\n",
    "        usage_mean = customer_stats['usage_distribution']['mean']\n",
    "        cv = usage_std / usage_mean if usage_mean > 0 else 0\n",
    "        \n",
    "        if cv > 1.0:\n",
    "            strategy = \"고객별 표준화 + 로그 변환\"\n",
    "        elif cv > 0.5:\n",
    "            strategy = \"고객별 표준화\"\n",
    "        else:\n",
    "            strategy = \"전체 Min-Max 정규화\"\n",
    "        \n",
    "        return {\n",
    "            'coefficient_of_variation': cv,\n",
    "            'recommended_strategy': strategy\n",
    "        }\n",
    "    \n",
    "    def _decide_feature_engineering(self, pattern_analysis, variability_analysis):\n",
    "        \"\"\"피처 엔지니어링 전략 결정\"\"\"\n",
    "        features_to_create = []\n",
    "        \n",
    "        # 시간 기반 피처\n",
    "        time_patterns = pattern_analysis['time_patterns']\n",
    "        if 'monthly' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'month_sin', 'month_cos',  # 계절 순환 피처\n",
    "                'is_summer', 'is_winter',  # 계절 더미 변수\n",
    "            ])\n",
    "        \n",
    "        # 요일 기반 피처\n",
    "        if 'weekday' in time_patterns:\n",
    "            features_to_create.extend([\n",
    "                'weekday_sin', 'weekday_cos',  # 요일 순환 피처\n",
    "                'is_weekend'                   # 주말 여부\n",
    "            ])\n",
    "        \n",
    "        # 변동성 기반 피처\n",
    "        if variability_analysis:\n",
    "            features_to_create.extend([\n",
    "                'rolling_mean_7d',       # 7일 이동평균\n",
    "                'rolling_std_7d',        # 7일 이동표준편차\n",
    "                'usage_volatility',      # 변동성 지수\n",
    "            ])\n",
    "        \n",
    "        # 고객 기반 피처\n",
    "        features_to_create.extend([\n",
    "            'customer_avg_usage',      # 고객 평균 사용량\n",
    "            'customer_usage_rank',     # 고객 사용량 순위\n",
    "            'deviation_from_avg'       # 평균 대비 편차\n",
    "        ])\n",
    "        \n",
    "        return {\n",
    "            'features_to_create': features_to_create,\n",
    "            'total_features': len(features_to_create)\n",
    "        }\n",
    "    \n",
    "    def _print_preprocessing_summary(self, strategy):\n",
    "        \"\"\"전처리 전략 요약 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🔧 전처리 전략 결정 완료\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"📋 결측치 처리: {strategy['missing_data_handling']['recommended_strategy']}\")\n",
    "        print(f\"🎯 이상치 처리: {strategy['outlier_handling']['recommended_strategy']}\")\n",
    "        print(f\"📊 정규화 방법: {strategy['normalization_method']['recommended_strategy']}\")\n",
    "        print(f\"🛠️ 생성할 피처: {strategy['feature_engineering']['total_features']}개\")\n",
    "        \n",
    "        print(\"\\n💡 변동계수 설계를 위한 인사이트:\")\n",
    "        print(\"- 어떤 변동성 지표가 실제 사업 변화를 잘 반영하는가?\")\n",
    "        print(\"- 업종별로 다른 임계값이 필요한가?\")\n",
    "        print(\"- 시간 윈도우는 얼마나 설정해야 하는가?\")\n",
    "        print(\"- 계절성 보정이 필요한가?\")\n",
    "    \n",
    "    # ============ 시각화 및 리포트 생성 ============\n",
    "    \n",
    "    def create_eda_visualizations(self, processed_data):\n",
    "        \"\"\"탐색적 데이터 분석 시각화\"\"\"\n",
    "        print(\"\\n📈 EDA 시각화 생성 중...\")\n",
    "        \n",
    "        # 1. 요일별 사용 패턴 (시간대별 대신)\n",
    "        self._plot_weekday_patterns_daily(processed_data)\n",
    "        \n",
    "        # 2. 요일별 사용 패턴 (바 차트)\n",
    "        self._plot_weekday_patterns(processed_data)\n",
    "        \n",
    "        # 3. 월별 사용량 박스플롯\n",
    "        self._plot_monthly_boxplot(processed_data)\n",
    "        \n",
    "        # 4. 고객별 사용량 분포\n",
    "        self._plot_customer_distribution(processed_data)\n",
    "        \n",
    "        print(\"✅ 시각화 생성 완료\")\n",
    "    \n",
    "    def _plot_weekday_patterns_daily(self, data):\n",
    "        \"\"\"요일별 평균 사용 패턴 (라인 차트)\"\"\"\n",
    "        if 'weekday' in data.columns:\n",
    "            weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(7), weekday_avg.values, marker='o', linewidth=2, markersize=8)\n",
    "            plt.title('Daily Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Weekday')\n",
    "            plt.ylabel('Average Usage (kWh)')\n",
    "            plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"  ⚠️ 요일별 패턴 시각화 불가 - weekday 컬럼 없음\")\n",
    "    \n",
    "    def _plot_weekday_patterns(self, data):\n",
    "        \"\"\"요일별 사용 패턴\"\"\"\n",
    "        if 'weekday' not in data.columns:\n",
    "            print(\"  ⚠️ 요일별 패턴 시각화 불가 - weekday 컬럼 없음\")\n",
    "            return\n",
    "            \n",
    "        weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        weekday_avg = data.groupby('weekday')['daily_mean'].mean()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(range(7), weekday_avg.values, color=['skyblue' if i < 5 else 'lightcoral' for i in range(7)])\n",
    "        plt.title('Average Power Usage by Weekday', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.ylabel('Average Usage (kWh)')\n",
    "        plt.xticks(range(7), weekday_names)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 주중/주말 구분 표시\n",
    "        for i, bar in enumerate(bars):\n",
    "            if i >= 5:  # 주말\n",
    "                bar.set_label('Weekend' if i == 5 else '')\n",
    "            else:  # 주중\n",
    "                bar.set_label('Weekday' if i == 0 else '')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_monthly_boxplot(self, data):\n",
    "        \"\"\"월별 사용량 박스플롯\"\"\"\n",
    "        if 'month' not in data.columns:\n",
    "            print(\"  ⚠️ 월별 패턴 시각화 불가 - month 컬럼 없음\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # 월별 데이터 준비\n",
    "        monthly_data = [data[data['month'] == m]['daily_mean'].values for m in range(1, 13)]\n",
    "        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "        \n",
    "        # 데이터가 있는 월만 표시\n",
    "        valid_months = []\n",
    "        valid_data = []\n",
    "        valid_names = []\n",
    "        \n",
    "        for i, month_data in enumerate(monthly_data):\n",
    "            if len(month_data) > 0:\n",
    "                valid_months.append(i + 1)\n",
    "                valid_data.append(month_data)\n",
    "                valid_names.append(month_names[i])\n",
    "        \n",
    "        if not valid_data:\n",
    "            print(\"  ⚠️ 월별 데이터 없음\")\n",
    "            return\n",
    "        \n",
    "        box_plot = plt.boxplot(valid_data, labels=valid_names, patch_artist=True)\n",
    "        \n",
    "        # 계절별 색상 구분\n",
    "        colors = []\n",
    "        for month in valid_months:\n",
    "            if month in [12, 1, 2]:  # 겨울\n",
    "                colors.append('lightblue')\n",
    "            elif month in [3, 4, 5]:  # 봄\n",
    "                colors.append('lightgreen')\n",
    "            elif month in [6, 7, 8]:  # 여름\n",
    "                colors.append('lightcoral')\n",
    "            else:  # 가을\n",
    "                colors.append('orange')\n",
    "        \n",
    "        for patch, color in zip(box_plot['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        plt.title('Monthly Power Usage Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Daily Average Usage (kWh)')\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def _plot_customer_distribution(self, data):\n",
    "        \"\"\"고객별 평균 사용량 분포\"\"\"\n",
    "        customer_avg = data.groupby('customer_id')['daily_mean'].mean()\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 히스토그램\n",
    "        ax1.hist(customer_avg.values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax1.set_title('Customer Average Usage Distribution', fontweight='bold')\n",
    "        ax1.set_xlabel('Average Usage (kWh)')\n",
    "        ax1.set_ylabel('Number of Customers')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 박스플롯\n",
    "        ax2.boxplot(customer_avg.values, vert=True)\n",
    "        ax2.set_title('Customer Average Usage Boxplot', fontweight='bold')\n",
    "        ax2.set_ylabel('Average Usage (kWh)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_eda_report(self):\n",
    "        \"\"\"EDA 종합 리포트 생성\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"KEPCO LP Data Preprocessing and EDA Report\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if hasattr(self, 'data_quality_report'):\n",
    "            print(\"\\n🔍 Step 1: Data Quality Check Results\")\n",
    "            print(\"-\" * 40)\n",
    "            quality = self.data_quality_report\n",
    "            print(f\"Total Records: {quality['lp_quality']['total_records']:,}\")\n",
    "            print(f\"Customers: {quality['lp_quality']['unique_customers']:,}\")\n",
    "            print(f\"Data Completeness: {quality['data_completeness']['completeness_rate']:.2f}%\")\n",
    "            print(f\"Outlier Rate: {quality['anomaly_detection']['outlier_rate_iqr']:.3f}%\")\n",
    "        \n",
    "        if hasattr(self, 'pattern_analysis'):\n",
    "            print(\"\\n📊 Step 2: Basic Pattern Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            pattern = self.pattern_analysis\n",
    "            if 'customer_segmentation' in pattern:\n",
    "                seg = pattern['customer_segmentation']\n",
    "                print(f\"Analyzed Customers: {seg['customer_count']:,}\")\n",
    "                print(f\"Average Usage: {seg['usage_distribution']['mean']:.2f} kWh\")\n",
    "                print(f\"Usage Std Dev: {seg['usage_distribution']['std']:.2f} kWh\")\n",
    "        \n",
    "        if hasattr(self, 'variability_analysis'):\n",
    "            print(\"\\n📈 Step 3: Variability Analysis Results\")\n",
    "            print(\"-\" * 40)\n",
    "            var = self.variability_analysis\n",
    "            if 'basic_variability' in var:\n",
    "                basic = var['basic_variability']\n",
    "                print(f\"Average Daily CV: {basic['daily_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['weekly_cv']['mean']):\n",
    "                    print(f\"Average Weekly CV: {basic['weekly_cv']['mean']:.4f}\")\n",
    "                if not pd.isna(basic['monthly_cv']['mean']):\n",
    "                    print(f\"Average Monthly CV: {basic['monthly_cv']['mean']:.4f}\")\n",
    "        \n",
    "        print(\"\\n💡 Next Steps Recommendations:\")\n",
    "        print(\"- Define and design variability coefficient\")\n",
    "        print(\"- Implement stacking ensemble model\") \n",
    "        print(\"- Apply overfitting prevention techniques\")\n",
    "        print(\"- Develop business activity change prediction algorithm\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# 사용 예시 - LP 데이터 파일들을 결합하여 분석\n",
    "\n",
    "# 데이터 로딩 (여러 LP 파일들 결합)\n",
    "lp_files = ['LP데이터1.csv', 'LP데이터2.csv']  # 한 달을 반으로 나눈 파일들\n",
    "customer_data = pd.read_excel('고객번호.xlsx')\n",
    "weather_data = pd.read_csv('weather_daily_processed.csv') \n",
    "calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')\n",
    "\n",
    "# 전처리 및 EDA 실행\n",
    "preprocessor = KepcoDataPreprocessor()\n",
    "\n",
    "# LP 데이터 결합\n",
    "combined_lp_data = preprocessor.load_and_combine_lp_data(lp_files)\n",
    "\n",
    "# 1단계: 데이터 품질 점검 (30분)\n",
    "quality_report = preprocessor.check_data_quality(combined_lp_data, customer_data)\n",
    "\n",
    "# 2단계: 기본 패턴 탐색 (60분) - 기상 데이터도 함께 병합!\n",
    "pattern_analysis = preprocessor.analyze_basic_patterns(combined_lp_data, customer_data, weather_data)\n",
    "\n",
    "# 3단계: 변동성 기초 분석 (90분)\n",
    "variability_analysis = preprocessor.analyze_variability(pattern_analysis['processed_data'])\n",
    "\n",
    "# 4단계: 이상 패턴 탐지 (60분)\n",
    "anomaly_results = preprocessor.detect_anomalous_patterns(pattern_analysis['processed_data'])\n",
    "\n",
    "# 5단계: 전처리 방향 결정 (30분)\n",
    "preprocessing_strategy = preprocessor.decide_preprocessing_strategy(\n",
    "    quality_report, pattern_analysis, variability_analysis\n",
    ")\n",
    "\n",
    "# 시각화 생성 (폰트 오류 없이)\n",
    "preprocessor.create_eda_visualizations(pattern_analysis['processed_data'])\n",
    "\n",
    "# 종합 리포트 생성\n",
    "preprocessor.generate_eda_report()\n",
    "\n",
    "print(\"✅ Complete LP data preprocessing and EDA finished!\")\n",
    "print(\"📤 Next: Define variability coefficient and implement stacking model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452490b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류 수정된 실행 코드\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.weather_data = None\n",
    "        self.calendar_data = None\n",
    "        self.customer_data = None\n",
    "        self.combined_data = None\n",
    "        self.volatility_metrics = {}\n",
    "        \n",
    "    def load_real_data(self, lp_files=None, weather_file='weather_daily_processed.csv', \n",
    "                      calendar_file='power_analysis_calendar_2022_2025.csv', \n",
    "                      customer_file='고객번호.xlsx'):\n",
    "        \"\"\"실제 파일들을 사용한 데이터 로딩\"\"\"\n",
    "        print(\"=== 실제 데이터 파일 로딩 및 전처리 ===\")\n",
    "        \n",
    "        # 현재 작업 디렉토리 확인\n",
    "        current_dir = os.getcwd()\n",
    "        print(f\"📁 현재 작업 디렉토리: {current_dir}\")\n",
    "        \n",
    "        # 1. 고객 기본정보 로딩\n",
    "        try:\n",
    "            if os.path.exists(customer_file):\n",
    "                self.customer_data = pd.read_excel(customer_file)\n",
    "                print(f\"✅ 고객 데이터 로딩: {len(self.customer_data):,}명\")\n",
    "            else:\n",
    "                print(f\"⚠️ {customer_file} 파일이 없습니다. 샘플 데이터를 생성합니다.\")\n",
    "                self.customer_data = self._create_sample_customer_data()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 고객 데이터 로딩 실패: {e}\")\n",
    "            self.customer_data = self._create_sample_customer_data()\n",
    "            \n",
    "        # 2. LP 데이터 로딩 (여러 파일 통합)\n",
    "        if lp_files:\n",
    "            lp_dataframes = []\n",
    "            for file in lp_files:\n",
    "                try:\n",
    "                    if os.path.exists(file):\n",
    "                        df = pd.read_csv(file)\n",
    "                        lp_dataframes.append(df)\n",
    "                        print(f\"✅ LP 데이터 로딩: {file} ({len(df):,}건)\")\n",
    "                    else:\n",
    "                        print(f\"⚠️ {file} 파일이 없습니다.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ LP 데이터 로딩 실패: {file} - {e}\")\n",
    "            \n",
    "            if lp_dataframes:\n",
    "                self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "                print(f\"✅ 전체 LP 데이터: {len(self.lp_data):,}건\")\n",
    "            else:\n",
    "                print(\"⚠️ LP 파일 로딩 실패, 샘플 데이터 생성\")\n",
    "                self.lp_data = self._create_test_lp_data()\n",
    "        else:\n",
    "            print(\"⚠️ LP 파일 미지정, 샘플 데이터 생성\")\n",
    "            self.lp_data = self._create_test_lp_data()\n",
    "            \n",
    "        # 3. 날씨 데이터 로딩\n",
    "        try:\n",
    "            if os.path.exists(weather_file):\n",
    "                self.weather_data = pd.read_csv(weather_file, encoding='utf-8')\n",
    "                print(f\"✅ 날씨 데이터 로딩: {len(self.weather_data):,}일\")\n",
    "                print(f\"   컬럼: {list(self.weather_data.columns)[:5]}...\")  # 처음 5개 컬럼만 표시\n",
    "            else:\n",
    "                print(f\"⚠️ {weather_file} 파일이 없습니다. 샘플 데이터를 생성합니다.\")\n",
    "                self.weather_data = self._create_weather_test_data()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 날씨 데이터 로딩 실패: {e}\")\n",
    "            self.weather_data = self._create_weather_test_data()\n",
    "            \n",
    "        # 4. 달력 데이터 로딩\n",
    "        try:\n",
    "            if os.path.exists(calendar_file):\n",
    "                self.calendar_data = pd.read_csv(calendar_file, encoding='utf-8')\n",
    "                print(f\"✅ 달력 데이터 로딩: {len(self.calendar_data):,}일\")\n",
    "                print(f\"   컬럼: {list(self.calendar_data.columns)[:5]}...\")  # 처음 5개 컬럼만 표시\n",
    "            else:\n",
    "                print(f\"⚠️ {calendar_file} 파일이 없습니다. 샘플 데이터를 생성합니다.\")\n",
    "                self.calendar_data = self._create_calendar_test_data()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 달력 데이터 로딩 실패: {e}\")\n",
    "            self.calendar_data = self._create_calendar_test_data()\n",
    "            \n",
    "        # 5. 데이터 전처리 및 병합\n",
    "        self._preprocess_data()\n",
    "        self.combined_data = self._merge_all_data()\n",
    "        \n",
    "        print(f\"\\n🎯 최종 통합 데이터셋: {len(self.combined_data):,}건\")\n",
    "        return self.combined_data\n",
    "    \n",
    "    def _create_sample_customer_data(self):\n",
    "        \"\"\"샘플 고객 데이터 생성\"\"\"\n",
    "        customers = []\n",
    "        for i in range(10):  # 10명 샘플\n",
    "            customers.append({\n",
    "                '대체고객번호': f'A{1001+i}',\n",
    "                '계약전력': np.random.choice([100, 200, 300, 500, 700]),\n",
    "                '계약종별': np.random.choice([222, 226, 311, 322, 726]),\n",
    "                '사용용도': np.random.choice(['02', '09']),\n",
    "                '산업분류': f'업종{np.random.randint(1, 6)}'\n",
    "            })\n",
    "        return pd.DataFrame(customers)\n",
    "    \n",
    "    def _create_test_lp_data(self):\n",
    "        \"\"\"테스트용 LP 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        for customer in customers:\n",
    "            # 고객별 고유 특성 (변동성 패턴)\n",
    "            base_usage = np.random.uniform(30, 150)\n",
    "            volatility_level = np.random.uniform(0.1, 0.4)\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                \n",
    "                for hour in range(0, 24, 6):  # 6시간 간격\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # 시간대별 패턴\n",
    "                        if 8 <= hour <= 18:  # 업무시간\n",
    "                            hour_factor = 1.2\n",
    "                        elif 19 <= hour <= 22:  # 저녁시간\n",
    "                            hour_factor = 0.8\n",
    "                        else:  # 심야시간\n",
    "                            hour_factor = 0.4\n",
    "                        \n",
    "                        # 요일별 패턴\n",
    "                        weekday = current_date.weekday()\n",
    "                        if weekday >= 5:  # 주말\n",
    "                            weekday_factor = 0.6\n",
    "                        else:  # 평일\n",
    "                            weekday_factor = 1.0\n",
    "                        \n",
    "                        # 최종 전력량 계산\n",
    "                        power = base_usage * hour_factor * weekday_factor\n",
    "                        power += np.random.normal(0, base_usage * volatility_level)\n",
    "                        power = max(5, power)  # 최소값 보장\n",
    "                        \n",
    "                        data.append({\n",
    "                            '대체고객번호': customer,\n",
    "                            'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            '순방향유효전력': round(power, 1),\n",
    "                            '지상무효': round(power * 0.2, 1),\n",
    "                            '진상무효': round(power * 0.1, 1),\n",
    "                            '피상전력': round(power * 1.05, 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _create_weather_test_data(self):\n",
    "        \"\"\"테스트용 날씨 데이터 생성\"\"\"\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        weather_data = []\n",
    "        for day in range(days):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            # 3월 기후 특성\n",
    "            base_temp = 12 + np.random.normal(0, 4)\n",
    "            temp_range = np.random.uniform(8, 15)\n",
    "            \n",
    "            최고기온 = base_temp + temp_range/2\n",
    "            최저기온 = base_temp - temp_range/2\n",
    "            평균기온 = (최고기온 + 최저기온) / 2\n",
    "            \n",
    "            # 습도 계산\n",
    "            평균습도 = max(30, min(90, 70 - (평균기온 - 12) * 2 + np.random.normal(0, 10)))\n",
    "            \n",
    "            # 강수량\n",
    "            강수여부 = 1 if np.random.random() < 0.1 else 0\n",
    "            총강수량 = np.random.exponential(5) if 강수여부 else 0\n",
    "            \n",
    "            # 냉난방도일\n",
    "            냉방도일 = max(0, 평균기온 - 18)\n",
    "            난방도일 = max(0, 18 - 평균기온)\n",
    "            \n",
    "            weather_data.append({\n",
    "                '날짜': current_date.strftime('%Y-%m-%d'),\n",
    "                '평균기온': round(평균기온, 1),\n",
    "                '평균습도': round(평균습도),\n",
    "                '총강수량': round(총강수량, 1),\n",
    "                '냉방도일': round(냉방도일, 1),\n",
    "                '난방도일': round(난방도일, 1),\n",
    "                '강수여부': 강수여부\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(weather_data)\n",
    "    \n",
    "    def _create_calendar_test_data(self):\n",
    "        \"\"\"테스트용 달력 데이터 생성\"\"\"\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        calendar_data = []\n",
    "        for day in range(days):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            weekday = current_date.weekday()\n",
    "            is_weekend = weekday >= 5\n",
    "            is_workday = not is_weekend\n",
    "            is_holiday = (current_date.month == 3 and current_date.day == 1)  # 3.1절\n",
    "            \n",
    "            calendar_data.append({\n",
    "                'date': current_date.strftime('%Y-%m-%d'),\n",
    "                'year': current_date.year,\n",
    "                'month': current_date.month,\n",
    "                'day': current_date.day,\n",
    "                'weekday': weekday,\n",
    "                'is_workday': is_workday,\n",
    "                'is_weekend': is_weekend,\n",
    "                'is_holiday': is_holiday,\n",
    "                'workday_indicator': 1 if is_workday and not is_holiday else 0,\n",
    "                'is_month_start': current_date.day == 1,\n",
    "                'is_month_end': current_date.day >= 29\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(calendar_data)\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"데이터 전처리\"\"\"\n",
    "        # LP 데이터 전처리\n",
    "        if self.lp_data is not None:\n",
    "            self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "            self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "            self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "            self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "        \n",
    "        # 날씨 데이터 전처리\n",
    "        if self.weather_data is not None:\n",
    "            self.weather_data['날짜'] = pd.to_datetime(self.weather_data['날짜']).dt.date\n",
    "        \n",
    "        # 달력 데이터 전처리\n",
    "        if self.calendar_data is not None:\n",
    "            self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date']).dt.date\n",
    "    \n",
    "    def _merge_all_data(self):\n",
    "        \"\"\"모든 데이터 병합\"\"\"\n",
    "        # 일별로 LP 데이터 집계\n",
    "        daily_lp = self.lp_data.groupby(['대체고객번호', 'date']).agg({\n",
    "            '순방향유효전력': ['mean', 'std', 'max', 'min', 'sum', 'count']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        daily_lp.columns = ['customer_id', 'date', 'power_mean', 'power_std', \n",
    "                           'power_max', 'power_min', 'power_sum', 'power_count']\n",
    "        \n",
    "        # 변동계수 계산\n",
    "        daily_lp['daily_cv'] = daily_lp['power_std'] / daily_lp['power_mean']\n",
    "        daily_lp['daily_cv'] = daily_lp['daily_cv'].fillna(0)\n",
    "        \n",
    "        # 날씨 데이터와 병합\n",
    "        merged = daily_lp.merge(self.weather_data, left_on='date', right_on='날짜', how='left')\n",
    "        \n",
    "        # 달력 데이터와 병합\n",
    "        merged = merged.merge(self.calendar_data, on='date', how='left')\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def calculate_comprehensive_volatility_score(self):\n",
    "        \"\"\"종합 변동성 스코어 계산\"\"\"\n",
    "        print(\"\\n=== 종합 변동성 스코어 계산 ===\")\n",
    "        \n",
    "        if self.combined_data is None:\n",
    "            print(\"❌ 먼저 데이터를 로딩해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        comprehensive_scores = {}\n",
    "        \n",
    "        for customer in self.combined_data['customer_id'].unique():\n",
    "            customer_data = self.combined_data[self.combined_data['customer_id'] == customer]\n",
    "            \n",
    "            if len(customer_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 1. 기본 변동계수\n",
    "            basic_cv = customer_data['daily_cv'].std()\n",
    "            \n",
    "            # 2. 날씨 보정 변동계수\n",
    "            try:\n",
    "                weather_features = customer_data[['평균기온', '평균습도', '냉방도일', '난방도일']].fillna(0)\n",
    "                power_target = customer_data['power_mean'].fillna(customer_data['power_mean'].mean())\n",
    "                \n",
    "                if len(weather_features) > 1 and len(weather_features) == len(power_target):\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(weather_features, power_target)\n",
    "                    predicted = model.predict(weather_features)\n",
    "                    residuals = power_target - predicted\n",
    "                    weather_adjusted_cv = residuals.std() / residuals.mean() if residuals.mean() != 0 else basic_cv\n",
    "                else:\n",
    "                    weather_adjusted_cv = basic_cv\n",
    "            except:\n",
    "                weather_adjusted_cv = basic_cv\n",
    "            \n",
    "            # 3. 달력 보정 변동계수\n",
    "            try:\n",
    "                calendar_features = customer_data[['workday_indicator', 'is_month_start', 'is_month_end']].fillna(0)\n",
    "                \n",
    "                if len(calendar_features) > 1 and len(calendar_features) == len(power_target):\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(calendar_features, power_target)\n",
    "                    predicted = model.predict(calendar_features)\n",
    "                    residuals = power_target - predicted\n",
    "                    calendar_adjusted_cv = residuals.std() / residuals.mean() if residuals.mean() != 0 else basic_cv\n",
    "                else:\n",
    "                    calendar_adjusted_cv = basic_cv\n",
    "            except:\n",
    "                calendar_adjusted_cv = basic_cv\n",
    "            \n",
    "            # 4. 외부 요인 민감도\n",
    "            temp_correlation = abs(customer_data['평균기온'].corr(customer_data['power_mean'])) or 0\n",
    "            \n",
    "            workday_data = customer_data[customer_data['is_workday'] == True]['power_mean']\n",
    "            weekend_data = customer_data[customer_data['is_weekend'] == True]['power_mean']\n",
    "            \n",
    "            if len(workday_data) > 0 and len(weekend_data) > 0:\n",
    "                weekday_effect = abs(workday_data.mean() - weekend_data.mean())\n",
    "                weekday_effect = weekday_effect / customer_data['power_mean'].mean() if customer_data['power_mean'].mean() > 0 else 0\n",
    "            else:\n",
    "                weekday_effect = 0\n",
    "            \n",
    "            # 5. 최종 순수 변동성 (날씨 + 달력 보정)\n",
    "            pure_volatility = (weather_adjusted_cv + calendar_adjusted_cv) / 2\n",
    "            \n",
    "            # 6. 종합 스코어 계산\n",
    "            weights = {\n",
    "                'pure_volatility': 0.5,\n",
    "                'basic_cv': 0.3,\n",
    "                'temp_sensitivity': 0.1,\n",
    "                'weekday_effect': 0.1\n",
    "            }\n",
    "            \n",
    "            final_score = (\n",
    "                weights['pure_volatility'] * pure_volatility +\n",
    "                weights['basic_cv'] * basic_cv +\n",
    "                weights['temp_sensitivity'] * temp_correlation +\n",
    "                weights['weekday_effect'] * weekday_effect\n",
    "            )\n",
    "            \n",
    "            # 7. 영업활동 변화 위험도 평가\n",
    "            if pure_volatility > 0.3:\n",
    "                risk_level = \"높음\"\n",
    "                recommendation = \"영업활동 변화 의심 - 정밀 분석 필요\"\n",
    "            elif pure_volatility > 0.15:\n",
    "                risk_level = \"보통\"\n",
    "                recommendation = \"주의 관찰 - 주기적 모니터링\"\n",
    "            else:\n",
    "                risk_level = \"낮음\"\n",
    "                recommendation = \"정상 운영 추정 - 안정적\"\n",
    "            \n",
    "            comprehensive_scores[customer] = {\n",
    "                'basic_cv': round(basic_cv, 4),\n",
    "                'weather_adjusted_cv': round(weather_adjusted_cv, 4),\n",
    "                'calendar_adjusted_cv': round(calendar_adjusted_cv, 4),\n",
    "                'pure_volatility': round(pure_volatility, 4),\n",
    "                'temp_sensitivity': round(temp_correlation, 4),\n",
    "                'weekday_effect': round(weekday_effect, 4),\n",
    "                'final_score': round(final_score, 4),\n",
    "                'risk_level': risk_level,\n",
    "                'recommendation': recommendation,\n",
    "                'mean_power': round(customer_data['power_mean'].mean(), 1),\n",
    "                'data_points': len(customer_data)\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\t기본CV\\t날씨보정CV\\t달력보정CV\\t순수변동성\\t최종점수\\t위험도\")\n",
    "        for customer, scores in comprehensive_scores.items():\n",
    "            print(f\"{customer}\\t{scores['basic_cv']}\\t{scores['weather_adjusted_cv']}\\t{scores['calendar_adjusted_cv']}\\t{scores['pure_volatility']}\\t{scores['final_score']}\\t{scores['risk_level']}\")\n",
    "        \n",
    "        self.volatility_metrics['comprehensive'] = comprehensive_scores\n",
    "        return comprehensive_scores\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"최종 분석 리포트 생성\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🏢 한국전력공사 전력 사용패턴 변동계수 개발 - 최종 리포트\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if not hasattr(self, 'volatility_metrics') or 'comprehensive' not in self.volatility_metrics:\n",
    "            comprehensive_scores = self.calculate_comprehensive_volatility_score()\n",
    "        else:\n",
    "            comprehensive_scores = self.volatility_metrics['comprehensive']\n",
    "        \n",
    "        if not comprehensive_scores:\n",
    "            print(\"❌ 변동성 스코어를 먼저 계산해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n📊 분석 개요:\")\n",
    "        print(f\"  • 분석 대상: {len(comprehensive_scores)}명 고객\")\n",
    "        print(f\"  • 데이터 기간: {self.combined_data['date'].min()} ~ {self.combined_data['date'].max()}\")\n",
    "        print(f\"  • 총 데이터 포인트: {len(self.combined_data):,}건\")\n",
    "        \n",
    "        # 변동성 분포 분석\n",
    "        pure_volatilities = [s['pure_volatility'] for s in comprehensive_scores.values()]\n",
    "        final_scores = [s['final_score'] for s in comprehensive_scores.values()]\n",
    "        \n",
    "        print(f\"\\n🎯 순수 변동성 통계:\")\n",
    "        print(f\"  • 평균: {np.mean(pure_volatilities):.4f}\")\n",
    "        print(f\"  • 표준편차: {np.std(pure_volatilities):.4f}\")\n",
    "        print(f\"  • 최소값: {np.min(pure_volatilities):.4f}\")\n",
    "        print(f\"  • 최대값: {np.max(pure_volatilities):.4f}\")\n",
    "        \n",
    "        # 위험도별 분류\n",
    "        risk_distribution = {}\n",
    "        for scores in comprehensive_scores.values():\n",
    "            risk = scores['risk_level']\n",
    "            risk_distribution[risk] = risk_distribution.get(risk, 0) + 1\n",
    "        \n",
    "        print(f\"\\n🚨 영업활동 변화 위험도 분포:\")\n",
    "        for risk, count in risk_distribution.items():\n",
    "            percentage = count / len(comprehensive_scores) * 100\n",
    "            print(f\"  • {risk}: {count}명 ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 상위 위험 고객 식별\n",
    "        high_risk_customers = {k: v for k, v in comprehensive_scores.items() \n",
    "                             if v['risk_level'] == '높음'}\n",
    "        \n",
    "        if high_risk_customers:\n",
    "            print(f\"\\n⚠️ 고위험 고객 상세 분석:\")\n",
    "            print(\"고객번호\\t순수변동성\\t평균전력\\t권장사항\")\n",
    "            for customer, scores in sorted(high_risk_customers.items(), \n",
    "                                         key=lambda x: x[1]['pure_volatility'], reverse=True):\n",
    "                print(f\"{customer}\\t{scores['pure_volatility']:.4f}\\t{scores['mean_power']}kW\\t{scores['recommendation']}\")\n",
    "        \n",
    "        print(f\"\\n💡 알고리즘 특징:\")\n",
    "        print(\"  ✅ 다차원 변동성 지표: 기본/날씨보정/달력보정/순수변동성\")\n",
    "        print(\"  ✅ 외부요인 제거: 날씨 및 달력 효과 자동 보정\")\n",
    "        print(\"  ✅ 위험도 자동분류: 3단계 위험도 + 맞춤 권장사항\")\n",
    "        print(\"  ✅ 실시간 모니터링: 영업활동 변화 조기 감지\")\n",
    "        print(\"  ✅ 확장 가능성: 추가 외부요인 쉽게 통합 가능\")\n",
    "        \n",
    "        return {\n",
    "            'comprehensive_scores': comprehensive_scores,\n",
    "            'statistics': {\n",
    "                'mean_pure_volatility': np.mean(pure_volatilities),\n",
    "                'std_pure_volatility': np.std(pure_volatilities),\n",
    "                'risk_distribution': risk_distribution\n",
    "            },\n",
    "            'high_risk_customers': high_risk_customers\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 실행 함수 - 오류 수정 버전\"\"\"\n",
    "    print(\"🚀 한국전력공사 전력 사용패턴 변동계수 분석 실행\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCOVolatilityAnalyzer()\n",
    "    \n",
    "    # 실제 파일들로 데이터 로딩\n",
    "    try:\n",
    "        print(\"📁 실제 데이터 파일 로딩 시도...\")\n",
    "        \n",
    "        # 실제 파일 경로 설정\n",
    "        lp_files = ['LP데이터1.csv', 'LP데이터2.csv']\n",
    "        weather_file = 'weather_daily_processed.csv'\n",
    "        calendar_file = 'power_analysis_calendar_2022_2025.csv'\n",
    "        customer_file = '고객번호.xlsx'\n",
    "        \n",
    "        # 데이터 로딩\n",
    "        combined_data = analyzer.load_real_data(\n",
    "            lp_files=lp_files,\n",
    "            weather_file=weather_file,\n",
    "            calendar_file=calendar_file,\n",
    "            customer_file=customer_file\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 데이터 로딩 완료: {len(combined_data):,}건\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 실제 파일 로딩 실패: {e}\")\n",
    "        print(\"📊 샘플 데이터로 분석을 진행합니다...\")\n",
    "        \n",
    "        # 샘플 데이터로 대체\n",
    "        combined_data = analyzer.load_real_data()\n",
    "    \n",
    "    # 종합 변동성 스코어 계산\n",
    "    print(\"\\n🔄 종합 변동성 분석 시작...\")\n",
    "    volatility_scores = analyzer.calculate_comprehensive_volatility_score()\n",
    "    \n",
    "    # 최종 리포트 생성\n",
    "    print(\"\\n📊 최종 리포트 생성...\")\n",
    "    final_report = analyzer.generate_final_report()\n",
    "    \n",
    "    # 결과 저장 (오류 수정)\n",
    "    if volatility_scores:\n",
    "        try:\n",
    "            # 현재 디렉토리에 파일 저장 (영어 파일명 사용)\n",
    "            results_df = pd.DataFrame(volatility_scores).T\n",
    "            \n",
    "            # 저장 경로 확인\n",
    "            output_file = 'volatility_analysis_result.csv'\n",
    "            results_df.to_csv(output_file, encoding='utf-8-sig')\n",
    "            print(f\"💾 결과가 '{output_file}'에 저장되었습니다.\")\n",
    "            \n",
    "            # 추가 요약 파일도 저장\n",
    "            summary_file = 'analysis_summary.txt'\n",
    "            with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"한국전력공사 전력 사용패턴 변동계수 분석 요약\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\")\n",
    "                f.write(f\"분석 고객 수: {len(volatility_scores)}명\\n\")\n",
    "                f.write(f\"분석 완료 시간: {datetime.now()}\\n\")\n",
    "                \n",
    "                # 위험도별 분포\n",
    "                risk_dist = {}\n",
    "                for scores in volatility_scores.values():\n",
    "                    risk = scores['risk_level']\n",
    "                    risk_dist[risk] = risk_dist.get(risk, 0) + 1\n",
    "                    \n",
    "                f.write(\"\\n위험도 분포:\\n\")\n",
    "                for risk, count in risk_dist.items():\n",
    "                    f.write(f\"  {risk}: {count}명\\n\")\n",
    "                    \n",
    "                # 고위험 고객\n",
    "                high_risk = {k: v for k, v in volatility_scores.items() if v['risk_level'] == '높음'}\n",
    "                if high_risk:\n",
    "                    f.write(f\"\\n고위험 고객 ({len(high_risk)}명):\\n\")\n",
    "                    for customer, scores in high_risk.items():\n",
    "                        f.write(f\"  {customer}: 순수변동성 {scores['pure_volatility']:.4f}\\n\")\n",
    "            \n",
    "            print(f\"📄 요약 리포트가 '{summary_file}'에 저장되었습니다.\")\n",
    "            \n",
    "        except Exception as save_error:\n",
    "            print(f\"⚠️ 파일 저장 중 오류 발생: {save_error}\")\n",
    "            print(\"📊 결과를 콘솔에 출력합니다:\")\n",
    "            \n",
    "            # 콘솔에 결과 출력\n",
    "            print(\"\\n=== 분석 결과 ===\")\n",
    "            for customer, scores in volatility_scores.items():\n",
    "                print(f\"{customer}: 위험도 {scores['risk_level']}, 순수변동성 {scores['pure_volatility']:.4f}\")\n",
    "        \n",
    "        # 요약 통계\n",
    "        print(\"\\n📈 분석 요약:\")\n",
    "        print(f\"  • 분석 고객 수: {len(volatility_scores)}명\")\n",
    "        \n",
    "        # 위험도별 분포\n",
    "        risk_dist = {}\n",
    "        for scores in volatility_scores.values():\n",
    "            risk = scores['risk_level']\n",
    "            risk_dist[risk] = risk_dist.get(risk, 0) + 1\n",
    "            \n",
    "        print(\"  • 위험도 분포:\")\n",
    "        for risk, count in risk_dist.items():\n",
    "            percentage = count / len(volatility_scores) * 100\n",
    "            print(f\"    - {risk}: {count}명 ({percentage:.1f}%)\")\n",
    "            \n",
    "        # 고위험 고객 출력\n",
    "        high_risk = {k: v for k, v in volatility_scores.items() if v['risk_level'] == '높음'}\n",
    "        if high_risk:\n",
    "            print(f\"\\n⚠️ 고위험 고객 ({len(high_risk)}명):\")\n",
    "            for customer, scores in sorted(high_risk.items(), \n",
    "                                         key=lambda x: x[1]['pure_volatility'], reverse=True):\n",
    "                print(f\"  {customer}: 순수변동성 {scores['pure_volatility']:.4f} - {scores['recommendation']}\")\n",
    "        else:\n",
    "            print(\"\\n✅ 고위험 고객이 발견되지 않았습니다.\")\n",
    "    \n",
    "    print(\"\\n🎯 분석 완료!\")\n",
    "    return final_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실행\n",
    "    try:\n",
    "        report = main()\n",
    "        \n",
    "        # 추가 분석 안내\n",
    "        print(\"\\n💡 추가 분석 옵션:\")\n",
    "        print(\"1. 시각화 생성\")\n",
    "        print(\"2. 상세 고객 분석\")\n",
    "        print(\"3. 예측 모델 구축\")\n",
    "        print(\"4. 실시간 모니터링 설정\")\n",
    "        \n",
    "    except Exception as main_error:\n",
    "        print(f\"❌ 실행 중 오류 발생: {main_error}\")\n",
    "        print(\"🔧 오류 해결 방법:\")\n",
    "        print(\"1. 파일 경로 확인\")\n",
    "        print(\"2. 필요한 라이브러리 설치 확인\")\n",
    "        print(\"3. 데이터 파일 형식 확인\")\n",
    "        \n",
    "        # 간단한 디버깅 정보\n",
    "        import traceback\n",
    "        print(\"\\n🐛 상세 오류 정보:\")\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
