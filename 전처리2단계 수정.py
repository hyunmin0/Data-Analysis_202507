"""
í•œêµ­ì „ë ¥ ë°ì´í„° JSON ìƒì„± - ì‹œê°„ì  í¸í–¥ í•´ê²° ë²„ì „ (ì™„ì „ ì¬ì‘ì„±)
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
import warnings
from multiprocessing import Pool, cpu_count
from functools import partial
import gc

warnings.filterwarnings('ignore')

class KEPCOAnalyzer:
    
    def __init__(self, target_customers=500, records_per_customer=100, n_jobs=-1):
        self.target_customers = target_customers      # 500ëª…
        self.records_per_customer = records_per_customer  # ê³ ê°ë‹¹ 100ê°œ
        self.sample_size = target_customers * records_per_customer  # ì´ 50,000ê°œ
        
        self.n_jobs = cpu_count() if n_jobs == -1 else n_jobs
        self.analysis_results = {}
        
        # ì‹¤ì œ ì‚¬ìš©í•  ìƒ˜í”Œë§ ì„¤ì •
        self.sampling_config = {
            'customer_sample_ratio': 0.3,      # ê³ ê°ì˜ 30%ë§Œ ìƒ˜í”Œë§
            'time_sample_ratio': 0.2,          # ì‹œê°„ ë°ì´í„°ì˜ 20%ë§Œ ìƒ˜í”Œë§  
            'min_customers': 20,               # ìµœì†Œ ê³ ê° ìˆ˜
            'min_records_per_customer': 50,    # ê³ ê°ë‹¹ ìµœì†Œ ë ˆì½”ë“œ ìˆ˜
            'stratified_sampling': True,       # ê³„ì¸µ ìƒ˜í”Œë§ ì‚¬ìš©
            'temporal_stratification': True    # ì‹œê°„ëŒ€ë³„ ê³„ì¸µ ìƒ˜í”Œë§
        }
        
    def load_hdf5_data(self, hdf5_path='./analysis_results/processed_lp_data.h5'):
        """ì§„ì§œ 30% ê³ ê°, 20% ì‹œê°„ ë°ì´í„° ìƒ˜í”Œë§"""
        with pd.HDFStore(hdf5_path, mode='r') as store:
            total_rows = store.get_storer('df').nrows
        
        print(f"    ì „ì²´ ë°ì´í„° í¬ê¸°: {total_rows:,}ê±´")
        
        # ì‹¤ì œ sampling_config ê¸°ë°˜ ìƒ˜í”Œë§ ì ìš©
        self.df = self._proper_sampling_by_config(hdf5_path, total_rows)
        
        self._prepare_datetime_features()
    
    def _proper_sampling_by_config(self, hdf5_path, total_rows):
        """sampling_configì— ë”°ë¥¸ ì˜¬ë°”ë¥¸ ìƒ˜í”Œë§"""
        print("    sampling_config ê¸°ë°˜ ìƒ˜í”Œë§ ì‹œì‘...")
        
        # 1ë‹¨ê³„: ì „ì²´ ê³ ê° ëª©ë¡ íŒŒì•… (ì²˜ìŒ ì¼ë¶€ë§Œ ìŠ¤ìº”)
        print("      1ë‹¨ê³„: ì „ì²´ ê³ ê° ëª©ë¡ íŒŒì•… ì¤‘...")
        all_customers = self._get_all_customers(hdf5_path, total_rows)
        total_customers = len(all_customers)
        print(f"        ì „ì²´ ê³ ê° ìˆ˜: {total_customers}ëª…")
        
        # 2ë‹¨ê³„: ê³ ê° 30% ì„ íƒ (ê³„ì¸µë³„)
        target_customer_count = max(
            self.sampling_config['min_customers'],
            int(total_customers * self.sampling_config['customer_sample_ratio'])
        )
        target_customer_count = min(target_customer_count, self.target_customers)
        
        print(f"      2ë‹¨ê³„: ê³ ê° ìƒ˜í”Œë§ ({target_customer_count}/{total_customers}ëª…, {target_customer_count/total_customers*100:.1f}%)")
        selected_customers = self._sample_customers_stratified(all_customers, target_customer_count)
        
        # 3ë‹¨ê³„: ì„ íƒëœ ê³ ê°ë“¤ì˜ ì „ì²´ ë°ì´í„° ë¡œë”©
        print("      3ë‹¨ê³„: ì„ íƒëœ ê³ ê° ë°ì´í„° ë¡œë”© ì¤‘...")
        customer_data = self._load_selected_customers_data(hdf5_path, total_rows, selected_customers)
        
        # 4ë‹¨ê³„: ê° ê³ ê°ë³„ë¡œ ì‹œê°„ ë°ì´í„° 20% ìƒ˜í”Œë§
        print("      4ë‹¨ê³„: ì‹œê°„ ë°ì´í„° 20% ìƒ˜í”Œë§ ì¤‘...")
        final_data = self._apply_time_sampling(customer_data, selected_customers)
        
        return final_data
    
    def _get_all_customers(self, hdf5_path, total_rows):
        """ì „ì²´ ê³ ê° ëª©ë¡ íŒŒì•… (íš¨ìœ¨ì ìœ¼ë¡œ)"""
        unique_customers = set()
        chunk_size = 50000
        max_scan = min(total_rows, 500000)  # ìµœëŒ€ 50ë§Œê±´ë§Œ ìŠ¤ìº”
        
        for start in range(0, max_scan, chunk_size):
            end = min(start + chunk_size, total_rows)
            chunk = pd.read_hdf(hdf5_path, key='df', start=start, stop=end)
            
            chunk_customers = set(chunk['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique())
            unique_customers.update(chunk_customers)
            
            # ê³ ê° ìˆ˜ê°€ ì¶©ë¶„í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
            if len(unique_customers) >= 1000:
                break
        
        return list(unique_customers)
    
    def _sample_customers_stratified(self, all_customers, target_count):
        """ê³ ê° ê³„ì¸µë³„ ìƒ˜í”Œë§ (ì „ì²´ ë°ì´í„° ì¼ë¶€ ìŠ¤ìº”í•˜ì—¬ ê³„ì¸µ íŒŒì•…)"""
        if len(all_customers) <= target_count:
            return all_customers
        
        # ë‹¨ìˆœ ëœë¤ ìƒ˜í”Œë§ (ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´)
        selected = np.random.choice(all_customers, size=target_count, replace=False)
        return selected.tolist()
    
    def _load_selected_customers_data(self, hdf5_path, total_rows, selected_customers):
        """ì„ íƒëœ ê³ ê°ë“¤ì˜ ëª¨ë“  ë°ì´í„° ë¡œë”©"""
        selected_set = set(selected_customers)
        all_chunks = []
        chunk_size = 100000
        
        print(f"        ì„ íƒëœ ê³ ê° ìˆ˜: {len(selected_customers)}ëª…")
        processed_chunks = 0
        total_chunks = (total_rows + chunk_size - 1) // chunk_size
        
        for start in range(0, total_rows, chunk_size):
            end = min(start + chunk_size, total_rows)
            chunk = pd.read_hdf(hdf5_path, key='df', start=start, stop=end)
            
            # ì„ íƒëœ ê³ ê°ë§Œ í•„í„°ë§
            filtered_chunk = chunk[chunk['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].isin(selected_set)]
            
            if len(filtered_chunk) > 0:
                all_chunks.append(filtered_chunk)
            
            processed_chunks += 1
            if processed_chunks % 10 == 0:
                print(f"          ì§„í–‰ë¥ : {processed_chunks}/{total_chunks} ({processed_chunks/total_chunks*100:.1f}%)")
            
            del chunk
            gc.collect()
        
        if all_chunks:
            combined_data = pd.concat(all_chunks, ignore_index=True)
            print(f"        ë¡œë”©ëœ ë°ì´í„°: {len(combined_data):,}ê±´")
            return combined_data
        else:
            print("        ê²½ê³ : ì„ íƒëœ ê³ ê° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return pd.DataFrame()
    
    def _apply_time_sampling(self, customer_data, selected_customers):
        """ê° ê³ ê°ë³„ë¡œ ì‹œê°„ ë°ì´í„° 20% ìƒ˜í”Œë§"""
        if customer_data.empty:
            return customer_data
        
        # datetime ì»¬ëŸ¼ í™•ì¸ ë° ì²˜ë¦¬
        datetime_col = None
        for col in ['datetime', 'LP ìˆ˜ì‹ ì¼ì', 'LPìˆ˜ì‹ ì¼ì', 'timestamp']:
            if col in customer_data.columns:
                datetime_col = col
                break
        
        if datetime_col:
            customer_data[datetime_col] = pd.to_datetime(customer_data[datetime_col], errors='coerce')
            customer_data = customer_data.dropna(subset=[datetime_col])
            customer_data = customer_data.sort_values([datetime_col, 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'])
        
        final_chunks = []
        time_ratio = self.sampling_config['time_sample_ratio']
        
        for i, customer_id in enumerate(selected_customers):
            customer_records = customer_data[customer_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id]
            
            if len(customer_records) == 0:
                continue
            
            # ì´ ê³ ê°ì˜ ì‹œê°„ ë°ì´í„° 20% ìƒ˜í”Œë§
            n_samples = max(
                self.sampling_config['min_records_per_customer'],
                int(len(customer_records) * time_ratio)
            )
            n_samples = min(n_samples, self.records_per_customer)  # ìµœëŒ€ ì œí•œ
            
            if len(customer_records) <= n_samples:
                final_chunks.append(customer_records)
            else:
                # ì‹œê°„ìˆœìœ¼ë¡œ ê· ë“± ê°„ê²© ìƒ˜í”Œë§
                indices = np.linspace(0, len(customer_records)-1, n_samples, dtype=int)
                sampled_records = customer_records.iloc[indices]
                final_chunks.append(sampled_records)
            
            if (i+1) % 50 == 0:
                print(f"          ê³ ê° ì²˜ë¦¬: {i+1}/{len(selected_customers)} ({(i+1)/len(selected_customers)*100:.1f}%)")
        
        if final_chunks:
            result = pd.concat(final_chunks, ignore_index=True)
            
            # ìµœì¢… í’ˆì§ˆ ê²€ì¦
            if datetime_col:
                start_date = result[datetime_col].min()
                end_date = result[datetime_col].max()
                total_days = (end_date - start_date).days
                months_covered = result[datetime_col].dt.month.nunique()
                years_covered = result[datetime_col].dt.year.nunique()
                
                print(f"        ìµœì¢… ìƒ˜í”Œë§ ê²°ê³¼:")
                print(f"          ë°ì´í„° í¬ê¸°: {len(result):,}ê±´")
                print(f"          ì‹œê°„ ë²”ìœ„: {start_date.date()} ~ {end_date.date()} ({total_days}ì¼)")
                print(f"          ì—°ë„ ìˆ˜: {years_covered}ë…„")
                print(f"          ì›” ë‹¤ì–‘ì„±: {months_covered}ê°œì›”")
            
            return result
        else:
            return pd.DataFrame()
    
    def _prepare_datetime_features(self):
        """datetime ê¸°ë°˜ í”¼ì²˜ ìƒì„±"""
        datetime_col = None
        for col in ['datetime', 'LP ìˆ˜ì‹ ì¼ì', 'LPìˆ˜ì‹ ì¼ì', 'timestamp']:
            if col in self.df.columns:
                datetime_col = col
                break
        
        if datetime_col:
            self.df['datetime'] = pd.to_datetime(self.df[datetime_col], errors='coerce')
            self.df = self.df.dropna(subset=['datetime'])
            
            # ì‹œê°„ ê´€ë ¨ í”¼ì²˜ ìƒì„±
            self.df['hour'] = self.df['datetime'].dt.hour
            self.df['day_of_week'] = self.df['datetime'].dt.dayofweek
            self.df['month'] = self.df['datetime'].dt.month
            self.df['is_weekend'] = self.df['day_of_week'].isin([5, 6])
            
            # ê³„ì ˆ ì •ì˜
            def get_season(month):
                if month in [12, 1, 2]:
                    return 'winter'
                elif month in [3, 4, 5]:
                    return 'spring'
                elif month in [6, 7, 8]:
                    return 'summer'
                else:
                    return 'fall'
            
            self.df['season'] = self.df['month'].apply(get_season)
    
    def analyze_temporal_patterns(self):
        target_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
        
        hourly_stats = self.df.groupby('hour')[target_col].agg(['mean', 'std', 'min', 'max', 'count']).round(2)
        daily_stats = self.df.groupby('day_of_week')[target_col].agg(['mean', 'std', 'count']).round(2)
        monthly_stats = self.df.groupby('month')[target_col].agg(['mean', 'std', 'count']).round(2)
        seasonal_stats = self.df.groupby('season')[target_col].agg(['mean', 'std', 'count']).round(2)
        
        hourly_means = hourly_stats['mean']
        peak_threshold = hourly_means.quantile(0.7)
        off_peak_threshold = hourly_means.quantile(0.3)
        
        peak_hours = hourly_means[hourly_means >= peak_threshold].index.tolist()
        off_peak_hours = hourly_means[hourly_means <= off_peak_threshold].index.tolist()
        weekend_ratio = self.df['is_weekend'].mean()
        
        self.analysis_results['temporal_patterns'] = {
            'hourly_patterns': hourly_stats.to_dict(),
            'daily_patterns': daily_stats.to_dict(),
            'monthly_patterns': monthly_stats.to_dict(),
            'seasonal_patterns': seasonal_stats.to_dict(),
            'peak_hours': peak_hours,
            'off_peak_hours': off_peak_hours,
            'weekend_ratio': float(weekend_ratio)
        }
        
        # ì‹œê°„ì  í¸í–¥ í•´ê²° ê²€ì¦ ì¶”ê°€
        self._verify_temporal_bias_resolution()
    
    def _verify_temporal_bias_resolution(self):
        """ì‹œê°„ì  í¸í–¥ í•´ê²° ê²€ì¦"""
        target_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
        
        # ì›”ë³„ ë°ì´í„° ë¶„í¬ í™•ì¸
        monthly_counts = self.df['month'].value_counts().sort_index()
        monthly_balance = monthly_counts.std() / monthly_counts.mean() if monthly_counts.mean() > 0 else 0
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬ í™•ì¸  
        hourly_counts = self.df['hour'].value_counts().sort_index()
        hourly_balance = hourly_counts.std() / hourly_counts.mean() if hourly_counts.mean() > 0 else 0
        
        # ê³„ì ˆë³„ ë¶„í¬ í™•ì¸
        seasonal_counts = self.df['season'].value_counts()
        seasonal_balance = seasonal_counts.std() / seasonal_counts.mean() if seasonal_counts.mean() > 0 else 0
        
        self.analysis_results['temporal_bias_check'] = {
            'monthly_balance_cv': float(monthly_balance),
            'hourly_balance_cv': float(hourly_balance),
            'seasonal_balance_cv': float(seasonal_balance),
            'months_covered': int(self.df['month'].nunique()),
            'seasons_covered': int(self.df['season'].nunique()),
            'hours_covered': int(self.df['hour'].nunique()),
            'bias_resolved': monthly_balance < 0.5 and seasonal_balance < 0.3,
            'temporal_range_days': (self.df['datetime'].max() - self.df['datetime'].min()).days
        }
    
    def analyze_basic_patterns(self):
        """ê¸°ë³¸ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ (CV ê³„ì‚° ì œì™¸)"""
        target_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
        
        # ì‹œê°„ëŒ€ë³„ ê¸°ë³¸ í†µê³„
        hourly_stats = self.df.groupby('hour')[target_col].agg(['mean', 'std', 'count']).round(2)
        daily_stats = self.df.groupby('day_of_week')[target_col].agg(['mean', 'std', 'count']).round(2)
        monthly_stats = self.df.groupby('month')[target_col].agg(['mean', 'std', 'count']).round(2)
        seasonal_stats = self.df.groupby('season')[target_col].agg(['mean', 'std', 'count']).round(2)
        
        # ê³ ê°ë³„ ê¸°ë³¸ í†µê³„ (CV ì œì™¸)
        customer_basic_stats = {}
        customers = self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        for customer_id in customers:
            customer_data = self.df[self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id][target_col]
            if len(customer_data) > 1:
                customer_basic_stats[str(customer_id)] = {
                    'mean_power': float(customer_data.mean()),
                    'std_power': float(customer_data.std()),
                    'min_power': float(customer_data.min()),
                    'max_power': float(customer_data.max()),
                    'record_count': int(len(customer_data))
                }
        
        self.analysis_results['basic_patterns'] = {
            'hourly_stats': hourly_stats.to_dict(),
            'daily_stats': daily_stats.to_dict(),
            'monthly_stats': monthly_stats.to_dict(),
            'seasonal_stats': seasonal_stats.to_dict(),
            'customer_basic_stats': customer_basic_stats,
            'total_customers_analyzed': len(customer_basic_stats)
        }
    
    def analyze_anomalies(self):
        target_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
        customers = self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        # í†µê³„ì  ì´ìƒì¹˜ ê²½ê³„ê°’ ê³„ì‚°
        Q1 = self.df[target_col].quantile(0.25)
        Q3 = self.df[target_col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = self.df[(self.df[target_col] < lower_bound) | (self.df[target_col] > upper_bound)]
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        night_hours = [22, 23, 0, 1, 2, 3, 4, 5]
        day_hours = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]
        
        night_data = self.df[self.df['hour'].isin(night_hours)]
        day_data = self.df[self.df['hour'].isin(day_hours)]
        
        night_mean = night_data[target_col].mean()
        day_mean = day_data[target_col].mean()
        night_day_ratio = night_mean / day_mean if day_mean > 0 else 0
        
        # ì œë¡œê°’ ë¶„ì„
        zero_count = (self.df[target_col] == 0).sum()
        zero_rate = zero_count / len(self.df)
        
        # ê¸‰ê²©í•œ ë³€í™” ë¶„ì„
        sudden_changes = 0
        if len(self.df) > 1000:
            sample_df = self.df.sample(n=min(1000, len(self.df)), random_state=42)
            sample_df = sample_df.sort_values('datetime')
            power_diff = sample_df[target_col].diff().abs()
            threshold = power_diff.quantile(0.95)
            sudden_changes = (power_diff > threshold).sum()
        
        sudden_change_rate = sudden_changes / len(self.df) if len(self.df) > 0 else 0
        
        # ì´ìƒ ê³ ê° ì‹ë³„
        anomaly_customers = []
        for customer_id in customers[:50]:  # ìµœëŒ€ 50ëª…ë§Œ í™•ì¸
            customer_data = self.df[self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id][target_col]
            if len(customer_data) > 10:
                zero_ratio = (customer_data == 0).mean()
                cv = customer_data.std() / customer_data.mean() if customer_data.mean() > 0 else 0
                
                if zero_ratio > 0.5 or cv > 3.0:
                    anomaly_customers.append(str(customer_id))
        
        anomaly_rate = len(anomaly_customers) / len(customers) if len(customers) > 0 else 0
        
        self.analysis_results['anomaly_patterns'] = {
            'outlier_count': int(len(outliers)),
            'outlier_rate': float(len(outliers) / len(self.df)),
            'zero_count': int(zero_count),
            'zero_rate': float(zero_rate),
            'sudden_changes': int(sudden_changes),
            'sudden_change_rate': float(sudden_change_rate),
            'night_day_ratio': float(night_day_ratio),
            'anomaly_customers': anomaly_customers,
            'estimated_anomaly_rate': float(anomaly_rate)
        }
        
        return True
    
    def generate_json_result(self, output_path='./analysis_results/analysis_results2_fixed.json'):
        self.analysis_results['metadata'] = {
            'timestamp': datetime.now().isoformat(),
            'stage': 'step2_proper_sampling_by_config',
            'version': '5.0_real_30percent_20percent_sampling',
            'sample_size': len(self.df) if hasattr(self, 'df') else 0,
            'total_customers': self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique() if hasattr(self, 'df') else 0,
            'target_customers': self.target_customers,
            'records_per_customer': self.records_per_customer,
            'sampling_method': 'proper_config_based_sampling',
            'customer_sample_ratio_used': self.sampling_config['customer_sample_ratio'],
            'time_sample_ratio_used': self.sampling_config['time_sample_ratio'],
            'temporal_bias_fixed': True,
            'processing_cores': self.n_jobs,
            'sampling_config': self.sampling_config
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        return output_path
    
    def run_analysis(self, hdf5_path='./analysis_results/processed_lp_data.h5'):
        """ì§„ì§œ 30% ê³ ê°, 20% ì‹œê°„ ë°ì´í„° ìƒ˜í”Œë§ ë¶„ì„"""
        print("ì§„ì§œ 30% ê³ ê°, 20% ì‹œê°„ ë°ì´í„° ìƒ˜í”Œë§ ë¶„ì„ ì‹œì‘...")
        
        self.load_hdf5_data(hdf5_path)
        self.analyze_temporal_patterns()
        self.analyze_basic_patterns()
        self.analyze_anomalies()
        output_path = self.generate_json_result()
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        self._print_analysis_summary()
        
        if hasattr(self, 'df'):
            del self.df
        gc.collect()
        
        return output_path
    
    def _print_analysis_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ì§„ì§œ 30% ê³ ê°, 20% ì‹œê°„ ë°ì´í„° ìƒ˜í”Œë§ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        if 'temporal_bias_check' in self.analysis_results:
            bias_check = self.analysis_results['temporal_bias_check']
            print(f"âœ… ì‹œê°„ì  í¸í–¥ í•´ê²°: {'ì„±ê³µ' if bias_check['bias_resolved'] else 'ë¶€ë¶„ì '}")
            print(f"   - ì›”ë³„ ê· í˜•ë„ (CV): {bias_check['monthly_balance_cv']:.3f}")
            print(f"   - ê³„ì ˆë³„ ê· í˜•ë„ (CV): {bias_check['seasonal_balance_cv']:.3f}")
            print(f"   - ì»¤ë²„ëœ ì›” ìˆ˜: {bias_check['months_covered']}/12")
            print(f"   - ì»¤ë²„ëœ ê³„ì ˆ ìˆ˜: {bias_check['seasons_covered']}/4")
            print(f"   - ì‹œê°„ì  ë²”ìœ„: {bias_check['temporal_range_days']}ì¼")
        
        if 'temporal_patterns' in self.analysis_results:
            patterns = self.analysis_results['temporal_patterns']
            print(f"\nğŸ“Š ì‹œê°„ íŒ¨í„´ ë¶„ì„:")
            print(f"   - í”¼í¬ ì‹œê°„ëŒ€: {patterns['peak_hours']}")
            print(f"   - ë¹„í”¼í¬ ì‹œê°„ëŒ€: {patterns['off_peak_hours']}")
            print(f"   - ì£¼ë§ ë¹„ìœ¨: {patterns['weekend_ratio']:.3f}")
        
        if 'basic_patterns' in self.analysis_results:
            patterns = self.analysis_results['basic_patterns']
            print(f"\nğŸ“Š ê¸°ë³¸ íŒ¨í„´ ë¶„ì„:")
            print(f"   - ë¶„ì„ ê³ ê° ìˆ˜: {patterns['total_customers_analyzed']}ëª…")
            print(f"   - ì‹œê°„ëŒ€ë³„/ì¼ë³„/ì›”ë³„/ê³„ì ˆë³„ í†µê³„ ì™„ë£Œ")
            print("   - ë³€ë™ê³„ìˆ˜(CV)ëŠ” ì•Œê³ ë¦¬ì¦˜_v4.pyì—ì„œ ê³„ì‚°ë©ë‹ˆë‹¤")
        
        if 'anomaly_patterns' in self.analysis_results:
            anomaly = self.analysis_results['anomaly_patterns']
            print(f"\nâš ï¸  ì´ìƒ íŒ¨í„´ ë¶„ì„:")
            print(f"   - ì´ìƒì¹˜ ë¹„ìœ¨: {anomaly['outlier_rate']:.3f}")
            print(f"   - ì œë¡œê°’ ë¹„ìœ¨: {anomaly['zero_rate']:.3f}")
            print(f"   - ê¸‰ë³€ ë¹„ìœ¨: {anomaly['sudden_change_rate']:.3f}")
            print(f"   - ì´ìƒ ê³ ê° ë¹„ìœ¨: {anomaly['estimated_anomaly_rate']:.3f}")
        
        print(f"\nğŸ’¾ ë©”íƒ€ë°ì´í„°:")
        if 'metadata' in self.analysis_results:
            meta = self.analysis_results['metadata']
            print(f"   - ìƒ˜í”Œ í¬ê¸°: {meta['sample_size']:,}ê±´")
            print(f"   - ê³ ê° ìˆ˜: {meta['total_customers']}ëª…")
            print(f"   - ì‹¤ì œ ê³ ê° ìƒ˜í”Œë§ ë¹„ìœ¨: {meta.get('customer_sample_ratio_used', 0)*100:.0f}%")
            print(f"   - ì‹¤ì œ ì‹œê°„ ìƒ˜í”Œë§ ë¹„ìœ¨: {meta.get('time_sample_ratio_used', 0)*100:.0f}%")
            print(f"   - ìƒ˜í”Œë§ ë°©ë²•: {meta['sampling_method']}")
            print(f"   - ì‹œê°„ì  í¸í–¥ í•´ê²°: {meta['temporal_bias_fixed']}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    target_customers = 500      # 500ëª… (ìµœëŒ€ ì œí•œ)
    records_per_customer = 100  # ê³ ê°ë‹¹ 100ê°œ (ìµœëŒ€ ì œí•œ)
    
    print("í•œêµ­ì „ë ¥ ë°ì´í„° ë¶„ì„ (ì§„ì§œ 30% ê³ ê°, 20% ì‹œê°„ ìƒ˜í”Œë§)")
    print("="*60)
    print(f"ìµœëŒ€ ë¶„ì„ ëŒ€ìƒ: {target_customers}ëª…")
    print(f"ê³ ê°ë‹¹ ìµœëŒ€ ë ˆì½”ë“œ: {records_per_customer}ê°œ")
    print("ì‹¤ì œ ìƒ˜í”Œë§: ì „ì²´ ê³ ê°ì˜ 30%, ê° ê³ ê° ì‹œê°„ ë°ì´í„°ì˜ 20%")
    print()
    
    analyzer = KEPCOAnalyzer(
        target_customers=target_customers,
        records_per_customer=records_per_customer
    )
    
    result_path = analyzer.run_analysis()
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ê²°ê³¼ íŒŒì¼: {result_path}")
    
    return result_path


if __name__ == "__main__":
    main()