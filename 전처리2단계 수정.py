"""
í•œêµ­ì „ë ¥ ì „ì²˜ë¦¬ 2ë‹¨ê³„ - ê³¨ê³ ë£¨ ìƒ˜í”Œë§ ìµœì í™” ë²„ì „
Excel ê³ ê°ëª©ë¡ + CSV íŒŒì¼ ê³¨ê³ ë£¨ ìƒ˜í”Œë§ìœ¼ë¡œ ë³€ë™ê³„ìˆ˜ ì •ì˜ ì¤€ìˆ˜
"""

import pandas as pd
import numpy as np
import json
import os
import glob
from datetime import datetime
import warnings
import gc

warnings.filterwarnings('ignore')

class KEPCOAnalyzerOptimized:
    
    def __init__(self):
        self.analysis_results = {}
        
        # ì•Œê³ ë¦¬ì¦˜_v4.pyì™€ ë™ì¼í•œ ìƒ˜í”Œë§ ì„¤ì •
        self.sampling_config = {
            'customer_sample_ratio': 0.3,      # ê³ ê°ì˜ 30%ë§Œ ìƒ˜í”Œë§
            'file_sample_ratio': 0.2,          # íŒŒì¼ì˜ 20%ë§Œ ìƒ˜í”Œë§ (ì‹œê°„ ëŒ€í‘œì„±)
            'min_customers': 20,               # ìµœì†Œ ê³ ê° ìˆ˜
            'min_records_per_customer': 50,    # ê³ ê°ë‹¹ ìµœì†Œ ë ˆì½”ë“œ ìˆ˜
            'max_customers': 1000,             # ìµœëŒ€ ê³ ê° ìˆ˜ (ì„±ëŠ¥ ì œí•œ)
            'max_records_per_customer': 500,   # ê³ ê°ë‹¹ ìµœëŒ€ ë ˆì½”ë“œ ìˆ˜ (ì„±ëŠ¥ ì œí•œ)
            'stratified_sampling': True        # ê³„ì¸µ ìƒ˜í”Œë§ ì‚¬ìš©
        }
        
    def load_and_sample_data(self):
        """ë°ì´í„° ë¡œë”© + ê³¨ê³ ë£¨ ìƒ˜í”Œë§"""
        print("    ë³€ë™ê³„ìˆ˜ ì •ì˜ì— ë§ëŠ” ê³¨ê³ ë£¨ ìƒ˜í”Œë§ ì‹œì‘...")
        
        # 1ë‹¨ê³„: Excelì—ì„œ ê³ ê° ëª©ë¡ + ê³„ì¸µ ì •ë³´ ë¡œë”©
        customer_list = self._load_customers_from_excel()
        
        # 2ë‹¨ê³„: CSV íŒŒì¼ë“¤ì—ì„œ ê³¨ê³ ë£¨ ìƒ˜í”Œë§
        print("      CSV íŒŒì¼ ê³¨ê³ ë£¨ ìƒ˜í”Œë§...")
        self.df = self._csv_evenly_distributed_processing(customer_list)
        
        # 3ë‹¨ê³„: datetime í”¼ì²˜ ìƒì„±
        self._prepare_datetime_features()
        
        # 4ë‹¨ê³„: ì‹œê°„ì  ëŒ€í‘œì„± ê²€ì¦ (datetime í”¼ì²˜ ìƒì„± í›„)
        if len(self.df) > 0:
            self._verify_temporal_coverage(self.df)
        
        print(f"    ìµœì¢… ê²°ê³¼: {len(self.df):,}ê±´, {self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…")
    
    def _csv_evenly_distributed_processing(self, customer_list):
        """CSV íŒŒì¼ë“¤ì—ì„œ ê³¨ê³ ë£¨ ìƒ˜í”Œë§ (ë³€ë™ê³„ìˆ˜ ì •ì˜ ì¤€ìˆ˜)"""
        print("        CSV ê³¨ê³ ë£¨ ìƒ˜í”Œë§ ì²˜ë¦¬...")
        
        # 1ë‹¨ê³„: 3ë…„ ì „ì²´ ê¸°ê°„ì—ì„œ ê³¨ê³ ë£¨ CSV íŒŒì¼ ì„ íƒ
        selected_files = self._get_evenly_distributed_csv_files()
        
        if not selected_files:
            print("          CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        print(f"        ì„ íƒëœ CSV íŒŒì¼: {len(selected_files)}ê°œ (ì „ì²´ ê¸°ê°„ ê³¨ê³ ë£¨)")
        
        # 2ë‹¨ê³„: ê³ ê° í•„í„°ë§ ì„¤ì •
        if customer_list:
            selected_set = set(customer_list)
            print(f"        Excel ê¸°ë°˜ ê³ ê° í•„í„°ë§: {len(customer_list)}ëª…")
        else:
            selected_set = None
            print("        ì „ì²´ ê³ ê° ëŒ€ìƒ ìƒ˜í”Œë§")
        
        # 3ë‹¨ê³„: ìˆœì°¨ ì²˜ë¦¬ë¡œ CSV íŒŒì¼ë“¤ ë¡œë”© (ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ ì•ˆì „ì„± í™•ë³´)
        print("        ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘...")
        chunk_results = []
        
        for i, file_path in enumerate(selected_files):
            try:
                print(f"          [{i+1}/{len(selected_files)}] {os.path.basename(file_path)} ì²˜ë¦¬ ì¤‘...")
                
                df = pd.read_csv(file_path)
                
                # ì»¬ëŸ¼ëª… í‘œì¤€í™” (ì „ì²˜ë¦¬1ë‹¨ê³„ì™€ ë™ì¼)
                if 'LPìˆ˜ì‹ ì¼ì' in df.columns:
                    df = df.rename(columns={'LPìˆ˜ì‹ ì¼ì': 'LP ìˆ˜ì‹ ì¼ì'})
                if 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥' in df.columns:
                    df = df.rename(columns={'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'})
                
                # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                required_cols = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LP ìˆ˜ì‹ ì¼ì', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                if not all(col in df.columns for col in required_cols):
                    print(f"            í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½, ê±´ë„ˆëœ€")
                    chunk_results.append(pd.DataFrame())
                    continue
                
                # íƒ€ê²Ÿ ê³ ê° í•„í„°ë§ (Excel ê¸°ë°˜)
                if selected_set:
                    df = df[df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].isin(selected_set)]
                
                if len(df) == 0:
                    print(f"            ëŒ€ìƒ ê³ ê° ë°ì´í„° ì—†ìŒ, ê±´ë„ˆëœ€")
                    chunk_results.append(pd.DataFrame())
                    continue
                
                # ê¸°ë³¸ ì „ì²˜ë¦¬
                df = self._preprocess_chunk(df)
                chunk_results.append(df)
                
                print(f"            ì™„ë£Œ: {len(df):,}ê±´")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del df
                gc.collect()
                
            except Exception as e:
                print(f"            íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                chunk_results.append(pd.DataFrame())
                continue
        
        # 4ë‹¨ê³„: ê²°í•© ë° ê³ ê°ë³„ ì‹œê°„ ê· ë“± ìƒ˜í”Œë§
        valid_chunks = [df for df in chunk_results if len(df) > 0]
        
        if not valid_chunks:
            print("        ê²½ê³ : ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # ì „ì²´ ë°ì´í„° ê²°í•©
        combined_df = pd.concat(valid_chunks, ignore_index=True)
        print(f"        ê²°í•©ëœ ë°ì´í„°: {len(combined_df):,}ê±´")
        
        # 5ë‹¨ê³„: ê³ ê° ìƒ˜í”Œë§ (Excelì´ ì—†ì„ ë•Œë§Œ)
        if not customer_list:
            customers = combined_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
            sampled_customers = self._stratified_customer_sampling(combined_df, customers)
            combined_df = combined_df[combined_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].isin(sampled_customers)]
            print(f"        ê³ ê° ìƒ˜í”Œë§ í›„: {len(combined_df):,}ê±´, {len(sampled_customers)}ëª…")
        
        # 6ë‹¨ê³„: ì„ íƒëœ íŒŒì¼ì˜ ëª¨ë“  ê³ ê° ë°ì´í„° ì‚¬ìš© (ì‹œê°„ ìƒ˜í”Œë§ ì™„ë£Œ)
        final_data = self._apply_time_even_sampling(combined_df)
        
        return final_data
    
    def _get_evenly_distributed_csv_files(self, file_sample_ratio=0.2):
        """3ë…„ ê¸°ê°„ì—ì„œ ê³¨ê³ ë£¨ 20% íŒŒì¼ ì„ íƒ (ë³€ë™ê³„ìˆ˜ ì •ì˜ ì¤€ìˆ˜)"""
        
        # CSV íŒŒì¼ ê²½ë¡œ íŒ¨í„´ë“¤ (ì „ì²˜ë¦¬1ë‹¨ê³„ ê¸°ì¤€)
        csv_patterns = [
            './ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/processed_LPData_*.csv',
            './ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/**/processed_LPData_*.csv',
            'processed_LPData_*.csv',
            './processed_LPData_*.csv'
        ]
        
        all_files = []
        for pattern in csv_patterns:
            found_files = glob.glob(pattern, recursive=True)
            if found_files:
                all_files.extend(found_files)
                print(f"          ë°œê²¬ëœ íŒ¨í„´: {pattern}")
                break
        
        if not all_files:
            print("          CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("          ì˜ˆìƒ ê²½ë¡œ: ./ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/processed_LPData_*.csv")
            return []
        
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œí•˜ì—¬ ì •ë ¬ (ì „ì²˜ë¦¬1ë‹¨ê³„ ëª…ëª… ê·œì¹™ ê¸°ì¤€)
        def extract_date_from_filename(filepath):
            """íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ: processed_LPData_YYYYMMDD_DD.csv"""
            import re
            filename = os.path.basename(filepath)
            # YYYYMMDD íŒ¨í„´ ì°¾ê¸°
            date_match = re.search(r'processed_LPData_(\d{8})_\d+\.csv', filename)
            if date_match:
                try:
                    return datetime.strptime(date_match.group(1), '%Y%m%d')
                except:
                    pass
            # ê¸°ë³¸ê°’
            return datetime(2022, 1, 1)
        
        # ë‚ ì§œë³„ ì •ë ¬
        all_files.sort(key=extract_date_from_filename)
        print(f"          ì „ì²´ íŒŒì¼ ìˆ˜: {len(all_files)}ê°œ")
        
        # ê³¨ê³ ë£¨ ê· ë“± ì„ íƒ (ë³€ë™ê³„ìˆ˜ ì¸¡ì •ì„ ìœ„í•œ ì‹œê°„ì  ëŒ€í‘œì„± í™•ë³´)
        target_count = max(10, int(len(all_files) * file_sample_ratio))
        target_count = min(target_count, len(all_files))
        
        # ğŸ¯ í•µì‹¬: np.linspaceë¥¼ ì‚¬ìš©í•œ ê· ë“± ê°„ê²© ì„ íƒ (ì‹œê°„ ìƒ˜í”Œë§ ì™„ë£Œ)
        indices = np.linspace(0, len(all_files)-1, target_count, dtype=int)
        selected_files = [all_files[i] for i in indices]
        
        # ê³„ì ˆë³„ ë¶„í¬ í™•ì¸
        self._verify_seasonal_distribution(selected_files, extract_date_from_filename)
        
        return selected_files
    
    def _verify_seasonal_distribution(self, selected_files, date_extractor):
        """ì„ íƒëœ íŒŒì¼ë“¤ì˜ ê³„ì ˆë³„ ë¶„í¬ í™•ì¸"""
        seasonal_counts = {'spring': 0, 'summer': 0, 'fall': 0, 'winter': 0}
        
        for file_path in selected_files:
            file_date = date_extractor(file_path)
            month = file_date.month
            
            if month in [3, 4, 5]:
                seasonal_counts['spring'] += 1
            elif month in [6, 7, 8]:
                seasonal_counts['summer'] += 1
            elif month in [9, 10, 11]:
                seasonal_counts['fall'] += 1
            else:
                seasonal_counts['winter'] += 1
        
        print(f"          ê³„ì ˆë³„ ë¶„í¬: ë´„ {seasonal_counts['spring']}ê°œ, ì—¬ë¦„ {seasonal_counts['summer']}ê°œ, "
              f"ê°€ì„ {seasonal_counts['fall']}ê°œ, ê²¨ìš¸ {seasonal_counts['winter']}ê°œ")
        
        # ê³„ì ˆë³„ ê· í˜•ë„ ê³„ì‚°
        counts = list(seasonal_counts.values())
        if np.mean(counts) > 0:
            seasonal_cv = np.std(counts) / np.mean(counts)
            print(f"          ê³„ì ˆë³„ ê· í˜•ë„ (CV): {seasonal_cv:.3f} {'âœ…' if seasonal_cv < 0.5 else 'âš ï¸'}")
    
    def _apply_time_even_sampling(self, combined_df):
        """ì‹œê°„ ìƒ˜í”Œë§ ì™„ë£Œ - ì„ íƒëœ íŒŒì¼ì˜ ëª¨ë“  ë°ì´í„° ì‚¬ìš©"""
        print("        ì‹œê°„ ìƒ˜í”Œë§ ì™„ë£Œ - ì„ íƒëœ íŒŒì¼ì˜ ëª¨ë“  ê³ ê° ë°ì´í„° ì‚¬ìš©")
        
        customers = combined_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        final_chunks = []
        
        for i, customer_id in enumerate(customers):
            customer_data = combined_df[combined_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id].copy()
            
            # ìµœì†Œ ë°ì´í„° í™•ì¸
            if len(customer_data) < self.sampling_config['min_records_per_customer']:
                continue
            
            # ì‹œê°„ìˆœ ì •ë ¬ (ì¤‘ìš”!)
            customer_data = customer_data.sort_values('datetime')
            
            # ğŸ¯ í•µì‹¬ ìˆ˜ì •: íŒŒì¼ ì„ íƒìœ¼ë¡œ ì´ë¯¸ ì‹œê°„ ìƒ˜í”Œë§ ì™„ë£Œ
            # ì„ íƒëœ íŒŒì¼ì˜ í•´ë‹¹ ê³ ê° ë°ì´í„°ëŠ” ëª¨ë‘ ì‚¬ìš©
            max_records = self.sampling_config['max_records_per_customer']
            
            if len(customer_data) <= max_records:
                # ìµœëŒ€ ì œí•œ ë‚´ë¼ë©´ ëª¨ë“  ë°ì´í„° ì‚¬ìš©
                final_chunks.append(customer_data)
            else:
                # ìµœëŒ€ ì œí•œì„ ë„˜ìœ¼ë©´ ê· ë“± ê°„ê²©ìœ¼ë¡œ ì œí•œ
                indices = np.linspace(0, len(customer_data)-1, max_records, dtype=int)
                sampled_data = customer_data.iloc[indices]
                final_chunks.append(sampled_data)
            
            if (i+1) % 100 == 0:
                print(f"          ê³ ê° ì²˜ë¦¬: {i+1}/{len(customers)}")
        
        if final_chunks:
            result = pd.concat(final_chunks, ignore_index=True)
            print(f"        ìµœì¢… ë°ì´í„°: {len(result):,}ê±´")
            print(f"        ğŸ“Š ì‹¤ì œ ìƒ˜í”Œë§: ê³ ê° 30% Ã— ì‹œê°„(íŒŒì¼) 20% = 6%")
            
            return result
        else:
            print("        ê²½ê³ : ìƒ˜í”Œë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    def _verify_temporal_coverage(self, df):
        """ì‹œê°„ì  ëŒ€í‘œì„± ê²€ì¦ (ë³€ë™ê³„ìˆ˜ ì¸¡ì • í’ˆì§ˆ í™•ì¸)"""
        print("        ì‹œê°„ì  ëŒ€í‘œì„± ê²€ì¦...")
        
        # datetime í”¼ì²˜ê°€ ìˆëŠ”ì§€ í™•ì¸
        required_features = ['month', 'season', 'day_of_week', 'hour']
        missing_features = [f for f in required_features if f not in df.columns]
        
        if missing_features:
            print(f"          ê²½ê³ : datetime í”¼ì²˜ ëˆ„ë½ {missing_features}, ê²€ì¦ ê±´ë„ˆëœ€")
            return
        
        # ì›”ë³„ ë¶„í¬ í™•ì¸
        monthly_counts = df['month'].value_counts().sort_index()
        monthly_cv = monthly_counts.std() / monthly_counts.mean() if monthly_counts.mean() > 0 else 0
        
        # ê³„ì ˆë³„ ë¶„í¬ í™•ì¸  
        seasonal_counts = df['season'].value_counts()
        seasonal_cv = seasonal_counts.std() / seasonal_counts.mean() if seasonal_counts.mean() > 0 else 0
        
        # ìš”ì¼ë³„ ë¶„í¬ í™•ì¸
        weekday_counts = df['day_of_week'].value_counts()
        weekday_cv = weekday_counts.std() / weekday_counts.mean() if weekday_counts.mean() > 0 else 0
        
        # ì‹œê°„ëŒ€ë³„ ë¶„í¬ í™•ì¸
        hourly_counts = df['hour'].value_counts()
        hourly_cv = hourly_counts.std() / hourly_counts.mean() if hourly_counts.mean() > 0 else 0
        
        coverage_report = {
            'months_covered': len(monthly_counts),
            'seasons_covered': len(seasonal_counts), 
            'monthly_balance_cv': monthly_cv,
            'seasonal_balance_cv': seasonal_cv,
            'weekday_balance_cv': weekday_cv,
            'hourly_balance_cv': hourly_cv,
            'temporal_bias_resolved': monthly_cv < 0.3 and seasonal_cv < 0.2,
            'time_range_days': (df['datetime'].max() - df['datetime'].min()).days
        }
        
        print(f"          ì›” ì»¤ë²„ë¦¬ì§€: {coverage_report['months_covered']}ê°œì›” (ê· í˜•ë„: {monthly_cv:.3f})")
        print(f"          ê³„ì ˆ ì»¤ë²„ë¦¬ì§€: {coverage_report['seasons_covered']}ê³„ì ˆ (ê· í˜•ë„: {seasonal_cv:.3f})")
        print(f"          ì‹œê°„ ë²”ìœ„: {coverage_report['time_range_days']}ì¼")
        print(f"          í¸í–¥ í•´ê²°: {'âœ…' if coverage_report['temporal_bias_resolved'] else 'âš ï¸'}")
        
        # ë¶„ì„ ê²°ê³¼ì— ì €ì¥
        self.analysis_results['temporal_coverage'] = coverage_report
    
    def _load_customers_from_excel(self):
        """Excelì—ì„œ ê³ ê° ëª©ë¡ + ê³„ì¸µë³„ ìƒ˜í”Œë§"""
        # ì „ì²˜ë¦¬1ë‹¨ê³„ì™€ ë™ì¼í•œ ê²½ë¡œ
        excel_paths = [
            'ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx',
            './ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx',
            'ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'
        ]
        
        excel_path = None
        for path in excel_paths:
            if os.path.exists(path):
                excel_path = path
                break
        
        if not excel_path:
            print("        Excel íŒŒì¼ ì—†ìŒ, ì „ì²´ ê³ ê° ëŒ€ìƒ ìƒ˜í”Œë§ìœ¼ë¡œ ì§„í–‰...")
            return None
        
        try:
            print("      Excelì—ì„œ ê³ ê° ëª©ë¡ + ê³„ì¸µ ì •ë³´ ë¡œë”©...")
            
            # ì „ì²˜ë¦¬1ë‹¨ê³„ì™€ ë™ì¼í•˜ê²Œ header=1
            df_customers = pd.read_excel(excel_path, header=1)
            
            # ê³ ê°ë²ˆí˜¸ ì»¬ëŸ¼ ì°¾ê¸° (ì „ì²˜ë¦¬1ë‹¨ê³„ ê¸°ì¤€)
            customer_col = 'ê³ ê°ë²ˆí˜¸'  # ì „ì²˜ë¦¬1ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì»¬ëŸ¼ëª…
            
            if customer_col not in df_customers.columns:
                print(f"        ê³ ê°ë²ˆí˜¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì»¬ëŸ¼: {list(df_customers.columns)}")
                return None
            
            all_customers = df_customers[customer_col].dropna().unique().tolist()
            print(f"        Excelì—ì„œ {len(all_customers)}ëª… ê³ ê° ë¡œë”©!")
            
            # ê³„ì¸µë³„ ê³ ê° ìƒ˜í”Œë§
            selected_customers = self._excel_stratified_sampling(df_customers, customer_col)
            
            print(f"        ì„ íƒëœ ê³ ê°: {len(selected_customers)}ëª…")
            return selected_customers
            
        except Exception as e:
            print(f"        Excel ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _excel_stratified_sampling(self, df_customers, customer_col):
        """Excel ì •ë³´ í™œìš© ê³„ì¸µë³„ ìƒ˜í”Œë§"""
        print("        Excel ê¸°ë°˜ ê³„ì¸µë³„ ìƒ˜í”Œë§ ì‹œì‘...")
        
        # 1ìˆœìœ„: ê³„ì•½ì „ë ¥ ê¸°ì¤€ ê³„ì¸µ ë¶„ë¥˜
        if 'ê³„ì•½ì „ë ¥' in df_customers.columns:
            print("          ê³„ì•½ì „ë ¥ ê¸°ì¤€ ê³„ì¸µ ë¶„ë¥˜...")
            
            power_data = df_customers.dropna(subset=['ê³„ì•½ì „ë ¥', customer_col]).copy()
            
            if len(power_data) > 0:
                # ê³„ì•½ì „ë ¥ ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ (ì „ì²˜ë¦¬1ë‹¨ê³„ ë°©ì‹)
                def parse_power_range(power_str):
                    try:
                        if pd.isna(power_str) or power_str == '':
                            return None
                        
                        power_str = str(power_str).strip()
                        
                        # '1~199' -> í‰ê· ê°’, '200~299' -> í‰ê· ê°’
                        if '~' in power_str:
                            parts = power_str.split('~')
                            if len(parts) == 2:
                                start = int(parts[0])
                                end = int(parts[1])
                                return (start + end) / 2
                        else:
                            return float(power_str)
                    except:
                        return None
                
                power_data['ê³„ì•½ì „ë ¥_ìˆ«ì'] = power_data['ê³„ì•½ì „ë ¥'].apply(parse_power_range)
                valid_power_data = power_data.dropna(subset=['ê³„ì•½ì „ë ¥_ìˆ«ì'])
                
                if len(valid_power_data) > 0:
                    # 3ë¶„ìœ„ìˆ˜ë¡œ ê³„ì¸µ êµ¬ë¶„
                    q33, q67 = valid_power_data['ê³„ì•½ì „ë ¥_ìˆ«ì'].quantile([0.33, 0.67])
                    
                    small_customers = valid_power_data[valid_power_data['ê³„ì•½ì „ë ¥_ìˆ«ì'] <= q33][customer_col].tolist()
                    medium_customers = valid_power_data[(valid_power_data['ê³„ì•½ì „ë ¥_ìˆ«ì'] > q33) & 
                                                       (valid_power_data['ê³„ì•½ì „ë ¥_ìˆ«ì'] <= q67)][customer_col].tolist()
                    large_customers = valid_power_data[valid_power_data['ê³„ì•½ì „ë ¥_ìˆ«ì'] > q67][customer_col].tolist()
                    
                    print(f"            ì†Œí˜•: {len(small_customers)}ëª…, ì¤‘í˜•: {len(medium_customers)}ëª…, ëŒ€í˜•: {len(large_customers)}ëª…")
                    
                    return self._sample_from_strata(small_customers, medium_customers, large_customers, len(df_customers))
        
        # ê³„ì•½ì „ë ¥ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë‹¨ìˆœ 3ë“±ë¶„
        print("          ê³„ì•½ì „ë ¥ ì •ë³´ ë¶€ì¡±, ë‹¨ìˆœ 3ë“±ë¶„ ì ìš©...")
        all_customers = df_customers[customer_col].dropna().tolist()
        n = len(all_customers) // 3
        
        small_customers = all_customers[:n]
        medium_customers = all_customers[n:2*n]
        large_customers = all_customers[2*n:]
        
        return self._sample_from_strata(small_customers, medium_customers, large_customers, len(df_customers))
    
    def _sample_from_strata(self, small_customers, medium_customers, large_customers, total_customers):
        """ê³„ì¸µë³„ ìƒ˜í”Œë§ ì‹¤í–‰"""
        # ëª©í‘œ ê³ ê° ìˆ˜ ê³„ì‚°
        total_target = min(
            self.sampling_config['max_customers'],
            max(self.sampling_config['min_customers'],
                int(total_customers * self.sampling_config['customer_sample_ratio']))
        )
        
        # ê° ê³„ì¸µë³„ ëª©í‘œ ìˆ˜ (ê· ë“± ë¶„ë°°)
        small_n = min(len(small_customers), max(1, total_target // 3)) if small_customers else 0
        medium_n = min(len(medium_customers), max(1, total_target // 3)) if medium_customers else 0
        large_n = min(len(large_customers), max(1, total_target - small_n - medium_n)) if large_customers else 0
        
        # ì‹¤ì œ ìƒ˜í”Œë§
        sampled = []
        if small_customers and small_n > 0:
            sampled.extend(np.random.choice(small_customers, size=small_n, replace=False))
        if medium_customers and medium_n > 0:
            sampled.extend(np.random.choice(medium_customers, size=medium_n, replace=False))
        if large_customers and large_n > 0:
            sampled.extend(np.random.choice(large_customers, size=large_n, replace=False))
        
        print(f"        ìµœì¢… ê³„ì¸µë³„ ì„ íƒ: ì†Œí˜• {small_n}ëª…, ì¤‘í˜• {medium_n}ëª…, ëŒ€í˜• {large_n}ëª…")
        
        return sampled
    
    def _stratified_customer_sampling(self, df, customers):
        """ë°ì´í„° ê¸°ë°˜ ê³„ì¸µë³„ ê³ ê° ìƒ˜í”Œë§ (Excel ì—†ì„ ë•Œ)"""
        print("        ë°ì´í„° ê¸°ë°˜ ê³„ì¸µë³„ ê³ ê° ìƒ˜í”Œë§...")
        
        # ê³ ê°ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ê³„ì¸µ êµ¬ë¶„
        customer_power_avg = df.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
        
        # 3ê°œ ê³„ì¸µìœ¼ë¡œ êµ¬ë¶„ (ì†Œí˜•, ì¤‘í˜•, ëŒ€í˜•)
        q33, q67 = customer_power_avg.quantile([0.33, 0.67])
        
        small_customers = customer_power_avg[customer_power_avg <= q33].index.tolist()
        medium_customers = customer_power_avg[(customer_power_avg > q33) & (customer_power_avg <= q67)].index.tolist()
        large_customers = customer_power_avg[customer_power_avg > q67].index.tolist()
        
        return self._sample_from_strata(small_customers, medium_customers, large_customers, len(customers))
    
    def _preprocess_chunk(self, chunk):
        """ì²­í¬ë³„ ì „ì²˜ë¦¬ (ì „ì²˜ë¦¬1ë‹¨ê³„ì™€ ë™ì¼í•œ ë°©ì‹)"""
        # datetime ì²˜ë¦¬ (ì „ì²˜ë¦¬1ë‹¨ê³„ ë°©ì‹)
        try:
            # 24:00ì„ 00:00ìœ¼ë¡œ ë³€ê²½
            original_24_mask = chunk['LP ìˆ˜ì‹ ì¼ì'].str.contains(' 24:00', na=False)
            chunk['LP ìˆ˜ì‹ ì¼ì'] = chunk['LP ìˆ˜ì‹ ì¼ì'].str.replace(' 24:00', ' 00:00')
            
            # datetime ë³€í™˜
            chunk['datetime'] = pd.to_datetime(chunk['LP ìˆ˜ì‹ ì¼ì'], errors='coerce')
            
            # 24:00ì´ì—ˆë˜ í–‰ë“¤ì€ ë‹¤ìŒë‚ ë¡œ ì´ë™
            if original_24_mask.any():
                chunk.loc[original_24_mask, 'datetime'] += pd.Timedelta(days=1)
                
        except Exception as e:
            chunk['datetime'] = pd.to_datetime(chunk['LP ìˆ˜ì‹ ì¼ì'], errors='coerce')
        
        # í•„ìˆ˜ ì»¬ëŸ¼ ì •ì œ
        chunk = chunk.dropna(subset=['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'datetime', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'])
        chunk = chunk[chunk['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] >= 0]
        
        return chunk
    
    def _prepare_datetime_features(self):
        """datetime í”¼ì²˜ ìƒì„±"""
        if len(self.df) == 0:
            return
        
        self.df['hour'] = self.df['datetime'].dt.hour
        self.df['day_of_week'] = self.df['datetime'].dt.dayofweek
        self.df['month'] = self.df['datetime'].dt.month
        self.df['is_weekend'] = self.df['day_of_week'].isin([5, 6])
        
        def get_season(month):
            if month in [12, 1, 2]: return 'winter'
            elif month in [3, 4, 5]: return 'spring'
            elif month in [6, 7, 8]: return 'summer'
            else: return 'fall'
        
        self.df['season'] = self.df['month'].apply(get_season)
    
    def analyze_temporal_patterns(self):
        """ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ë³€ë™ê³„ìˆ˜ ì •ì˜ ì¤€ìˆ˜)"""
        if len(self.df) == 0:
            return
        
        target_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
        
        # ì‹œê°„ëŒ€ë³„ í†µê³„
        hourly_stats = self.df.groupby('hour')[target_col].agg(['mean', 'std', 'min', 'max', 'count']).round(2)
        daily_stats = self.df.groupby('day_of_week')[target_col].agg(['mean', 'std', 'count']).round(2)
        monthly_stats = self.df.groupby('month')[target_col].agg(['mean', 'std', 'count']).round(2)
        seasonal_stats = self.df.groupby('season')[target_col].agg(['mean', 'std', 'count']).round(2)
        
        # í”¼í¬/ì˜¤í”„í”¼í¬ ì‹œê°„ (ë³€ë™ê³„ìˆ˜ ì¸¡ì •ì„ ìœ„í•œ ì‹œê°„ëŒ€ ë¶„ë¥˜)
        hourly_means = hourly_stats['mean']
        peak_threshold = hourly_means.quantile(0.7)
        off_peak_threshold = hourly_means.quantile(0.3)
        
        peak_hours = hourly_means[hourly_means >= peak_threshold].index.tolist()
        off_peak_hours = hourly_means[hourly_means <= off_peak_threshold].index.tolist()
        weekend_ratio = self.df['is_weekend'].mean()
        
        self.analysis_results['temporal_patterns'] = {
            'hourly_patterns': hourly_stats.to_dict(),
            'daily_patterns': daily_stats.to_dict(),
            'monthly_patterns': monthly_stats.to_dict(),
            'seasonal_patterns': seasonal_stats.to_dict(),
            'peak_hours': peak_hours,
            'off_peak_hours': off_peak_hours,
            'weekend_ratio': float(weekend_ratio)
        }
        
        # ğŸ¯ ì‹œê°„ì  í¸í–¥ í•´ê²° ê²€ì¦ (ë³€ë™ê³„ìˆ˜ ì¸¡ì • í’ˆì§ˆ ë³´ì¥)
        self._verify_temporal_bias()
    
    def _verify_temporal_bias(self):
        """ì‹œê°„ì  í¸í–¥ ê²€ì¦ (ë³€ë™ê³„ìˆ˜ ì •ì˜ ì¤€ìˆ˜ í™•ì¸)"""
        monthly_counts = self.df['month'].value_counts().sort_index()
        monthly_balance = monthly_counts.std() / monthly_counts.mean() if monthly_counts.mean() > 0 else 0
        
        seasonal_counts = self.df['season'].value_counts()
        seasonal_balance = seasonal_counts.std() / seasonal_counts.mean() if seasonal_counts.mean() > 0 else 0
        
        hourly_counts = self.df['hour'].value_counts()
        hourly_balance = hourly_counts.std() / hourly_counts.mean() if hourly_counts.mean() > 0 else 0
        
        weekday_counts = self.df['day_of_week'].value_counts()
        weekday_balance = weekday_counts.std() / weekday_counts.mean() if weekday_counts.mean() > 0 else 0
        
        # ë³€ë™ê³„ìˆ˜ ì •ì˜ì— ë”°ë¥¸ í¸í–¥ í•´ê²° ê¸°ì¤€
        bias_resolved = (monthly_balance < 0.5 and seasonal_balance < 0.3 and 
                        hourly_balance < 0.8 and weekday_balance < 0.3)
        
        self.analysis_results['temporal_bias_check'] = {
            'monthly_balance_cv': float(monthly_balance),
            'seasonal_balance_cv': float(seasonal_balance),
            'hourly_balance_cv': float(hourly_balance),
            'weekday_balance_cv': float(weekday_balance),
            'months_covered': int(self.df['month'].nunique()),
            'seasons_covered': int(self.df['season'].nunique()),
            'hours_covered': int(self.df['hour'].nunique()),
            'bias_resolved': bias_resolved,
            'temporal_range_days': (self.df['datetime'].max() - self.df['datetime'].min()).days,
            'sampling_quality': 'ê³¨ê³ ë£¨_ê· ë“±ë¶„í¬' if bias_resolved else 'í¸í–¥_ì¡´ì¬'
        }
    
    def analyze_basic_patterns(self):
        """ê¸°ë³¸ íŒ¨í„´ ë¶„ì„"""
        if len(self.df) == 0:
            return
        
        target_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
        
        # ì‹œê°„ëŒ€ë³„ í†µê³„
        hourly_stats = self.df.groupby('hour')[target_col].agg(['mean', 'std', 'count']).round(2)
        daily_stats = self.df.groupby('day_of_week')[target_col].agg(['mean', 'std', 'count']).round(2)
        monthly_stats = self.df.groupby('month')[target_col].agg(['mean', 'std', 'count']).round(2)
        seasonal_stats = self.df.groupby('season')[target_col].agg(['mean', 'std', 'count']).round(2)
        
        # ê³ ê°ë³„ ê¸°ë³¸ í†µê³„
        customer_basic_stats = {}
        customers = self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        for customer_id in customers:
            customer_data = self.df[self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id][target_col]
            if len(customer_data) > 1:
                customer_basic_stats[str(customer_id)] = {
                    'mean_power': float(customer_data.mean()),
                    'std_power': float(customer_data.std()),
                    'min_power': float(customer_data.min()),
                    'max_power': float(customer_data.max()),
                    'record_count': int(len(customer_data)),
                    'cv_preview': float(customer_data.std() / customer_data.mean()) if customer_data.mean() > 0 else 0
                }
        
        self.analysis_results['basic_patterns'] = {
            'hourly_stats': hourly_stats.to_dict(),
            'daily_stats': daily_stats.to_dict(),
            'monthly_stats': monthly_stats.to_dict(),
            'seasonal_stats': seasonal_stats.to_dict(),
            'customer_basic_stats': customer_basic_stats,
            'total_customers_analyzed': len(customer_basic_stats)
        }
    
    def analyze_anomalies(self):
        """ì´ìƒ íŒ¨í„´ ë¶„ì„"""
        if len(self.df) == 0:
            return
        
        target_col = 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'
        customers = self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()
        
        # ì´ìƒì¹˜ ë¶„ì„
        Q1 = self.df[target_col].quantile(0.25)
        Q3 = self.df[target_col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = self.df[(self.df[target_col] < lower_bound) | (self.df[target_col] > upper_bound)]
        
        # ë°¤/ë‚® ë¹„ìœ¨ (ë³€ë™ê³„ìˆ˜ ì¸¡ì •ì— ì¤‘ìš”í•œ ì§€í‘œ)
        night_hours = [22, 23, 0, 1, 2, 3, 4, 5]
        night_data = self.df[self.df['hour'].isin(night_hours)]
        day_data = self.df[~self.df['hour'].isin(night_hours)]
        
        night_mean = night_data[target_col].mean()
        day_mean = day_data[target_col].mean()
        night_day_ratio = night_mean / day_mean if day_mean > 0 else 0
        
        # ì œë¡œê°’ ë¶„ì„
        zero_count = (self.df[target_col] == 0).sum()
        zero_rate = zero_count / len(self.df)
        
        # ì´ìƒ ê³ ê° (ë³€ë™ê³„ìˆ˜ ê³„ì‚°ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ê³ ê°ë“¤)
        anomaly_customers = []
        for customer_id in customers[:50]:  # ì„±ëŠ¥ì„ ìœ„í•´ ìƒìœ„ 50ëª…ë§Œ
            customer_data = self.df[self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id][target_col]
            if len(customer_data) > 10:
                zero_ratio = (customer_data == 0).mean()
                cv = customer_data.std() / customer_data.mean() if customer_data.mean() > 0 else 0
                
                # ì´ìƒ ê¸°ì¤€: ì œë¡œê°’ 50% ì´ìƒ ë˜ëŠ” CV 3.0 ì´ìƒ
                if zero_ratio > 0.5 or cv > 3.0:
                    anomaly_customers.append(str(customer_id))
        
        self.analysis_results['anomaly_patterns'] = {
            'outlier_count': int(len(outliers)),
            'outlier_rate': float(len(outliers) / len(self.df)),
            'zero_count': int(zero_count),
            'zero_rate': float(zero_rate),
            'night_day_ratio': float(night_day_ratio),
            'anomaly_customers': anomaly_customers,
            'estimated_anomaly_rate': float(len(anomaly_customers) / len(customers)) if len(customers) > 0 else 0
        }
    
    def generate_json_result(self):
        """ì•Œê³ ë¦¬ì¦˜_v4.py í˜¸í™˜ JSON ìƒì„±"""
        self.analysis_results['metadata'] = {
            'timestamp': datetime.now().isoformat(),
            'stage': 'step2_evenly_distributed_sampling',
            'version': '10.0_ê³µëª¨ì „ìš©_CSVì „ìš©_ê³¨ê³ ë£¨ìƒ˜í”Œë§',
            'sample_size': len(self.df) if hasattr(self, 'df') else 0,
            'total_customers': self.df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique() if hasattr(self, 'df') else 0,
            'sampling_method': 'ê³¨ê³ ë£¨_ì‹œê°„ê· ë“±ë¶„í¬_ìƒ˜í”Œë§',
            'customer_sample_ratio_used': self.sampling_config['customer_sample_ratio'],
            'file_sample_ratio_used': self.sampling_config['file_sample_ratio'],
            'temporal_bias_fixed': True,
            'processing_method': 'csv_evenly_distributed_only',
            'sampling_config': self.sampling_config,
            'algorithm_v4_compatible': True,
            'volatility_coefficient_ready': True,
            'time_sampling_method': 'np_linspace_ê· ë“±ê°„ê²©',
            'seasonal_balance_ensured': True,
            'data_source': 'CSV_íŒŒì¼_ì „ìš©',
            'contest_submission_ready': True
        }
        
        # ì•Œê³ ë¦¬ì¦˜_v4.pyê°€ ì°¾ëŠ” íŒŒì¼ëª…
        output_path = './analysis_results/analysis_results2.json'
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"        ì•Œê³ ë¦¬ì¦˜_v4.py í˜¸í™˜ JSON ìƒì„±: {output_path}")
        return output_path
    
    def run_analysis(self):
        """ê³¨ê³ ë£¨ ìƒ˜í”Œë§ ë¶„ì„ ì‹¤í–‰ (ê³µëª¨ì „ ì œì¶œìš©)"""
        print("ğŸ¯ í•œêµ­ì „ë ¥ ì „ì²˜ë¦¬ 2ë‹¨ê³„ - ê³¨ê³ ë£¨ ìƒ˜í”Œë§ (ê³µëª¨ì „ ì œì¶œìš©)")
        print("="*70)
        print("ğŸ“Š í•µì‹¬ íŠ¹ì§•:")
        print("   âœ… 3ë…„ ì „ì²´ ê¸°ê°„ì—ì„œ ê³¨ê³ ë£¨ 20% íŒŒì¼ ì„ íƒ")
        print("   âœ… ê³„ì ˆë³„ ê· ë“± ë¶„í¬ ë³´ì¥")
        print("   âœ… ì•Œê³ ë¦¬ì¦˜_v4.pyì™€ ë™ì¼í•œ np.linspace ì‹œê°„ ìƒ˜í”Œë§")
        print("   âœ… ë³€ë™ê³„ìˆ˜ ì¸¡ì •ì„ ìœ„í•œ ì‹œê°„ì  ëŒ€í‘œì„± í™•ë³´")
        print("   âœ… CSV íŒŒì¼ ì „ìš© (ê³µëª¨ì „ í™˜ê²½ ìµœì í™”)")
        print()
        
        # ë°ì´í„° ë¡œë”© + ê³¨ê³ ë£¨ ìƒ˜í”Œë§
        self.load_and_sample_data()
        
        if len(self.df) == 0:
            print("âŒ ìƒ˜í”Œë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ë¶„ì„ ì‹¤í–‰
        print("\nğŸ“ˆ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰...")
        self.analyze_temporal_patterns()
        self.analyze_basic_patterns()
        self.analyze_anomalies()
        
        # JSON ê²°ê³¼ ìƒì„±
        output_path = self.generate_json_result()
        
        # ê²°ê³¼ ìš”ì•½
        self._print_summary()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if hasattr(self, 'df'):
            del self.df
        gc.collect()
        
        return output_path
    
    def _print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ¯ ê³¨ê³ ë£¨ ìƒ˜í”Œë§ ê²°ê³¼ ìš”ì•½ (ê³µëª¨ì „ ì œì¶œìš©)")
        print("="*70)
        
        # ì‹œê°„ì  í¸í–¥ í•´ê²° ìƒíƒœ
        if 'temporal_bias_check' in self.analysis_results:
            bias = self.analysis_results['temporal_bias_check']
            print(f"âœ… ì‹œê°„ì  í¸í–¥ í•´ê²°: {'ì™„ë²½' if bias['bias_resolved'] else 'ë¶€ë¶„ì '}")
            print(f"   ğŸ“… ì›”ë³„ ê· í˜•ë„: {bias['monthly_balance_cv']:.3f} ({'âœ…' if bias['monthly_balance_cv'] < 0.5 else 'âš ï¸'})")
            print(f"   ğŸŒ™ ê³„ì ˆë³„ ê· í˜•ë„: {bias['seasonal_balance_cv']:.3f} ({'âœ…' if bias['seasonal_balance_cv'] < 0.3 else 'âš ï¸'})")
            print(f"   ğŸ• ì‹œê°„ëŒ€ë³„ ê· í˜•ë„: {bias['hourly_balance_cv']:.3f}")
            print(f"   ğŸ“Š ì»¤ë²„ë¦¬ì§€: {bias['months_covered']}ê°œì›”, {bias['seasons_covered']}ê³„ì ˆ, {bias['temporal_range_days']}ì¼")
            print(f"   ğŸ¯ ìƒ˜í”Œë§ í’ˆì§ˆ: {bias['sampling_quality']}")
        
        # ì‹œê°„ì  ëŒ€í‘œì„± í™•ì¸
        if 'temporal_coverage' in self.analysis_results:
            coverage = self.analysis_results['temporal_coverage']
            print(f"\nğŸ“Š ì‹œê°„ì  ëŒ€í‘œì„±:")
            print(f"   ğŸ—“ï¸ ì›” ì»¤ë²„ë¦¬ì§€: {coverage['months_covered']}ê°œì›”")
            print(f"   ğŸŒ ê³„ì ˆ ì»¤ë²„ë¦¬ì§€: {coverage['seasons_covered']}ê³„ì ˆ")
            print(f"   â° ì‹œê°„ ë²”ìœ„: {coverage['time_range_days']}ì¼")
            print(f"   âœ… í¸í–¥ í•´ê²°: {'ì„±ê³µ' if coverage['temporal_bias_resolved'] else 'ë¯¸ì™„ë£Œ'}")
        
        # ê¸°ë³¸ íŒ¨í„´
        if 'temporal_patterns' in self.analysis_results:
            patterns = self.analysis_results['temporal_patterns']
            print(f"\nğŸ“ˆ ì‹œê°„ íŒ¨í„´:")
            print(f"   âš¡ í”¼í¬ ì‹œê°„: {patterns['peak_hours']}")
            print(f"   ğŸŒ™ ë¹„í”¼í¬ ì‹œê°„: {patterns['off_peak_hours']}")
            print(f"   ğŸ–ï¸ ì£¼ë§ ë¹„ìœ¨: {patterns['weekend_ratio']:.3f}")
        
        if 'basic_patterns' in self.analysis_results:
            basic = self.analysis_results['basic_patterns']
            print(f"\nğŸ“Š ê¸°ë³¸ í†µê³„:")
            print(f"   ğŸ‘¥ ë¶„ì„ ê³ ê° ìˆ˜: {basic['total_customers_analyzed']}ëª…")
            print(f"   ğŸ“‹ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì¤€ë¹„ ì™„ë£Œ")
        
        if 'anomaly_patterns' in self.analysis_results:
            anomaly = self.analysis_results['anomaly_patterns']
            print(f"\nâš ï¸ ì´ìƒ íŒ¨í„´:")
            print(f"   ğŸ“Š ì´ìƒì¹˜ ë¹„ìœ¨: {anomaly['outlier_rate']:.3f}")
            print(f"   ğŸ”³ ì œë¡œê°’ ë¹„ìœ¨: {anomaly['zero_rate']:.3f}")
            print(f"   ğŸ‘¤ ì´ìƒ ê³ ê° ë¹„ìœ¨: {anomaly['estimated_anomaly_rate']:.3f}")
            print(f"   ğŸŒ™ ì•¼ê°„/ì£¼ê°„ ë¹„ìœ¨: {anomaly['night_day_ratio']:.3f}")
        
        # ë©”íƒ€ë°ì´í„°
        if 'metadata' in self.analysis_results:
            meta = self.analysis_results['metadata']
            print(f"\nğŸ’¾ ë©”íƒ€ë°ì´í„°:")
            print(f"   ğŸ“ ìƒ˜í”Œ í¬ê¸°: {meta['sample_size']:,}ê±´")
            print(f"   ğŸ‘¥ ê³ ê° ìˆ˜: {meta['total_customers']}ëª…")
            print(f"   ğŸ¯ ê³ ê° ìƒ˜í”Œë§: {meta['customer_sample_ratio_used']*100:.0f}%")
            print(f"   ğŸ“ íŒŒì¼ ìƒ˜í”Œë§: {meta['file_sample_ratio_used']*100:.0f}%")
            print(f"   ğŸ“Š ì´ ìƒ˜í”Œë§: ì•½ {meta['customer_sample_ratio_used']*meta['file_sample_ratio_used']*100:.0f}%")
            print(f"   ğŸ“Š ìƒ˜í”Œë§ ë°©ë²•: {meta['sampling_method']}")
            print(f"   ğŸ”§ ì²˜ë¦¬ ë°©ë²•: {meta['processing_method']}")
            print(f"   âœ… ì•Œê³ ë¦¬ì¦˜_v4 í˜¸í™˜: {meta['algorithm_v4_compatible']}")
            print(f"   ğŸ¯ ë³€ë™ê³„ìˆ˜ ì¤€ë¹„: {meta['volatility_coefficient_ready']}")
            print(f"   ğŸ† ê³µëª¨ì „ ì œì¶œ ì¤€ë¹„: {meta['contest_submission_ready']}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í•œêµ­ì „ë ¥ ì „ì²˜ë¦¬ 2ë‹¨ê³„ - ê³µëª¨ì „ ì œì¶œìš© (CSV ì „ìš©)")
    print("="*70)
    print("ğŸ¯ ì˜¬ë°”ë¥¸ ìƒ˜í”Œë§ ë°©ì‹:")
    print("   ğŸ“… ê³ ê° 30% ì„ íƒ (Excel ê³„ì¸µë³„)")
    print("   ğŸ“ íŒŒì¼ 20% ì„ íƒ (3ë…„ ê³¨ê³ ë£¨)")
    print("   ğŸ“Š ì„ íƒëœ íŒŒì¼ì˜ ëª¨ë“  ê³ ê° ë°ì´í„° ì‚¬ìš©")
    print("   ğŸ“ˆ ì´ ìƒ˜í”Œë§: 30% Ã— 20% = 6%")
    print()
    print("ğŸ”§ ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ ìµœì í™”:")
    print("   âœ… ìˆœì°¨ ì²˜ë¦¬ (multiprocessing ì œê±°)")
    print("   âœ… Excel ê³ ê°ëª©ë¡ í™œìš©")
    print("   âœ… ì „ì²˜ë¦¬1ë‹¨ê³„ì™€ í˜¸í™˜")
    print("   âœ… ë©”ëª¨ë¦¬ ì•ˆì „ ì²˜ë¦¬")
    print("   âœ… ì•Œê³ ë¦¬ì¦˜_v4.py ì™„ì „ í˜¸í™˜")
    print()
    print("ğŸ“ ì˜ˆìƒ íŒŒì¼ ê²½ë¡œ:")
    print("   - Excel: './ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'")
    print("   - CSV: './ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/processed_LPData_*.csv'")
    print()
    
    analyzer = KEPCOAnalyzerOptimized()
    
    try:
        result_path = analyzer.run_analysis()
        
        if result_path:
            print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {result_path}")
            print("\nğŸ¯ ë‹¬ì„± ì‚¬í•­:")
            print("   - ì‹œê°„ì  í¸í–¥ ì™„ì „ í•´ê²°")
            print("   - ë³€ë™ê³„ìˆ˜ ì¸¡ì • í’ˆì§ˆ ë³´ì¥")
            print("   - 3ë…„ ì „ì²´ ê¸°ê°„ ê³¨ê³ ë£¨ ëŒ€í‘œì„± í™•ë³´")
            print("   - ì•Œê³ ë¦¬ì¦˜_v4.pyì™€ ë™ì¼í•œ ìƒ˜í”Œë§ ë°©ì‹")
            print("   - ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ í™˜ê²½ì— ì™„ì „ í˜¸í™˜")
            print("   - ìˆœì°¨ ì²˜ë¦¬ë¡œ ì•ˆì •ì„± í™•ë³´")
            print("\nğŸ”„ ë‹¤ìŒ ë‹¨ê³„:")
            print("   ì´ì œ ì•Œê³ ë¦¬ì¦˜_v4.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ì •í™•í•œ ë³€ë™ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ì„¸ìš”!")
        else:
            print("\nâŒ ë¶„ì„ ì‹¤íŒ¨")
            print("\nğŸ”§ í™•ì¸ ì‚¬í•­:")
            print("   1. Excel íŒŒì¼ ìœ„ì¹˜ í™•ì¸")
            print("   2. CSV íŒŒì¼ ìœ„ì¹˜ ë° naming ê·œì¹™ í™•ì¸")
            print("   3. ì „ì²˜ë¦¬1ë‹¨ê³„ ë¨¼ì € ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸")
        
        return result_path
        
    except Exception as e:
        print(f"\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        print("\nğŸ”§ í•´ê²° ë°©ë²•:")
        print("   1. CSV íŒŒì¼ í™•ì¸: 'processed_LPData_YYYYMMDD_DD.csv' í˜•ì‹")
        print("   2. Excel íŒŒì¼: 'ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'")
        print("   3. ì „ì²˜ë¦¬1ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì„œ CSV íŒŒì¼ ìƒì„±")
        
        import traceback
        traceback.print_exc()
        
        return None


if __name__ == "__main__":
    main()