import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
import glob
import os
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class KEPCODataAnalyzer:
    def __init__(self):
        self.customer_data = None
        self.lp_data = None
        
        self.analysis_results = {}
        
    def load_customer_data(self, file_path='ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx'):
        """ì‹¤ì œ ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ê¸°ë³¸ ë¶„ì„"""
        print("=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===")
        
        try:
            # ì‹¤ì œ Excel íŒŒì¼ ì½ê¸°
            self.customer_data = pd.read_excel(file_path, header=1)
            
            print(f"ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…")
            print(f"ì»¬ëŸ¼: {list(self.customer_data.columns)}")
            print("\nê¸°ë³¸ ì •ë³´:")
            print(self.customer_data.head())
            
            return self._analyze_customer_distribution()
            
        except Exception as e:
            print(f"ê³ ê° ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _analyze_customer_distribution(self):
        """ê³ ê° ë¶„í¬ ë¶„ì„"""
        print("\n=== ê³ ê° ë¶„í¬ ë¶„ì„ ===")
        
        # ê³„ì•½ì¢…ë³„ ë¶„í¬
        contract_counts = self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts()
        print("\nğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:")
        for contract, count in contract_counts.items():
            pct = (count / len(self.customer_data)) * 100
            print(f"  {contract}: {count}ëª… ({pct:.1f}%)")
        
        # ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬
        usage_counts = self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts()
        print("\nğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:")
        for usage, count in usage_counts.items():
            pct = (count / len(self.customer_data)) * 100
            print(f"  {usage}: {count}ëª… ({pct:.1f}%)")
        
        # ê³„ì•½ì „ë ¥ ë¶„í¬
        print("\nâš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:")
        power_stats = self.customer_data['ê³„ì•½ì „ë ¥'].describe()
        print(power_stats)
        
        return {
            'contract_distribution': contract_counts,
            'usage_distribution': usage_counts,
            'power_stats': power_stats
        }
    
    def load_lp_data(self, data_directory='./ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/'):
        """ì‹¤ì œ LP ë°ì´í„° ë¡œë”© (ì—¬ëŸ¬ CSV íŒŒì¼)"""
        print("\n=== LP ë°ì´í„° ë¡œë”© ===")
        
        try:
            # processed_LPData_YYYYMMDD_DD.csv íŒ¨í„´ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
            lp_files = glob.glob(os.path.join(data_directory, 'processed_LPData_*.csv'))
            
            if not lp_files:
                print("LP ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            print(f"ë°œê²¬ëœ LP íŒŒì¼ ìˆ˜: {len(lp_files)}ê°œ")
            
            # ëª¨ë“  LP íŒŒì¼ ì½ê¸° ë° ê²°í•©
            lp_dataframes = []
            total_records = 0
            
            for i, file_path in enumerate(sorted(lp_files)):
                try:
                    filename = os.path.basename(file_path)
                    print(f"   [{i+1}/{len(lp_files)}] {filename} ì²˜ë¦¬ ì¤‘...")

                    #ì²­í¬ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ ë°”ë¡œ ì²˜ë¦¬
                    chunk_list = []

                    for chunk in pd.read_csv(file_path, chunksize=5000):  # 5000í–‰ì”© ì²˜ë¦¬
                        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
                        if 'LPìˆ˜ì‹ ì¼ì' in chunk.columns:
                            chunk = chunk.rename(columns={'LPìˆ˜ì‹ ì¼ì': 'LP ìˆ˜ì‹ ì¼ì'})
                        if 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥' in chunk.columns:
                            chunk = chunk.rename(columns={'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'})

                        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
                        required_cols = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LP ìˆ˜ì‹ ì¼ì', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                        if all(col in chunk.columns for col in required_cols):

                            # â­ 24:00 ì²˜ë¦¬ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë°”ë¡œ ì²˜ë¦¬
                            chunk = self._process_datetime_chunk(chunk)

                            # ë°ì´í„° í’ˆì§ˆ ê¸°ë³¸ ì²´í¬
                            chunk = chunk.dropna(subset=required_cols)
                            chunk = chunk[chunk['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] >= 0]

                            chunk_list.append(chunk)

                    # íŒŒì¼ë³„ ì²­í¬ ê²°í•©
                    if chunk_list:
                        file_df = pd.concat(chunk_list, ignore_index=True)
                        lp_dataframes.append(file_df)
                        total_records += len(file_df)
                        print(f"      ë ˆì½”ë“œ: {len(file_df):,}ê°œ, ê³ ê°: {file_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…")
                        
                except Exception as e:
                    print(f"  âœ— íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            if not lp_dataframes:
                print("ìœ íš¨í•œ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ëª¨ë“  ë°ì´í„° ê²°í•©
            self.lp_data = pd.concat(lp_dataframes, ignore_index=True)
            
            # ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬
            self.lp_data = self.lp_data.sort_values(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'datetime']).reset_index(drop=True)
            
            print(f"\nâœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:")
            print(f"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}")
            print(f"  ì´ ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}")
            print(f"   - ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}")
            
            return self._analyze_lp_quality()
            
        except Exception as e:
            print(f"LP ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    
    def _analyze_lp_quality(self):
        """LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        print("\n=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===")

        # ê¸°ë³¸ í†µê³„
        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']
        available_cols = [col for col in numeric_columns if col in self.lp_data.columns]

        print(f"ğŸ“ˆ ê¸°ë³¸ í†µê³„:")
        print(self.lp_data[available_cols].describe())

        # ì‹œê°„ ê°„ê²© ì²´í¬ (ìƒ˜í”Œë§Œ)
        sample_customers = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]
        print(f"\nâ° ì‹œê°„ ê°„ê²© ì²´í¬:")
        for customer in sample_customers:
            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('datetime')
            if len(customer_data) > 1:
                time_diffs = customer_data['datetime'].diff().dt.total_seconds() / 60
                avg_interval = time_diffs.dropna().mean()
                std_interval = time_diffs.dropna().std()
                print(f"  {customer}: í‰ê·  ê°„ê²© {avg_interval:.1f}ë¶„, í‘œì¤€í¸ì°¨ {std_interval:.1f}ë¶„")

        # ë°ì´í„° í’ˆì§ˆ ì²´í¬
        print(f"\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:")
        for col in available_cols:
            null_count = self.lp_data[col].isnull().sum()
            null_pct = null_count / len(self.lp_data) * 100
            zero_count = (self.lp_data[col] == 0).sum()
            zero_pct = zero_count / len(self.lp_data) * 100
            print(f"  {col}:")
            print(f"    ê²°ì¸¡ì¹˜: {null_count}ê±´ ({null_pct:.2f}%)")
            print(f"    0ê°’: {zero_count}ê±´ ({zero_pct:.2f}%)")

        # ì´ìƒì¹˜ íƒì§€
        print(f"\nğŸš¨ ì´ìƒì¹˜ íƒì§€:")
        for col in available_cols:
            Q1 = self.lp_data[col].quantile(0.25)
            Q3 = self.lp_data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = self.lp_data[(self.lp_data[col] < Q1 - 1.5 * IQR) | (self.lp_data[col] > Q3 + 1.5 * IQR)]
            outlier_pct = len(outliers) / len(self.lp_data) * 100
            print(f"  {col}: {len(outliers)}ê±´ ({outlier_pct:.2f}%)")

        return True
    
    def _process_datetime_chunk(self, chunk):
        """ì²­í¬ ë‹¨ìœ„ë¡œ datetime ì²˜ë¦¬"""
        try:
            # 24:00ì„ 00:00ìœ¼ë¡œ ë³€ê²½í•˜ë©´ì„œ ë‹¤ìŒë‚  í‘œì‹œ ì €ì¥
            original_24_mask = chunk['LP ìˆ˜ì‹ ì¼ì'].str.contains(' 24:00', na=False)

            # 24:00ì„ 00:00ìœ¼ë¡œ ë³€ê²½
            chunk['LP ìˆ˜ì‹ ì¼ì'] = chunk['LP ìˆ˜ì‹ ì¼ì'].str.replace(' 24:00', ' 00:00')

            # datetime ë³€í™˜
            chunk['datetime'] = pd.to_datetime(chunk['LP ìˆ˜ì‹ ì¼ì'], errors='coerce')

            # ì›ë˜ 24:00ì´ì—ˆë˜ í–‰ë“¤ì€ ë‹¤ìŒë‚ ë¡œ ì´ë™
            if original_24_mask.any():
                chunk.loc[original_24_mask, 'datetime'] += pd.Timedelta(days=1)

            return chunk

        except Exception as e:
            print(f"   âš ï¸ datetime ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            chunk['datetime'] = pd.to_datetime(chunk['LP ìˆ˜ì‹ ì¼ì'], errors='coerce')
            return chunk
    
    def detect_outliers(self, method='iqr'):
        """ì´ìƒì¹˜ íƒì§€"""
        outlier_summary = {}
        numeric_columns = ['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']
        
        for col in numeric_columns:
            if col in self.lp_data.columns:
                if method == 'iqr':
                    Q1 = self.lp_data[col].quantile(0.25)
                    Q3 = self.lp_data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outliers = self.lp_data[
                        (self.lp_data[col] < lower_bound) | 
                        (self.lp_data[col] > upper_bound)
                    ]
                    
                    outlier_count = len(outliers)
                    outlier_pct = (outlier_count / len(self.lp_data)) * 100
                    
                    print(f"  {col}: {outlier_count}ê±´ ({outlier_pct:.2f}%)")
                    outlier_summary[col] = {
                        'count': outlier_count,
                        'percentage': outlier_pct,
                        'lower_bound': lower_bound,
                        'upper_bound': upper_bound
                    }
        
        return outlier_summary
    

    def generate_quality_report(self):
        """ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ë° ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        import json
        from datetime import datetime
        import os
        
        print("\n" + "="*60)
        print("ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸")
        print("="*60)

        # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if self.customer_data is None or self.lp_data is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        # ê³ ê° ë°ì´í„° ìš”ì•½
        if self.customer_data is not None:
            print(f"\nğŸ‘¥ ê³ ê° ë°ì´í„°:")
            print(f"  ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…")
            print(f"  ê³„ì•½ì¢…ë³„ ìœ í˜•: {self.customer_data['ê³„ì•½ì¢…ë³„'].nunique()}ê°œ")
            print(f"  ì‚¬ìš©ìš©ë„ ìœ í˜•: {self.customer_data['ì‚¬ìš©ìš©ë„'].nunique()}ê°œ")

            # â­ analysis_resultsì— ê³ ê° ì •ë³´ ì €ì¥
            self.analysis_results['customer_summary'] = {
                'total_customers': len(self.customer_data),
                'contract_types': self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts().to_dict(),
                'usage_types': self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts().to_dict()
            }

        # LP ë°ì´í„° ìš”ì•½
        if self.lp_data is not None:
            print(f"\nâš¡ LP ë°ì´í„°:")
            print(f"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}ê±´")
            print(f"  ì¸¡ì • ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}")
            print(f"  ë°ì´í„° ì»¤ë²„ë¦¬ì§€: {(self.lp_data['datetime'].max() - self.lp_data['datetime'].min()).days}ì¼")

            # í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰
            avg_power = self.lp_data['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
            print(f"  í‰ê·  ìœ íš¨ì „ë ¥: {avg_power:.2f}kW")

            # â­ analysis_resultsì— LP ë°ì´í„° ì •ë³´ ì €ì¥
            self.analysis_results['lp_data_summary'] = {
                'total_records': len(self.lp_data),
                'total_customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),
                'date_range': {
                    'start': str(self.lp_data['datetime'].min()),
                    'end': str(self.lp_data['datetime'].max())
                },
                'avg_power': float(avg_power)
            }

        # â­â­â­ í•µì‹¬: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        print(f"\nğŸ’¾ ì „ì²˜ë¦¬ëœ LP ë°ì´í„° ì €ì¥ ì¤‘...")

        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            import os
            os.makedirs('./analysis_results', exist_ok=True)

            # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
            '''
            processed_csv = './analysis_results/processed_lp_data.csv'
            #processed_parquet = './analysis_results/processed_lp_data.parquet'

            print(f"   ğŸ“Š ì €ì¥ ëŒ€ìƒ: {len(self.lp_data):,}ê°œ ë ˆì½”ë“œ")
            print(f"   ğŸ’¾ ì €ì¥ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")

            # 1. CSV ì €ì¥ (í˜¸í™˜ì„±ìš©)
            print(f"      ğŸ“„ CSV ì €ì¥ ì¤‘...")
            #self.lp_data.to_csv(processed_csv, index=False, encoding='utf-8-sig')
            csv_size_gb = os.path.getsize(processed_csv) / 1024**3
            '''

            # 2. â­ HDF5 ì €ì¥ (ì„±ëŠ¥ ìµœì í™”ìš©)
            processed_hdf5 = './analysis_results/processed_lp_data.h5'
            print(f"   ğŸ“Š ì €ì¥ ëŒ€ìƒ: {len(self.lp_data):,}ê°œ ë ˆì½”ë“œ")
            print(f"      ğŸ“¦ HDF5 ì €ì¥ ì¤‘...")
            try:
                self.lp_data.to_hdf(processed_hdf5, key='df', mode='w', format='table')
                hdf5_size_gb = os.path.getsize(processed_hdf5) / 1024**3
                hdf5_success = True
            except Exception as hdf5_error:
                print(f"         âš ï¸ HDF5 ì €ì¥ ì‹¤íŒ¨: {hdf5_error}")
                print(f"         ğŸ’¡ í•´ê²°ë°©ë²•: pip install tables")
                hdf5_success = False

            print(f"   âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
            #print(f"      ğŸ“„ CSV: {processed_csv} ({csv_size_gb:.2f}GB)")

            if hdf5_success:
                print(f"      ğŸ“¦ HDF5: {processed_hdf5} ({hdf5_size_gb:.2f}GB)")
                #print(f"      ğŸš€ í¬ê¸° ì ˆì•½: {((csv_size_gb - hdf5_size_gb) / csv_size_gb * 100):.1f}%")
                print(f"      âš¡ ë¡œë”© ì†ë„ í–¥ìƒ: ì•½ 2-3ë°° ë¹¨ë¼ì§!")

            # ë©”íƒ€ ì •ë³´ ì €ì¥ (â­ Parquet ì •ë³´ ì¶”ê°€)
            meta_info = {
                'total_records': len(self.lp_data),
                'total_customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),
                'date_range': {
                    'start': str(self.lp_data['datetime'].min()),
                    'end': str(self.lp_data['datetime'].max())
                },
                'file_info': {
                    'hdf5_file': 'processed_lp_data.hdf5' if hdf5_success else None,
                    'hdf5_size_gb': hdf5_size_gb if hdf5_success else None,
                    'hdf5_available': hdf5_success,
                    'compression': 'table_format' if hdf5_success else None,
                    'encoding': 'utf-8-sig'
                },
                'processed_timestamp': datetime.now().isoformat()
            }

            # analysis_resultsì— ë©”íƒ€ ì •ë³´ ì¶”ê°€
            self.analysis_results['processed_lp_data'] = meta_info

            if hdf5_success:
                print(f"   ğŸš€ 2-3ë‹¨ê³„ì—ì„œ 30ë¶„ â†’ 3-5ë¶„ìœ¼ë¡œ ì‹œê°„ ë‹¨ì¶• ì˜ˆìƒ!")
            else:
                print(f"   ğŸ“„ CSVë¡œ ì €ì¥ ì™„ë£Œ (30ë¶„ â†’ 8ë¶„ ì‹œê°„ ë‹¨ì¶•)")

        except Exception as save_error:
            print(f"   âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {save_error}")
            print(f"      (ë¶„ì„ì€ ê³„ì† ì§„í–‰ë©ë‹ˆë‹¤)")

        # â­â­â­ í•„ìˆ˜: JSON ê²°ê³¼ ì €ì¥ (2-3ë‹¨ê³„ ì—°ê³„ìš©)
        print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì¤‘...")

        try:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
            self.analysis_results['metadata'] = {
                'timestamp': datetime.now().isoformat(),
                'analysis_stage': 'step1_preprocessing_optimized',
                'version': '2.0',
                'total_customers': len(self.customer_data) if self.customer_data is not None else 0,
                'total_lp_records': len(self.lp_data) if self.lp_data is not None else 0
            }

            # JSON íŒŒì¼ë¡œ ì €ì¥
            output_file = os.path.join('./analysis_results', 'analysis_results.json')

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, 
                         ensure_ascii=False, 
                         indent=2, 
                         default=str)

            print(f"âœ… ë¶„ì„ ê²°ê³¼ JSON ì €ì¥: {output_file}")
            print(f"   ì €ì¥ëœ í•­ëª©: {len(self.analysis_results)}ê°œ")

            # ì €ì¥ëœ êµ¬ì¡° í™•ì¸
            print(f"   ğŸ“ ì €ì¥ëœ êµ¬ì¡°:")
            for key in self.analysis_results.keys():
                if key == 'metadata':
                    print(f"      - metadata: ì‹œê°„ì •ë³´ ë° ë²„ì „")
                elif key == 'customer_summary':
                    print(f"      - customer_summary: ê³ ê° ê¸°ë³¸ ì •ë³´")
                elif key == 'lp_data_summary':
                    print(f"      - lp_data_summary: LP ë°ì´í„° ìš”ì•½")
                elif key == 'processed_lp_data':
                    print(f"      - processed_lp_data: ì „ì²˜ë¦¬ëœ ë°ì´í„° ë©”íƒ€ì •ë³´")
                else:
                    print(f"      - {key}: {type(self.analysis_results[key])}")

        except Exception as json_error:
            print(f"âŒ JSON ì €ì¥ ì‹¤íŒ¨: {json_error}")
            import traceback
            traceback.print_exc()
            return False

        # ê¶Œì¥ì‚¬í•­
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:")
        print("  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì „ì²˜ë¦¬ëœ ë°ì´í„° í™œìš©)")
        print("  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§")
        print("  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ")
        print("  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ")

        print(f"\nğŸ¯ 1ë‹¨ê³„ ìµœì í™” ì™„ë£Œ!")
        print(f"   ğŸ“ ìƒì„± íŒŒì¼:")
        print(f"      - analysis_results.json (2-3ë‹¨ê³„ ì—°ê³„ìš©)")
        print(f"      - processed_lp_data.csv (ì „ì²˜ë¦¬ëœ LP ë°ì´í„°)")
        if 'processed_lp_data' in self.analysis_results and self.analysis_results['processed_lp_data']['file_info']['hdf5_available']:
            print(f"      - processed_lp_data.hdf5 (ê³ ì„±ëŠ¥ ì „ì²˜ë¦¬ëœ ë°ì´í„°)")

        return True

# ì‚¬ìš© ì˜ˆì œ (ì‹¤ì œ ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ì—ì„œ ì‹¤í–‰)
if __name__ == "__main__":
    print("í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ í”„ë¡œì íŠ¸")
    print("ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ ì „ìš© - ì‹¤ì œ ë°ì´í„° ë¶„ì„")
    print("="*60)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = KEPCODataAnalyzer()
    
    # 1ë‹¨ê³„: ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„
    print("\n[1ë‹¨ê³„] ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ë¶„ì„")
    customer_analysis = analyzer.load_customer_data('ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°/ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê°.xlsx')
    
    # 2ë‹¨ê³„: LP ë°ì´í„° ë¶„ì„
    print("\n[2ë‹¨ê³„] LP ë°ì´í„° ë¡œë”© ë° í’ˆì§ˆ ë¶„ì„")
    lp_analysis = analyzer.load_lp_data('./ì œ13íšŒ ì‚°ì—…ë¶€ ê³µëª¨ì „ ëŒ€ìƒê³ ê° LPë°ì´í„°/')  # í˜„ì¬ ë””ë ‰í„°ë¦¬ì—ì„œ LP íŒŒì¼ ì°¾ê¸°
    
    # 3ë‹¨ê³„: ì´ìƒì¹˜ íƒì§€
    print("\n[3ë‹¨ê³„] ì´ìƒì¹˜ íƒì§€ ë° ë°ì´í„° ì •ì œ")
    outliers = analyzer.detect_outliers('iqr')
    
    # 4ë‹¨ê³„: ì¢…í•© ë¦¬í¬íŠ¸
    print("\n[4ë‹¨ê³„] ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€")
    analyzer.generate_quality_report()
    
    print("\nğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!")
    print("ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ")