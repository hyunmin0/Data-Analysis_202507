"""
í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ê°œë°œ (ì²­í¬ ì²˜ë¦¬ ìµœì í™” ë²„ì „)
ì „ì²˜ë¦¬ 2ë‹¨ê³„ ìƒ˜í”Œë§ ë°ì´í„° í™œìš© + ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ì²˜ë¦¬
"""

import pandas as pd
import numpy as np
import json
import os
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_absolute_error, r2_score
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
from math import pi
from sklearn.metrics import mean_squared_error
import matplotlib
import gc
matplotlib.rcParams['font.family'] = 'DejaVu Sans'

class KEPCOChunkVolatilityAnalyzer:
    """KEPCO ë³€ë™ê³„ìˆ˜ ë¶„ì„ê¸° (ì²­í¬ ì²˜ë¦¬ ìµœì í™” ë²„ì „)"""
    
    def __init__(self, results_dir='./analysis_results', chunk_size=5000):
        self.results_dir = results_dir
        self.chunk_size = chunk_size  # ì²­í¬ í¬ê¸° ì„¤ì •
        self.scaler = StandardScaler()
        self.robust_scaler = RobustScaler()
        self.level0_models = {}
        self.meta_model = None
        
        # ê¸°ì¡´ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©
        self.step1_results = self._load_step1_results()
        self.step2_results = self._load_step2_results()
        self.sampled_data_path = None
        
        print("ğŸ”§ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ì²­í¬ ì²˜ë¦¬ ë¶„ì„ê¸° ì´ˆê¸°í™”")
        print(f"   ğŸ“¦ ì²­í¬ í¬ê¸°: {self.chunk_size:,}ê±´")
        
    def _load_step1_results(self):
        """1ë‹¨ê³„ ì „ì²˜ë¦¬ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   âŒ 1ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def _load_step2_results(self):
        """2ë‹¨ê³„ ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼ ë¡œë”©"""
        try:
            file_path = os.path.join(self.results_dir, 'analysis_results2.json')
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                print(f"   âœ… 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”©: {len(results)}ê°œ í•­ëª©")
                return results
            else:
                return {}
        except Exception as e:
            print(f"   âŒ 2ë‹¨ê³„ ê²°ê³¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return {}
    
    def find_sampled_data(self):
        """ì „ì²˜ë¦¬ 2ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ìƒ˜í”Œë§ ë°ì´í„° ì°¾ê¸°"""
        print("\nğŸ“‚ ì „ì²˜ë¦¬ 2ë‹¨ê³„ ìƒ˜í”Œë§ ë°ì´í„° ê²€ìƒ‰ ì¤‘...")
        
        # ê°€ëŠ¥í•œ íŒŒì¼ ê²½ë¡œë“¤
        possible_paths = [
            os.path.join(self.results_dir, 'sampled_lp_data.csv'),
            os.path.join(self.results_dir, 'processed_lp_data.csv'),
            './sampled_lp_data.csv',
            './processed_lp_data.csv'
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = os.path.getsize(path) / (1024 * 1024)  # MB
                print(f"   âœ… ìƒ˜í”Œë§ ë°ì´í„° ë°œê²¬: {path}")
                print(f"   ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.1f} MB")
                
                # ê°„ë‹¨í•œ ë°ì´í„° ê²€ì¦
                try:
                    sample_df = pd.read_csv(path, nrows=1000)  # ë” ë§ì€ í–‰ìœ¼ë¡œ í™•ì¸
                    print(f"   ğŸ“‹ ì»¬ëŸ¼: {list(sample_df.columns)}")
                    
                    # ì „ì²´ íŒŒì¼ì—ì„œ ê³ ê° ìˆ˜ ì¶”ì •
                    total_rows = sum(1 for line in open(path)) - 1  # í—¤ë” ì œì™¸
                    sample_customers = sample_df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()
                    avg_records_per_customer = len(sample_df) / sample_customers if sample_customers > 0 else 1
                    estimated_customers = int(total_rows / avg_records_per_customer)
                    
                    print(f"   ğŸ‘¥ ì²« 1,000í–‰ ê³ ê° ìˆ˜: {sample_customers}ëª…")
                    print(f"   ğŸ“Š ì „ì²´ ì¶”ì • ê³ ê° ìˆ˜: ì•½ {estimated_customers}ëª…")
                    
                    self.sampled_data_path = path
                    return True
                except Exception as e:
                    print(f"   âš ï¸ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
                    continue
        
        print("   âŒ ìƒ˜í”Œë§ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ğŸ’¡ ì „ì²˜ë¦¬ 2ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return False
    
    def load_data_in_chunks(self):
        """ì²­í¬ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬"""
        print(f"\nğŸ“Š ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ë¡œë”© ì¤‘... (ì²­í¬ í¬ê¸°: {self.chunk_size:,})")
        
        if not self.sampled_data_path:
            print("   âŒ ìƒ˜í”Œë§ ë°ì´í„° ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        # ì „ì²´ í–‰ ìˆ˜ í™•ì¸
        total_rows = sum(1 for line in open(self.sampled_data_path)) - 1  # í—¤ë” ì œì™¸
        total_chunks = (total_rows + self.chunk_size - 1) // self.chunk_size
        
        print(f"   ğŸ“Š ì „ì²´ ë°ì´í„°: {total_rows:,}ê±´")
        print(f"   ğŸ“¦ ì˜ˆìƒ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
        
        # ì²­í¬ë³„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³ ê° ì •ë³´ ìˆ˜ì§‘
        self.customer_data_summary = {}
        processed_rows = 0
        
        # ì²« ë²ˆì§¸ íŒ¨ìŠ¤: ê³ ê°ë³„ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        print("   ğŸ” 1ë‹¨ê³„: ê³ ê°ë³„ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        chunk_reader = pd.read_csv(self.sampled_data_path, chunksize=self.chunk_size)
        
        for chunk_idx, chunk in enumerate(chunk_reader):
            try:
                # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ì •ë¦¬
                chunk = self._prepare_chunk_columns(chunk)
                
                if chunk is None or len(chunk) == 0:
                    continue
                
                # ê³ ê°ë³„ ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
                for customer_id in chunk['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():
                    customer_chunk = chunk[chunk['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id]
                    
                    if customer_id not in self.customer_data_summary:
                        self.customer_data_summary[customer_id] = {
                            'total_records': 0,
                            'power_sum': 0.0,
                            'power_sum_sq': 0.0,
                            'min_power': float('inf'),
                            'max_power': float('-inf'),
                            'min_datetime': None,
                            'max_datetime': None,
                            'zero_count': 0
                        }
                    
                    summary = self.customer_data_summary[customer_id]
                    power_values = customer_chunk['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].values
                    
                    # í†µê³„ ì •ë³´ ëˆ„ì 
                    summary['total_records'] += len(customer_chunk)
                    summary['power_sum'] += power_values.sum()
                    summary['power_sum_sq'] += (power_values ** 2).sum()
                    summary['min_power'] = min(summary['min_power'], power_values.min())
                    summary['max_power'] = max(summary['max_power'], power_values.max())
                    summary['zero_count'] += (power_values == 0).sum()
                    
                    # ë‚ ì§œ ë²”ìœ„ ì—…ë°ì´íŠ¸
                    chunk_min_date = customer_chunk['datetime'].min()
                    chunk_max_date = customer_chunk['datetime'].max()
                    
                    if summary['min_datetime'] is None or chunk_min_date < summary['min_datetime']:
                        summary['min_datetime'] = chunk_min_date
                    if summary['max_datetime'] is None or chunk_max_date > summary['max_datetime']:
                        summary['max_datetime'] = chunk_max_date
                
                processed_rows += len(chunk)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunk
                gc.collect()
                
                if (chunk_idx + 1) % 5 == 0:
                    print(f"      ì²­í¬ {chunk_idx + 1}/{total_chunks} ì²˜ë¦¬ ì™„ë£Œ ({processed_rows:,}/{total_rows:,})")
                
            except Exception as e:
                print(f"      âš ï¸ ì²­í¬ {chunk_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        print(f"   âœ… ê³ ê° ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(self.customer_data_summary)}ëª…")
        
        # ìµœì†Œ ë ˆì½”ë“œ ìˆ˜ í•„í„°ë§
        min_records = 50
        valid_customers = [
            cid for cid, summary in self.customer_data_summary.items()
            if summary['total_records'] >= min_records
        ]
        
        print(f"   ğŸ“‹ ìœ íš¨ ê³ ê° (ìµœì†Œ {min_records}ê±´): {len(valid_customers)}ëª…")
        
        if len(valid_customers) == 0:
            print("   âŒ ë¶„ì„ ê°€ëŠ¥í•œ ê³ ê°ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ìœ íš¨ ê³ ê°ìœ¼ë¡œ í•„í„°ë§
        self.customer_data_summary = {
            cid: summary for cid, summary in self.customer_data_summary.items()
            if cid in valid_customers
        }
        
        return True
    
    def _prepare_chunk_columns(self, chunk):
        """ì²­í¬ë³„ ì»¬ëŸ¼ ì¤€ë¹„"""
        try:
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
            datetime_columns = ['datetime', 'LP ìˆ˜ì‹ ì¼ì', 'LPìˆ˜ì‹ ì¼ì', 'timestamp']
            
            # datetime ì»¬ëŸ¼ ì°¾ê¸°
            datetime_col = None
            for col in datetime_columns:
                if col in chunk.columns:
                    datetime_col = col
                    break
            
            if datetime_col is None:
                print(f"      âš ï¸ datetime ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {list(chunk.columns)}")
                return None
            
            # datetime ë³€í™˜
            if datetime_col != 'datetime':
                chunk['datetime'] = pd.to_datetime(chunk[datetime_col], errors='coerce')
            else:
                chunk['datetime'] = pd.to_datetime(chunk['datetime'], errors='coerce')
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬
            for col in required_columns:
                if col not in chunk.columns:
                    print(f"      âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {col}")
                    return None
            
            # ë°ì´í„° ì •ì œ
            chunk = chunk.dropna(subset=['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'datetime', 'ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'])
            chunk = chunk[chunk['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'] >= 0]
            
            # ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
            chunk['hour'] = chunk['datetime'].dt.hour
            chunk['weekday'] = chunk['datetime'].dt.weekday
            chunk['is_weekend'] = chunk['weekday'].isin([5, 6])
            chunk['month'] = chunk['datetime'].dt.month
            chunk['date'] = chunk['datetime'].dt.date
            
            return chunk
            
        except Exception as e:
            print(f"      âš ï¸ ì²­í¬ ì»¬ëŸ¼ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return None
    
    def calculate_volatility_from_chunks(self):
        """ì²­í¬ ê¸°ë°˜ ë³€ë™ê³„ìˆ˜ ê³„ì‚°"""
        print("\nğŸ“ ì²­í¬ ê¸°ë°˜ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì¤‘...")
        
        # 2ë‹¨ê³„ ê²°ê³¼ì—ì„œ ì‹œê°„ íŒ¨í„´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        temporal_patterns = self.step2_results.get('temporal_patterns', {})
        peak_hours = temporal_patterns.get('peak_hours', [9, 10, 11, 14, 15, 18, 19])
        off_peak_hours = temporal_patterns.get('off_peak_hours', [0, 1, 2, 3, 4, 5])
        weekend_ratio = temporal_patterns.get('weekend_ratio', 1.0)
        
        print(f"   ğŸ• í”¼í¬ ì‹œê°„: {peak_hours}")
        print(f"   ğŸŒ™ ë¹„í”¼í¬ ì‹œê°„: {off_peak_hours}")
        
        volatility_results = {}
        volatility_components = []
        
        # ê³ ê°ë³„ ìƒì„¸ ë³€ë™ì„± ë¶„ì„ (ì²­í¬ ë‹¨ìœ„)
        print("   ğŸ” 2ë‹¨ê³„: ê³ ê°ë³„ ìƒì„¸ ë³€ë™ì„± ë¶„ì„ ì¤‘...")
        
        customer_list = list(self.customer_data_summary.keys())
        processed_customers = 0
        
        # ê³ ê°ì„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
        customer_batch_size = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  ê³ ê° ìˆ˜
        
        for batch_start in range(0, len(customer_list), customer_batch_size):
            batch_end = min(batch_start + customer_batch_size, len(customer_list))
            batch_customers = customer_list[batch_start:batch_end]
            
            print(f"      ë°°ì¹˜ {batch_start//customer_batch_size + 1}: ê³ ê° {batch_start+1}-{batch_end} ì²˜ë¦¬ ì¤‘...")
            
            # í•´ë‹¹ ê³ ê°ë“¤ì˜ ë°ì´í„°ë§Œ ì²­í¬ ë‹¨ìœ„ë¡œ ë¡œë”©
            batch_volatility = self._process_customer_batch_chunks(
                batch_customers, peak_hours, off_peak_hours, weekend_ratio
            )
            
            # ê²°ê³¼ ë³‘í•©
            for customer_id, metrics in batch_volatility.items():
                if metrics:
                    volatility_components.append({
                        'customer_id': customer_id,
                        **metrics
                    })
                    processed_customers += 1
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
        
        print(f"   âœ… {processed_customers}ëª… ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        
        if len(volatility_components) < 10:
            raise ValueError(f"ê°€ì¤‘ì¹˜ ìµœì í™”ë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 10ê°œì˜ ê³ ê° ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. (í˜„ì¬: {len(volatility_components)}ê°œ)")
        
        # ê°€ì¤‘ì¹˜ ìµœì í™”
        optimal_weights = self.optimize_volatility_weights(volatility_components)
        print(f"   ğŸ¯ ìµœì¢… ê°€ì¤‘ì¹˜: {[round(w, 3) for w in optimal_weights]}")
        
        # ìµœì¢… ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        for component in volatility_components:
            customer_id = component['customer_id']
            
            enhanced_volatility_coefficient = (
                optimal_weights[0] * component['basic_cv'] +
                optimal_weights[1] * component['hourly_cv'] +
                optimal_weights[2] * component['peak_cv'] +
                optimal_weights[3] * component['weekend_diff'] +
                optimal_weights[4] * component['seasonal_cv']
            )
            
            volatility_results[customer_id] = {
                'enhanced_volatility_coefficient': round(enhanced_volatility_coefficient, 4),
                'basic_cv': round(component['basic_cv'], 4),
                'hourly_cv': round(component['hourly_cv'], 4),
                'peak_cv': round(component['peak_cv'], 4),
                'off_peak_cv': round(component['off_peak_cv'], 4),
                'weekday_cv': round(component['weekday_cv'], 4),
                'weekend_cv': round(component['weekend_cv'], 4),
                'weekend_diff': round(component['weekend_diff'], 4),
                'seasonal_cv': round(component['seasonal_cv'], 4),
                'load_factor': round(component['load_factor'], 4),
                'peak_load_ratio': round(component['peak_load_ratio'], 4),
                'mean_power': round(component['mean_power'], 4),
                'zero_ratio': round(component['zero_ratio'], 4),
                'extreme_changes': int(component['extreme_changes']),
                'data_points': component['data_points'],
                'optimized_weights': [round(w, 3) for w in optimal_weights]
            }
        
        if len(volatility_results) > 0:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   ğŸ“ˆ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
            print(f"   ğŸ“Š ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        return volatility_results
    
    def _process_customer_batch_chunks(self, batch_customers, peak_hours, off_peak_hours, weekend_ratio):
        """ê³ ê° ë°°ì¹˜ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
        batch_results = {}
        
        # ê³ ê°ë³„ ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        customer_detailed_data = {cid: {
            'power_values': [],
            'hourly_data': {},
            'peak_data': [],
            'off_peak_data': [],
            'weekday_data': [],
            'weekend_data': [],
            'daily_averages': {},
            'extreme_changes': 0
        } for cid in batch_customers}
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ íŒŒì¼ ì½ê¸°
        chunk_reader = pd.read_csv(self.sampled_data_path, chunksize=self.chunk_size)
        
        for chunk in chunk_reader:
            try:
                # ì²­í¬ ì „ì²˜ë¦¬
                chunk = self._prepare_chunk_columns(chunk)
                if chunk is None or len(chunk) == 0:
                    continue
                
                # ë°°ì¹˜ ê³ ê°ë§Œ í•„í„°ë§
                batch_chunk = chunk[chunk['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].isin(batch_customers)]
                if len(batch_chunk) == 0:
                    continue
                
                # ê³ ê°ë³„ ë°ì´í„° ìˆ˜ì§‘
                for customer_id in batch_customers:
                    customer_chunk = batch_chunk[batch_chunk['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer_id]
                    if len(customer_chunk) == 0:
                        continue
                    
                    customer_data = customer_detailed_data[customer_id]
                    power_values = customer_chunk['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].values
                    
                    # ì „ë ¥ ë°ì´í„° ìˆ˜ì§‘
                    customer_data['power_values'].extend(power_values)
                    
                    # ì‹œê°„ëŒ€ë³„ ë°ì´í„°
                    for hour in range(24):
                        hour_data = customer_chunk[customer_chunk['hour'] == hour]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                        if len(hour_data) > 0:
                            if hour not in customer_data['hourly_data']:
                                customer_data['hourly_data'][hour] = []
                            customer_data['hourly_data'][hour].extend(hour_data.tolist())
                    
                    # í”¼í¬/ì˜¤í”„í”¼í¬ ë°ì´í„°
                    peak_data = customer_chunk[customer_chunk['hour'].isin(peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    off_peak_data = customer_chunk[customer_chunk['hour'].isin(off_peak_hours)]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    
                    customer_data['peak_data'].extend(peak_data.tolist())
                    customer_data['off_peak_data'].extend(off_peak_data.tolist())
                    
                    # ì£¼ì¤‘/ì£¼ë§ ë°ì´í„°
                    weekday_data = customer_chunk[~customer_chunk['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    weekend_data = customer_chunk[customer_chunk['is_weekend']]['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥']
                    
                    customer_data['weekday_data'].extend(weekday_data.tolist())
                    customer_data['weekend_data'].extend(weekend_data.tolist())
                    
                    # ì¼ë³„ í‰ê·  (ê³„ì ˆë³„ ë³€ë™ì„±ìš©)
                    daily_groups = customer_chunk.groupby('date')['ìˆœë°©í–¥ ìœ íš¨ì „ë ¥'].mean()
                    for date, avg_power in daily_groups.items():
                        customer_data['daily_averages'][date] = avg_power
                    
                    # ê¸‰ê²©í•œ ë³€í™” ê°ì§€
                    if len(power_values) > 1:
                        power_series = pd.Series(power_values)
                        pct_changes = power_series.pct_change().dropna()
                        customer_data['extreme_changes'] += (np.abs(pct_changes) > 1.5).sum()
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del chunk, batch_chunk
                gc.collect()
                
            except Exception as e:
                print(f"         âš ï¸ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        # ê³ ê°ë³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°
        for customer_id in batch_customers:
            try:
                customer_data = customer_detailed_data[customer_id]
                
                if len(customer_data['power_values']) < 10:
                    continue
                
                metrics = self._calculate_customer_volatility_metrics(
                    customer_data, peak_hours, off_peak_hours, weekend_ratio
                )
                
                if metrics:
                    batch_results[customer_id] = metrics
                    
            except Exception as e:
                print(f"         âš ï¸ ê³ ê° {customer_id} ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        return batch_results
    
    def _calculate_customer_volatility_metrics(self, customer_data, peak_hours, off_peak_hours, weekend_ratio):
        """ê°œë³„ ê³ ê°ì˜ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° (ì²­í¬ ê¸°ë°˜)"""
        try:
            power_values = np.array(customer_data['power_values'])
            
            if len(power_values) == 0 or np.mean(power_values) <= 0:
                return None
            
            mean_power = np.mean(power_values)
            
            # 1. ê¸°ë³¸ ë³€ë™ê³„ìˆ˜
            basic_cv = np.std(power_values) / mean_power
            
            # 2. ì‹œê°„ëŒ€ë³„ ë³€ë™ê³„ìˆ˜
            hourly_means = []
            for hour in range(24):
                if hour in customer_data['hourly_data'] and len(customer_data['hourly_data'][hour]) > 0:
                    hourly_means.append(np.mean(customer_data['hourly_data'][hour]))
            
            hourly_cv = (np.std(hourly_means) / np.mean(hourly_means)) if len(hourly_means) > 1 and np.mean(hourly_means) > 0 else basic_cv
            
            # 3. í”¼í¬/ë¹„í”¼í¬ ë³€ë™ì„±
            peak_data = customer_data['peak_data']
            off_peak_data = customer_data['off_peak_data']
            
            peak_cv = (np.std(peak_data) / np.mean(peak_data)) if len(peak_data) > 0 and np.mean(peak_data) > 0 else basic_cv
            off_peak_cv = (np.std(off_peak_data) / np.mean(off_peak_data)) if len(off_peak_data) > 0 and np.mean(off_peak_data) > 0 else basic_cv
            
            # 4. ì£¼ë§/í‰ì¼ ë³€ë™ì„±
            weekday_data = customer_data['weekday_data']
            weekend_data = customer_data['weekend_data']
            
            weekday_cv = (np.std(weekday_data) / np.mean(weekday_data)) if len(weekday_data) > 0 and np.mean(weekday_data) > 0 else basic_cv
            weekend_cv = (np.std(weekend_data) / np.mean(weekend_data)) if len(weekend_data) > 0 and np.mean(weekend_data) > 0 else basic_cv
            weekend_diff = abs(weekday_cv - weekend_cv) * weekend_ratio
            
            # 5. ê³„ì ˆë³„ ë³€ë™ì„± (ì¼ë³„ ì§‘ê³„)
            daily_averages = list(customer_data['daily_averages'].values())
            seasonal_cv = (np.std(daily_averages) / np.mean(daily_averages)) if len(daily_averages) > 3 and np.mean(daily_averages) > 0 else basic_cv
            
            # 6. ì¶”ê°€ ì§€í‘œë“¤
            max_power = np.max(power_values)
            load_factor = mean_power / max_power if max_power > 0 else 0
            zero_ratio = (power_values == 0).sum() / len(power_values)
            
            # í”¼í¬/ë¹„í”¼í¬ ë¶€í•˜ ë¹„ìœ¨
            peak_avg = np.mean(peak_data) if len(peak_data) > 0 else mean_power
            off_peak_avg = np.mean(off_peak_data) if len(off_peak_data) > 0 else mean_power
            peak_load_ratio = peak_avg / off_peak_avg if off_peak_avg > 0 else 1.0
            
            return {
                'basic_cv': basic_cv,
                'hourly_cv': hourly_cv,
                'peak_cv': peak_cv,
                'off_peak_cv': off_peak_cv,
                'weekday_cv': weekday_cv,
                'weekend_cv': weekend_cv,
                'weekend_diff': weekend_diff,
                'seasonal_cv': seasonal_cv,
                'load_factor': load_factor,
                'zero_ratio': zero_ratio,
                'extreme_changes': customer_data['extreme_changes'],
                'peak_load_ratio': peak_load_ratio,
                'mean_power': mean_power,
                'data_points': len(power_values)
            }
            
        except Exception as e:
            return None
    
    def optimize_volatility_weights(self, volatility_components):
        """ê°€ì¤‘ì¹˜ ìµœì í™”"""
        print("\nâš™ï¸ ê°€ì¤‘ì¹˜ ìµœì í™” ì¤‘...")
        
        try:
            from scipy.optimize import minimize
        except ImportError:
            raise ImportError("scipyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install scipy'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        
        # ëª©í‘œ í•¨ìˆ˜ ì •ì˜
        components_df = pd.DataFrame(volatility_components)
        
        # ëª©í‘œ ë³€ìˆ˜: ì˜ì—…í™œë™ ë¶ˆì•ˆì •ì„± ì§€í‘œ
        target_instability = (
            components_df['basic_cv'] * 2.0 +
            components_df['zero_ratio'] * 1.0 +
            (1 - components_df['load_factor']) * 0.5
        ).values
        
        X = components_df[['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']].values
        y = target_instability
        
        # ìµœì í™” ëª©í‘œ í•¨ìˆ˜
        def objective(weights):
            predicted = X @ weights
            return np.mean((predicted - y) ** 2)
        
        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}]
        bounds = [(0, 1) for _ in range(5)]
        initial_weights = [0.35, 0.25, 0.20, 0.10, 0.10]
        
        result = minimize(objective, initial_weights, method='SLSQP', 
                        bounds=bounds, constraints=constraints)
        
        if not result.success:
            raise RuntimeError(f"ê°€ì¤‘ì¹˜ ìµœì í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {result.message}")
        
        print(f"   âœ… ê°€ì¤‘ì¹˜ ìµœì í™” ì™„ë£Œ")
        return result.x.tolist()
    
    def train_stacking_ensemble_model(self, volatility_results):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸ¯ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        if len(volatility_results) < 5:
            print("   âŒ í›ˆë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 5ê°œ í•„ìš”)")
            return None
        
        # íŠ¹ì„± ì¶”ì¶œ
        features = []
        targets = []
        
        for customer_id, data in volatility_results.items():
            try:
                feature_vector = [
                    data['basic_cv'], data['hourly_cv'], data['peak_cv'],
                    data['off_peak_cv'], data['weekday_cv'], data['weekend_cv'],
                    data['seasonal_cv'], data['load_factor'], data['peak_load_ratio'],
                    data['mean_power'], data['zero_ratio'],
                    data['extreme_changes'] / data['data_points']
                ]
                
                if any(np.isnan(x) or np.isinf(x) for x in feature_vector):
                    continue
                    
                features.append(feature_vector)
                targets.append(data['enhanced_volatility_coefficient'])
                
            except KeyError:
                continue
        
        X = np.array(features)
        y = np.array(targets)
        
        print(f"   ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(X)}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # ì •ê·œí™”
        X_train_scaled = self.robust_scaler.fit_transform(X_train)
        X_test_scaled = self.robust_scaler.transform(X_test)
        
        # Level-0 ëª¨ë¸ë“¤
        self.level0_models = {
            'rf': RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42),
            'gbm': GradientBoostingRegressor(n_estimators=50, max_depth=6, random_state=42),
            'ridge': Ridge(alpha=1.0),
            'linear': LinearRegression()
        }
        
        # êµì°¨ê²€ì¦ìœ¼ë¡œ ë©”íƒ€ íŠ¹ì„± ìƒì„±
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        
        meta_features_train = np.zeros((len(X_train_scaled), len(self.level0_models)))
        meta_features_test = np.zeros((len(X_test_scaled), len(self.level0_models)))
        
        print(f"   ğŸ”„ Level-0 ëª¨ë¸ í›ˆë ¨ (5-Fold CV):")
        for i, (name, model) in enumerate(self.level0_models.items()):
            fold_predictions = np.zeros(len(X_train_scaled))
            
            for train_idx, val_idx in kf.split(X_train_scaled):
                try:
                    fold_model = type(model)(**model.get_params())
                    fold_model.fit(X_train_scaled[train_idx], y_train[train_idx])
                    fold_predictions[val_idx] = fold_model.predict(X_train_scaled[val_idx])
                except Exception as e:
                    fold_predictions[val_idx] = np.mean(y_train[train_idx])
            
            meta_features_train[:, i] = fold_predictions
            
            # ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ë¡œ ì¬í›ˆë ¨
            try:
                model.fit(X_train_scaled, y_train)
                meta_features_test[:, i] = model.predict(X_test_scaled)
                
                test_pred = model.predict(X_test_scaled)
                test_mae = mean_absolute_error(y_test, test_pred)
                test_r2 = r2_score(y_test, test_pred) if len(set(y_test)) > 1 else 0.0
                print(f"      {name}: MAE={test_mae:.4f}, RÂ²={test_r2:.4f}")
                
            except Exception as e:
                meta_features_test[:, i] = np.mean(y_train)
        
        # Level-1 ë©”íƒ€ ëª¨ë¸ (ì„ í˜• íšŒê·€)
        self.meta_model = LinearRegression()
        try:
            self.meta_model.fit(meta_features_train, y_train)
            final_pred = self.meta_model.predict(meta_features_test)
            final_mae = mean_absolute_error(y_test, final_pred)
            final_r2 = r2_score(y_test, final_pred) if len(set(y_test)) > 1 else 0.0
        except:
            final_pred = np.mean(meta_features_test, axis=1)
            final_mae = mean_absolute_error(y_test, final_pred)
            final_r2 = r2_score(y_test, final_pred) if len(set(y_test)) > 1 else 0.0
        
        print(f"   âœ… ìŠ¤íƒœí‚¹ ì•™ìƒë¸” í›ˆë ¨ ì™„ë£Œ")
        print(f"      ìµœì¢… MAE: {final_mae:.4f}")
        print(f"      ìµœì¢… RÂ²: {final_r2:.4f}")
        
        return {
            'final_mae': final_mae,
            'final_r2': final_r2,
            'level0_models': list(self.level0_models.keys()),
            'meta_model': 'LinearRegression',
            'n_samples': len(X),
            'n_features': X.shape[1],
            'chunk_optimized': True
        }

    def analyze_business_stability(self, volatility_results):
        """ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„"""
        print("\nğŸ” ì˜ì—…í™œë™ ì•ˆì •ì„± ë¶„ì„ ì¤‘...")
        
        if not volatility_results:
            return {}
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
        
        # ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë“±ê¸‰ ë¶„ë¥˜
        p25, p75 = np.percentile(coefficients, [25, 75])
        
        stability_analysis = {}
        grade_counts = {'ì•ˆì •': 0, 'ë³´í†µ': 0, 'ì£¼ì˜': 0}
        
        for customer_id, data in volatility_results.items():
            coeff = data['enhanced_volatility_coefficient']
            
            # 3ë‹¨ê³„ ë“±ê¸‰ ë¶„ë¥˜
            if coeff <= p25:
                grade = 'ì•ˆì •'
                risk_level = 'low'
            elif coeff <= p75:
                grade = 'ë³´í†µ'
                risk_level = 'medium'
            else:
                grade = 'ì£¼ì˜'
                risk_level = 'high'
            
            grade_counts[grade] += 1
            
            # ìœ„í—˜ ìš”ì¸ ë¶„ì„
            risk_factors = []
            if data.get('zero_ratio', 0) > 0.1:
                risk_factors.append('ë¹ˆë²ˆí•œ_ì‚¬ìš©ì¤‘ë‹¨')
            if data.get('load_factor', 1) < 0.3:
                risk_factors.append('ë‚®ì€_ë¶€í•˜ìœ¨')
            if data.get('peak_cv', 0) > data.get('basic_cv', 0) * 2:
                risk_factors.append('í”¼í¬ì‹œê°„_ë¶ˆì•ˆì •')
            if data.get('weekend_diff', 0) > 0.3:
                risk_factors.append('ì£¼ë§_íŒ¨í„´_ê¸‰ë³€')
            if data.get('extreme_changes', 0) > data.get('data_points', 1) * 0.05:
                risk_factors.append('ê¸‰ê²©í•œ_ë³€í™”_ë¹ˆë°œ')
            
            # ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚° (0-100ì )
            stability_score = max(0, 100 - (coeff * 400))
            
            stability_analysis[customer_id] = {
                'enhanced_volatility_coefficient': round(coeff, 4),
                'stability_grade': grade,
                'risk_level': risk_level,
                'stability_score': round(stability_score, 1),
                'risk_factors': risk_factors,
                'load_factor': data.get('load_factor', 0.0),
                'peak_load_ratio': data.get('peak_load_ratio', 1.0),
                'zero_ratio': data.get('zero_ratio', 0.0),
                'extreme_changes': data.get('extreme_changes', 0)
            }
        
        print(f"   ğŸ“‹ ì•ˆì •ì„± ë“±ê¸‰ ë¶„í¬:")
        total = len(stability_analysis)
        for grade, count in grade_counts.items():
            percentage = count / total * 100 if total > 0 else 0
            print(f"      {grade}: {count}ëª… ({percentage:.1f}%)")
        
        return stability_analysis

    def generate_report(self, volatility_results, model_performance, stability_analysis):
        """ì²­í¬ ì²˜ë¦¬ ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“‹ ì²­í¬ ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        coefficients = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()] if volatility_results else []
        
        # ìœ„í—˜ ê³ ê° ì‹ë³„
        high_risk_customers = [
            customer_id for customer_id, analysis in stability_analysis.items()
            if analysis['risk_level'] == 'high'
        ] if stability_analysis else []
        
        # ì£¼ìš” ìœ„í—˜ ìš”ì¸ ì§‘ê³„
        all_risk_factors = []
        for analysis in stability_analysis.values():
            all_risk_factors.extend(analysis['risk_factors'])
        
        from collections import Counter
        risk_factor_counts = Counter(all_risk_factors)
        
        report = {
            'analysis_metadata': {
                'timestamp': pd.Timestamp.now().isoformat(),
                'algorithm_version': 'chunk_optimized_v1',
                'chunk_size': self.chunk_size,
                'total_customers_analyzed': len(volatility_results),
                'execution_mode': 'chunk_processing',
                'data_source': 'preprocessed_sampled_data'
            },
            
            'chunk_processing_summary': {
                'chunk_size_used': self.chunk_size,
                'memory_efficient': True,
                'batch_processing': True,
                'sampled_data_path': self.sampled_data_path
            },
            
            'volatility_coefficient_summary': {
                'total_customers': len(volatility_results),
                'mean_coefficient': round(np.mean(coefficients), 4) if coefficients else 0,
                'std_coefficient': round(np.std(coefficients), 4) if coefficients else 0,
                'percentiles': {
                    '25%': round(np.percentile(coefficients, 25), 4) if coefficients else 0,
                    '50%': round(np.percentile(coefficients, 50), 4) if coefficients else 0,
                    '75%': round(np.percentile(coefficients, 75), 4) if coefficients else 0
                }
            },
            
            'model_performance': model_performance or {},
            
            'business_stability_distribution': {
                grade: sum(1 for a in stability_analysis.values() if a['stability_grade'] == grade)
                for grade in ['ì•ˆì •', 'ë³´í†µ', 'ì£¼ì˜']
            } if stability_analysis else {},
            
            'risk_analysis': {
                'high_risk_customers': len(high_risk_customers),
                'high_risk_percentage': round(len(high_risk_customers) / len(stability_analysis) * 100, 1) if stability_analysis else 0,
                'top_risk_factors': dict(risk_factor_counts.most_common(5))
            },
            
            'performance_optimization': {
                'chunk_processing_achieved': True,
                'memory_efficient': True,
                'accuracy_maintained': model_performance['final_r2'] >= 0.3 if model_performance else False
            },
            
            'business_insights': [
                f"ì²­í¬ ì²˜ë¦¬ë¥¼ í†µí•´ {len(volatility_results)}ëª… ê³ ê° ë¶„ì„ ì™„ë£Œ",
                f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì•ˆì •ì  ë¶„ì„",
                f"ëª¨ë¸ ì˜ˆì¸¡ ì •í™•ë„(RÂ²): {model_performance['final_r2']:.3f}" if model_performance else "ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ë¶ˆê°€",
                f"ê³ ìœ„í—˜ ê³ ê° {len(high_risk_customers)}ëª… ì‹ë³„",
                "ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ í™˜ê²½ì— ìµœì í™”ëœ ì•ˆì •ì  ë¶„ì„ ì‹œìŠ¤í…œ"
            ],
            
            'recommendations': [
                "ì²­í¬ í¬ê¸° ì¡°ì •ì„ í†µí•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”",
                "ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì•ˆì •ì  ì²˜ë¦¬",
                "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ íš¨ìœ¨ì  ë¶„ì„ ì²´ê³„",
                "ì£¼ê¸°ì  ì „ì²´ ë°ì´í„° ê²€ì¦ìœ¼ë¡œ í’ˆì§ˆ í™•ë³´"
            ]
        }
        
        return report

    def create_volatility_components_radar_chart(self, volatility_results, save_path='./analysis_results'):
        """ë ˆì´ë” ì°¨íŠ¸ ìƒì„± (ì˜ë¬¸ ë²„ì „)"""
        import matplotlib.pyplot as plt
        import numpy as np
        from math import pi
        import os
        
        if not volatility_results:
            print("   âŒ ë³€ë™ê³„ìˆ˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # êµ¬ì„±ìš”ì†Œ ì´ë¦„ (ì˜ë¬¸)
        components = ['Basic CV', 'Hourly CV', 'Peak CV', 'Weekend Diff', 'Seasonal CV']
        component_keys = ['basic_cv', 'hourly_cv', 'peak_cv', 'weekend_diff', 'seasonal_cv']
        
        # ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”
        customers_data = {}
        all_values = {key: [] for key in component_keys}
        
        # ëª¨ë“  ê³ ê°ì˜ ë°ì´í„° ìˆ˜ì§‘
        for customer_id, data in volatility_results.items():
            customer_values = []
            for key in component_keys:
                value = data.get(key, 0)
                # ì´ìƒê°’ ì²˜ë¦¬
                if np.isnan(value) or np.isinf(value):
                    value = 0
                customer_values.append(value)
                all_values[key].append(value)
            customers_data[customer_id] = customer_values
        
        # ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ê°’ ê³„ì‚° (ê° êµ¬ì„±ìš”ì†Œë³„)
        max_values = []
        for key in component_keys:
            values = all_values[key]
            if values:
                max_val = max(values) if max(values) > 0 else 1
                max_values.append(max_val)
            else:
                max_values.append(1)
        
        # ìƒìœ„ 5ëª…ì˜ ê³ ê° ì„ íƒ (ë³€ë™ê³„ìˆ˜ê°€ ë†’ì€ ìˆœ)
        top_customers = sorted(
            volatility_results.items(),
            key=lambda x: x[1].get('enhanced_volatility_coefficient', 0),
            reverse=True
        )[:5]
        
        # ë ˆì´ë” ì°¨íŠ¸ ì„¤ì •
        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))
        
        # ê°ë„ ê³„ì‚° (5ê°œ í•­ëª©)
        angles = [n / float(len(components)) * 2 * pi for n in range(len(components))]
        angles += angles[:1] 
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57']
        
        # ê° ê³ ê°ë³„ ë ˆì´ë” ì°¨íŠ¸ ê·¸ë¦¬ê¸°
        for i, (customer_id, data) in enumerate(top_customers):
            if i >= 5:  # ìµœëŒ€ 5ëª…ë§Œ
                break
                
            # ë°ì´í„° ì •ê·œí™” (0-1 ë²”ìœ„)
            values = []
            for j, key in enumerate(component_keys):
                raw_value = data.get(key, 0)
                if np.isnan(raw_value) or np.isinf(raw_value):
                    raw_value = 0
                normalized_value = raw_value / max_values[j] if max_values[j] > 0 else 0
                values.append(min(normalized_value, 1.0))  # 1.0ìœ¼ë¡œ í´ë¦¬í•‘
            
            values += values[:1] 
            
            # ì„  ê·¸ë¦¬ê¸°
            ax.plot(angles, values, 'o-', linewidth=2, label=f'{customer_id}', color=colors[i], markersize=6)
            # ì˜ì—­ ì±„ìš°ê¸° (íˆ¬ëª…ë„ ì ìš©)
            ax.fill(angles, values, alpha=0.15, color=colors[i])
        
        # ë¼ë²¨ ì„¤ì •
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(components, fontsize=11, fontweight='bold')
        
        # Yì¶• ì„¤ì • (0-1 ë²”ìœ„)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)
        ax.grid(True, alpha=0.3)
        
        # ì œëª© ë° ë²”ë¡€ (ì˜ë¬¸)
        plt.title('Volatility Coefficient Components Analysis (Top 5 Customers)', 
                  fontsize=16, fontweight='bold', pad=30)
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=10)
        
        # ì„œë¸Œ ì œëª© (ì˜ë¬¸)
        fig.text(0.5, 0.02, 'Each component is normalized by maximum value (0-1 range)', 
                 ha='center', fontsize=9, style='italic')
        
        # í†µê³„ ì •ë³´ ì¶”ê°€ (ì˜ë¬¸)
        stats_text = f"Analyzed Customers: {len(volatility_results)}\n"
        stats_text += f"Average Volatility Coeff: {np.mean([v.get('enhanced_volatility_coefficient', 0) for v in volatility_results.values()]):.4f}"
        fig.text(0.02, 0.95, stats_text, fontsize=9, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        
        # ì €ì¥
        os.makedirs(save_path, exist_ok=True)
        chart_path = os.path.join(save_path, 'volatility_components_radar_chunk.png')
        plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        print(f"   âœ… ë ˆì´ë” ì°¨íŠ¸ ì €ì¥: {chart_path}")
        
        return {
            'chart_path': chart_path,
            'top_customers': [customer_id for customer_id, _ in top_customers]
        }

def save_chunk_results(volatility_results, stability_analysis, report):
    """ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥"""
    try:
        os.makedirs('./analysis_results', exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ë³€ë™ê³„ìˆ˜ ê²°ê³¼
        if volatility_results:
            df = pd.DataFrame.from_dict(volatility_results, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
            csv_path = f'./analysis_results/volatility_chunk_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ ë³€ë™ê³„ìˆ˜ (ì²­í¬): {csv_path}")
        
        # ì•ˆì •ì„± ë¶„ì„
        if stability_analysis:
            df = pd.DataFrame.from_dict(stability_analysis, orient='index')
            df.reset_index(inplace=True)
            df.rename(columns={'index': 'ëŒ€ì²´ê³ ê°ë²ˆí˜¸'}, inplace=True)
            if 'risk_factors' in df.columns:
                df['risk_factors_str'] = df['risk_factors'].apply(
                    lambda x: ', '.join(x) if isinstance(x, list) else ''
                )
            csv_path = f'./analysis_results/stability_chunk_{timestamp}.csv'
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"   ğŸ’¾ ì•ˆì •ì„± (ì²­í¬): {csv_path}")
        
        # ì²­í¬ ë¦¬í¬íŠ¸
        if report:
            json_path = f'./analysis_results/chunk_report_{timestamp}.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            print(f"   ğŸ’¾ ì²­í¬ ë¦¬í¬íŠ¸: {json_path}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def main_chunk():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì²­í¬ ì²˜ë¦¬ ë²„ì „)"""
    print("ğŸ† í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ë¶„ì„ (ì²­í¬ ì²˜ë¦¬ ìµœì í™”)")
    print("=" * 80)
    print("ğŸ“¦ ì£¼ìš” íŠ¹ì§•:")
    print("   âœ… ì „ì²˜ë¦¬ 2ë‹¨ê³„ ìƒ˜í”Œë§ ë°ì´í„° í™œìš©")
    print("   âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²­í¬ ì²˜ë¦¬")
    print("   âœ… ë°°ì¹˜ ë‹¨ìœ„ ê³ ê° ë¶„ì„")
    print("   âœ… ë°ì´í„°ì•ˆì‹¬êµ¬ì—­ í™˜ê²½ ìµœì í™”")
    print("   âœ… ê¸°ì¡´ ì¶œë ¥ í˜•ì‹ ì™„ì „ í˜¸í™˜")
    print()
    
    start_time = datetime.now()
    
    try:
        # 1. ë¶„ì„ê¸° ì´ˆê¸°í™” (ì²­í¬ í¬ê¸° ì¡°ì • ê°€ëŠ¥)
        chunk_size = 5000  # ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥
        analyzer = KEPCOChunkVolatilityAnalyzer('./analysis_results', chunk_size)
        
        # 2. ìƒ˜í”Œë§ ë°ì´í„° ì°¾ê¸°
        if not analyzer.find_sampled_data():
            print("âŒ ìƒ˜í”Œë§ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("\nğŸ”§ í•´ê²° ë°©ë²•:")
            print("   1. ì „ì²˜ë¦¬ 2ë‹¨ê³„ (ì „ì²˜ë¦¬2ë‹¨ê³„ ìˆ˜ì •.py)ë¥¼ ë¨¼ì € ì‹¤í–‰")
            print("   2. sampled_lp_data.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸")
            return None
        
        # 3. ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ë¡œë”©
        if not analyzer.load_data_in_chunks():
            print("âŒ ì²­í¬ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            return None
        
        # 4. ì²­í¬ ê¸°ë°˜ ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        volatility_results = analyzer.calculate_volatility_from_chunks()
        if not volatility_results:
            print("âŒ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
            return None
        
        # 5. ëª¨ë¸ í›ˆë ¨
        model_performance = analyzer.train_stacking_ensemble_model(volatility_results)
        
        # 6. ì•ˆì •ì„± ë¶„ì„
        stability_analysis = analyzer.analyze_business_stability(volatility_results)
        
        # 7. ë¦¬í¬íŠ¸ ìƒì„±
        report = analyzer.generate_report(volatility_results, model_performance, stability_analysis)
        
        # 8. ì‹œê°í™” ìƒì„±
        try:
            radar_result = analyzer.create_volatility_components_radar_chart(volatility_results)
            if radar_result:
                print(f"   ğŸ“Š ë ˆì´ë” ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {radar_result['chart_path']}")
        except Exception as e:
            print(f"   âš ï¸ ë ˆì´ë” ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
        
        # 9. ê²°ê³¼ ì €ì¥
        save_chunk_results(volatility_results, stability_analysis, report)
        
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"\nğŸ‰ ì²­í¬ ì²˜ë¦¬ ë¶„ì„ ì™„ë£Œ!")
        print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ")
        print(f"   ğŸ‘¥ ë¶„ì„ ê³ ê°: {len(volatility_results)}ëª…")
        print(f"   ğŸ“¦ ì²­í¬ í¬ê¸°: {chunk_size:,}ê±´")
        
        if volatility_results:
            cv_values = [v['enhanced_volatility_coefficient'] for v in volatility_results.values()]
            print(f"   ğŸ“ˆ í‰ê·  ë³€ë™ê³„ìˆ˜: {np.mean(cv_values):.4f}")
            print(f"   ğŸ“Š ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {np.min(cv_values):.4f} ~ {np.max(cv_values):.4f}")
        
        if model_performance:
            print(f"   ğŸ¯ ëª¨ë¸ ì„±ëŠ¥: RÂ²={model_performance['final_r2']:.3f}, MAE={model_performance['final_mae']:.4f}")
        
        print(f"   ğŸ’¾ ê²°ê³¼ íŒŒì¼: ./analysis_results/ ë””ë ‰í† ë¦¬")
        
        return {
            'volatility_results': volatility_results,
            'model_performance': model_performance,
            'stability_analysis': stability_analysis,
            'report': report,
            'execution_time': execution_time,
            'chunk_size': chunk_size
        }
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    print("ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ë¶„ì„ ì‹œì‘ (ì²­í¬ ì²˜ë¦¬ ë²„ì „)!")
    print("=" * 80)
    print("ğŸ“¦ ì²­í¬ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”")
    print("ğŸ¯ ì „ì²˜ë¦¬ 2ë‹¨ê³„ ìƒ˜í”Œë§ ë°ì´í„° í™œìš©")
    print("ğŸ“Š ê¸°ì¡´ ì¶œë ¥ í˜•ì‹ ì™„ì „ ìœ ì§€")
    print()
    
    # ë©”ì¸ ì‹¤í–‰
    results = main_chunk()
    
    if results:
        print(f"\nğŸŠ ì²­í¬ ì²˜ë¦¬ ë¶„ì„ ì„±ê³µ!")
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì´ ./analysis_results/ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        print(f"   âš¡ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ë¡œ ì•ˆì •ì  ì‹¤í–‰ ì™„ë£Œ")
        print(f"   ğŸ¯ ë™ì¼í•œ ì •í™•ë„, í–¥ìƒëœ ì•ˆì •ì„±")
        
        print(f"\nğŸ’¡ ì²­í¬ í¬ê¸° ì¡°ì •:")
        print(f"   â€¢ ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ: chunk_sizeë¥¼ 2000~3000ìœ¼ë¡œ ê°ì†Œ")
        print(f"   â€¢ ë©”ëª¨ë¦¬ ì—¬ìœ ì‹œ: chunk_sizeë¥¼ 10000~20000ìœ¼ë¡œ ì¦ê°€")
        print(f"   â€¢ í˜„ì¬ ì„¤ì •: {results['chunk_size']:,}ê±´")
        
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨")
        print(f"   ğŸ”§ í™•ì¸ ì‚¬í•­:")
        print(f"   1. ì „ì²˜ë¦¬ 2ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print(f"   2. sampled_lp_data.csv íŒŒì¼ ì¡´ì¬ ì—¬ë¶€")
        print(f"   3. ë©”ëª¨ë¦¬ ìš©ëŸ‰ ë° ì²­í¬ í¬ê¸° ì„¤ì •")

print("\n" + "=" * 80)
print("ğŸ† í•œêµ­ì „ë ¥ê³µì‚¬ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ (ì²­í¬ ì²˜ë¦¬ ìµœì í™”)")
print("ğŸ“¦ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± | ğŸ¯ ì•ˆì •ì  ì²˜ë¦¬ | ğŸ“Š ë™ì¼í•œ ì¶œë ¥ í˜•ì‹")
print("=" * 80)