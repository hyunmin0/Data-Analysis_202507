{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7203f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\n",
      "ì´ ê³ ê° ìˆ˜: 200ëª…\n",
      "ì»¬ëŸ¼: ['ìˆœë²ˆ', 'ê³ ê°ë²ˆí˜¸', 'ê³„ì•½ì „ë ¥', 'ê³„ì•½ì¢…ë³„', 'ì‚¬ìš©ìš©ë„', 'ì£¼ìƒì‚°í’ˆ', 'ì‚°ì—…ë¶„ë¥˜']\n",
      "\n",
      "ê¸°ë³¸ ì •ë³´:\n",
      "   ìˆœë²ˆ   ê³ ê°ë²ˆí˜¸     ê³„ì•½ì „ë ¥            ê³„ì•½ì¢…ë³„     ì‚¬ìš©ìš©ë„ ì£¼ìƒì‚°í’ˆ     ì‚°ì—…ë¶„ë¥˜\n",
      "0   1  A1001  500~599  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A  09 ê´‘ê³µì—…ìš©   ë§ˆíŠ¸  996ê¸°íƒ€ì—…ì¢…\n",
      "1   2  A1002  400~499  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A   02 ìƒì—…ìš©   ë³‘ì›  391ê¸°íƒ€ì—…ì¢…\n",
      "2   3  A1003  200~299  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A  09 ê´‘ê³µì—…ìš©   êµíšŒ  304ê¸°íƒ€ì—…ì¢…\n",
      "3   4  A1004  200~299  726 ì‚°ì—…ìš©(ì„) ê³ ì••A  09 ê´‘ê³µì—…ìš©  ì œì¡°ì—…  457ê¸°íƒ€ì—…ì¢…\n",
      "4   5  A1005  200~299  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A  09 ê´‘ê³µì—…ìš©  ì˜¨ì²œíƒ•  825ê¸°íƒ€ì—…ì¢…\n",
      "\n",
      "=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\n",
      "\n",
      "ğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\n",
      "  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A: 60ëª… (30.0%)\n",
      "  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A: 55ëª… (27.5%)\n",
      "  226 ì¼ë°˜ìš©(ì„) ê³ ì••A: 45ëª… (22.5%)\n",
      "  726 ì‚°ì—…ìš©(ì„) ê³ ì••A: 40ëª… (20.0%)\n",
      "\n",
      "ğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\n",
      "  09 ê´‘ê³µì—…ìš©: 104ëª… (52.0%)\n",
      "  02 ìƒì—…ìš©: 96ëª… (48.0%)\n",
      "\n",
      "âš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\n",
      "  100~199kW: 39ëª… (19.5%)\n",
      "  200~299kW: 35ëª… (17.5%)\n",
      "  400~499kW: 30ëª… (15.0%)\n",
      "  500~599kW: 25ëª… (12.5%)\n",
      "  700~799kW: 34ëª… (17.0%)\n",
      "  800~899kW: 37ëª… (18.5%)\n",
      "\n",
      "=== LP ë°ì´í„° ë¡œë”© ===\n",
      "íŒŒì¼ 1 ë¡œë”©: LPë°ì´í„°1.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 14,400\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "íŒŒì¼ 2 ë¡œë”©: LPë°ì´í„°2.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 15,360\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-16-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "âœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\n",
      "  ì´ ë ˆì½”ë“œ: 29,760\n",
      "  ì´ ê³ ê°: 10\n",
      "\n",
      "=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\n",
      "ğŸ“ˆ ê¸°ë³¸ í†µê³„:\n",
      "            ìˆœë°©í–¥ìœ íš¨ì „ë ¥          ì§€ìƒë¬´íš¨          ì§„ìƒë¬´íš¨          í”¼ìƒì „ë ¥\n",
      "count  29760.000000  29760.000000  29760.000000  29760.000000\n",
      "mean      79.940981     10.069745      5.137708     79.981331\n",
      "std       24.013893      4.828842      2.803947     23.695466\n",
      "min        4.600000      0.000000      0.000000      3.900000\n",
      "25%       60.200000      6.600000      3.000000     60.200000\n",
      "50%       79.900000     10.000000      5.000000     80.200000\n",
      "75%       99.600000     13.300000      7.000000     99.700000\n",
      "max      151.200000     29.900000     16.200000    147.600000\n",
      "\n",
      "â° ì‹œê°„ ê°„ê²© ì²´í¬:\n",
      "  A1001: í‰ê·  ê°„ê²© 15.0ë¶„, í‘œì¤€í¸ì°¨ 0.0ë¶„\n",
      "  A1002: í‰ê·  ê°„ê²© 15.0ë¶„, í‘œì¤€í¸ì°¨ 0.0ë¶„\n",
      "  A1003: í‰ê·  ê°„ê²© 15.0ë¶„, í‘œì¤€í¸ì°¨ 0.0ë¶„\n",
      "\n",
      "ğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\n",
      "  ìˆœë°©í–¥ìœ íš¨ì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "  ì§€ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 34ê±´ (0.11%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "  ì§„ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 105ê±´ (0.35%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "  í”¼ìƒì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "\n",
      "ğŸ‘¥ ê³ ê°ë³„ ë°ì´í„° ì™„ì •ì„±:\n",
      "  ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜: 2,976ê°œ/ê³ ê°\n",
      "  ì‹¤ì œ ë ˆì½”ë“œ ìˆ˜: 2,976~2,976ê°œ/ê³ ê°\n",
      "  âœ… ëª¨ë“  ê³ ê° ë°ì´í„° ì™„ì •ì„± ì–‘í˜¸\n",
      "\n",
      "=== ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•) ===\n",
      "\n",
      "ğŸ“Š ìˆœë°©í–¥ìœ íš¨ì „ë ¥:\n",
      "  ì´ìƒì¹˜: 0ê±´ (0.00%)\n",
      "\n",
      "ğŸ“Š ì§€ìƒë¬´íš¨:\n",
      "  ì´ìƒì¹˜: 114ê±´ (0.38%)\n",
      "  ë²”ìœ„: 23.4 ~ 29.9\n",
      "\n",
      "ğŸ“Š ì§„ìƒë¬´íš¨:\n",
      "  ì´ìƒì¹˜: 105ê±´ (0.35%)\n",
      "  ë²”ìœ„: 13.1 ~ 16.2\n",
      "\n",
      "ğŸ“Š í”¼ìƒì „ë ¥:\n",
      "  ì´ìƒì¹˜: 0ê±´ (0.00%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ”¢ ë°ì´í„° ê·œëª¨:\n",
      "  â€¢ ê³ ê° ê¸°ë³¸ì •ë³´: 200ëª…\n",
      "  â€¢ LP ë°ì´í„°: 29,760ë ˆì½”ë“œ\n",
      "  â€¢ ë¶„ì„ ëŒ€ìƒ ê³ ê°: 10ëª…\n",
      "  â€¢ ë¶„ì„ ê¸°ê°„: 2024-03-01-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "âœ… ë°ì´í„° í’ˆì§ˆ ìƒíƒœ:\n",
      "  â€¢ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: 0.00%\n",
      "  â€¢ 0ê°’ ë¹„ìœ¨: 0.12%\n",
      "  â€¢ ìŒìˆ˜ê°’ ë¹„ìœ¨: 0.00%\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\n",
      "  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì¼/ì£¼/ì›”ë³„ ì‚¬ìš© íŒ¨í„´)\n",
      "  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\n",
      "  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\n",
      "  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n",
      "\n",
      "ğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "    def load_customer_data(self, file_path):\n",
    "        \"\"\"ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ê¸°ë³¸ ë¶„ì„\"\"\"\n",
    "        print(\"=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\")\n",
    "        \n",
    "        # Excel íŒŒì¼ ì½ê¸° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ ë¶€ë¶„ ì‚¬ìš©)\n",
    "        # self.customer_data = pd.read_excel(file_path)\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
    "        self.customer_data = self._create_sample_customer_data()\n",
    "        \n",
    "        print(f\"ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…\")\n",
    "        print(f\"ì»¬ëŸ¼: {list(self.customer_data.columns)}\")\n",
    "        print(\"\\nê¸°ë³¸ ì •ë³´:\")\n",
    "        print(self.customer_data.head())\n",
    "        \n",
    "        return self._analyze_customer_distribution()\n",
    "    \n",
    "    def _create_sample_customer_data(self):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê³ ê° ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        contract_types = ['222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A', '226 ì¼ë°˜ìš©(ì„) ê³ ì••A', '322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A', '726 ì‚°ì—…ìš©(ì„) ê³ ì••A']\n",
    "        usage_types = ['02 ìƒì—…ìš©', '09 ê´‘ê³µì—…ìš©']\n",
    "        power_ranges = ['100~199', '200~299', '400~499', '500~599', '700~799', '800~899']\n",
    "        industries = ['ë³‘ì›', 'êµíšŒ', 'ìƒê°€', 'CNGì¶©ì „', 'ê¸ˆì†ê°€ê³µ', 'ì í¬', 'ì˜¨ì²œíƒ•', 'ì œì¡°ì—…', 'ë§ˆíŠ¸']\n",
    "        \n",
    "        for i in range(1, 201):  # 200ëª…\n",
    "            data.append({\n",
    "                'ìˆœë²ˆ': i,\n",
    "                'ê³ ê°ë²ˆí˜¸': f'A{1000+i}',\n",
    "                'ê³„ì•½ì „ë ¥': np.random.choice(power_ranges),\n",
    "                'ê³„ì•½ì¢…ë³„': np.random.choice(contract_types),\n",
    "                'ì‚¬ìš©ìš©ë„': np.random.choice(usage_types),\n",
    "                'ì£¼ìƒì‚°í’ˆ': np.random.choice(industries),\n",
    "                'ì‚°ì—…ë¶„ë¥˜': f'{np.random.randint(100,999)}ê¸°íƒ€ì—…ì¢…'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"ê³ ê° ë¶„í¬ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê³„ì•½ì¢…ë³„ ë¶„í¬\n",
    "        contract_dist = self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts()\n",
    "        print(\"\\nğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\")\n",
    "        for contract, count in contract_dist.items():\n",
    "            print(f\"  {contract}: {count}ëª… ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬\n",
    "        usage_dist = self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts()\n",
    "        print(\"\\nğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\")\n",
    "        for usage, count in usage_dist.items():\n",
    "            print(f\"  {usage}: {count}ëª… ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # ê³„ì•½ì „ë ¥ ë¶„í¬\n",
    "        power_dist = self.customer_data['ê³„ì•½ì „ë ¥'].value_counts().sort_index()\n",
    "        print(\"\\nâš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\")\n",
    "        for power, count in power_dist.items():\n",
    "            print(f\"  {power}kW: {count}ëª… ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_dist,\n",
    "            'usage_distribution': usage_dist,\n",
    "            'power_distribution': power_dist\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, file_paths):\n",
    "        \"\"\"LP ë°ì´í„° ë¡œë”© ë° ê²°í•©\"\"\"\n",
    "        print(\"\\n=== LP ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        lp_dataframes = []\n",
    "        \n",
    "        for i, file_path in enumerate(file_paths, 1):\n",
    "            print(f\"íŒŒì¼ {i} ë¡œë”©: {file_path}\")\n",
    "            \n",
    "            # CSV íŒŒì¼ ì½ê¸° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ ë¶€ë¶„ ì‚¬ìš©)\n",
    "            # df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
    "            df = self._create_sample_lp_data(i)\n",
    "            \n",
    "            print(f\"  ë ˆì½”ë“œ ìˆ˜: {len(df):,}\")\n",
    "            print(f\"  ê³ ê° ìˆ˜: {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}\")\n",
    "            print(f\"  ê¸°ê°„: {df['LPìˆ˜ì‹ ì¼ì'].min()} ~ {df['LPìˆ˜ì‹ ì¼ì'].max()}\")\n",
    "            \n",
    "            lp_dataframes.append(df)\n",
    "        \n",
    "        # ë°ì´í„° ê²°í•©\n",
    "        self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "        print(f\"\\nâœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\")\n",
    "        print(f\"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}\")\n",
    "        print(f\"  ì´ ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}\")\n",
    "        \n",
    "        return self._analyze_lp_quality()\n",
    "    \n",
    "    def _create_sample_lp_data(self, file_num):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ LP ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        \n",
    "        # íŒŒì¼ë³„ë¡œ ë‹¤ë¥¸ ê¸°ê°„ ì„¤ì •\n",
    "        if file_num == 1:\n",
    "            start_date = datetime(2024, 3, 1)\n",
    "            days = 15\n",
    "        else:\n",
    "            start_date = datetime(2024, 3, 16) \n",
    "            days = 16\n",
    "        \n",
    "        for customer in customers:\n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ì„ ë°˜ì˜í•œ ê°€ìƒ ë°ì´í„°\n",
    "                        base_power = 80 + 30 * np.sin(2 * np.pi * hour / 24) + np.random.normal(0, 10)\n",
    "                        base_power = max(0, base_power)\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(base_power + np.random.normal(0, 5), 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(abs(np.random.normal(10, 5)), 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(abs(np.random.normal(5, 3)), 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(base_power + np.random.normal(0, 3), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        print(\"ğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
    "        numeric_cols = ['ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        print(self.lp_data[numeric_cols].describe())\n",
    "        \n",
    "        # ì‹œê°„ ê°„ê²© ì²´í¬\n",
    "        print(\"\\nâ° ì‹œê°„ ê°„ê²© ì²´í¬:\")\n",
    "        self.lp_data['LPìˆ˜ì‹ ì¼ì_dt'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]:  # ì²˜ìŒ 3ëª…ë§Œ ì²´í¬\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('LPìˆ˜ì‹ ì¼ì_dt')\n",
    "            time_diffs = customer_data['LPìˆ˜ì‹ ì¼ì_dt'].diff().dt.total_seconds() / 60  # ë¶„ ë‹¨ìœ„\n",
    "            \n",
    "            print(f\"  {customer}: í‰ê·  ê°„ê²© {time_diffs.mean():.1f}ë¶„, í‘œì¤€í¸ì°¨ {time_diffs.std():.1f}ë¶„\")\n",
    "            \n",
    "            # 15ë¶„ì´ ì•„ë‹Œ ê°„ê²© ì°¾ê¸°\n",
    "            non_15min = time_diffs[(time_diffs != 15.0) & (~time_diffs.isna())]\n",
    "            if len(non_15min) > 0:\n",
    "                print(f\"    âš ï¸ ë¹„ì •ìƒ ê°„ê²©: {len(non_15min)}ê±´\")\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ì²´í¬\n",
    "        print(\"\\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
    "        for col in numeric_cols:\n",
    "            null_count = self.lp_data[col].isnull().sum()\n",
    "            zero_count = (self.lp_data[col] == 0).sum()\n",
    "            negative_count = (self.lp_data[col] < 0).sum()\n",
    "            \n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    ê²°ì¸¡ì¹˜: {null_count:,}ê±´ ({null_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    0ê°’: {zero_count:,}ê±´ ({zero_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    ìŒìˆ˜: {negative_count:,}ê±´ ({negative_count/len(self.lp_data)*100:.2f}%)\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ë°ì´í„° ì™„ì •ì„±\n",
    "        print(\"\\nğŸ‘¥ ê³ ê°ë³„ ë°ì´í„° ì™„ì •ì„±:\")\n",
    "        customer_counts = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].value_counts()\n",
    "        expected_records = 24 * 4 * 31  # 15ë¶„ ê°„ê²© Ã— 1ê°œì›”\n",
    "        \n",
    "        print(f\"  ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜: {expected_records:,}ê°œ/ê³ ê°\")\n",
    "        print(f\"  ì‹¤ì œ ë ˆì½”ë“œ ìˆ˜: {customer_counts.min():,}~{customer_counts.max():,}ê°œ/ê³ ê°\")\n",
    "        \n",
    "        incomplete_customers = customer_counts[customer_counts < expected_records * 0.95]  # 95% ë¯¸ë§Œ\n",
    "        if len(incomplete_customers) > 0:\n",
    "            print(f\"  âš ï¸ ë¶ˆì™„ì „í•œ ê³ ê°: {len(incomplete_customers)}ëª…\")\n",
    "        else:\n",
    "            print(\"  âœ… ëª¨ë“  ê³ ê° ë°ì´í„° ì™„ì •ì„± ì–‘í˜¸\")\n",
    "        \n",
    "        return {\n",
    "            'total_records': len(self.lp_data),\n",
    "            'customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "            'date_range': (self.lp_data['LPìˆ˜ì‹ ì¼ì_dt'].min(), self.lp_data['LPìˆ˜ì‹ ì¼ì_dt'].max()),\n",
    "            'data_quality': {col: {'null': self.lp_data[col].isnull().sum(), \n",
    "                                  'zero': (self.lp_data[col] == 0).sum(),\n",
    "                                  'negative': (self.lp_data[col] < 0).sum()} for col in numeric_cols}\n",
    "        }\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        print(f\"\\n=== ì´ìƒì¹˜ íƒì§€ ({method.upper()} ë°©ë²•) ===\")\n",
    "        \n",
    "        numeric_cols = ['ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        outliers_summary = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            data = self.lp_data[col].dropna()\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs((data - data.mean()) / data.std())\n",
    "                outliers = data[z_scores > 3]\n",
    "            \n",
    "            outliers_summary[col] = {\n",
    "                'count': len(outliers),\n",
    "                'percentage': len(outliers) / len(data) * 100,\n",
    "                'min_outlier': outliers.min() if len(outliers) > 0 else None,\n",
    "                'max_outlier': outliers.max() if len(outliers) > 0 else None\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ“Š {col}:\")\n",
    "            print(f\"  ì´ìƒì¹˜: {len(outliers):,}ê±´ ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"  ë²”ìœ„: {outliers.min():.1f} ~ {outliers.max():.1f}\")\n",
    "        \n",
    "        return outliers_summary\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. ë°ì´í„° ê·œëª¨\n",
    "        print(\"\\nğŸ”¢ ë°ì´í„° ê·œëª¨:\")\n",
    "        print(f\"  â€¢ ê³ ê° ê¸°ë³¸ì •ë³´: {len(self.customer_data):,}ëª…\")\n",
    "        print(f\"  â€¢ LP ë°ì´í„°: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "        print(f\"  â€¢ ë¶„ì„ ëŒ€ìƒ ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        print(f\"  â€¢ ë¶„ì„ ê¸°ê°„: {self.lp_data['LPìˆ˜ì‹ ì¼ì'].min()} ~ {self.lp_data['LPìˆ˜ì‹ ì¼ì'].max()}\")\n",
    "        \n",
    "        # 2. ë°ì´í„° í’ˆì§ˆ ìš”ì•½\n",
    "        print(\"\\nâœ… ë°ì´í„° í’ˆì§ˆ ìƒíƒœ:\")\n",
    "        numeric_cols = ['ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        \n",
    "        total_records = len(self.lp_data)\n",
    "        total_nulls = sum(self.lp_data[col].isnull().sum() for col in numeric_cols)\n",
    "        total_zeros = sum((self.lp_data[col] == 0).sum() for col in numeric_cols)\n",
    "        total_negatives = sum((self.lp_data[col] < 0).sum() for col in numeric_cols)\n",
    "        \n",
    "        print(f\"  â€¢ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {total_nulls/(total_records*4)*100:.2f}%\")\n",
    "        print(f\"  â€¢ 0ê°’ ë¹„ìœ¨: {total_zeros/(total_records*4)*100:.2f}%\") \n",
    "        print(f\"  â€¢ ìŒìˆ˜ê°’ ë¹„ìœ¨: {total_negatives/(total_records*4)*100:.2f}%\")\n",
    "        \n",
    "        # 3. ê¶Œì¥ì‚¬í•­\n",
    "        print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\")\n",
    "        print(\"  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì¼/ì£¼/ì›”ë³„ ì‚¬ìš© íŒ¨í„´)\")\n",
    "        print(\"  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\")\n",
    "        print(\"  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\")\n",
    "        print(\"  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1ë‹¨ê³„: ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\n",
    "    customer_analysis = analyzer.load_customer_data('ê³ ê°ë²ˆí˜¸.xlsx')\n",
    "    \n",
    "    # 2ë‹¨ê³„: LP ë°ì´í„° ë¶„ì„  \n",
    "    lp_analysis = analyzer.load_lp_data(['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv'])\n",
    "    \n",
    "    # 3ë‹¨ê³„: ì´ìƒì¹˜ íƒì§€\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4ë‹¨ê³„: ì¢…í•© ë¦¬í¬íŠ¸\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\nğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\")\n",
    "    print(\"ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a4c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LP ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ===\n",
      "âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: 29,760ë ˆì½”ë“œ\n",
      "ê¸°ê°„: 2024-03-01 00:00:00 ~ 2024-03-31 23:45:00\n",
      "ê³ ê° ìˆ˜: 10ëª…\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\n",
      "============================================================\n",
      "\n",
      "=== ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ===\n",
      "ğŸ“Š ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (kW):\n",
      "ì‹œê°„\tí‰ê· \tí‘œì¤€í¸ì°¨\tìµœì†Œ\tìµœëŒ€\n",
      "00ì‹œ\t58.3\t24.3\t7.8\t120.5\n",
      "01ì‹œ\t66.2\t27.2\t9.3\t137.0\n",
      "02ì‹œ\t73.6\t30.3\t9.3\t154.6\n",
      "03ì‹œ\t80.0\t33.0\t9.0\t167.9\n",
      "04ì‹œ\t85.0\t35.1\t10.9\t180.6\n",
      "05ì‹œ\t87.9\t36.3\t12.3\t196.4\n",
      "06ì‹œ\t93.1\t37.6\t11.6\t184.0\n",
      "07ì‹œ\t103.7\t55.5\t17.9\t268.5\n",
      "08ì‹œ\t111.0\t54.1\t14.4\t265.8\n",
      "09ì‹œ\t108.2\t51.8\t16.5\t252.3\n",
      "10ì‹œ\t93.9\t38.6\t8.8\t181.5\n",
      "11ì‹œ\t79.1\t34.4\t8.7\t164.4\n",
      "12ì‹œ\t68.0\t34.9\t11.8\t183.4\n",
      "13ì‹œ\t63.1\t31.1\t9.6\t155.6\n",
      "14ì‹œ\t55.4\t26.4\t8.6\t125.2\n",
      "15ì‹œ\t45.7\t18.7\t4.7\t93.9\n",
      "16ì‹œ\t34.9\t14.2\t3.9\t79.4\n",
      "17ì‹œ\t32.9\t14.9\t5.1\t86.9\n",
      "18ì‹œ\t32.6\t15.8\t5.2\t87.8\n",
      "19ì‹œ\t37.6\t19.2\t5.2\t86.5\n",
      "20ì‹œ\t38.4\t19.3\t3.7\t90.4\n",
      "21ì‹œ\t42.2\t21.0\t5.1\t98.3\n",
      "22ì‹œ\t44.2\t17.9\t5.2\t93.5\n",
      "23ì‹œ\t50.1\t20.6\t6.8\t105.2\n",
      "\n",
      "âš¡ í”¼í¬ ì‹œê°„ëŒ€ (ìƒìœ„ 20%): [6, 7, 8, 9, 10]ì‹œ\n",
      "ğŸ’¤ ë¹„í”¼í¬ ì‹œê°„ëŒ€ (í•˜ìœ„ 30%): [16, 17, 18, 19, 20, 21, 22]ì‹œ\n",
      "\n",
      "=== ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ===\n",
      "ğŸ“… ìš”ì¼ë³„ í‰ê·  ì¼ê°„ ì‚¬ìš©ëŸ‰ (kWh):\n",
      "ì›”ìš”ì¼: 7,085.5 Â± 2014.9\n",
      "í™”ìš”ì¼: 7,075.1 Â± 2014.0\n",
      "ìˆ˜ìš”ì¼: 7,044.2 Â± 1986.6\n",
      "ëª©ìš”ì¼: 7,063.5 Â± 2006.7\n",
      "ê¸ˆìš”ì¼: 7,087.1 Â± 2046.0\n",
      "í† ìš”ì¼: 4,800.1 Â± 2987.5\n",
      "ì¼ìš”ì¼: 4,804.2 Â± 2982.3\n",
      "\n",
      "ğŸ“Š í‰ì¼ vs ì£¼ë§ ë¹„êµ:\n",
      "í‰ì¼ í‰ê· : 7,071.9 kWh\n",
      "ì£¼ë§ í‰ê· : 4,802.2 kWh\n",
      "ì£¼ë§/í‰ì¼ ë¹„ìœ¨: 0.68\n",
      "\n",
      "=== ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„ ===\n",
      "ğŸ‘¥ ê³ ê°ë³„ ê¸°ë³¸ í†µê³„ (kW):\n",
      "ê³ ê°ë²ˆí˜¸\tí‰ê· \tí‘œì¤€í¸ì°¨\të³€ë™ê³„ìˆ˜\tìµœì†Œ\tìµœëŒ€\n",
      "A1001\t85.1\t36.3\t0.427\t21.4\t205.5\n",
      "A1002\t45.7\t29.6\t0.648\t5.7\t141.9\n",
      "A1003\t79.0\t28.7\t0.363\t21.8\t181.2\n",
      "A1004\t81.7\t64.2\t0.786\t3.7\t268.5\n",
      "A1005\t66.0\t24.2\t0.367\t22.6\t138.6\n",
      "A1006\t58.0\t26.8\t0.462\t18.0\t172.3\n",
      "A1007\t59.7\t41.5\t0.695\t5.1\t186.5\n",
      "A1008\t94.9\t43.9\t0.463\t28.6\t244.2\n",
      "A1009\t36.6\t19.9\t0.544\t6.8\t109.5\n",
      "A1010\t53.8\t25.9\t0.481\t11.2\t146.4\n",
      "\n",
      "ğŸ“ˆ ëŒ€ìš©ëŸ‰ ì‚¬ìš©ì (ìƒìœ„ 20%): ['A1001', 'A1008']\n",
      "ğŸ“‰ ì†Œìš©ëŸ‰ ì‚¬ìš©ì (í•˜ìœ„ 20%): ['A1002', 'A1009']\n",
      "\n",
      "ğŸŒŠ ê³ ë³€ë™ì„± ê³ ê°: ['A1004', 'A1007']\n",
      "ğŸ“Š ì €ë³€ë™ì„± ê³ ê°: ['A1003', 'A1005']\n",
      "\n",
      "=== ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ===\n",
      "âš¡ ê³ ê°ë³„ ë¶€í•˜ìœ¨ ë° í”¼í¬ ì§‘ì¤‘ë„:\n",
      "ê³ ê°ë²ˆí˜¸\të¶€í•˜ìœ¨\tí”¼í¬ì§‘ì¤‘ë„\tí‰ê· ë¶€í•˜\tìµœëŒ€ë¶€í•˜\n",
      "A1001\t0.414\t1.039\t85.1\t205.5\n",
      "A1002\t0.322\t1.076\t45.7\t141.9\n",
      "A1003\t0.436\t0.928\t79.0\t181.2\n",
      "A1004\t0.304\t1.099\t81.7\t268.5\n",
      "A1005\t0.476\t0.858\t66.0\t138.6\n",
      "A1006\t0.337\t0.806\t58.0\t172.3\n",
      "A1007\t0.32\t1.081\t59.7\t186.5\n",
      "A1008\t0.389\t0.963\t94.9\t244.2\n",
      "A1009\t0.334\t0.977\t36.6\t109.5\n",
      "A1010\t0.367\t1.083\t53.8\t146.4\n",
      "\n",
      "ğŸ“Š ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬:\n",
      "í‰ê·  ë¶€í•˜ìœ¨: 0.370\n",
      "ë¶€í•˜ìœ¨ ë²”ìœ„: 0.304 ~ 0.476\n",
      "\n",
      "=== ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€ ===\n",
      "ğŸš¨ ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼:\n",
      "ê³ ê°ë²ˆí˜¸\tê¸‰ê²©ë³€í™”\tì¥ê¸°0ê°’\tí†µê³„ì´ìƒì¹˜\n",
      "A1001\t0\t0\t10\n",
      "A1002\t4\t0\t6\n",
      "A1003\t0\t0\t9\n",
      "A1004\t4\t0\t0\n",
      "A1005\t0\t0\t1\n",
      "A1006\t0\t0\t54\n",
      "A1007\t4\t0\t2\n",
      "A1008\t0\t0\t9\n",
      "A1009\t0\t0\t21\n",
      "A1010\t0\t0\t6\n",
      "\n",
      "ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\n",
      "  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: [6, 7, 8, 9, 10]ì‹œ\n",
      "  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: 0.68\n",
      "  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: 0.363 ~ 0.786\n",
      "  â€¢ í‰ê·  ë¶€í•˜ìœ¨: 0.370\n",
      "  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: 10ëª…\n",
      "\n",
      "ğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\n",
      "  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\n",
      "  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\n",
      "  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\n",
      "  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\n",
      "  5. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \n",
      "\n",
      "ğŸ¯ 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 3ë‹¨ê³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.weather_data = None\n",
    "        self.calendar_data = None\n",
    "        \n",
    "    def load_sample_data(self, customer_df=None):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ LP ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨)\"\"\"\n",
    "        print(\"=== í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± ===\")\n",
    "        print(\"âš ï¸  ì£¼ì˜: ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” load_real_lp_data() ì‚¬ìš©\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "        self.lp_data = self._create_comprehensive_sample_data(customer_df)\n",
    "        \n",
    "        # ë‚ ì§œ/ì‹œê°„ ì „ì²˜ë¦¬\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "        \n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "        print(f\"ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "        print(f\"ê³ ê° ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def load_real_lp_data(self, lp_files):\n",
    "        \"\"\"ì‹¤ì œ LP ë°ì´í„° ë¡œë”© (ì‹¤ì œ í™˜ê²½ì—ì„œ ì‚¬ìš©)\"\"\"\n",
    "        print(\"=== ì‹¤ì œ LP ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        lp_data_list = []\n",
    "        \n",
    "        for file_path in lp_files:\n",
    "            print(f\"ğŸ“‚ ë¡œë”© ì¤‘: {file_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                lp_data_list.append(df)\n",
    "                print(f\"   âœ… ì™„ë£Œ: {len(df):,}ë ˆì½”ë“œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        if lp_data_list:\n",
    "            self.lp_data = pd.concat(lp_data_list, ignore_index=True)\n",
    "            \n",
    "            # ë‚ ì§œ/ì‹œê°„ ì „ì²˜ë¦¬\n",
    "            self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "            self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "            self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "            self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "            self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "            self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "            \n",
    "            print(f\"âœ… ì „ì²´ ë°ì´í„° ê²°í•© ì™„ë£Œ: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "            print(f\"ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "            print(f\"ê³ ê° ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def load_external_data(self):\n",
    "        \"\"\"ê¸°ìƒ ë° ë‹¬ë ¥ ë°ì´í„° ë¡œë”©\"\"\"\n",
    "        print(\"\\n=== ì™¸ë¶€ ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        try:\n",
    "            # ê¸°ìƒ ë°ì´í„° ë¡œë”©\n",
    "            print(\"ğŸ“Š ê¸°ìƒ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "            self.weather_data = pd.read_csv('weather_daily_processed.csv')\n",
    "            \n",
    "            # ë‚ ì§œ ì»¬ëŸ¼ ì „ì²˜ë¦¬\n",
    "            self.weather_data['date'] = pd.to_datetime(self.weather_data['ë‚ ì§œ'])\n",
    "            \n",
    "            print(f\"âœ… ê¸°ìƒ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.weather_data):,}ì¼\")\n",
    "            print(f\"   ê¸°ê°„: {self.weather_data['date'].min().date()} ~ {self.weather_data['date'].max().date()}\")\n",
    "            print(f\"   ì»¬ëŸ¼: {len(self.weather_data.columns)}ê°œ - {list(self.weather_data.columns[:5])}...\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"âš ï¸  ê¸°ìƒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (weather_daily_processed.csv)\")\n",
    "            self.weather_data = None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê¸°ìƒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.weather_data = None\n",
    "        \n",
    "        try:\n",
    "            # ë‹¬ë ¥ ë°ì´í„° ë¡œë”©\n",
    "            print(\"\\nğŸ“… ë‹¬ë ¥ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "            self.calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')\n",
    "            \n",
    "            # ë‚ ì§œ ì»¬ëŸ¼ ì „ì²˜ë¦¬\n",
    "            self.calendar_data['date'] = pd.to_datetime(self.calendar_data['date'])\n",
    "            \n",
    "            print(f\"âœ… ë‹¬ë ¥ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.calendar_data):,}ì¼\")\n",
    "            print(f\"   ê¸°ê°„: {self.calendar_data['date'].min().date()} ~ {self.calendar_data['date'].max().date()}\")\n",
    "            print(f\"   ì»¬ëŸ¼: {len(self.calendar_data.columns)}ê°œ - {list(self.calendar_data.columns[:5])}...\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(\"âš ï¸  ë‹¬ë ¥ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (power_analysis_calendar_2022_2025.csv)\")\n",
    "            self.calendar_data = None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë‹¬ë ¥ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "            self.calendar_data = None\n",
    "    \n",
    "    def _create_customer_profiles_from_data(self, customer_df=None):\n",
    "        \"\"\"ì‹¤ì œ ê³ ê° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ê° í”„ë¡œíŒŒì¼ ìƒì„±\"\"\"\n",
    "        profiles = {}\n",
    "        \n",
    "        if customer_df is not None:\n",
    "            # ì‹¤ì œ ê³ ê° ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°\n",
    "            for _, customer in customer_df.iterrows():\n",
    "                customer_id = customer['ê³ ê°ë²ˆí˜¸']\n",
    "                \n",
    "                # ê³„ì•½ì „ë ¥ì—ì„œ base_power ì¶”ì •\n",
    "                contract_power = customer['ê³„ì•½ì „ë ¥']\n",
    "                if '100~199' in str(contract_power):\n",
    "                    base_power = np.random.uniform(80, 120)\n",
    "                elif '200~299' in str(contract_power):\n",
    "                    base_power = np.random.uniform(120, 180)\n",
    "                elif '400~499' in str(contract_power):\n",
    "                    base_power = np.random.uniform(200, 280)\n",
    "                elif '500~599' in str(contract_power):\n",
    "                    base_power = np.random.uniform(280, 360)\n",
    "                elif '700~799' in str(contract_power):\n",
    "                    base_power = np.random.uniform(400, 500)\n",
    "                elif '800~899' in str(contract_power):\n",
    "                    base_power = np.random.uniform(500, 650)\n",
    "                else:\n",
    "                    base_power = np.random.uniform(100, 200)\n",
    "                \n",
    "                # ì‚¬ìš©ìš©ë„ë³„ íŒ¨í„´ ì •ì˜\n",
    "                usage_type = customer['ì‚¬ìš©ìš©ë„']\n",
    "                if 'ìƒì—…ìš©' in str(usage_type):\n",
    "                    peak_hours = [9, 14, 19]  # ìƒì—…ìš©: ì˜¤ì „, ì˜¤í›„, ì €ë… í”¼í¬\n",
    "                    weekend_factor = np.random.uniform(0.7, 1.3)  # ìƒì—…ìš©ì€ ì£¼ë§ ë³€ë™ í¼\n",
    "                else:  # ê´‘ê³µì—…ìš©\n",
    "                    peak_hours = [8, 13, 18]  # ì‚°ì—…ìš©: ì‘ì—…ì‹œê°„ í”¼í¬\n",
    "                    weekend_factor = np.random.uniform(0.1, 0.4)  # ì‚°ì—…ìš©ì€ ì£¼ë§ ë‚®ìŒ\n",
    "                \n",
    "                # ê³„ì•½ì¢…ë³„ ì„¸ë¶€ ì¡°ì •\n",
    "                contract_type = customer['ê³„ì•½ì¢…ë³„']\n",
    "                if 'ì¼ë°˜ìš©' in str(contract_type):\n",
    "                    # ì¼ë°˜ìš©ì€ ë” ë¶ˆê·œì¹™í•œ íŒ¨í„´\n",
    "                    weekend_factor *= np.random.uniform(0.8, 1.5)\n",
    "                \n",
    "                # ì‚°ì—…ë¶„ë¥˜ë³„ íŠ¹ì„± ë°˜ì˜ (ìˆëŠ” ê²½ìš°)\n",
    "                industry = customer.get('ì‚°ì—…ë¶„ë¥˜(ì†Œ)', '')\n",
    "                if 'ì œì¡°' in str(industry) or 'ìƒì‚°' in str(industry):\n",
    "                    # ì œì¡°ì—…ì€ ë” ì¼ì •í•œ íŒ¨í„´\n",
    "                    weekend_factor *= 0.3\n",
    "                    peak_hours = [8, 13, 18, 22]  # êµëŒ€ê·¼ë¬´ ê³ ë ¤\n",
    "                \n",
    "                profiles[customer_id] = {\n",
    "                    'type': self._classify_business_type(customer),\n",
    "                    'base_power': round(base_power, 1),\n",
    "                    'peak_hours': peak_hours,\n",
    "                    'weekend_factor': round(weekend_factor, 2),\n",
    "                    'contract_power': contract_power,\n",
    "                    'usage_type': usage_type\n",
    "                }\n",
    "        else:\n",
    "            # ìƒ˜í”Œ ë°ì´í„°ìš© ê¸°ë³¸ í”„ë¡œíŒŒì¼\n",
    "            profiles = self._get_default_sample_profiles()\n",
    "        \n",
    "        return profiles\n",
    "    \n",
    "    def _classify_business_type(self, customer):\n",
    "        \"\"\"ê³ ê° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—…ì¢… ë¶„ë¥˜\"\"\"\n",
    "        usage_type = str(customer['ì‚¬ìš©ìš©ë„'])\n",
    "        contract_type = str(customer['ê³„ì•½ì¢…ë³„'])\n",
    "        industry = str(customer.get('ì‚°ì—…ë¶„ë¥˜(ì†Œ)', ''))\n",
    "        product = str(customer.get('ì£¼ìƒì‚°í’ˆ', ''))\n",
    "        \n",
    "        # ì‚¬ìš©ìš©ë„ ê¸°ë°˜ 1ì°¨ ë¶„ë¥˜\n",
    "        if 'ìƒì—…ìš©' in usage_type:\n",
    "            if 'ì¼ë°˜ìš©' in contract_type:\n",
    "                return 'commercial'  # ìƒê°€, ì‚¬ë¬´ì†Œ ë“±\n",
    "            else:\n",
    "                return 'service'     # ëŒ€í˜• ì„œë¹„ìŠ¤ì—…\n",
    "        else:  # ê´‘ê³µì—…ìš©\n",
    "            if any(keyword in industry.lower() for keyword in ['ì œì¡°', 'ìƒì‚°', 'ê³µì¥']):\n",
    "                return 'manufacturing'\n",
    "            else:\n",
    "                return 'industrial'\n",
    "        \n",
    "    def _get_default_sample_profiles(self):\n",
    "        \"\"\"ìƒ˜í”Œ ë°ì´í„°ìš© ê¸°ë³¸ í”„ë¡œíŒŒì¼\"\"\"\n",
    "        return {\n",
    "            'A1001': {'type': 'hospital', 'base_power': 120, 'peak_hours': [9, 14, 20], 'weekend_factor': 0.8},\n",
    "            'A1002': {'type': 'office', 'base_power': 80, 'peak_hours': [9, 14], 'weekend_factor': 0.3},\n",
    "            'A1003': {'type': 'retail', 'base_power': 100, 'peak_hours': [11, 15, 19], 'weekend_factor': 1.2},\n",
    "            'A1004': {'type': 'factory', 'base_power': 150, 'peak_hours': [8, 13, 18], 'weekend_factor': 0.1},\n",
    "            'A1005': {'type': 'restaurant', 'base_power': 90, 'peak_hours': [12, 18], 'weekend_factor': 1.1},\n",
    "            'A1006': {'type': 'gym', 'base_power': 70, 'peak_hours': [7, 18, 21], 'weekend_factor': 1.3},\n",
    "            'A1007': {'type': 'school', 'base_power': 110, 'peak_hours': [10, 14], 'weekend_factor': 0.2},\n",
    "            'A1008': {'type': 'hotel', 'base_power': 130, 'peak_hours': [8, 20], 'weekend_factor': 1.0},\n",
    "            'A1009': {'type': 'warehouse', 'base_power': 60, 'peak_hours': [9, 16], 'weekend_factor': 0.5},\n",
    "            'A1010': {'type': 'clinic', 'base_power': 85, 'peak_hours': [10, 15], 'weekend_factor': 0.6}\n",
    "        }\n",
    "\n",
    "    def _create_comprehensive_sample_data(self, customer_df=None):\n",
    "        \"\"\"í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31  # 3ì›” ì „ì²´\n",
    "        \n",
    "        # ë™ì  ê³ ê° í”„ë¡œíŒŒì¼ ìƒì„±\n",
    "        customer_profiles = self._create_customer_profiles_from_data(customer_df)\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = customer_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                is_weekend = current_date.weekday() >= 5\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ê¸°ë³¸ ì „ë ¥ ê³„ì‚°\n",
    "                        base_power = profile['base_power']\n",
    "                        \n",
    "                        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ (ì‚¬ì¸íŒŒ ê¸°ë°˜)\n",
    "                        time_factor = 0.3 + 0.7 * (np.sin(2 * np.pi * hour / 24) + 1) / 2\n",
    "                        \n",
    "                        # í”¼í¬ ì‹œê°„ ë³´ì •\n",
    "                        peak_factor = 1.0\n",
    "                        for peak_hour in profile['peak_hours']:\n",
    "                            if abs(hour - peak_hour) <= 1:\n",
    "                                peak_factor = 1.5\n",
    "                        \n",
    "                        # ì£¼ë§ ë³´ì •\n",
    "                        weekend_factor = profile['weekend_factor'] if is_weekend else 1.0\n",
    "                        \n",
    "                        # ìµœì¢… ì „ë ¥ ê³„ì‚°\n",
    "                        power = base_power * time_factor * peak_factor * weekend_factor\n",
    "                        power += np.random.normal(0, power * 0.1)  # 10% ë…¸ì´ì¦ˆ\n",
    "                        power = max(0, power)\n",
    "                        \n",
    "                        # ë¬´íš¨ì „ë ¥ ê³„ì‚°\n",
    "                        reactive_lag = power * np.random.uniform(0.1, 0.3)\n",
    "                        reactive_lead = power * np.random.uniform(0.05, 0.15)\n",
    "                        apparent_power = np.sqrt(power**2 + (reactive_lag - reactive_lead)**2)\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(reactive_lag, 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(reactive_lead, 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(apparent_power, 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def merge_with_external_data(self):\n",
    "        \"\"\"LP ë°ì´í„°ì™€ ì™¸ë¶€ ë°ì´í„° ê²°í•©\"\"\"\n",
    "        if self.lp_data is None:\n",
    "            print(\"âŒ LP ë°ì´í„°ê°€ ë¡œë”©ë˜ì§€ ì•ŠìŒ\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n=== ì™¸ë¶€ ë°ì´í„°ì™€ ê²°í•© ===\")\n",
    "        \n",
    "        # ì¼ë³„ LP ë°ì´í„° ì§‘ê³„\n",
    "        daily_lp = self.lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date'])['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\n",
    "        daily_lp.columns = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date', 'daily_sum', 'daily_mean', 'daily_std', 'daily_min', 'daily_max']\n",
    "        daily_lp['date'] = pd.to_datetime(daily_lp['date'])\n",
    "        \n",
    "        # ê¸°ìƒ ë°ì´í„°ì™€ ê²°í•©\n",
    "        if self.weather_data is not None:\n",
    "            print(\"ğŸŒ¤ï¸  ê¸°ìƒ ë°ì´í„° ê²°í•© ì¤‘...\")\n",
    "            merged_data = daily_lp.merge(\n",
    "                self.weather_data[['date', 'í‰ê· ê¸°ì˜¨', 'ìµœê³ ê¸°ì˜¨', 'ìµœì €ê¸°ì˜¨', 'í‰ê· ìŠµë„', 'ì´ê°•ìˆ˜ëŸ‰', 'ë¶ˆì¾Œì§€ìˆ˜', 'ëƒ‰ë°©í•„ìš”ë„', 'ë‚œë°©í•„ìš”ë„']],\n",
    "                on='date',\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"   âœ… ê¸°ìƒ ë°ì´í„° ê²°í•© ì™„ë£Œ: {merged_data['í‰ê· ê¸°ì˜¨'].notna().sum():,}ì¼ ë§¤ì¹­\")\n",
    "        else:\n",
    "            merged_data = daily_lp.copy()\n",
    "        \n",
    "        # ë‹¬ë ¥ ë°ì´í„°ì™€ ê²°í•©\n",
    "        if self.calendar_data is not None:\n",
    "            print(\"ğŸ“… ë‹¬ë ¥ ë°ì´í„° ê²°í•© ì¤‘...\")\n",
    "            merged_data = merged_data.merge(\n",
    "                self.calendar_data[['date', 'is_workday', 'is_weekend', 'is_holiday', 'is_consecutive_holiday', 'day_type']],\n",
    "                on='date',\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"   âœ… ë‹¬ë ¥ ë°ì´í„° ê²°í•© ì™„ë£Œ: {merged_data['is_workday'].notna().sum():,}ì¼ ë§¤ì¹­\")\n",
    "        \n",
    "        # ê²°í•©ëœ ë°ì´í„° ì €ì¥\n",
    "        self.merged_daily_data = merged_data\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ê²°í•©ëœ ë°ì´í„° ìš”ì•½:\")\n",
    "        print(f\"   ì´ ë ˆì½”ë“œ: {len(merged_data):,}ê°œ\")\n",
    "        print(f\"   ê³ ê° ìˆ˜: {merged_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique():,}ëª…\")\n",
    "        print(f\"   ê¸°ê°„: {merged_data['date'].min().date()} ~ {merged_data['date'].max().date()}\")\n",
    "        print(f\"   ì»¬ëŸ¼ ìˆ˜: {len(merged_data.columns)}ê°œ\")\n",
    "        \n",
    "        return merged_data\n",
    "    \n",
    "    def analyze_weather_impact(self):\n",
    "        \"\"\"ê¸°ìƒ ìš”ì¸ì´ ì „ë ¥ ì‚¬ìš©ëŸ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„\"\"\"\n",
    "        if not hasattr(self, 'merged_daily_data') or self.weather_data is None:\n",
    "            print(\"âš ï¸  ê¸°ìƒ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ìƒ ì˜í–¥ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n=== ê¸°ìƒ ìš”ì¸ ì˜í–¥ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê¸°ìƒ ë³€ìˆ˜ë³„ ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "        weather_cols = ['í‰ê· ê¸°ì˜¨', 'ìµœê³ ê¸°ì˜¨', 'ìµœì €ê¸°ì˜¨', 'í‰ê· ìŠµë„', 'ì´ê°•ìˆ˜ëŸ‰', 'ë¶ˆì¾Œì§€ìˆ˜', 'ëƒ‰ë°©í•„ìš”ë„', 'ë‚œë°©í•„ìš”ë„']\n",
    "        power_cols = ['daily_sum', 'daily_mean']\n",
    "        \n",
    "        correlations = {}\n",
    "        for weather_col in weather_cols:\n",
    "            if weather_col in self.merged_daily_data.columns:\n",
    "                corr_sum = self.merged_daily_data[weather_col].corr(self.merged_daily_data['daily_sum'])\n",
    "                corr_mean = self.merged_daily_data[weather_col].corr(self.merged_daily_data['daily_mean'])\n",
    "                correlations[weather_col] = {'sum': corr_sum, 'mean': corr_mean}\n",
    "        \n",
    "        print(\"ğŸŒ¡ï¸  ê¸°ìƒ ìš”ì¸ë³„ ìƒê´€ê´€ê³„ (ì „ë ¥ì‚¬ìš©ëŸ‰):\")\n",
    "        print(\"ê¸°ìƒìš”ì¸\\t\\tì¼ì´ì‚¬ìš©ëŸ‰\\tì¼í‰ê· ì‚¬ìš©ëŸ‰\")\n",
    "        for weather_factor, corrs in correlations.items():\n",
    "            print(f\"{weather_factor}\\t{corrs['sum']:.3f}\\t\\t{corrs['mean']:.3f}\")\n",
    "        \n",
    "        # ì˜¨ë„ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´\n",
    "        temp_bins = [-10, 0, 10, 20, 25, 30, 35, 50]\n",
    "        temp_labels = ['ê·¹í•œì €ì˜¨', 'ì €ì˜¨', 'ì„œëŠ˜', 'ì ì •', 'ë”°ëœ»', 'ë”ì›€', 'ê³ ì˜¨']\n",
    "        \n",
    "        if 'í‰ê· ê¸°ì˜¨' in self.merged_daily_data.columns:\n",
    "            self.merged_daily_data['temp_category'] = pd.cut(\n",
    "                self.merged_daily_data['í‰ê· ê¸°ì˜¨'], \n",
    "                bins=temp_bins, \n",
    "                labels=temp_labels, \n",
    "                include_lowest=True\n",
    "            )\n",
    "            \n",
    "            temp_power = self.merged_daily_data.groupby('temp_category')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "            \n",
    "            print(f\"\\nğŸŒ¡ï¸  ì˜¨ë„ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´:\")\n",
    "            print(\"ì˜¨ë„êµ¬ê°„\\tì¼ìˆ˜\\tí‰ê· ì‚¬ìš©ëŸ‰\\tí‘œì¤€í¸ì°¨\")\n",
    "            for temp_cat in temp_power.index:\n",
    "                stats = temp_power.loc[temp_cat]\n",
    "                print(f\"{temp_cat}\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        return correlations\n",
    "    \n",
    "    def analyze_calendar_patterns(self):\n",
    "        \"\"\"ë‹¬ë ¥ ìš”ì¸ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        if not hasattr(self, 'merged_daily_data') or self.calendar_data is None:\n",
    "            print(\"âš ï¸  ë‹¬ë ¥ ë°ì´í„°ê°€ ì—†ì–´ ë‹¬ë ¥ íŒ¨í„´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"\\n=== ë‹¬ë ¥ íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # í‰ì¼/ì£¼ë§/íœ´ì¼ë³„ íŒ¨í„´\n",
    "        if 'day_type' in self.merged_daily_data.columns:\n",
    "            day_type_stats = self.merged_daily_data.groupby('day_type')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "            \n",
    "            print(\"ğŸ“… ì¼ì ìœ í˜•ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´:\")\n",
    "            print(\"ì¼ììœ í˜•\\tì¼ìˆ˜\\tí‰ê· ì‚¬ìš©ëŸ‰\\tí‘œì¤€í¸ì°¨\")\n",
    "            for day_type in day_type_stats.index:\n",
    "                stats = day_type_stats.loc[day_type]\n",
    "                print(f\"{day_type}\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        # ì—°íœ´ íš¨ê³¼ ë¶„ì„\n",
    "        if 'is_consecutive_holiday' in self.merged_daily_data.columns:\n",
    "            holiday_effect = self.merged_daily_data.groupby('is_consecutive_holiday')['daily_mean'].agg(['count', 'mean']).round(1)\n",
    "            \n",
    "            print(f\"\\nğŸŠ ì—°íœ´ íš¨ê³¼ ë¶„ì„:\")\n",
    "            for is_holiday, stats in holiday_effect.iterrows():\n",
    "                holiday_label = \"ì—°íœ´ê¸°ê°„\" if is_holiday else \"ì¼ë°˜ê¸°ê°„\"\n",
    "                print(f\"{holiday_label}: ì¼ìˆ˜ {stats['count']}ì¼, í‰ê·  {stats['mean']}kW\")\n",
    "        \n",
    "        # ì›”ë³„ íŒ¨í„´ (ê³„ì ˆì„±)\n",
    "        self.merged_daily_data['month'] = self.merged_daily_data['date'].dt.month\n",
    "        monthly_stats = self.merged_daily_data.groupby('month')['daily_mean'].agg(['count', 'mean', 'std']).round(1)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì›”ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´:\")\n",
    "        print(\"ì›”\\tì¼ìˆ˜\\tí‰ê· ì‚¬ìš©ëŸ‰\\tí‘œì¤€í¸ì°¨\")\n",
    "        for month in monthly_stats.index:\n",
    "            stats = monthly_stats.loc[month]\n",
    "            print(f\"{month}ì›”\\t{stats['count']}\\t{stats['mean']}\\t{stats['std']}\")\n",
    "        \n",
    "        return {\n",
    "            'day_type_stats': day_type_stats if 'day_type' in self.merged_daily_data.columns else None,\n",
    "            'holiday_effect': holiday_effect if 'is_consecutive_holiday' in self.merged_daily_data.columns else None,\n",
    "            'monthly_stats': monthly_stats\n",
    "        }\n",
    "    \n",
    "    def analyze_hourly_patterns(self):\n",
    "        \"\"\"ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "        hourly_avg = self.lp_data.groupby('hour')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "        \n",
    "        print(\"ğŸ“Š ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (kW):\")\n",
    "        print(\"ì‹œê°„\\tí‰ê· \\tí‘œì¤€í¸ì°¨\\tìµœì†Œ\\tìµœëŒ€\")\n",
    "        for hour in range(24):\n",
    "            stats = hourly_avg.loc[hour]\n",
    "            print(f\"{hour:02d}ì‹œ\\t{stats['mean']}\\t{stats['std']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # í”¼í¬/ë¹„í”¼í¬ ì‹œê°„ëŒ€ ì‹ë³„\n",
    "        peak_threshold = hourly_avg['mean'].quantile(0.8)\n",
    "        peak_hours = hourly_avg[hourly_avg['mean'] >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = hourly_avg[hourly_avg['mean'] < hourly_avg['mean'].quantile(0.3)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nâš¡ í”¼í¬ ì‹œê°„ëŒ€ (ìƒìœ„ 20%): {peak_hours}ì‹œ\")\n",
    "        print(f\"ğŸ’¤ ë¹„í”¼í¬ ì‹œê°„ëŒ€ (í•˜ìœ„ 30%): {off_peak_hours}ì‹œ\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_stats': hourly_avg,\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours\n",
    "        }\n",
    "    \n",
    "    def analyze_daily_patterns(self):\n",
    "        \"\"\"ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ì¼ë³„ ì´ ì‚¬ìš©ëŸ‰\n",
    "        daily_usage = self.lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date'])['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].sum().reset_index()\n",
    "        daily_usage['weekday'] = pd.to_datetime(daily_usage['date']).dt.weekday\n",
    "        daily_usage['is_weekend'] = daily_usage['weekday'].isin([5, 6])\n",
    "        \n",
    "        # ìš”ì¼ë³„ í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "        weekday_avg = daily_usage.groupby('weekday')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['mean', 'std']).round(1)\n",
    "        weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']\n",
    "        \n",
    "        print(\"ğŸ“… ìš”ì¼ë³„ í‰ê·  ì¼ê°„ ì‚¬ìš©ëŸ‰ (kWh):\")\n",
    "        for i, day_name in enumerate(weekday_names):\n",
    "            stats = weekday_avg.loc[i]\n",
    "            print(f\"{day_name}ìš”ì¼: {stats['mean']:,.1f} Â± {stats['std']:.1f}\")\n",
    "        \n",
    "        # í‰ì¼ vs ì£¼ë§ ë¹„êµ\n",
    "        weekday_mean = daily_usage[~daily_usage['is_weekend']]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "        weekend_mean = daily_usage[daily_usage['is_weekend']]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "        weekend_ratio = weekend_mean / weekday_mean\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í‰ì¼ vs ì£¼ë§ ë¹„êµ:\")\n",
    "        print(f\"í‰ì¼ í‰ê· : {weekday_mean:,.1f} kWh\")\n",
    "        print(f\"ì£¼ë§ í‰ê· : {weekend_mean:,.1f} kWh\")\n",
    "        print(f\"ì£¼ë§/í‰ì¼ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'daily_usage': daily_usage,\n",
    "            'weekday_stats': weekday_avg,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "    \n",
    "    def analyze_customer_profiles(self):\n",
    "        \"\"\"ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ê¸°ë³¸ í†µê³„\n",
    "        customer_stats = self.lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max'\n",
    "        ]).round(1)\n",
    "        customer_stats['cv'] = (customer_stats['std'] / customer_stats['mean']).round(3)  # ë³€ë™ê³„ìˆ˜\n",
    "        \n",
    "        print(\"ğŸ‘¥ ê³ ê°ë³„ ê¸°ë³¸ í†µê³„ (kW):\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tí‰ê· \\tí‘œì¤€í¸ì°¨\\të³€ë™ê³„ìˆ˜\\tìµœì†Œ\\tìµœëŒ€\")\n",
    "        for customer in customer_stats.index:\n",
    "            stats = customer_stats.loc[customer]\n",
    "            print(f\"{customer}\\t{stats['mean']}\\t{stats['std']}\\t{stats['cv']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë¶„ë¥˜\n",
    "        mean_usage = customer_stats['mean']\n",
    "        high_users = mean_usage[mean_usage >= mean_usage.quantile(0.8)].index.tolist()\n",
    "        low_users = mean_usage[mean_usage <= mean_usage.quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ëŒ€ìš©ëŸ‰ ì‚¬ìš©ì (ìƒìœ„ 20%): {high_users}\")\n",
    "        print(f\"ğŸ“‰ ì†Œìš©ëŸ‰ ì‚¬ìš©ì (í•˜ìœ„ 20%): {low_users}\")\n",
    "        \n",
    "        # ë³€ë™ì„±ë³„ ë¶„ë¥˜\n",
    "        high_volatility = customer_stats[customer_stats['cv'] >= customer_stats['cv'].quantile(0.8)].index.tolist()\n",
    "        low_volatility = customer_stats[customer_stats['cv'] <= customer_stats['cv'].quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nğŸŒŠ ê³ ë³€ë™ì„± ê³ ê°: {high_volatility}\")\n",
    "        print(f\"ğŸ“Š ì €ë³€ë™ì„± ê³ ê°: {low_volatility}\")\n",
    "        \n",
    "        return {\n",
    "            'customer_stats': customer_stats,\n",
    "            'usage_segments': {\n",
    "                'high_users': high_users,\n",
    "                'low_users': low_users,\n",
    "                'high_volatility': high_volatility,\n",
    "                'low_volatility': low_volatility\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_load_factors(self):\n",
    "        \"\"\"ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ===\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ë¶€í•˜ìœ¨ ê³„ì‚°\n",
    "        customer_load_factors = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "            \n",
    "            avg_load = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            max_load = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].max()\n",
    "            load_factor = avg_load / max_load if max_load > 0 else 0\n",
    "            \n",
    "            # í”¼í¬ ì§‘ì¤‘ë„ (í”¼í¬ ì‹œê°„ëŒ€ ì‚¬ìš©ëŸ‰ ë¹„ì¤‘)\n",
    "            peak_hours = [9, 14, 18]  # ëŒ€í‘œ í”¼í¬ ì‹œê°„\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            total_avg = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            peak_concentration = peak_usage / total_avg if total_avg > 0 else 0\n",
    "            \n",
    "            customer_load_factors[customer] = {\n",
    "                'load_factor': round(load_factor, 3),\n",
    "                'peak_concentration': round(peak_concentration, 3),\n",
    "                'avg_load': round(avg_load, 1),\n",
    "                'max_load': round(max_load, 1)\n",
    "            }\n",
    "        \n",
    "        print(\"âš¡ ê³ ê°ë³„ ë¶€í•˜ìœ¨ ë° í”¼í¬ ì§‘ì¤‘ë„:\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\të¶€í•˜ìœ¨\\tí”¼í¬ì§‘ì¤‘ë„\\tí‰ê· ë¶€í•˜\\tìµœëŒ€ë¶€í•˜\")\n",
    "        for customer, metrics in customer_load_factors.items():\n",
    "            print(f\"{customer}\\t{metrics['load_factor']}\\t{metrics['peak_concentration']}\\t{metrics['avg_load']}\\t{metrics['max_load']}\")\n",
    "        \n",
    "        # ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬\n",
    "        load_factors = [metrics['load_factor'] for metrics in customer_load_factors.values()]\n",
    "        avg_load_factor = np.mean(load_factors)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬:\")\n",
    "        print(f\"í‰ê·  ë¶€í•˜ìœ¨: {avg_load_factor:.3f}\")\n",
    "        print(f\"ë¶€í•˜ìœ¨ ë²”ìœ„: {min(load_factors):.3f} ~ {max(load_factors):.3f}\")\n",
    "        \n",
    "        return customer_load_factors\n",
    "    \n",
    "    def detect_usage_anomalies(self):\n",
    "        \"\"\"ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€\"\"\"\n",
    "        print(\"\\n=== ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€ ===\")\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()\n",
    "            customer_data = customer_data.sort_values('datetime')\n",
    "            \n",
    "            # 1. ê¸‰ê²©í•œ ë³€í™” íƒì§€ (ì „ì‹œì  ëŒ€ë¹„ 200% ì´ìƒ ë³€í™”)\n",
    "            customer_data['power_change'] = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['power_change']) > 2.0]\n",
    "            \n",
    "            # 2. ì—°ì†ì ì¸ 0ê°’ íƒì§€ (2ì‹œê°„ ì´ìƒ)\n",
    "            customer_data['is_zero'] = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] == 0\n",
    "            customer_data['zero_group'] = (customer_data['is_zero'] != customer_data['is_zero'].shift()).cumsum()\n",
    "            zero_periods = customer_data[customer_data['is_zero']].groupby('zero_group').size()\n",
    "            long_zero_periods = zero_periods[zero_periods >= 8]  # 2ì‹œê°„ = 8ê°œ 15ë¶„ êµ¬ê°„\n",
    "            \n",
    "            # 3. í†µê³„ì  ì´ìƒì¹˜ (Z-score > 3)\n",
    "            mean_power = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            std_power = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].std()\n",
    "            if std_power > 0:\n",
    "                customer_data['z_score'] = abs(customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] - mean_power) / std_power\n",
    "                statistical_outliers = customer_data[customer_data['z_score'] > 3]\n",
    "            else:\n",
    "                statistical_outliers = pd.DataFrame()\n",
    "            \n",
    "            # ì´ìƒì¹˜ ì •ë³´ ì €ì¥\n",
    "            if len(sudden_changes) > 0 or len(long_zero_periods) > 0 or len(statistical_outliers) > 0:\n",
    "                anomalies.append({\n",
    "                    'customer': customer,\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'long_zero_periods': len(long_zero_periods),\n",
    "                    'statistical_outliers': len(statistical_outliers)\n",
    "                })\n",
    "        \n",
    "        print(\"ğŸš¨ ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼:\")\n",
    "        if anomalies:\n",
    "            print(\"ê³ ê°ë²ˆí˜¸\\tê¸‰ê²©ë³€í™”\\tì¥ê¸°0ê°’\\tí†µê³„ì´ìƒì¹˜\")\n",
    "            for anomaly in anomalies:\n",
    "                print(f\"{anomaly['customer']}\\t{anomaly['sudden_changes']}\\t{anomaly['long_zero_periods']}\\t{anomaly['statistical_outliers']}\")\n",
    "        else:\n",
    "            print(\"âœ… ì‹¬ê°í•œ ì´ìƒ íŒ¨í„´ ì—†ìŒ\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def generate_enhanced_pattern_summary(self):\n",
    "        \"\"\"ê°•í™”ëœ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š ê°•í™”ëœ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ì™¸ë¶€ ë°ì´í„° ë¡œë”©\n",
    "        self.load_external_data()\n",
    "        \n",
    "        # ë°ì´í„° ê²°í•©\n",
    "        merged_data = self.merge_with_external_data()\n",
    "        \n",
    "        # ê¸°ì¡´ íŒ¨í„´ ë¶„ì„\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        # ê°•í™”ëœ ë¶„ì„ (ì™¸ë¶€ ë°ì´í„° í™œìš©)\n",
    "        weather_impact = self.analyze_weather_impact()\n",
    "        calendar_patterns = self.analyze_calendar_patterns()\n",
    "        \n",
    "        print(\"\\nğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "        \n",
    "        # 1. ì‹œê°„ íŒ¨í„´\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: {peak_hours}ì‹œ\")\n",
    "        \n",
    "        # 2. ìš”ì¼ íŒ¨í„´  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. ê³ ê° ë‹¤ì–‘ì„±\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. ë¶€í•˜ìœ¨\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  â€¢ í‰ê·  ë¶€í•˜ìœ¨: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. ì´ìƒ íŒ¨í„´\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: {anomaly_customers}ëª…\")\n",
    "        \n",
    "        # 6. ê¸°ìƒ ì˜í–¥ (ìˆëŠ” ê²½ìš°)\n",
    "        if weather_impact:\n",
    "            temp_corr = weather_impact.get('í‰ê· ê¸°ì˜¨', {}).get('mean', 0)\n",
    "            humidity_corr = weather_impact.get('í‰ê· ìŠµë„', {}).get('mean', 0)\n",
    "            print(f\"  â€¢ ê¸°ì˜¨ê³¼ ì „ë ¥ì‚¬ìš©ëŸ‰ ìƒê´€ê´€ê³„: {temp_corr:.3f}\")\n",
    "            print(f\"  â€¢ ìŠµë„ì™€ ì „ë ¥ì‚¬ìš©ëŸ‰ ìƒê´€ê´€ê³„: {humidity_corr:.3f}\")\n",
    "        \n",
    "        # 7. ë‹¬ë ¥ íš¨ê³¼ (ìˆëŠ” ê²½ìš°)\n",
    "        if calendar_patterns and calendar_patterns.get('holiday_effect') is not None:\n",
    "            holiday_effect = calendar_patterns['holiday_effect']\n",
    "            if len(holiday_effect) >= 2:\n",
    "                normal_avg = holiday_effect.loc[False, 'mean'] if False in holiday_effect.index else 0\n",
    "                holiday_avg = holiday_effect.loc[True, 'mean'] if True in holiday_effect.index else 0\n",
    "                if normal_avg > 0:\n",
    "                    holiday_ratio = holiday_avg / normal_avg\n",
    "                    print(f\"  â€¢ ì—°íœ´/ì¼ë°˜ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {holiday_ratio:.3f}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ê°•í™”ëœ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\")\n",
    "        print(\"  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\") \n",
    "        print(\"  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\")\n",
    "        print(\"  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
    "        print(\"  5. ê¸°ìƒ ìš”ì¸ ë³´ì • (ì˜¨ë„, ìŠµë„, ê°•ìˆ˜ëŸ‰)\")\n",
    "        print(\"  6. ë‹¬ë ¥ íš¨ê³¼ ë°˜ì˜ (íœ´ì¼, ì—°íœ´, ê³„ì ˆì„±)\")\n",
    "        print(\"  7. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \")\n",
    "        print(\"  8. ì´ìƒ íŒ¨í„´ í•„í„°ë§ ë©”ì»¤ë‹ˆì¦˜\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies,\n",
    "            'weather_impact': weather_impact,\n",
    "            'calendar_patterns': calendar_patterns,\n",
    "            'merged_data': merged_data\n",
    "        }\n",
    "    \n",
    "    def generate_pattern_summary(self):\n",
    "        \"\"\"ê¸°ì¡´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½ (í•˜ìœ„ í˜¸í™˜ì„±)\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ì£¼ìš” íŒ¨í„´ íŠ¹ì„±\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        print(\"\\nğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "        \n",
    "        # 1. ì‹œê°„ íŒ¨í„´\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: {peak_hours}ì‹œ\")\n",
    "        \n",
    "        # 2. ìš”ì¼ íŒ¨í„´  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. ê³ ê° ë‹¤ì–‘ì„±\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. ë¶€í•˜ìœ¨\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  â€¢ í‰ê·  ë¶€í•˜ìœ¨: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. ì´ìƒ íŒ¨í„´\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: {anomaly_customers}ëª…\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\")\n",
    "        print(\"  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\") \n",
    "        print(\"  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\")\n",
    "        print(\"  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
    "        print(\"  5. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies\n",
    "        }\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer()\n",
    "    \n",
    "    print(\"ğŸ”§ ì‹¤ì œ í™˜ê²½ì—ì„œ ì‚¬ìš© ë°©ë²• (3000í˜¸):\")\n",
    "    print(\"\"\"\n",
    "    # 1. ì‹¤ì œ LP ë°ì´í„° ë¡œë”©\n",
    "    lp_files = ['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv']\n",
    "    analyzer.load_real_lp_data(lp_files)\n",
    "    \n",
    "    # 2. ê¸°ìƒ ë° ë‹¬ë ¥ ë°ì´í„° ìë™ ë¡œë”©í•˜ì—¬ ê°•í™”ëœ ë¶„ì„\n",
    "    enhanced_summary = analyzer.generate_enhanced_pattern_summary()\n",
    "    \n",
    "    # generate_enhanced_pattern_summary() ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ:\n",
    "    # - weather_daily_processed.csv ë¡œë”©\n",
    "    # - power_analysis_calendar_2022_2025.csv ë¡œë”©\n",
    "    # - LP ë°ì´í„°ì™€ ê²°í•©í•˜ì—¬ ë¶„ì„\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ§ª í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹œì—°\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    lp_data = analyzer.load_sample_data()\n",
    "    \n",
    "    # ê°•í™”ëœ íŒ¨í„´ ë¶„ì„ ì‹¤í–‰ (weather + calendar ë°ì´í„° í¬í•¨)\n",
    "    print(\"\\nğŸŒ¤ï¸ğŸ“… ê¸°ìƒ ë° ë‹¬ë ¥ ë°ì´í„°ë¥¼ í¬í•¨í•œ ê°•í™”ëœ ë¶„ì„:\")\n",
    "    enhanced_summary = analyzer.generate_enhanced_pattern_summary()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\n",
    "    print(\"  âœ… ì‹¤ì œ í™˜ê²½: ì‹¤ì œ LP ë°ì´í„° + ê¸°ìƒ ë°ì´í„° + ë‹¬ë ¥ ë°ì´í„°\")\n",
    "    print(\"  âœ… weather_daily_processed.csv: ì˜¨ë„, ìŠµë„, ê°•ìˆ˜ëŸ‰ ë“± ê¸°ìƒ ìš”ì¸\")\n",
    "    print(\"  âœ… power_analysis_calendar_2022_2025.csv: íœ´ì¼, ì—°íœ´, í‰ì¼/ì£¼ë§\")\n",
    "    print(\"  âœ… ê²°í•© ë¶„ì„: ê¸°ìƒ/ë‹¬ë ¥ ìš”ì¸ì´ ì „ë ¥ ì‚¬ìš©ëŸ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥\")\n",
    "    print(\"  âœ… ë³€ë™ì„± ì§€í‘œ: ì™¸ë¶€ ìš”ì¸ì„ ê³ ë ¤í•œ ì •êµí•œ ë³€ë™ê³„ìˆ˜ ê°œë°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77981da6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ===\n",
      "âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: 29,760ë ˆì½”ë“œ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ë³€ë™ì„± ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸\n",
      "============================================================\n",
      "\n",
      "=== ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ===\n",
      "ê³ ê°ë²ˆí˜¸\tCV\të²”ìœ„ë³€ë™ì„±\tIQRë³€ë™ì„±\tMADë³€ë™ì„±\tìˆ˜ìµë¥ ë³€ë™ì„±\n",
      "A1001\t0.049\t0.3511\t0.0651\t0.0385\t0.07\n",
      "A1002\t0.4806\t4.2717\t0.4656\t0.3194\tnan\n",
      "A1003\t0.2469\t1.4009\t0.4039\t0.2095\t0.1471\n",
      "A1004\t0.1861\t1.1296\t0.2589\t0.1497\t0.229\n",
      "A1005\t0.242\t1.5726\t0.3375\t0.1944\t0.3202\n",
      "A1006\t0.1341\t1.9821\t0.1355\t0.0888\t0.1678\n",
      "A1007\t0.2917\t2.3547\t0.2276\t0.1914\tnan\n",
      "A1008\t0.4125\t2.3961\t0.5556\t0.3295\tnan\n",
      "A1009\t0.1757\t1.2397\t0.235\t0.1402\t0.2752\n",
      "A1010\t0.3447\t2.3695\t0.4607\t0.2746\tnan\n",
      "\n",
      "=== ì‹œê°„ ìœˆë„ìš°ë³„ ë³€ë™ì„± ë¶„ì„ ===\n",
      "ìœˆë„ìš°ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜:\n",
      "ê³ ê°ë²ˆí˜¸\tì‹œê°„ë³„\tì¼ë³„\tì£¼ë³„\n",
      "A1001\t0.0453\t0.0489\t0.049\n",
      "A1002\t0.3818\t0.4716\t0.4805\n",
      "A1003\t0.0941\t0.2393\t0.2462\n",
      "A1004\t0.138\t0.1473\t0.1495\n",
      "A1005\t0.1825\t0.1958\t0.2031\n",
      "A1006\t0.0985\t0.1259\t0.1332\n",
      "A1007\t0.2312\t0.2437\t0.2844\n",
      "A1008\t0.3991\t0.4132\t0.4118\n",
      "A1009\t0.163\t0.1749\t0.1753\n",
      "A1010\t0.3284\t0.3449\t0.3452\n",
      "\n",
      "=== ë°©í–¥ì„± ë³€ë™ì„± ë¶„ì„ ===\n",
      "ê³ ê°ë²ˆí˜¸\tìƒìŠ¹ë³€ë™ì„±\tí•˜ë½ë³€ë™ì„±\të¹„ëŒ€ì¹­ë¹„ìœ¨\tê¸‰ì¦íšŸìˆ˜\tê¸‰ê°íšŸìˆ˜\n",
      "A1001\t0.0456\t0.0386\t1.1793\t0\t0\n",
      "A1002\tnan\t0.2268\tnan\t676\t370\n",
      "A1003\t0.1041\t0.0767\t1.3563\t13\t0\n",
      "A1004\t0.1792\t0.1065\t1.6822\t105\t4\n",
      "A1005\t0.2735\t0.132\t2.0712\t233\t33\n",
      "A1006\t0.1446\t0.0819\t1.7652\t21\t6\n",
      "A1007\tnan\t0.2183\tnan\t314\t181\n",
      "A1008\tnan\t0.2381\tnan\t782\t409\n",
      "A1009\t0.2223\t0.1193\t1.8638\t160\t15\n",
      "A1010\tnan\t0.2079\tnan\t641\t300\n",
      "\n",
      "=== íŒ¨í„´ ì•ˆì •ì„± ë¶„ì„ ===\n",
      "ê³ ê°ë²ˆí˜¸\tíŒ¨í„´ì¼ê´€ì„±\tì¼ì¼ì£¼ê¸°ì„±\tìê¸°ìƒê´€\n",
      "A1001\t0.0302\t0.6329\t0.9976\n",
      "A1002\t0.0039\t2.0012\t0.8167\n",
      "A1003\t0.9462\t68.7362\t0.9899\n",
      "A1004\t-0.0045\t0.3761\t0.9779\n",
      "A1005\t-0.01\t0.5237\t0.9634\n",
      "A1006\t0.0461\t3.0904\t0.988\n",
      "A1007\t-0.0079\t1.0726\t0.9217\n",
      "A1008\t-0.0115\t1.4058\t0.8554\n",
      "A1009\t0.0084\t1.1471\t0.9701\n",
      "A1010\t-0.0024\t0.6642\t0.8931\n",
      "\n",
      "=== ë³µí•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ê³„ì‚° ===\n",
      "ê³ ê°ë²ˆí˜¸\tê¸°ë³¸\tìœˆë„ìš°\të°©í–¥ì„±\tì•ˆì •ì„±\të³µí•©ì ìˆ˜\n",
      "A1001\t0.049\t0.0477\t0.0421\t0.9698\t0.2314\n",
      "A1002\t0.4806\t0.4446\tnan\t0.9961\tnan\n",
      "A1003\t0.2469\t0.1932\t0.0904\t0.0538\t0.1609\n",
      "A1004\t0.1861\t0.1449\t0.1429\t1.0045\t0.3288\n",
      "A1005\t0.242\t0.1938\t0.2028\t1.01\t0.3733\n",
      "A1006\t0.1341\t0.1192\t0.1133\t0.9539\t0.2894\n",
      "A1007\t0.2917\t0.2531\tnan\t1.0079\tnan\n",
      "A1008\t0.4125\t0.408\tnan\t1.0115\tnan\n",
      "A1009\t0.1757\t0.1711\t0.1708\t0.9916\t0.3365\n",
      "A1010\t0.3447\t0.3395\tnan\t1.0024\tnan\n",
      "\n",
      "ğŸ“Š ë³€ë™ì„± ë“±ê¸‰ ê¸°ì¤€:\n",
      "  ì €ë³€ë™ì„±: < nan\n",
      "  ì¤‘ë³€ë™ì„±: nan ~ nan\n",
      "  ê³ ë³€ë™ì„±: > nan\n",
      "\n",
      "ë“±ê¸‰ë³„ ê³ ê° ë¶„ë¥˜:\n",
      "  A1001: 0.231 (ê³ ë³€ë™ì„±)\n",
      "  A1002: nan (ê³ ë³€ë™ì„±)\n",
      "  A1003: 0.161 (ê³ ë³€ë™ì„±)\n",
      "  A1004: 0.329 (ê³ ë³€ë™ì„±)\n",
      "  A1005: 0.373 (ê³ ë³€ë™ì„±)\n",
      "  A1006: 0.289 (ê³ ë³€ë™ì„±)\n",
      "  A1007: nan (ê³ ë³€ë™ì„±)\n",
      "  A1008: nan (ê³ ë³€ë™ì„±)\n",
      "  A1009: 0.337 (ê³ ë³€ë™ì„±)\n",
      "  A1010: nan (ê³ ë³€ë™ì„±)\n",
      "\n",
      "ğŸ¯ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ì¸ì‚¬ì´íŠ¸:\n",
      "  1. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œì˜ í•„ìš”ì„± í™•ì¸\n",
      "  2. ì‹œê°„ ìœˆë„ìš°ë³„ ì°¨ë³„í™”ëœ ê°€ì¤‘ì¹˜ ì ìš©\n",
      "  3. ë°©í–¥ì„± ë³€ë™ì„±ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ë¹„ëŒ€ì¹­ì„± í¬ì°©\n",
      "  4. íŒ¨í„´ ì•ˆì •ì„±ìœ¼ë¡œ ì˜ˆì¸¡ê°€ëŠ¥ì„± í‰ê°€\n",
      "  5. ë³µí•© ìŠ¤ì½”ì–´ë¥¼ í†µí•œ ì¢…í•©ì  ë³€ë™ì„± í‰ê°€\n",
      "\n",
      "ğŸ’¡ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ë°©í–¥:\n",
      "  â€¢ Level-0 ëª¨ë¸: ê° ë³€ë™ì„± ì§€í‘œë¥¼ ê°œë³„ ëª¨ë¸ë¡œ êµ¬ì„±\n",
      "  â€¢ Level-1 ë©”íƒ€ëª¨ë¸: ê°€ì¤‘ ê²°í•©ìœ¼ë¡œ ìµœì¢… ë³€ë™ê³„ìˆ˜ ì‚°ì¶œ\n",
      "  â€¢ ê³¼ì í•© ë°©ì§€: êµì°¨ê²€ì¦ ë° ì •ê·œí™” ì ìš©\n",
      "\n",
      "ğŸ¯ 3ë‹¨ê³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 4ë‹¨ê³„ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.volatility_metrics = {}\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\"\"\"\n",
    "        print(\"=== ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ===\")\n",
    "        \n",
    "        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” CSV íŒŒì¼ ì½ê¸°\n",
    "        # self.lp_data = pd.concat([\n",
    "        #     pd.read_csv('LPë°ì´í„°1.csv'),\n",
    "        #     pd.read_csv('LPë°ì´í„°2.csv')\n",
    "        # ], ignore_index=True)\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±\n",
    "        self.lp_data = self._create_volatility_test_data()\n",
    "        \n",
    "        # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì¶”ê°€\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "        return self.lp_data\n",
    "    \n",
    "    def _create_volatility_test_data(self):\n",
    "        \"\"\"ë³€ë™ì„± í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        # ë‹¤ì–‘í•œ ë³€ë™ì„± íŒ¨í„´ì„ ê°€ì§„ ê³ ê° ì •ì˜\n",
    "        volatility_profiles = {\n",
    "            'A1001': {'type': 'stable', 'base': 100, 'noise': 0.05},      # ì•ˆì •ì \n",
    "            'A1002': {'type': 'high_volatility', 'base': 80, 'noise': 0.3}, # ê³ ë³€ë™ì„±\n",
    "            'A1003': {'type': 'periodic', 'base': 120, 'noise': 0.1},     # ì£¼ê¸°ì \n",
    "            'A1004': {'type': 'trending', 'base': 90, 'noise': 0.15},     # íŠ¸ë Œë“œ\n",
    "            'A1005': {'type': 'seasonal', 'base': 110, 'noise': 0.2},     # ê³„ì ˆì„±\n",
    "            'A1006': {'type': 'jumpy', 'base': 85, 'noise': 0.1},         # ì í”„í˜•\n",
    "            'A1007': {'type': 'clustered', 'base': 95, 'noise': 0.25},    # í´ëŸ¬ìŠ¤í„°í˜•\n",
    "            'A1008': {'type': 'low_usage', 'base': 30, 'noise': 0.4},     # ì €ì‚¬ìš©ëŸ‰\n",
    "            'A1009': {'type': 'medium', 'base': 70, 'noise': 0.18},       # ì¤‘ê°„\n",
    "            'A1010': {'type': 'irregular', 'base': 105, 'noise': 0.35}    # ë¶ˆê·œì¹™\n",
    "        }\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = volatility_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ê¸°ë³¸ íŒ¨í„´ ìƒì„±\n",
    "                        base_power = profile['base']\n",
    "                        \n",
    "                        # íƒ€ì…ë³„ íŒ¨í„´ ì ìš©\n",
    "                        if profile['type'] == 'stable':\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'high_volatility':\n",
    "                            # ë†’ì€ ë³€ë™ì„± - í° ë¬´ì‘ìœ„ ë³€í™”\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                            if np.random.random() < 0.1:  # 10% í™•ë¥ ë¡œ í° ì í”„\n",
    "                                power *= np.random.choice([0.3, 2.5])\n",
    "                        \n",
    "                        elif profile['type'] == 'periodic':\n",
    "                            # ì£¼ê¸°ì  íŒ¨í„´\n",
    "                            daily_cycle = np.sin(2 * np.pi * hour / 24)\n",
    "                            weekly_cycle = np.sin(2 * np.pi * day / 7)\n",
    "                            power = base_power * (1 + 0.3 * daily_cycle + 0.1 * weekly_cycle)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'trending':\n",
    "                            # íŠ¸ë Œë“œ íŒ¨í„´\n",
    "                            trend = 0.5 * day / days  # 30ì¼ê°„ 50% ì¦ê°€\n",
    "                            power = base_power * (1 + trend)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'seasonal':\n",
    "                            # ê³„ì ˆì„± íŒ¨í„´\n",
    "                            seasonal = 0.2 * np.sin(2 * np.pi * day / 30)\n",
    "                            power = base_power * (1 + seasonal)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'jumpy':\n",
    "                            # ê¸‰ê²©í•œ ë³€í™”ê°€ ìˆëŠ” íŒ¨í„´\n",
    "                            power = base_power\n",
    "                            if day % 7 == 0 and hour == 9:  # ì£¼ 1íšŒ í° ë³€í™”\n",
    "                                power *= 2.0\n",
    "                            elif day % 7 == 3 and hour == 15:\n",
    "                                power *= 0.4\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'clustered':\n",
    "                            # ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ (ë³€ë™ì„±ì´ ë†’ì€ êµ¬ê°„ê³¼ ë‚®ì€ êµ¬ê°„)\n",
    "                            if day % 10 < 3:  # 30% ê¸°ê°„ ë™ì•ˆ ë†’ì€ ë³€ë™ì„±\n",
    "                                noise_factor = profile['noise'] * 2\n",
    "                            else:\n",
    "                                noise_factor = profile['noise'] * 0.5\n",
    "                            power = base_power + np.random.normal(0, base_power * noise_factor)\n",
    "                        \n",
    "                        else:  # irregular, low_usage, medium\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        power = max(0, power)  # ìŒìˆ˜ ë°©ì§€\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(power * np.random.uniform(0.1, 0.3), 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(power * np.random.uniform(0.05, 0.15), 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(power * np.random.uniform(1.0, 1.1), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_basic_volatility_metrics(self):\n",
    "        \"\"\"ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ===\")\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            # 1. ì „í†µì  ë³€ë™ê³„ìˆ˜ (CV)\n",
    "            cv = customer_data.std() / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 2. ë²”ìœ„ ê¸°ë°˜ ë³€ë™ì„±\n",
    "            range_volatility = (customer_data.max() - customer_data.min()) / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 3. ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë³€ë™ì„± (IQR/Median)\n",
    "            q75, q25 = np.percentile(customer_data, [75, 25])\n",
    "            iqr_volatility = (q75 - q25) / np.median(customer_data) if np.median(customer_data) > 0 else 0\n",
    "            \n",
    "            # 4. í‰ê· ì ˆëŒ€í¸ì°¨ (MAD)\n",
    "            mad = np.mean(np.abs(customer_data - customer_data.mean()))\n",
    "            mad_volatility = mad / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 5. ë³€í™”ìœ¨ ê¸°ë°˜ ë³€ë™ì„±\n",
    "            returns = customer_data.pct_change().dropna()\n",
    "            return_volatility = returns.std() if len(returns) > 0 else 0\n",
    "            \n",
    "            metrics[customer] = {\n",
    "                'cv': round(cv, 4),\n",
    "                'range_vol': round(range_volatility, 4),\n",
    "                'iqr_vol': round(iqr_volatility, 4),\n",
    "                'mad_vol': round(mad_volatility, 4),\n",
    "                'return_vol': round(return_volatility, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tCV\\të²”ìœ„ë³€ë™ì„±\\tIQRë³€ë™ì„±\\tMADë³€ë™ì„±\\tìˆ˜ìµë¥ ë³€ë™ì„±\")\n",
    "        for customer, metrics_dict in metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['cv']}\\t{metrics_dict['range_vol']}\\t{metrics_dict['iqr_vol']}\\t{metrics_dict['mad_vol']}\\t{metrics_dict['return_vol']}\")\n",
    "        \n",
    "        self.volatility_metrics['basic'] = metrics\n",
    "        return metrics\n",
    "    \n",
    "    def calculate_time_window_volatility(self):\n",
    "        \"\"\"ì‹œê°„ ìœˆë„ìš°ë³„ ë³€ë™ì„± ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ì‹œê°„ ìœˆë„ìš°ë³„ ë³€ë™ì„± ë¶„ì„ ===\")\n",
    "        \n",
    "        window_metrics = {}\n",
    "        windows = {\n",
    "            'hourly': 4,    # 1ì‹œê°„ (4ê°œ 15ë¶„ êµ¬ê°„)\n",
    "            'daily': 96,    # 1ì¼ (96ê°œ 15ë¶„ êµ¬ê°„)\n",
    "            'weekly': 672   # 1ì£¼ (672ê°œ 15ë¶„ êµ¬ê°„)\n",
    "        }\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            window_metrics[customer] = {}\n",
    "            \n",
    "            for window_name, window_size in windows.items():\n",
    "                # ë¡¤ë§ ìœˆë„ìš°ë¡œ ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "                rolling_std = power_series.rolling(window=window_size, min_periods=window_size//2).std()\n",
    "                rolling_mean = power_series.rolling(window=window_size, min_periods=window_size//2).mean()\n",
    "                rolling_cv = rolling_std / rolling_mean\n",
    "                \n",
    "                # ìœˆë„ìš°ë³„ ë³€ë™ì„± í†µê³„\n",
    "                window_metrics[customer][window_name] = {\n",
    "                    'mean_cv': round(rolling_cv.mean(), 4),\n",
    "                    'std_cv': round(rolling_cv.std(), 4),\n",
    "                    'max_cv': round(rolling_cv.max(), 4),\n",
    "                    'min_cv': round(rolling_cv.min(), 4)\n",
    "                }\n",
    "        \n",
    "        print(\"ìœˆë„ìš°ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜:\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tì‹œê°„ë³„\\tì¼ë³„\\tì£¼ë³„\")\n",
    "        for customer in window_metrics.keys():\n",
    "            hourly_cv = window_metrics[customer]['hourly']['mean_cv']\n",
    "            daily_cv = window_metrics[customer]['daily']['mean_cv']\n",
    "            weekly_cv = window_metrics[customer]['weekly']['mean_cv']\n",
    "            print(f\"{customer}\\t{hourly_cv}\\t{daily_cv}\\t{weekly_cv}\")\n",
    "        \n",
    "        self.volatility_metrics['time_windows'] = window_metrics\n",
    "        return window_metrics\n",
    "    \n",
    "    def calculate_directional_volatility(self):\n",
    "        \"\"\"ë°©í–¥ì„± ë³€ë™ì„± ë¶„ì„ (ìƒìŠ¹/í•˜ë½ ë¹„ëŒ€ì¹­ì„±)\"\"\"\n",
    "        print(\"\\n=== ë°©í–¥ì„± ë³€ë™ì„± ë¶„ì„ ===\")\n",
    "        \n",
    "        directional_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            # ë³€í™”ìœ¨ ê³„ì‚°\n",
    "            returns = power_series.pct_change().dropna()\n",
    "            \n",
    "            # ìƒìŠ¹/í•˜ë½ ë¶„ë¦¬\n",
    "            upside_returns = returns[returns > 0]\n",
    "            downside_returns = returns[returns < 0]\n",
    "            \n",
    "            # ë°©í–¥ë³„ ë³€ë™ì„±\n",
    "            upside_volatility = upside_returns.std() if len(upside_returns) > 0 else 0\n",
    "            downside_volatility = abs(downside_returns.std()) if len(downside_returns) > 0 else 0\n",
    "            \n",
    "            # ë¹„ëŒ€ì¹­ì„± ì§€ìˆ˜\n",
    "            asymmetry_ratio = upside_volatility / downside_volatility if downside_volatility > 0 else 0\n",
    "            \n",
    "            # ê¸‰ê²©í•œ ë³€í™” íšŸìˆ˜\n",
    "            large_increases = len(returns[returns > 0.5])  # 50% ì´ìƒ ì¦ê°€\n",
    "            large_decreases = len(returns[returns < -0.5]) # 50% ì´ìƒ ê°ì†Œ\n",
    "            \n",
    "            directional_metrics[customer] = {\n",
    "                'upside_vol': round(upside_volatility, 4),\n",
    "                'downside_vol': round(downside_volatility, 4),\n",
    "                'asymmetry_ratio': round(asymmetry_ratio, 4),\n",
    "                'large_increases': large_increases,\n",
    "                'large_decreases': large_decreases\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tìƒìŠ¹ë³€ë™ì„±\\tí•˜ë½ë³€ë™ì„±\\të¹„ëŒ€ì¹­ë¹„ìœ¨\\tê¸‰ì¦íšŸìˆ˜\\tê¸‰ê°íšŸìˆ˜\")\n",
    "        for customer, metrics_dict in directional_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['upside_vol']}\\t{metrics_dict['downside_vol']}\\t{metrics_dict['asymmetry_ratio']}\\t{metrics_dict['large_increases']}\\t{metrics_dict['large_decreases']}\")\n",
    "        \n",
    "        self.volatility_metrics['directional'] = directional_metrics\n",
    "        return directional_metrics\n",
    "    \n",
    "    def calculate_pattern_stability(self):\n",
    "        \"\"\"íŒ¨í„´ ì•ˆì •ì„± ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== íŒ¨í„´ ì•ˆì •ì„± ë¶„ì„ ===\")\n",
    "        \n",
    "        stability_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "            \n",
    "            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ì¼ê´€ì„±\n",
    "            hourly_patterns = []\n",
    "            for day in customer_data['date'].unique():\n",
    "                day_data = customer_data[customer_data['date'] == day]\n",
    "                hourly_avg = day_data.groupby('hour')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "                hourly_patterns.append(hourly_avg.values)\n",
    "            \n",
    "            # íŒ¨í„´ ê°„ ìƒê´€ê´€ê³„ (ì¼ê´€ì„± ì¸¡ì •)\n",
    "            if len(hourly_patterns) > 1:\n",
    "                correlations = []\n",
    "                for i in range(len(hourly_patterns)):\n",
    "                    for j in range(i+1, len(hourly_patterns)):\n",
    "                        if len(hourly_patterns[i]) == len(hourly_patterns[j]):\n",
    "                            corr = np.corrcoef(hourly_patterns[i], hourly_patterns[j])[0,1]\n",
    "                            if not np.isnan(corr):\n",
    "                                correlations.append(corr)\n",
    "                \n",
    "                pattern_consistency = np.mean(correlations) if correlations else 0\n",
    "            else:\n",
    "                pattern_consistency = 0\n",
    "            \n",
    "            # ì£¼ê¸°ì„± ê°•ë„ (FFT ê¸°ë°˜)\n",
    "            power_series = customer_data.sort_values('datetime')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values\n",
    "            if len(power_series) > 100:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ\n",
    "                fft = np.fft.fft(power_series)\n",
    "                fft_magnitude = np.abs(fft)\n",
    "                \n",
    "                # ì¼ì¼ ì£¼ê¸° (96í¬ì¸íŠ¸) ê°•ë„\n",
    "                daily_freq_idx = len(fft_magnitude) // 96 if len(fft_magnitude) >= 96 else 1\n",
    "                daily_periodicity = fft_magnitude[daily_freq_idx] / np.mean(fft_magnitude) if np.mean(fft_magnitude) > 0 else 0\n",
    "            else:\n",
    "                daily_periodicity = 0\n",
    "            \n",
    "            # ì˜ˆì¸¡ê°€ëŠ¥ì„± (ìê¸°ìƒê´€)\n",
    "            autocorr_1lag = power_series[1:].dot(power_series[:-1]) / (np.linalg.norm(power_series[1:]) * np.linalg.norm(power_series[:-1])) if len(power_series) > 1 else 0\n",
    "            \n",
    "            stability_metrics[customer] = {\n",
    "                'pattern_consistency': round(pattern_consistency, 4),\n",
    "                'daily_periodicity': round(daily_periodicity, 4),\n",
    "                'autocorrelation': round(autocorr_1lag, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tíŒ¨í„´ì¼ê´€ì„±\\tì¼ì¼ì£¼ê¸°ì„±\\tìê¸°ìƒê´€\")\n",
    "        for customer, metrics_dict in stability_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['pattern_consistency']}\\t{metrics_dict['daily_periodicity']}\\t{metrics_dict['autocorrelation']}\")\n",
    "        \n",
    "        self.volatility_metrics['stability'] = stability_metrics\n",
    "        return stability_metrics\n",
    "    \n",
    "    def create_composite_volatility_score(self):\n",
    "        \"\"\"ë³µí•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ìƒì„±\"\"\"\n",
    "        print(\"\\n=== ë³µí•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ê³„ì‚° ===\")\n",
    "        \n",
    "        if not all(key in self.volatility_metrics for key in ['basic', 'time_windows', 'directional', 'stability']):\n",
    "            print(\"âŒ ëª¨ë“  ë³€ë™ì„± ì§€í‘œë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        composite_scores = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° (0-1 ì •ê·œí™”)\n",
    "            basic_score = self.volatility_metrics['basic'][customer]['cv']\n",
    "            window_score = np.mean([\n",
    "                self.volatility_metrics['time_windows'][customer]['hourly']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['daily']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['weekly']['mean_cv']\n",
    "            ])\n",
    "            directional_score = (\n",
    "                self.volatility_metrics['directional'][customer]['upside_vol'] + \n",
    "                self.volatility_metrics['directional'][customer]['downside_vol']\n",
    "            ) / 2\n",
    "            \n",
    "            # ì•ˆì •ì„±ì€ ì—­ìˆ˜ë¡œ (ë‚®ì€ ì•ˆì •ì„± = ë†’ì€ ë³€ë™ì„±)\n",
    "            stability_score = 1 - self.volatility_metrics['stability'][customer]['pattern_consistency']\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "            weights = {\n",
    "                'basic': 0.3,       # ê¸°ë³¸ ë³€ë™ê³„ìˆ˜\n",
    "                'window': 0.3,      # ì‹œê°„ ìœˆë„ìš° ë³€ë™ì„±\n",
    "                'directional': 0.2, # ë°©í–¥ì„± ë³€ë™ì„±\n",
    "                'stability': 0.2    # íŒ¨í„´ ë¶ˆì•ˆì •ì„±\n",
    "            }\n",
    "            \n",
    "            composite_score = (\n",
    "                weights['basic'] * basic_score +\n",
    "                weights['window'] * window_score +\n",
    "                weights['directional'] * directional_score +\n",
    "                weights['stability'] * stability_score\n",
    "            )\n",
    "            \n",
    "            composite_scores[customer] = {\n",
    "                'basic_score': round(basic_score, 4),\n",
    "                'window_score': round(window_score, 4),\n",
    "                'directional_score': round(directional_score, 4),\n",
    "                'stability_score': round(stability_score, 4),\n",
    "                'composite_score': round(composite_score, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tê¸°ë³¸\\tìœˆë„ìš°\\të°©í–¥ì„±\\tì•ˆì •ì„±\\të³µí•©ì ìˆ˜\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            print(f\"{customer}\\t{scores['basic_score']}\\t{scores['window_score']}\\t{scores['directional_score']}\\t{scores['stability_score']}\\t{scores['composite_score']}\")\n",
    "        \n",
    "        # ë³€ë™ì„± ë“±ê¸‰ ë¶„ë¥˜\n",
    "        composite_values = [scores['composite_score'] for scores in composite_scores.values()]\n",
    "        percentiles = np.percentile(composite_values, [33, 67])\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ë³€ë™ì„± ë“±ê¸‰ ê¸°ì¤€:\")\n",
    "        print(f\"  ì €ë³€ë™ì„±: < {percentiles[0]:.3f}\")\n",
    "        print(f\"  ì¤‘ë³€ë™ì„±: {percentiles[0]:.3f} ~ {percentiles[1]:.3f}\")\n",
    "        print(f\"  ê³ ë³€ë™ì„±: > {percentiles[1]:.3f}\")\n",
    "        \n",
    "        print(\"\\në“±ê¸‰ë³„ ê³ ê° ë¶„ë¥˜:\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            score = scores['composite_score']\n",
    "            if score < percentiles[0]:\n",
    "                grade = \"ì €ë³€ë™ì„±\"\n",
    "            elif score < percentiles[1]:\n",
    "                grade = \"ì¤‘ë³€ë™ì„±\"\n",
    "            else:\n",
    "                grade = \"ê³ ë³€ë™ì„±\"\n",
    "            print(f\"  {customer}: {score:.3f} ({grade})\")\n",
    "        \n",
    "        self.volatility_metrics['composite'] = composite_scores\n",
    "        return composite_scores\n",
    "    \n",
    "    def generate_volatility_report(self):\n",
    "        \"\"\"ë³€ë™ì„± ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š ë³€ë™ì„± ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ëª¨ë“  ì§€í‘œ ê³„ì‚°\n",
    "        basic_metrics = self.calculate_basic_volatility_metrics()\n",
    "        window_metrics = self.calculate_time_window_volatility()\n",
    "        directional_metrics = self.calculate_directional_volatility()\n",
    "        stability_metrics = self.calculate_pattern_stability()\n",
    "        composite_scores = self.create_composite_volatility_score()\n",
    "        \n",
    "        print(\"\\nğŸ¯ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"  1. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œì˜ í•„ìš”ì„± í™•ì¸\")\n",
    "        print(\"  2. ì‹œê°„ ìœˆë„ìš°ë³„ ì°¨ë³„í™”ëœ ê°€ì¤‘ì¹˜ ì ìš©\")\n",
    "        print(\"  3. ë°©í–¥ì„± ë³€ë™ì„±ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ë¹„ëŒ€ì¹­ì„± í¬ì°©\")\n",
    "        print(\"  4. íŒ¨í„´ ì•ˆì •ì„±ìœ¼ë¡œ ì˜ˆì¸¡ê°€ëŠ¥ì„± í‰ê°€\")\n",
    "        print(\"  5. ë³µí•© ìŠ¤ì½”ì–´ë¥¼ í†µí•œ ì¢…í•©ì  ë³€ë™ì„± í‰ê°€\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ë°©í–¥:\")\n",
    "        print(\"  â€¢ Level-0 ëª¨ë¸: ê° ë³€ë™ì„± ì§€í‘œë¥¼ ê°œë³„ ëª¨ë¸ë¡œ êµ¬ì„±\")\n",
    "        print(\"  â€¢ Level-1 ë©”íƒ€ëª¨ë¸: ê°€ì¤‘ ê²°í•©ìœ¼ë¡œ ìµœì¢… ë³€ë™ê³„ìˆ˜ ì‚°ì¶œ\")\n",
    "        print(\"  â€¢ ê³¼ì í•© ë°©ì§€: êµì°¨ê²€ì¦ ë° ì •ê·œí™” ì ìš©\")\n",
    "        \n",
    "        return {\n",
    "            'basic': basic_metrics,\n",
    "            'time_windows': window_metrics,\n",
    "            'directional': directional_metrics,\n",
    "            'stability': stability_metrics,\n",
    "            'composite': composite_scores\n",
    "        }\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCOVolatilityAnalyzer()\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë”©\n",
    "    data = analyzer.load_and_prepare_data()\n",
    "    \n",
    "    # ì¢…í•© ë³€ë™ì„± ë¶„ì„\n",
    "    volatility_report = analyzer.generate_volatility_report()\n",
    "    \n",
    "    print(\"\\nğŸ¯ 3ë‹¨ê³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ!\")\n",
    "    print(\"ë‹¤ìŒ: 4ë‹¨ê³„ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityStackingModel:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜\n",
    "    \n",
    "    Level-0: ë‹¤ì–‘í•œ ë³€ë™ì„± ì§€í‘œë“¤ì„ ê°œë³„ ëª¨ë¸ë¡œ êµ¬ì„±\n",
    "    Level-1: ë©”íƒ€ëª¨ë¸ë¡œ ìµœì¢… ë³€ë™ê³„ìˆ˜ ì‚°ì¶œ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.level0_models = {}\n",
    "        self.level1_model = None\n",
    "        self.scaler = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Level-0 ëª¨ë¸ ì •ì˜\n",
    "        self._initialize_level0_models()\n",
    "        \n",
    "        # ê³¼ì í•© ë°©ì§€ ì„¤ì •\n",
    "        self.cv_folds = 5\n",
    "        self.random_state = 42\n",
    "        \n",
    "    def _initialize_level0_models(self):\n",
    "        \"\"\"Level-0 ê¸°ë³¸ ëª¨ë¸ë“¤ ì´ˆê¸°í™”\"\"\"\n",
    "        self.level0_models = {\n",
    "            'traditional_cv': self._traditional_cv_model,\n",
    "            'range_volatility': self._range_volatility_model,\n",
    "            'iqr_volatility': self._iqr_volatility_model,\n",
    "            'mad_volatility': self._mad_volatility_model,\n",
    "            'return_volatility': self._return_volatility_model,\n",
    "            'window_volatility': self._window_volatility_model,\n",
    "            'percentile_volatility': self._percentile_volatility_model\n",
    "        }\n",
    "        \n",
    "    def _traditional_cv_model(self, data):\n",
    "        \"\"\"ì „í†µì  ë³€ë™ê³„ìˆ˜ ëª¨ë¸\"\"\"\n",
    "        return data.std() / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _range_volatility_model(self, data):\n",
    "        \"\"\"ë²”ìœ„ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        return (data.max() - data.min()) / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _iqr_volatility_model(self, data):\n",
    "        \"\"\"ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        q75, q25 = np.percentile(data, [75, 25])\n",
    "        median = np.median(data)\n",
    "        return (q75 - q25) / median if median > 0 else 0\n",
    "    \n",
    "    def _mad_volatility_model(self, data):\n",
    "        \"\"\"í‰ê· ì ˆëŒ€í¸ì°¨ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        mad = np.mean(np.abs(data - data.mean()))\n",
    "        return mad / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _return_volatility_model(self, data):\n",
    "        \"\"\"ìˆ˜ìµë¥  ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return 0\n",
    "        returns = data.pct_change().dropna()\n",
    "        return returns.std() if len(returns) > 0 else 0\n",
    "    \n",
    "    def _window_volatility_model(self, data, window_size=96):\n",
    "        \"\"\"ìœˆë„ìš° ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸ (ì¼ë³„)\"\"\"\n",
    "        if len(data) < window_size:\n",
    "            return self._traditional_cv_model(data)\n",
    "        \n",
    "        rolling_cv = []\n",
    "        for i in range(window_size, len(data) + 1):\n",
    "            window_data = data.iloc[i-window_size:i]\n",
    "            cv = self._traditional_cv_model(window_data)\n",
    "            rolling_cv.append(cv)\n",
    "        \n",
    "        return np.mean(rolling_cv) if rolling_cv else 0\n",
    "    \n",
    "    def _percentile_volatility_model(self, data):\n",
    "        \"\"\"ë°±ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        p90 = np.percentile(data, 90)\n",
    "        p10 = np.percentile(data, 10)\n",
    "        p50 = np.percentile(data, 50)\n",
    "        return (p90 - p10) / p50 if p50 > 0 else 0\n",
    "    \n",
    "    def extract_features(self, lp_data):\n",
    "        \"\"\"\n",
    "        LP ë°ì´í„°ì—ì„œ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ\n",
    "        \n",
    "        Parameters:\n",
    "        lp_data: DataFrame with columns ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LPìˆ˜ì‹ ì¼ì', 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥', ...]\n",
    "        \n",
    "        Returns:\n",
    "        features_df: DataFrame with volatility features for each customer\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”§ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ ì¤‘...\")\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for customer in lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "            power_data = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            # ì‹œê°„ ì •ë³´ ì¶”ê°€\n",
    "            if 'datetime' not in customer_data.columns:\n",
    "                customer_data = customer_data.copy()\n",
    "                customer_data['datetime'] = pd.to_datetime(customer_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "                customer_data['hour'] = customer_data['datetime'].dt.hour\n",
    "                customer_data['weekday'] = customer_data['datetime'].dt.weekday\n",
    "            \n",
    "            # Level-0 ëª¨ë¸ë“¤ë¡œ íŠ¹ì„± ê³„ì‚°\n",
    "            features = {'customer_id': customer}\n",
    "            \n",
    "            for model_name, model_func in self.level0_models.items():\n",
    "                try:\n",
    "                    if model_name == 'window_volatility':\n",
    "                        features[model_name] = model_func(power_data, window_size=96)\n",
    "                    else:\n",
    "                        features[model_name] = model_func(power_data)\n",
    "                except:\n",
    "                    features[model_name] = 0\n",
    "            \n",
    "            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„±\n",
    "            features.update({\n",
    "                'mean_usage': power_data.mean(),\n",
    "                'total_usage': power_data.sum(),\n",
    "                'peak_ratio': power_data.max() / power_data.mean() if power_data.mean() > 0 else 0,\n",
    "                'zero_ratio': (power_data == 0).sum() / len(power_data),\n",
    "                'weekend_effect': self._calculate_weekend_effect(customer_data),\n",
    "                'peak_hour_concentration': self._calculate_peak_concentration(customer_data)\n",
    "            })\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        print(f\"âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(features_df)}ëª… ê³ ê°, {len(features_df.columns)-1}ê°œ íŠ¹ì„±\")\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def _calculate_weekend_effect(self, customer_data):\n",
    "        \"\"\"ì£¼ë§ íš¨ê³¼ ê³„ì‚°\"\"\"\n",
    "        try:\n",
    "            weekday_avg = customer_data[customer_data['weekday'] < 5]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            weekend_avg = customer_data[customer_data['weekday'] >= 5]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            \n",
    "            if weekday_avg > 0:\n",
    "                return abs(weekend_avg / weekday_avg - 1)\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _calculate_peak_concentration(self, customer_data):\n",
    "        \"\"\"í”¼í¬ ì‹œê°„ ì§‘ì¤‘ë„ ê³„ì‚°\"\"\"\n",
    "        try:\n",
    "            peak_hours = [9, 14, 18]  # ì¼ë°˜ì ì¸ í”¼í¬ ì‹œê°„\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            total_avg = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            \n",
    "            return peak_usage / total_avg if total_avg > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def prepare_training_data(self, features_df, target_column='traditional_cv'):\n",
    "        \"\"\"\n",
    "        í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„\n",
    "        target_column: íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ëª…\n",
    "        \n",
    "        Returns:\n",
    "        X, y: íŠ¹ì„± í–‰ë ¬ê³¼ íƒ€ê²Ÿ ë²¡í„°\n",
    "        \"\"\"\n",
    "        # íŠ¹ì„± ì„ íƒ (Level-0 ëª¨ë¸ ì¶œë ¥ë“¤)\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        y = features_df[target_column].copy()\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        X = X.fillna(0)\n",
    "        y = y.fillna(0)\n",
    "        \n",
    "        # ì´ìƒì¹˜ ì²˜ë¦¬ (ìƒìœ„ 5%, í•˜ìœ„ 5% í´ë¦¬í•‘)\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                lower_bound = X[col].quantile(0.05)\n",
    "                upper_bound = X[col].quantile(0.95)\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X.shape[0]}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨\n",
    "        \n",
    "        Parameters:\n",
    "        X: íŠ¹ì„± í–‰ë ¬\n",
    "        y: íƒ€ê²Ÿ ë²¡í„° (ì‹¤ì œ ë³€ë™ê³„ìˆ˜)\n",
    "        \"\"\"\n",
    "        print(\"ğŸš€ ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "        \n",
    "        # ë°ì´í„° ì •ê·œí™”\n",
    "        self.scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ëŸ¬\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Level-1 ë©”íƒ€ëª¨ë¸ í›„ë³´ë“¤\n",
    "        meta_models = {\n",
    "            'linear': LinearRegression(),\n",
    "            'ridge': Ridge(alpha=1.0, random_state=self.random_state),\n",
    "            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state),\n",
    "            'rf': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=self.random_state),\n",
    "            'gbm': GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        # êµì°¨ê²€ì¦ìœ¼ë¡œ ìµœì  ë©”íƒ€ëª¨ë¸ ì„ íƒ\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "        best_model_name = None\n",
    "        \n",
    "        # ì‹œê³„ì—´ êµì°¨ê²€ì¦\n",
    "        tscv = TimeSeriesSplit(n_splits=self.cv_folds)\n",
    "        \n",
    "        print(\"ğŸ” ë©”íƒ€ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "        print(\"ëª¨ë¸ëª…\\t\\tMAE\\t\\tMSE\\t\\tRÂ²\")\n",
    "        \n",
    "        for model_name, model in meta_models.items():\n",
    "            try:\n",
    "                # êµì°¨ê²€ì¦ ì ìˆ˜ ê³„ì‚°\n",
    "                mae_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "                mse_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "                r2_scores = cross_val_score(model, X_scaled, y, cv=tscv, scoring='r2')\n",
    "                \n",
    "                avg_mae = np.mean(mae_scores)\n",
    "                avg_mse = np.mean(mse_scores)\n",
    "                avg_r2 = np.mean(r2_scores)\n",
    "                \n",
    "                print(f\"{model_name:<12}\\t{avg_mae:.4f}\\t\\t{avg_mse:.4f}\\t\\t{avg_r2:.4f}\")\n",
    "                \n",
    "                # RÂ² ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ\n",
    "                if avg_r2 > best_score:\n",
    "                    best_score = avg_r2\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{model_name:<12}\\tì—ëŸ¬: {str(e)[:30]}...\")\n",
    "        \n",
    "        # ìµœì  ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° í›ˆë ¨\n",
    "        if best_model is not None:\n",
    "            self.level1_model = best_model\n",
    "            self.level1_model.fit(X_scaled, y)\n",
    "            \n",
    "            print(f\"\\nğŸ† ìµœì  ë©”íƒ€ëª¨ë¸: {best_model_name} (RÂ² = {best_score:.4f})\")\n",
    "            \n",
    "            # íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "            if hasattr(self.level1_model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': self.level1_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\nğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 5ê°œ):\")\n",
    "                for _, row in importance_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "            \n",
    "            elif hasattr(self.level1_model, 'coef_'):\n",
    "                coef_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'coefficient': self.level1_model.coef_\n",
    "                }).sort_values('coefficient', key=abs, ascending=False)\n",
    "                \n",
    "                print(\"\\nğŸ“Š ëª¨ë¸ ê³„ìˆ˜ (ì ˆëŒ“ê°’ ìƒìœ„ 5ê°œ):\")\n",
    "                for _, row in coef_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['coefficient']:.4f}\")\n",
    "            \n",
    "            self.is_fitted = True\n",
    "            print(\"âœ… ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"âŒ ì í•©í•œ ë©”íƒ€ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, features_df):\n",
    "        \"\"\"\n",
    "        ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„\n",
    "        \n",
    "        Returns:\n",
    "        predictions: ì˜ˆì¸¡ëœ ë³€ë™ê³„ìˆ˜\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "        \n",
    "        # í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ íŠ¹ì„± ì„ íƒ\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # ì •ê·œí™”\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # ì˜ˆì¸¡\n",
    "        predictions = self.level1_model.predict(X_scaled)\n",
    "        \n",
    "        # ìŒìˆ˜ ë°©ì§€\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ì˜ˆì¸¡\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        y_pred = self.level1_model.predict(X_test_scaled)\n",
    "        y_pred = np.maximum(y_pred, 0)  # ìŒìˆ˜ ë°©ì§€\n",
    "        \n",
    "        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚°\n",
    "        relative_errors = np.abs((y_test - y_pred) / (y_test + 1e-8))\n",
    "        mape = np.mean(relative_errors) * 100\n",
    "        \n",
    "        print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\")\n",
    "        print(f\"  MAE (í‰ê· ì ˆëŒ€ì˜¤ì°¨): {mae:.4f}\")\n",
    "        print(f\"  MSE (í‰ê· ì œê³±ì˜¤ì°¨): {mse:.4f}\")\n",
    "        print(f\"  RMSE (ì œê³±ê·¼í‰ê· ì œê³±ì˜¤ì°¨): {rmse:.4f}\")\n",
    "        print(f\"  RÂ² (ê²°ì •ê³„ìˆ˜): {r2:.4f}\")\n",
    "        print(f\"  MAPE (í‰ê· ì ˆëŒ€ë°±ë¶„ìœ¨ì˜¤ì°¨): {mape:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape\n",
    "        }\n",
    "    \n",
    "    def detect_anomalous_volatility(self, features_df, threshold_percentile=95):\n",
    "        \"\"\"\n",
    "        ì´ìƒ ë³€ë™ì„± íƒì§€\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„\n",
    "        threshold_percentile: ì´ìƒì¹˜ ê¸°ì¤€ ë°±ë¶„ìœ„ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "        anomaly_results: ì´ìƒì¹˜ íƒì§€ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(\"ğŸš¨ ì´ìƒ ë³€ë™ì„± íƒì§€ ì¤‘...\")\n",
    "        \n",
    "        # ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # ì´ìƒì¹˜ ê¸°ì¤€ ì„¤ì •\n",
    "        threshold = np.percentile(predicted_volatility, threshold_percentile)\n",
    "        \n",
    "        # ì´ìƒì¹˜ íƒì§€\n",
    "        anomaly_mask = predicted_volatility > threshold\n",
    "        \n",
    "        anomaly_results = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'is_anomaly': anomaly_mask,\n",
    "            'anomaly_score': (predicted_volatility - threshold) / threshold\n",
    "        })\n",
    "        \n",
    "        anomaly_customers = anomaly_results[anomaly_results['is_anomaly']]\n",
    "        \n",
    "        print(f\"ğŸ¯ íƒì§€ ê²°ê³¼:\")\n",
    "        print(f\"  ì „ì²´ ê³ ê°: {len(anomaly_results)}ëª…\")\n",
    "        print(f\"  ì´ìƒ ë³€ë™ì„± ê³ ê°: {len(anomaly_customers)}ëª…\")\n",
    "        print(f\"  ì´ìƒì¹˜ ë¹„ìœ¨: {len(anomaly_customers)/len(anomaly_results)*100:.1f}%\")\n",
    "        \n",
    "        if len(anomaly_customers) > 0:\n",
    "            print(f\"\\nâš ï¸ ì£¼ì˜ í•„ìš” ê³ ê°:\")\n",
    "            for _, row in anomaly_customers.sort_values('anomaly_score', ascending=False).iterrows():\n",
    "                print(f\"  {row['customer_id']}: ë³€ë™ê³„ìˆ˜ {row['predicted_volatility']:.3f} (ì ìˆ˜: {row['anomaly_score']:.2f})\")\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def generate_volatility_report(self, features_df):\n",
    "        \"\"\"ì¢…í•© ë³€ë™ì„± ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # í†µê³„ ìš”ì•½\n",
    "        print(\"\\nğŸ“Š ë³€ë™ê³„ìˆ˜ ë¶„í¬:\")\n",
    "        print(f\"  í‰ê· : {np.mean(predicted_volatility):.3f}\")\n",
    "        print(f\"  í‘œì¤€í¸ì°¨: {np.std(predicted_volatility):.3f}\")\n",
    "        print(f\"  ìµœì†Œê°’: {np.min(predicted_volatility):.3f}\")\n",
    "        print(f\"  ìµœëŒ€ê°’: {np.max(predicted_volatility):.3f}\")\n",
    "        print(f\"  ì¤‘ìœ„ìˆ˜: {np.median(predicted_volatility):.3f}\")\n",
    "        \n",
    "        # ë“±ê¸‰ë³„ ë¶„ë¥˜\n",
    "        p33 = np.percentile(predicted_volatility, 33)\n",
    "        p67 = np.percentile(predicted_volatility, 67)\n",
    "        \n",
    "        print(f\"\\nğŸ·ï¸ ë³€ë™ì„± ë“±ê¸‰:\")\n",
    "        print(f\"  ì•ˆì •í˜• (< {p33:.3f}): {np.sum(predicted_volatility < p33)}ëª…\")\n",
    "        print(f\"  ë³´í†µí˜• ({p33:.3f} ~ {p67:.3f}): {np.sum((predicted_volatility >= p33) & (predicted_volatility < p67))}ëª…\")\n",
    "        print(f\"  ë³€ë™í˜• (â‰¥ {p67:.3f}): {np.sum(predicted_volatility >= p67)}ëª…\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ê²°ê³¼\n",
    "        results_df = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'actual_cv': features_df['traditional_cv'] if 'traditional_cv' in features_df.columns else np.nan\n",
    "        })\n",
    "        \n",
    "        results_df['grade'] = pd.cut(results_df['predicted_volatility'], \n",
    "                                   bins=[0, p33, p67, np.inf], \n",
    "                                   labels=['ì•ˆì •í˜•', 'ë³´í†µí˜•', 'ë³€ë™í˜•'])\n",
    "        \n",
    "        print(f\"\\nğŸ‘¥ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜:\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tì˜ˆì¸¡ë³€ë™ê³„ìˆ˜\\të“±ê¸‰\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['customer_id']}\\t{row['predicted_volatility']:.3f}\\t\\t{row['grade']}\")\n",
    "        \n",
    "        # ì´ìƒ ë³€ë™ì„± íƒì§€\n",
    "        anomaly_results = self.detect_anomalous_volatility(features_df)\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ í™œìš© ë°©ì•ˆ:\")\n",
    "        print(\"  1. ë¹„ì •ìƒì  ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ì¡°ê¸° ê°ì§€\")\n",
    "        print(\"  2. ê³ ê°ë³„ ë§ì¶¤í˜• ì „ë ¥ ê´€ë¦¬ ì„œë¹„ìŠ¤\")\n",
    "        print(\"  3. ì˜ì—… ë¦¬ìŠ¤í¬ í‰ê°€ ë° ê´€ë¦¬\")\n",
    "        print(\"  4. ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ\")\n",
    "        \n",
    "        return {\n",
    "            'predictions': results_df,\n",
    "            'anomalies': anomaly_results,\n",
    "            'statistics': {\n",
    "                'mean': np.mean(predicted_volatility),\n",
    "                'std': np.std(predicted_volatility),\n",
    "                'percentiles': {'p33': p33, 'p67': p67}\n",
    "            }\n",
    "        }\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ ë° í…ŒìŠ¤íŠ¸\n",
    "def create_sample_lp_data():\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ LP ë°ì´í„° ìƒì„±\"\"\"\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    data = []\n",
    "    customers = [f'A{1001+i}' for i in range(10)]\n",
    "    start_date = datetime(2024, 3, 1)\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ë³€ë™ì„± íŒ¨í„´ì˜ ê³ ê°ë“¤\n",
    "    volatility_patterns = {\n",
    "        'A1001': 0.3,   # ì•ˆì •ì \n",
    "        'A1002': 1.2,   # ë†’ì€ ë³€ë™ì„±\n",
    "        'A1003': 0.25,  # ë§¤ìš° ì•ˆì •ì \n",
    "        'A1004': 0.8,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1005': 0.9,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1006': 0.2,   # ì•ˆì •ì \n",
    "        'A1007': 1.5,   # ë§¤ìš° ë†’ì€ ë³€ë™ì„±\n",
    "        'A1008': 0.7,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1009': 0.6,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1010': 0.4    # ì•ˆì •ì \n",
    "    }\n",
    "    \n",
    "    for customer in customers:\n",
    "        target_cv = volatility_patterns[customer]\n",
    "        base_power = random.uniform(50, 150)\n",
    "        \n",
    "        for day in range(31):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            for hour in range(24):\n",
    "                for minute in [0, 15, 30, 45]:\n",
    "                    timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                    \n",
    "                    # ë³€ë™ì„±ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ìƒì„±\n",
    "                    noise_std = base_power * target_cv\n",
    "                    power = base_power + random.gauss(0, noise_std)\n",
    "                    power = max(0, power)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                        'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                        'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1)\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    print(\"1ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ìƒì„±...\")\n",
    "    lp_data = create_sample_lp_data()\n",
    "    print(f\"   ìƒì„± ì™„ë£Œ: {len(lp_data):,}ë ˆì½”ë“œ\")\n",
    "    \n",
    "    # 2. ìŠ¤íƒœí‚¹ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    print(\"\\n2ï¸âƒ£ ìŠ¤íƒœí‚¹ ëª¨ë¸ ì´ˆê¸°í™”...\")\n",
    "    stacking_model = KEPCOVolatilityStackingModel()\n",
    "    \n",
    "    # 3. íŠ¹ì„± ì¶”ì¶œ\n",
    "    print(\"\\n3ï¸âƒ£ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ...\")\n",
    "    features_df = stacking_model.extract_features(lp_data)\n",
    "    \n",
    "    # 4. í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "    print(\"\\n4ï¸âƒ£ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„...\")\n",
    "    X, y = stacking_model.prepare_training_data(features_df)\n",
    "    \n",
    "    # 5. ëª¨ë¸ í›ˆë ¨\n",
    "    print(\"\\n5ï¸âƒ£ ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨...\")\n",
    "    stacking_model.fit(X, y)\n",
    "    \n",
    "    # 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    print(\"\\n6ï¸âƒ£ ì¢…í•© ë³€ë™ì„± ë¦¬í¬íŠ¸ ìƒì„±...\")\n",
    "    report = stacking_model.generate_volatility_report(features_df)\n",
    "    \n",
    "    print(\"\\nğŸ¯ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì™„ë£Œ!\")\n",
    "    print(\"   â€¢ Level-0: 7ê°œ ê¸°ë³¸ ë³€ë™ì„± ëª¨ë¸\")\n",
    "    print(\"   â€¢ Level-1: ìµœì í™”ëœ ë©”íƒ€ëª¨ë¸\")\n",
    "    print(\"   â€¢ ê³¼ì í•© ë°©ì§€: êµì°¨ê²€ì¦ + ì •ê·œí™”\")\n",
    "    print(\"   â€¢ ì´ìƒ íƒì§€: ìë™ ì„ê³„ê°’ ì„¤ì •\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
