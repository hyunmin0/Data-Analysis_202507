{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7203f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\n",
      "ì´ ê³ ê° ìˆ˜: 200ëª…\n",
      "ì»¬ëŸ¼: ['ìˆœë²ˆ', 'ê³ ê°ë²ˆí˜¸', 'ê³„ì•½ì „ë ¥', 'ê³„ì•½ì¢…ë³„', 'ì‚¬ìš©ìš©ë„', 'ì£¼ìƒì‚°í’ˆ', 'ì‚°ì—…ë¶„ë¥˜']\n",
      "\n",
      "ê¸°ë³¸ ì •ë³´:\n",
      "   ìˆœë²ˆ   ê³ ê°ë²ˆí˜¸     ê³„ì•½ì „ë ¥            ê³„ì•½ì¢…ë³„     ì‚¬ìš©ìš©ë„  ì£¼ìƒì‚°í’ˆ     ì‚°ì—…ë¶„ë¥˜\n",
      "0   1  A1001  100~199  226 ì¼ë°˜ìš©(ì„) ê³ ì••A   02 ìƒì—…ìš©   ì œì¡°ì—…  299ê¸°íƒ€ì—…ì¢…\n",
      "1   2  A1002  500~599  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A   02 ìƒì—…ìš©    êµíšŒ  769ê¸°íƒ€ì—…ì¢…\n",
      "2   3  A1003  700~799  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A  09 ê´‘ê³µì—…ìš©  ê¸ˆì†ê°€ê³µ  858ê¸°íƒ€ì—…ì¢…\n",
      "3   4  A1004  800~899  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A   02 ìƒì—…ìš©    ìƒê°€  401ê¸°íƒ€ì—…ì¢…\n",
      "4   5  A1005  800~899  226 ì¼ë°˜ìš©(ì„) ê³ ì••A   02 ìƒì—…ìš©   ì œì¡°ì—…  364ê¸°íƒ€ì—…ì¢…\n",
      "\n",
      "=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\n",
      "\n",
      "ğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\n",
      "  322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A: 56ëª… (28.0%)\n",
      "  222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A: 52ëª… (26.0%)\n",
      "  226 ì¼ë°˜ìš©(ì„) ê³ ì••A: 46ëª… (23.0%)\n",
      "  726 ì‚°ì—…ìš©(ì„) ê³ ì••A: 46ëª… (23.0%)\n",
      "\n",
      "ğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\n",
      "  02 ìƒì—…ìš©: 101ëª… (50.5%)\n",
      "  09 ê´‘ê³µì—…ìš©: 99ëª… (49.5%)\n",
      "\n",
      "âš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\n",
      "  100~199kW: 28ëª… (14.0%)\n",
      "  200~299kW: 30ëª… (15.0%)\n",
      "  400~499kW: 31ëª… (15.5%)\n",
      "  500~599kW: 38ëª… (19.0%)\n",
      "  700~799kW: 27ëª… (13.5%)\n",
      "  800~899kW: 46ëª… (23.0%)\n",
      "\n",
      "=== LP ë°ì´í„° ë¡œë”© ===\n",
      "íŒŒì¼ 1 ë¡œë”©: LPë°ì´í„°1.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 14,400\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "íŒŒì¼ 2 ë¡œë”©: LPë°ì´í„°2.csv\n",
      "  ë ˆì½”ë“œ ìˆ˜: 15,360\n",
      "  ê³ ê° ìˆ˜: 10\n",
      "  ê¸°ê°„: 2024-03-16-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "âœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\n",
      "  ì´ ë ˆì½”ë“œ: 29,760\n",
      "  ì´ ê³ ê°: 10\n",
      "\n",
      "=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\n",
      "ğŸ“ˆ ê¸°ë³¸ í†µê³„:\n",
      "            ìˆœë°©í–¥ìœ íš¨ì „ë ¥          ì§€ìƒë¬´íš¨          ì§„ìƒë¬´íš¨          í”¼ìƒì „ë ¥\n",
      "count  29760.000000  29760.000000  29760.000000  29760.000000\n",
      "mean      79.859856     10.063058      5.113498     79.838306\n",
      "std       23.934716      4.852545      2.804499     23.595475\n",
      "min       12.600000      0.000000      0.000000     15.800000\n",
      "25%       60.000000      6.500000      3.000000     60.100000\n",
      "50%       80.100000     10.000000      5.000000     79.900000\n",
      "75%       99.600000     13.400000      7.000000     99.500000\n",
      "max      155.100000     31.200000     18.200000    143.200000\n",
      "\n",
      "â° ì‹œê°„ ê°„ê²© ì²´í¬:\n",
      "  A1001: í‰ê·  ê°„ê²© 15.0ë¶„, í‘œì¤€í¸ì°¨ 0.0ë¶„\n",
      "  A1002: í‰ê·  ê°„ê²© 15.0ë¶„, í‘œì¤€í¸ì°¨ 0.0ë¶„\n",
      "  A1003: í‰ê·  ê°„ê²© 15.0ë¶„, í‘œì¤€í¸ì°¨ 0.0ë¶„\n",
      "\n",
      "ğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\n",
      "  ìˆœë°©í–¥ìœ íš¨ì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "  ì§€ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 35ê±´ (0.12%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "  ì§„ìƒë¬´íš¨:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 104ê±´ (0.35%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "  í”¼ìƒì „ë ¥:\n",
      "    ê²°ì¸¡ì¹˜: 0ê±´ (0.00%)\n",
      "    0ê°’: 0ê±´ (0.00%)\n",
      "    ìŒìˆ˜: 0ê±´ (0.00%)\n",
      "\n",
      "ğŸ‘¥ ê³ ê°ë³„ ë°ì´í„° ì™„ì •ì„±:\n",
      "  ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜: 2,976ê°œ/ê³ ê°\n",
      "  ì‹¤ì œ ë ˆì½”ë“œ ìˆ˜: 2,976~2,976ê°œ/ê³ ê°\n",
      "  âœ… ëª¨ë“  ê³ ê° ë°ì´í„° ì™„ì •ì„± ì–‘í˜¸\n",
      "\n",
      "=== ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•) ===\n",
      "\n",
      "ğŸ“Š ìˆœë°©í–¥ìœ íš¨ì „ë ¥:\n",
      "  ì´ìƒì¹˜: 0ê±´ (0.00%)\n",
      "\n",
      "ğŸ“Š ì§€ìƒë¬´íš¨:\n",
      "  ì´ìƒì¹˜: 86ê±´ (0.29%)\n",
      "  ë²”ìœ„: 23.8 ~ 31.2\n",
      "\n",
      "ğŸ“Š ì§„ìƒë¬´íš¨:\n",
      "  ì´ìƒì¹˜: 106ê±´ (0.36%)\n",
      "  ë²”ìœ„: 13.1 ~ 18.2\n",
      "\n",
      "ğŸ“Š í”¼ìƒì „ë ¥:\n",
      "  ì´ìƒì¹˜: 0ê±´ (0.00%)\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\n",
      "============================================================\n",
      "\n",
      "ğŸ”¢ ë°ì´í„° ê·œëª¨:\n",
      "  â€¢ ê³ ê° ê¸°ë³¸ì •ë³´: 200ëª…\n",
      "  â€¢ LP ë°ì´í„°: 29,760ë ˆì½”ë“œ\n",
      "  â€¢ ë¶„ì„ ëŒ€ìƒ ê³ ê°: 10ëª…\n",
      "  â€¢ ë¶„ì„ ê¸°ê°„: 2024-03-01-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "âœ… ë°ì´í„° í’ˆì§ˆ ìƒíƒœ:\n",
      "  â€¢ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: 0.00%\n",
      "  â€¢ 0ê°’ ë¹„ìœ¨: 0.12%\n",
      "  â€¢ ìŒìˆ˜ê°’ ë¹„ìœ¨: 0.00%\n",
      "\n",
      "ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\n",
      "  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì¼/ì£¼/ì›”ë³„ ì‚¬ìš© íŒ¨í„´)\n",
      "  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\n",
      "  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\n",
      "  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n",
      "\n",
      "ğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "    def load_customer_data(self, file_path):\n",
    "        \"\"\"ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ë° ê¸°ë³¸ ë¶„ì„\"\"\"\n",
    "        print(\"=== ê³ ê° ê¸°ë³¸ì •ë³´ ë¡œë”© ===\")\n",
    "        \n",
    "        # Excel íŒŒì¼ ì½ê¸° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ ë¶€ë¶„ ì‚¬ìš©)\n",
    "        # self.customer_data = pd.read_excel(file_path)\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
    "        self.customer_data = self._create_sample_customer_data()\n",
    "        \n",
    "        print(f\"ì´ ê³ ê° ìˆ˜: {len(self.customer_data):,}ëª…\")\n",
    "        print(f\"ì»¬ëŸ¼: {list(self.customer_data.columns)}\")\n",
    "        print(\"\\nê¸°ë³¸ ì •ë³´:\")\n",
    "        print(self.customer_data.head())\n",
    "        \n",
    "        return self._analyze_customer_distribution()\n",
    "    \n",
    "    def _create_sample_customer_data(self):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ê³ ê° ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        contract_types = ['222 ì¼ë°˜ìš©(ê°‘)â€–ê³ ì••A', '226 ì¼ë°˜ìš©(ì„) ê³ ì••A', '322 ì‚°ì—…ìš©(ê°‘)â€–ê³ ì••A', '726 ì‚°ì—…ìš©(ì„) ê³ ì••A']\n",
    "        usage_types = ['02 ìƒì—…ìš©', '09 ê´‘ê³µì—…ìš©']\n",
    "        power_ranges = ['100~199', '200~299', '400~499', '500~599', '700~799', '800~899']\n",
    "        industries = ['ë³‘ì›', 'êµíšŒ', 'ìƒê°€', 'CNGì¶©ì „', 'ê¸ˆì†ê°€ê³µ', 'ì í¬', 'ì˜¨ì²œíƒ•', 'ì œì¡°ì—…', 'ë§ˆíŠ¸']\n",
    "        \n",
    "        for i in range(1, 201):  # 200ëª…\n",
    "            data.append({\n",
    "                'ìˆœë²ˆ': i,\n",
    "                'ê³ ê°ë²ˆí˜¸': f'A{1000+i}',\n",
    "                'ê³„ì•½ì „ë ¥': np.random.choice(power_ranges),\n",
    "                'ê³„ì•½ì¢…ë³„': np.random.choice(contract_types),\n",
    "                'ì‚¬ìš©ìš©ë„': np.random.choice(usage_types),\n",
    "                'ì£¼ìƒì‚°í’ˆ': np.random.choice(industries),\n",
    "                'ì‚°ì—…ë¶„ë¥˜': f'{np.random.randint(100,999)}ê¸°íƒ€ì—…ì¢…'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"ê³ ê° ë¶„í¬ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ê³ ê° ë¶„í¬ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê³„ì•½ì¢…ë³„ ë¶„í¬\n",
    "        contract_dist = self.customer_data['ê³„ì•½ì¢…ë³„'].value_counts()\n",
    "        print(\"\\nğŸ“Š ê³„ì•½ì¢…ë³„ ë¶„í¬:\")\n",
    "        for contract, count in contract_dist.items():\n",
    "            print(f\"  {contract}: {count}ëª… ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬\n",
    "        usage_dist = self.customer_data['ì‚¬ìš©ìš©ë„'].value_counts()\n",
    "        print(\"\\nğŸ­ ì‚¬ìš©ìš©ë„ë³„ ë¶„í¬:\")\n",
    "        for usage, count in usage_dist.items():\n",
    "            print(f\"  {usage}: {count}ëª… ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # ê³„ì•½ì „ë ¥ ë¶„í¬\n",
    "        power_dist = self.customer_data['ê³„ì•½ì „ë ¥'].value_counts().sort_index()\n",
    "        print(\"\\nâš¡ ê³„ì•½ì „ë ¥ ë¶„í¬:\")\n",
    "        for power, count in power_dist.items():\n",
    "            print(f\"  {power}kW: {count}ëª… ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_dist,\n",
    "            'usage_distribution': usage_dist,\n",
    "            'power_distribution': power_dist\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, file_paths):\n",
    "        \"\"\"LP ë°ì´í„° ë¡œë”© ë° ê²°í•©\"\"\"\n",
    "        print(\"\\n=== LP ë°ì´í„° ë¡œë”© ===\")\n",
    "        \n",
    "        lp_dataframes = []\n",
    "        \n",
    "        for i, file_path in enumerate(file_paths, 1):\n",
    "            print(f\"íŒŒì¼ {i} ë¡œë”©: {file_path}\")\n",
    "            \n",
    "            # CSV íŒŒì¼ ì½ê¸° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ ë¶€ë¶„ ì‚¬ìš©)\n",
    "            # df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
    "            df = self._create_sample_lp_data(i)\n",
    "            \n",
    "            print(f\"  ë ˆì½”ë“œ ìˆ˜: {len(df):,}\")\n",
    "            print(f\"  ê³ ê° ìˆ˜: {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}\")\n",
    "            print(f\"  ê¸°ê°„: {df['LPìˆ˜ì‹ ì¼ì'].min()} ~ {df['LPìˆ˜ì‹ ì¼ì'].max()}\")\n",
    "            \n",
    "            lp_dataframes.append(df)\n",
    "        \n",
    "        # ë°ì´í„° ê²°í•©\n",
    "        self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "        print(f\"\\nâœ… ì „ì²´ LP ë°ì´í„° ê²°í•© ì™„ë£Œ:\")\n",
    "        print(f\"  ì´ ë ˆì½”ë“œ: {len(self.lp_data):,}\")\n",
    "        print(f\"  ì´ ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}\")\n",
    "        \n",
    "        return self._analyze_lp_quality()\n",
    "    \n",
    "    def _create_sample_lp_data(self, file_num):\n",
    "        \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ LP ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        \n",
    "        # íŒŒì¼ë³„ë¡œ ë‹¤ë¥¸ ê¸°ê°„ ì„¤ì •\n",
    "        if file_num == 1:\n",
    "            start_date = datetime(2024, 3, 1)\n",
    "            days = 15\n",
    "        else:\n",
    "            start_date = datetime(2024, 3, 16) \n",
    "            days = 16\n",
    "        \n",
    "        for customer in customers:\n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ì„ ë°˜ì˜í•œ ê°€ìƒ ë°ì´í„°\n",
    "                        base_power = 80 + 30 * np.sin(2 * np.pi * hour / 24) + np.random.normal(0, 10)\n",
    "                        base_power = max(0, base_power)\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(base_power + np.random.normal(0, 5), 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(abs(np.random.normal(10, 5)), 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(abs(np.random.normal(5, 3)), 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(base_power + np.random.normal(0, 3), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== LP ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        print(\"ğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
    "        numeric_cols = ['ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        print(self.lp_data[numeric_cols].describe())\n",
    "        \n",
    "        # ì‹œê°„ ê°„ê²© ì²´í¬\n",
    "        print(\"\\nâ° ì‹œê°„ ê°„ê²© ì²´í¬:\")\n",
    "        self.lp_data['LPìˆ˜ì‹ ì¼ì_dt'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique()[:3]:  # ì²˜ìŒ 3ëª…ë§Œ ì²´í¬\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('LPìˆ˜ì‹ ì¼ì_dt')\n",
    "            time_diffs = customer_data['LPìˆ˜ì‹ ì¼ì_dt'].diff().dt.total_seconds() / 60  # ë¶„ ë‹¨ìœ„\n",
    "            \n",
    "            print(f\"  {customer}: í‰ê·  ê°„ê²© {time_diffs.mean():.1f}ë¶„, í‘œì¤€í¸ì°¨ {time_diffs.std():.1f}ë¶„\")\n",
    "            \n",
    "            # 15ë¶„ì´ ì•„ë‹Œ ê°„ê²© ì°¾ê¸°\n",
    "            non_15min = time_diffs[(time_diffs != 15.0) & (~time_diffs.isna())]\n",
    "            if len(non_15min) > 0:\n",
    "                print(f\"    âš ï¸ ë¹„ì •ìƒ ê°„ê²©: {len(non_15min)}ê±´\")\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ì²´í¬\n",
    "        print(\"\\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:\")\n",
    "        for col in numeric_cols:\n",
    "            null_count = self.lp_data[col].isnull().sum()\n",
    "            zero_count = (self.lp_data[col] == 0).sum()\n",
    "            negative_count = (self.lp_data[col] < 0).sum()\n",
    "            \n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    ê²°ì¸¡ì¹˜: {null_count:,}ê±´ ({null_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    0ê°’: {zero_count:,}ê±´ ({zero_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    ìŒìˆ˜: {negative_count:,}ê±´ ({negative_count/len(self.lp_data)*100:.2f}%)\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ë°ì´í„° ì™„ì •ì„±\n",
    "        print(\"\\nğŸ‘¥ ê³ ê°ë³„ ë°ì´í„° ì™„ì •ì„±:\")\n",
    "        customer_counts = self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].value_counts()\n",
    "        expected_records = 24 * 4 * 31  # 15ë¶„ ê°„ê²© Ã— 1ê°œì›”\n",
    "        \n",
    "        print(f\"  ì˜ˆìƒ ë ˆì½”ë“œ ìˆ˜: {expected_records:,}ê°œ/ê³ ê°\")\n",
    "        print(f\"  ì‹¤ì œ ë ˆì½”ë“œ ìˆ˜: {customer_counts.min():,}~{customer_counts.max():,}ê°œ/ê³ ê°\")\n",
    "        \n",
    "        incomplete_customers = customer_counts[customer_counts < expected_records * 0.95]  # 95% ë¯¸ë§Œ\n",
    "        if len(incomplete_customers) > 0:\n",
    "            print(f\"  âš ï¸ ë¶ˆì™„ì „í•œ ê³ ê°: {len(incomplete_customers)}ëª…\")\n",
    "        else:\n",
    "            print(\"  âœ… ëª¨ë“  ê³ ê° ë°ì´í„° ì™„ì •ì„± ì–‘í˜¸\")\n",
    "        \n",
    "        return {\n",
    "            'total_records': len(self.lp_data),\n",
    "            'customers': self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),\n",
    "            'date_range': (self.lp_data['LPìˆ˜ì‹ ì¼ì_dt'].min(), self.lp_data['LPìˆ˜ì‹ ì¼ì_dt'].max()),\n",
    "            'data_quality': {col: {'null': self.lp_data[col].isnull().sum(), \n",
    "                                  'zero': (self.lp_data[col] == 0).sum(),\n",
    "                                  'negative': (self.lp_data[col] < 0).sum()} for col in numeric_cols}\n",
    "        }\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"ì´ìƒì¹˜ íƒì§€\"\"\"\n",
    "        print(f\"\\n=== ì´ìƒì¹˜ íƒì§€ ({method.upper()} ë°©ë²•) ===\")\n",
    "        \n",
    "        numeric_cols = ['ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        outliers_summary = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            data = self.lp_data[col].dropna()\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs((data - data.mean()) / data.std())\n",
    "                outliers = data[z_scores > 3]\n",
    "            \n",
    "            outliers_summary[col] = {\n",
    "                'count': len(outliers),\n",
    "                'percentage': len(outliers) / len(data) * 100,\n",
    "                'min_outlier': outliers.min() if len(outliers) > 0 else None,\n",
    "                'max_outlier': outliers.max() if len(outliers) > 0 else None\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ“Š {col}:\")\n",
    "            print(f\"  ì´ìƒì¹˜: {len(outliers):,}ê±´ ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"  ë²”ìœ„: {outliers.min():.1f} ~ {outliers.max():.1f}\")\n",
    "        \n",
    "        return outliers_summary\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. ë°ì´í„° ê·œëª¨\n",
    "        print(\"\\nğŸ”¢ ë°ì´í„° ê·œëª¨:\")\n",
    "        print(f\"  â€¢ ê³ ê° ê¸°ë³¸ì •ë³´: {len(self.customer_data):,}ëª…\")\n",
    "        print(f\"  â€¢ LP ë°ì´í„°: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "        print(f\"  â€¢ ë¶„ì„ ëŒ€ìƒ ê³ ê°: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        print(f\"  â€¢ ë¶„ì„ ê¸°ê°„: {self.lp_data['LPìˆ˜ì‹ ì¼ì'].min()} ~ {self.lp_data['LPìˆ˜ì‹ ì¼ì'].max()}\")\n",
    "        \n",
    "        # 2. ë°ì´í„° í’ˆì§ˆ ìš”ì•½\n",
    "        print(\"\\nâœ… ë°ì´í„° í’ˆì§ˆ ìƒíƒœ:\")\n",
    "        numeric_cols = ['ìˆœë°©í–¥ìœ íš¨ì „ë ¥', 'ì§€ìƒë¬´íš¨', 'ì§„ìƒë¬´íš¨', 'í”¼ìƒì „ë ¥']\n",
    "        \n",
    "        total_records = len(self.lp_data)\n",
    "        total_nulls = sum(self.lp_data[col].isnull().sum() for col in numeric_cols)\n",
    "        total_zeros = sum((self.lp_data[col] == 0).sum() for col in numeric_cols)\n",
    "        total_negatives = sum((self.lp_data[col] < 0).sum() for col in numeric_cols)\n",
    "        \n",
    "        print(f\"  â€¢ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {total_nulls/(total_records*4)*100:.2f}%\")\n",
    "        print(f\"  â€¢ 0ê°’ ë¹„ìœ¨: {total_zeros/(total_records*4)*100:.2f}%\") \n",
    "        print(f\"  â€¢ ìŒìˆ˜ê°’ ë¹„ìœ¨: {total_negatives/(total_records*4)*100:.2f}%\")\n",
    "        \n",
    "        # 3. ê¶Œì¥ì‚¬í•­\n",
    "        print(\"\\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­:\")\n",
    "        print(\"  1. ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ (ì¼/ì£¼/ì›”ë³„ ì‚¬ìš© íŒ¨í„´)\")\n",
    "        print(\"  2. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§\")\n",
    "        print(\"  3. ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ë° ë¹„êµ\")\n",
    "        print(\"  4. ì´ìƒ íŒ¨í„´ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1ë‹¨ê³„: ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„\n",
    "    customer_analysis = analyzer.load_customer_data('ê³ ê°ë²ˆí˜¸.xlsx')\n",
    "    \n",
    "    # 2ë‹¨ê³„: LP ë°ì´í„° ë¶„ì„  \n",
    "    lp_analysis = analyzer.load_lp_data(['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv'])\n",
    "    \n",
    "    # 3ë‹¨ê³„: ì´ìƒì¹˜ íƒì§€\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4ë‹¨ê³„: ì¢…í•© ë¦¬í¬íŠ¸\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\nğŸ¯ 1ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ!\")\n",
    "    print(\"ë‹¤ìŒ: 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a4c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LP ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ===\n",
      "âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: 29,760ë ˆì½”ë“œ\n",
      "ê¸°ê°„: 2024-03-01 00:00:00 ~ 2024-03-31 23:45:00\n",
      "ê³ ê° ìˆ˜: 10ëª…\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\n",
      "============================================================\n",
      "\n",
      "=== ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ===\n",
      "ğŸ“Š ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (kW):\n",
      "ì‹œê°„\tí‰ê· \tí‘œì¤€í¸ì°¨\tìµœì†Œ\tìµœëŒ€\n",
      "00ì‹œ\t57.9\t23.7\t7.4\t117.3\n",
      "01ì‹œ\t65.9\t27.1\t9.0\t136.7\n",
      "02ì‹œ\t73.3\t30.1\t10.8\t149.4\n",
      "03ì‹œ\t80.0\t33.2\t11.5\t175.2\n",
      "04ì‹œ\t84.8\t34.7\t12.1\t171.1\n",
      "05ì‹œ\t87.9\t36.3\t12.0\t179.3\n",
      "06ì‹œ\t92.8\t37.4\t12.9\t173.8\n",
      "07ì‹œ\t103.9\t55.6\t16.9\t267.3\n",
      "08ì‹œ\t110.7\t53.5\t15.3\t256.7\n",
      "09ì‹œ\t107.8\t51.3\t16.4\t255.3\n",
      "10ì‹œ\t94.3\t38.5\t10.8\t177.4\n",
      "11ì‹œ\t79.3\t34.8\t8.6\t157.4\n",
      "12ì‹œ\t67.8\t34.6\t11.1\t180.9\n",
      "13ì‹œ\t62.6\t30.7\t10.0\t162.1\n",
      "14ì‹œ\t55.0\t26.4\t8.4\t138.2\n",
      "15ì‹œ\t45.6\t18.6\t5.1\t88.3\n",
      "16ì‹œ\t35.0\t14.3\t3.9\t77.4\n",
      "17ì‹œ\t32.8\t14.8\t4.8\t87.4\n",
      "18ì‹œ\t32.5\t16.0\t4.8\t89.2\n",
      "19ì‹œ\t37.5\t19.2\t5.5\t92.0\n",
      "20ì‹œ\t38.2\t19.0\t4.0\t88.8\n",
      "21ì‹œ\t42.4\t20.9\t4.6\t94.4\n",
      "22ì‹œ\t44.3\t17.9\t5.2\t86.1\n",
      "23ì‹œ\t50.0\t20.7\t6.4\t101.0\n",
      "\n",
      "âš¡ í”¼í¬ ì‹œê°„ëŒ€ (ìƒìœ„ 20%): [6, 7, 8, 9, 10]ì‹œ\n",
      "ğŸ’¤ ë¹„í”¼í¬ ì‹œê°„ëŒ€ (í•˜ìœ„ 30%): [16, 17, 18, 19, 20, 21, 22]ì‹œ\n",
      "\n",
      "=== ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ===\n",
      "ğŸ“… ìš”ì¼ë³„ í‰ê·  ì¼ê°„ ì‚¬ìš©ëŸ‰ (kWh):\n",
      "ì›”ìš”ì¼: 7,063.7 Â± 1993.7\n",
      "í™”ìš”ì¼: 7,074.2 Â± 1988.2\n",
      "ìˆ˜ìš”ì¼: 7,053.0 Â± 1999.1\n",
      "ëª©ìš”ì¼: 7,054.8 Â± 2016.7\n",
      "ê¸ˆìš”ì¼: 7,036.9 Â± 1997.1\n",
      "í† ìš”ì¼: 4,800.4 Â± 2989.7\n",
      "ì¼ìš”ì¼: 4,807.6 Â± 2996.2\n",
      "\n",
      "ğŸ“Š í‰ì¼ vs ì£¼ë§ ë¹„êµ:\n",
      "í‰ì¼ í‰ê· : 7,055.6 kWh\n",
      "ì£¼ë§ í‰ê· : 4,804.0 kWh\n",
      "ì£¼ë§/í‰ì¼ ë¹„ìœ¨: 0.68\n",
      "\n",
      "=== ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„ ===\n",
      "ğŸ‘¥ ê³ ê°ë³„ ê¸°ë³¸ í†µê³„ (kW):\n",
      "ê³ ê°ë²ˆí˜¸\tí‰ê· \tí‘œì¤€í¸ì°¨\të³€ë™ê³„ìˆ˜\tìµœì†Œ\tìµœëŒ€\n",
      "A1001\t84.6\t36.1\t0.427\t22.1\t221.2\n",
      "A1002\t45.6\t29.6\t0.649\t5.3\t148.2\n",
      "A1003\t78.8\t28.7\t0.364\t23.0\t176.6\n",
      "A1004\t81.2\t63.6\t0.783\t3.9\t267.3\n",
      "A1005\t66.0\t24.3\t0.368\t22.9\t131.0\n",
      "A1006\t58.1\t26.8\t0.461\t16.9\t180.8\n",
      "A1007\t59.7\t41.5\t0.695\t4.8\t183.7\n",
      "A1008\t95.0\t44.0\t0.463\t27.6\t251.8\n",
      "A1009\t36.6\t19.8\t0.541\t5.6\t113.9\n",
      "A1010\t53.8\t26.0\t0.483\t11.7\t140.5\n",
      "\n",
      "ğŸ“ˆ ëŒ€ìš©ëŸ‰ ì‚¬ìš©ì (ìƒìœ„ 20%): ['A1001', 'A1008']\n",
      "ğŸ“‰ ì†Œìš©ëŸ‰ ì‚¬ìš©ì (í•˜ìœ„ 20%): ['A1002', 'A1009']\n",
      "\n",
      "ğŸŒŠ ê³ ë³€ë™ì„± ê³ ê°: ['A1004', 'A1007']\n",
      "ğŸ“Š ì €ë³€ë™ì„± ê³ ê°: ['A1003', 'A1005']\n",
      "\n",
      "=== ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ===\n",
      "âš¡ ê³ ê°ë³„ ë¶€í•˜ìœ¨ ë° í”¼í¬ ì§‘ì¤‘ë„:\n",
      "ê³ ê°ë²ˆí˜¸\të¶€í•˜ìœ¨\tí”¼í¬ì§‘ì¤‘ë„\tí‰ê· ë¶€í•˜\tìµœëŒ€ë¶€í•˜\n",
      "A1001\t0.383\t1.045\t84.6\t221.2\n",
      "A1002\t0.307\t1.057\t45.6\t148.2\n",
      "A1003\t0.446\t0.926\t78.8\t176.6\n",
      "A1004\t0.304\t1.097\t81.2\t267.3\n",
      "A1005\t0.504\t0.861\t66.0\t131.0\n",
      "A1006\t0.321\t0.797\t58.1\t180.8\n",
      "A1007\t0.325\t1.09\t59.7\t183.7\n",
      "A1008\t0.377\t0.958\t95.0\t251.8\n",
      "A1009\t0.321\t0.968\t36.6\t113.9\n",
      "A1010\t0.383\t1.076\t53.8\t140.5\n",
      "\n",
      "ğŸ“Š ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬:\n",
      "í‰ê·  ë¶€í•˜ìœ¨: 0.367\n",
      "ë¶€í•˜ìœ¨ ë²”ìœ„: 0.304 ~ 0.504\n",
      "\n",
      "=== ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€ ===\n",
      "ğŸš¨ ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼:\n",
      "ê³ ê°ë²ˆí˜¸\tê¸‰ê²©ë³€í™”\tì¥ê¸°0ê°’\tí†µê³„ì´ìƒì¹˜\n",
      "A1001\t0\t0\t5\n",
      "A1002\t4\t0\t5\n",
      "A1003\t0\t0\t5\n",
      "A1004\t4\t0\t0\n",
      "A1006\t0\t0\t43\n",
      "A1007\t4\t0\t0\n",
      "A1008\t0\t0\t4\n",
      "A1009\t0\t0\t8\n",
      "A1010\t0\t0\t6\n",
      "\n",
      "ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\n",
      "  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: [6, 7, 8, 9, 10]ì‹œ\n",
      "  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: 0.68\n",
      "  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: 0.364 ~ 0.783\n",
      "  â€¢ í‰ê·  ë¶€í•˜ìœ¨: 0.367\n",
      "  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: 9ëª…\n",
      "\n",
      "ğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\n",
      "  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\n",
      "  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\n",
      "  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\n",
      "  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\n",
      "  5. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \n",
      "\n",
      "ğŸ¯ 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 3ë‹¨ê³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        \n",
    "    def load_sample_data(self):\n",
    "        \"\"\"ìƒ˜í”Œ LP ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\"\"\"\n",
    "        print(\"=== LP ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ===\")\n",
    "        \n",
    "        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” CSV íŒŒì¼ ì½ê¸°\n",
    "        # lp1 = pd.read_csv('LPë°ì´í„°1.csv')\n",
    "        # lp2 = pd.read_csv('LPë°ì´í„°2.csv') \n",
    "        # self.lp_data = pd.concat([lp1, lp2], ignore_index=True)\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "        self.lp_data = self._create_comprehensive_sample_data()\n",
    "        \n",
    "        # ë‚ ì§œ/ì‹œê°„ ì „ì²˜ë¦¬\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday  # 0=ì›”ìš”ì¼\n",
    "        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "        print(f\"ê¸°ê°„: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "        print(f\"ê³ ê° ìˆ˜: {self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def _create_comprehensive_sample_data(self):\n",
    "        \"\"\"í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31  # 3ì›” ì „ì²´\n",
    "        \n",
    "        # ê³ ê°ë³„ íŠ¹ì„± ì •ì˜ (ë‹¤ì–‘í•œ íŒ¨í„´)\n",
    "        customer_profiles = {\n",
    "            'A1001': {'type': 'hospital', 'base_power': 120, 'peak_hours': [9, 14, 20], 'weekend_factor': 0.8},\n",
    "            'A1002': {'type': 'office', 'base_power': 80, 'peak_hours': [9, 14], 'weekend_factor': 0.3},\n",
    "            'A1003': {'type': 'retail', 'base_power': 100, 'peak_hours': [11, 15, 19], 'weekend_factor': 1.2},\n",
    "            'A1004': {'type': 'factory', 'base_power': 150, 'peak_hours': [8, 13, 18], 'weekend_factor': 0.1},\n",
    "            'A1005': {'type': 'restaurant', 'base_power': 90, 'peak_hours': [12, 18], 'weekend_factor': 1.1},\n",
    "            'A1006': {'type': 'gym', 'base_power': 70, 'peak_hours': [7, 18, 21], 'weekend_factor': 1.3},\n",
    "            'A1007': {'type': 'school', 'base_power': 110, 'peak_hours': [10, 14], 'weekend_factor': 0.2},\n",
    "            'A1008': {'type': 'hotel', 'base_power': 130, 'peak_hours': [8, 20], 'weekend_factor': 1.0},\n",
    "            'A1009': {'type': 'warehouse', 'base_power': 60, 'peak_hours': [9, 16], 'weekend_factor': 0.5},\n",
    "            'A1010': {'type': 'clinic', 'base_power': 85, 'peak_hours': [10, 15], 'weekend_factor': 0.6}\n",
    "        }\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = customer_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                is_weekend = current_date.weekday() >= 5\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ê¸°ë³¸ ì „ë ¥ ê³„ì‚°\n",
    "                        base_power = profile['base_power']\n",
    "                        \n",
    "                        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ (ì‚¬ì¸íŒŒ ê¸°ë°˜)\n",
    "                        time_factor = 0.3 + 0.7 * (np.sin(2 * np.pi * hour / 24) + 1) / 2\n",
    "                        \n",
    "                        # í”¼í¬ ì‹œê°„ ë³´ì •\n",
    "                        peak_factor = 1.0\n",
    "                        for peak_hour in profile['peak_hours']:\n",
    "                            if abs(hour - peak_hour) <= 1:\n",
    "                                peak_factor = 1.5\n",
    "                        \n",
    "                        # ì£¼ë§ ë³´ì •\n",
    "                        weekend_factor = profile['weekend_factor'] if is_weekend else 1.0\n",
    "                        \n",
    "                        # ìµœì¢… ì „ë ¥ ê³„ì‚°\n",
    "                        power = base_power * time_factor * peak_factor * weekend_factor\n",
    "                        power += np.random.normal(0, power * 0.1)  # 10% ë…¸ì´ì¦ˆ\n",
    "                        power = max(0, power)\n",
    "                        \n",
    "                        # ë¬´íš¨ì „ë ¥ ê³„ì‚°\n",
    "                        reactive_lag = power * np.random.uniform(0.1, 0.3)\n",
    "                        reactive_lead = power * np.random.uniform(0.05, 0.15)\n",
    "                        apparent_power = np.sqrt(power**2 + (reactive_lag - reactive_lead)**2)\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(reactive_lag, 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(reactive_lead, 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(apparent_power, 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def analyze_hourly_patterns(self):\n",
    "        \"\"\"ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ì‹œê°„ëŒ€ë³„ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ì‹œê°„ëŒ€ë³„ í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "        hourly_avg = self.lp_data.groupby('hour')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "        \n",
    "        print(\"ğŸ“Š ì‹œê°„ëŒ€ë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ (kW):\")\n",
    "        print(\"ì‹œê°„\\tí‰ê· \\tí‘œì¤€í¸ì°¨\\tìµœì†Œ\\tìµœëŒ€\")\n",
    "        for hour in range(24):\n",
    "            stats = hourly_avg.loc[hour]\n",
    "            print(f\"{hour:02d}ì‹œ\\t{stats['mean']}\\t{stats['std']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # í”¼í¬/ë¹„í”¼í¬ ì‹œê°„ëŒ€ ì‹ë³„\n",
    "        peak_threshold = hourly_avg['mean'].quantile(0.8)\n",
    "        peak_hours = hourly_avg[hourly_avg['mean'] >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = hourly_avg[hourly_avg['mean'] < hourly_avg['mean'].quantile(0.3)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nâš¡ í”¼í¬ ì‹œê°„ëŒ€ (ìƒìœ„ 20%): {peak_hours}ì‹œ\")\n",
    "        print(f\"ğŸ’¤ ë¹„í”¼í¬ ì‹œê°„ëŒ€ (í•˜ìœ„ 30%): {off_peak_hours}ì‹œ\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_stats': hourly_avg,\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours\n",
    "        }\n",
    "    \n",
    "    def analyze_daily_patterns(self):\n",
    "        \"\"\"ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ì¼ë³„/ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ì¼ë³„ ì´ ì‚¬ìš©ëŸ‰\n",
    "        daily_usage = self.lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date'])['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].sum().reset_index()\n",
    "        daily_usage['weekday'] = pd.to_datetime(daily_usage['date']).dt.weekday\n",
    "        daily_usage['is_weekend'] = daily_usage['weekday'].isin([5, 6])\n",
    "        \n",
    "        # ìš”ì¼ë³„ í‰ê·  ì‚¬ìš©ëŸ‰\n",
    "        weekday_avg = daily_usage.groupby('weekday')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg(['mean', 'std']).round(1)\n",
    "        weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']\n",
    "        \n",
    "        print(\"ğŸ“… ìš”ì¼ë³„ í‰ê·  ì¼ê°„ ì‚¬ìš©ëŸ‰ (kWh):\")\n",
    "        for i, day_name in enumerate(weekday_names):\n",
    "            stats = weekday_avg.loc[i]\n",
    "            print(f\"{day_name}ìš”ì¼: {stats['mean']:,.1f} Â± {stats['std']:.1f}\")\n",
    "        \n",
    "        # í‰ì¼ vs ì£¼ë§ ë¹„êµ\n",
    "        weekday_mean = daily_usage[~daily_usage['is_weekend']]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "        weekend_mean = daily_usage[daily_usage['is_weekend']]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "        weekend_ratio = weekend_mean / weekday_mean\n",
    "        \n",
    "        print(f\"\\nğŸ“Š í‰ì¼ vs ì£¼ë§ ë¹„êµ:\")\n",
    "        print(f\"í‰ì¼ í‰ê· : {weekday_mean:,.1f} kWh\")\n",
    "        print(f\"ì£¼ë§ í‰ê· : {weekend_mean:,.1f} kWh\")\n",
    "        print(f\"ì£¼ë§/í‰ì¼ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'daily_usage': daily_usage,\n",
    "            'weekday_stats': weekday_avg,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "    \n",
    "    def analyze_customer_profiles(self):\n",
    "        \"\"\"ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ ë¶„ì„ ===\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ê¸°ë³¸ í†µê³„\n",
    "        customer_stats = self.lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max'\n",
    "        ]).round(1)\n",
    "        customer_stats['cv'] = (customer_stats['std'] / customer_stats['mean']).round(3)  # ë³€ë™ê³„ìˆ˜\n",
    "        \n",
    "        print(\"ğŸ‘¥ ê³ ê°ë³„ ê¸°ë³¸ í†µê³„ (kW):\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tí‰ê· \\tí‘œì¤€í¸ì°¨\\të³€ë™ê³„ìˆ˜\\tìµœì†Œ\\tìµœëŒ€\")\n",
    "        for customer in customer_stats.index:\n",
    "            stats = customer_stats.loc[customer]\n",
    "            print(f\"{customer}\\t{stats['mean']}\\t{stats['std']}\\t{stats['cv']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë¶„ë¥˜\n",
    "        mean_usage = customer_stats['mean']\n",
    "        high_users = mean_usage[mean_usage >= mean_usage.quantile(0.8)].index.tolist()\n",
    "        low_users = mean_usage[mean_usage <= mean_usage.quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ëŒ€ìš©ëŸ‰ ì‚¬ìš©ì (ìƒìœ„ 20%): {high_users}\")\n",
    "        print(f\"ğŸ“‰ ì†Œìš©ëŸ‰ ì‚¬ìš©ì (í•˜ìœ„ 20%): {low_users}\")\n",
    "        \n",
    "        # ë³€ë™ì„±ë³„ ë¶„ë¥˜\n",
    "        high_volatility = customer_stats[customer_stats['cv'] >= customer_stats['cv'].quantile(0.8)].index.tolist()\n",
    "        low_volatility = customer_stats[customer_stats['cv'] <= customer_stats['cv'].quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\nğŸŒŠ ê³ ë³€ë™ì„± ê³ ê°: {high_volatility}\")\n",
    "        print(f\"ğŸ“Š ì €ë³€ë™ì„± ê³ ê°: {low_volatility}\")\n",
    "        \n",
    "        return {\n",
    "            'customer_stats': customer_stats,\n",
    "            'usage_segments': {\n",
    "                'high_users': high_users,\n",
    "                'low_users': low_users,\n",
    "                'high_volatility': high_volatility,\n",
    "                'low_volatility': low_volatility\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_load_factors(self):\n",
    "        \"\"\"ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ë¶€í•˜ìœ¨ ë° íš¨ìœ¨ì„± ì§€í‘œ ê³„ì‚° ===\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ë¶€í•˜ìœ¨ ê³„ì‚°\n",
    "        customer_load_factors = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "            \n",
    "            avg_load = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            max_load = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].max()\n",
    "            load_factor = avg_load / max_load if max_load > 0 else 0\n",
    "            \n",
    "            # í”¼í¬ ì§‘ì¤‘ë„ (í”¼í¬ ì‹œê°„ëŒ€ ì‚¬ìš©ëŸ‰ ë¹„ì¤‘)\n",
    "            peak_hours = [9, 14, 18]  # ëŒ€í‘œ í”¼í¬ ì‹œê°„\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            total_avg = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            peak_concentration = peak_usage / total_avg if total_avg > 0 else 0\n",
    "            \n",
    "            customer_load_factors[customer] = {\n",
    "                'load_factor': round(load_factor, 3),\n",
    "                'peak_concentration': round(peak_concentration, 3),\n",
    "                'avg_load': round(avg_load, 1),\n",
    "                'max_load': round(max_load, 1)\n",
    "            }\n",
    "        \n",
    "        print(\"âš¡ ê³ ê°ë³„ ë¶€í•˜ìœ¨ ë° í”¼í¬ ì§‘ì¤‘ë„:\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\të¶€í•˜ìœ¨\\tí”¼í¬ì§‘ì¤‘ë„\\tí‰ê· ë¶€í•˜\\tìµœëŒ€ë¶€í•˜\")\n",
    "        for customer, metrics in customer_load_factors.items():\n",
    "            print(f\"{customer}\\t{metrics['load_factor']}\\t{metrics['peak_concentration']}\\t{metrics['avg_load']}\\t{metrics['max_load']}\")\n",
    "        \n",
    "        # ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬\n",
    "        load_factors = [metrics['load_factor'] for metrics in customer_load_factors.values()]\n",
    "        avg_load_factor = np.mean(load_factors)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ì „ì²´ ë¶€í•˜ìœ¨ ë¶„í¬:\")\n",
    "        print(f\"í‰ê·  ë¶€í•˜ìœ¨: {avg_load_factor:.3f}\")\n",
    "        print(f\"ë¶€í•˜ìœ¨ ë²”ìœ„: {min(load_factors):.3f} ~ {max(load_factors):.3f}\")\n",
    "        \n",
    "        return customer_load_factors\n",
    "    \n",
    "    def detect_usage_anomalies(self):\n",
    "        \"\"\"ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€\"\"\"\n",
    "        print(\"\\n=== ì‚¬ìš©ëŸ‰ ì´ìƒ íŒ¨í„´ íƒì§€ ===\")\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].copy()\n",
    "            customer_data = customer_data.sort_values('datetime')\n",
    "            \n",
    "            # 1. ê¸‰ê²©í•œ ë³€í™” íƒì§€ (ì „ì‹œì  ëŒ€ë¹„ 200% ì´ìƒ ë³€í™”)\n",
    "            customer_data['power_change'] = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['power_change']) > 2.0]\n",
    "            \n",
    "            # 2. ì—°ì†ì ì¸ 0ê°’ íƒì§€ (2ì‹œê°„ ì´ìƒ)\n",
    "            customer_data['is_zero'] = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] == 0\n",
    "            customer_data['zero_group'] = (customer_data['is_zero'] != customer_data['is_zero'].shift()).cumsum()\n",
    "            zero_periods = customer_data[customer_data['is_zero']].groupby('zero_group').size()\n",
    "            long_zero_periods = zero_periods[zero_periods >= 8]  # 2ì‹œê°„ = 8ê°œ 15ë¶„ êµ¬ê°„\n",
    "            \n",
    "            # 3. í†µê³„ì  ì´ìƒì¹˜ (Z-score > 3)\n",
    "            mean_power = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            std_power = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].std()\n",
    "            if std_power > 0:\n",
    "                customer_data['z_score'] = abs(customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] - mean_power) / std_power\n",
    "                statistical_outliers = customer_data[customer_data['z_score'] > 3]\n",
    "            else:\n",
    "                statistical_outliers = pd.DataFrame()\n",
    "            \n",
    "            # ì´ìƒì¹˜ ì •ë³´ ì €ì¥\n",
    "            if len(sudden_changes) > 0 or len(long_zero_periods) > 0 or len(statistical_outliers) > 0:\n",
    "                anomalies.append({\n",
    "                    'customer': customer,\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'long_zero_periods': len(long_zero_periods),\n",
    "                    'statistical_outliers': len(statistical_outliers)\n",
    "                })\n",
    "        \n",
    "        print(\"ğŸš¨ ì´ìƒ íŒ¨í„´ íƒì§€ ê²°ê³¼:\")\n",
    "        if anomalies:\n",
    "            print(\"ê³ ê°ë²ˆí˜¸\\tê¸‰ê²©ë³€í™”\\tì¥ê¸°0ê°’\\tí†µê³„ì´ìƒì¹˜\")\n",
    "            for anomaly in anomalies:\n",
    "                print(f\"{anomaly['customer']}\\t{anomaly['sudden_changes']}\\t{anomaly['long_zero_periods']}\\t{anomaly['statistical_outliers']}\")\n",
    "        else:\n",
    "            print(\"âœ… ì‹¬ê°í•œ ì´ìƒ íŒ¨í„´ ì—†ìŒ\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def generate_pattern_summary(self):\n",
    "        \"\"\"íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì¢…í•© ìš”ì•½\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ì£¼ìš” íŒ¨í„´ íŠ¹ì„±\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        print(\"\\nğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:\")\n",
    "        \n",
    "        # 1. ì‹œê°„ íŒ¨í„´\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  â€¢ ì£¼ìš” í”¼í¬ ì‹œê°„: {peak_hours}ì‹œ\")\n",
    "        \n",
    "        # 2. ìš”ì¼ íŒ¨í„´  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  â€¢ ì£¼ë§/í‰ì¼ ì‚¬ìš©ëŸ‰ ë¹„ìœ¨: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. ê³ ê° ë‹¤ì–‘ì„±\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  â€¢ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ë²”ìœ„: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. ë¶€í•˜ìœ¨\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  â€¢ í‰ê·  ë¶€í•˜ìœ¨: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. ì´ìƒ íŒ¨í„´\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  â€¢ ì´ìƒ íŒ¨í„´ ê³ ê°: {anomaly_customers}ëª…\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"  1. ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜ í•„ìš” (í”¼í¬/ë¹„í”¼í¬ êµ¬ë¶„)\")\n",
    "        print(\"  2. ìš”ì¼ë³„ ë³´ì • ê³„ìˆ˜ ê³ ë ¤\") \n",
    "        print(\"  3. ê³ ê°ë³„ ê¸°ì¤€ ë³€ë™ì„± ì„¤ì •\")\n",
    "        print(\"  4. ë¶€í•˜ìœ¨ê³¼ ë³€ë™ì„±ì˜ ìƒê´€ê´€ê³„ ë¶„ì„\")\n",
    "        print(\"  5. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œ ì¡°í•© ê²€í† \")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies\n",
    "        }\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer()\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë”©\n",
    "    lp_data = analyzer.load_sample_data()\n",
    "    \n",
    "    # íŒ¨í„´ ë¶„ì„ ì‹¤í–‰\n",
    "    pattern_summary = analyzer.generate_pattern_summary()\n",
    "    \n",
    "    print(\"\\nğŸ¯ 2ë‹¨ê³„ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ!\")\n",
    "    print(\"ë‹¤ìŒ: 3ë‹¨ê³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77981da6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ===\n",
      "âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: 29,760ë ˆì½”ë“œ\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š ë³€ë™ì„± ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸\n",
      "============================================================\n",
      "\n",
      "=== ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ===\n",
      "ê³ ê°ë²ˆí˜¸\tCV\të²”ìœ„ë³€ë™ì„±\tIQRë³€ë™ì„±\tMADë³€ë™ì„±\tìˆ˜ìµë¥ ë³€ë™ì„±\n",
      "A1001\t0.0506\t0.3199\t0.069\t0.0404\t0.0712\n",
      "A1002\t0.4685\t3.819\t0.4496\t0.3145\t1.132\n",
      "A1003\t0.246\t1.2983\t0.397\t0.2091\t0.151\n",
      "A1004\t0.1925\t1.4063\t0.2638\t0.1541\t0.2313\n",
      "A1005\t0.2487\t1.6808\t0.3404\t0.1992\t0.3534\n",
      "A1006\t0.1393\t2.1185\t0.1366\t0.0893\t0.1682\n",
      "A1007\t0.2909\t2.5063\t0.2315\t0.1917\tnan\n",
      "A1008\t0.3906\t2.33\t0.5331\t0.313\tnan\n",
      "A1009\t0.1806\t1.309\t0.2369\t0.1441\t0.2859\n",
      "A1010\t0.3419\t2.1765\t0.4417\t0.2714\tnan\n",
      "\n",
      "=== ì‹œê°„ ìœˆë„ìš°ë³„ ë³€ë™ì„± ë¶„ì„ ===\n",
      "ìœˆë„ìš°ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜:\n",
      "ê³ ê°ë²ˆí˜¸\tì‹œê°„ë³„\tì¼ë³„\tì£¼ë³„\n",
      "A1001\t0.0469\t0.0507\t0.051\n",
      "A1002\t0.3781\t0.4642\t0.4682\n",
      "A1003\t0.0964\t0.2381\t0.2449\n",
      "A1004\t0.1401\t0.1515\t0.1545\n",
      "A1005\t0.1923\t0.2052\t0.2138\n",
      "A1006\t0.0987\t0.1282\t0.1374\n",
      "A1007\t0.2277\t0.2414\t0.28\n",
      "A1008\t0.3766\t0.3906\t0.3891\n",
      "A1009\t0.1682\t0.1808\t0.181\n",
      "A1010\t0.3256\t0.3406\t0.3404\n",
      "\n",
      "=== ë°©í–¥ì„± ë³€ë™ì„± ë¶„ì„ ===\n",
      "ê³ ê°ë²ˆí˜¸\tìƒìŠ¹ë³€ë™ì„±\tí•˜ë½ë³€ë™ì„±\të¹„ëŒ€ì¹­ë¹„ìœ¨\tê¸‰ì¦íšŸìˆ˜\tê¸‰ê°íšŸìˆ˜\n",
      "A1001\t0.0454\t0.0392\t1.1583\t0\t0\n",
      "A1002\t1.3262\t0.2305\t5.7542\t667\t379\n",
      "A1003\t0.1081\t0.0758\t1.4276\t11\t0\n",
      "A1004\t0.1816\t0.1085\t1.6741\t101\t3\n",
      "A1005\t0.3157\t0.1414\t2.2324\t272\t51\n",
      "A1006\t0.1446\t0.0839\t1.7235\t20\t5\n",
      "A1007\tnan\t0.2168\tnan\t283\t160\n",
      "A1008\tnan\t0.2293\tnan\t706\t373\n",
      "A1009\t0.2321\t0.1255\t1.8492\t183\t28\n",
      "A1010\tnan\t0.2107\tnan\t610\t295\n",
      "\n",
      "=== íŒ¨í„´ ì•ˆì •ì„± ë¶„ì„ ===\n",
      "ê³ ê°ë²ˆí˜¸\tíŒ¨í„´ì¼ê´€ì„±\tì¼ì¼ì£¼ê¸°ì„±\tìê¸°ìƒê´€\n",
      "A1001\t0.0044\t0.6002\t0.9975\n",
      "A1002\t-0.0153\t1.5469\t0.824\n",
      "A1003\t0.9524\t68.3914\t0.9896\n",
      "A1004\t-0.0102\t0.9968\t0.9778\n",
      "A1005\t-0.0033\t0.7561\t0.9581\n",
      "A1006\t0.0267\t3.4987\t0.9874\n",
      "A1007\t-0.0021\t0.4741\t0.9234\n",
      "A1008\t-0.0118\t0.5864\t0.8678\n",
      "A1009\t0.0153\t1.4262\t0.9683\n",
      "A1010\t0.0051\t0.2849\t0.8935\n",
      "\n",
      "=== ë³µí•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ê³„ì‚° ===\n",
      "ê³ ê°ë²ˆí˜¸\tê¸°ë³¸\tìœˆë„ìš°\të°©í–¥ì„±\tì•ˆì •ì„±\të³µí•©ì ìˆ˜\n",
      "A1001\t0.0506\t0.0495\t0.0423\t0.9956\t0.2376\n",
      "A1002\t0.4685\t0.4368\t0.7783\t1.0153\t0.6303\n",
      "A1003\t0.246\t0.1931\t0.092\t0.0476\t0.1596\n",
      "A1004\t0.1925\t0.1487\t0.1451\t1.0102\t0.3334\n",
      "A1005\t0.2487\t0.2038\t0.2285\t1.0033\t0.3821\n",
      "A1006\t0.1393\t0.1214\t0.1143\t0.9733\t0.2957\n",
      "A1007\t0.2909\t0.2497\tnan\t1.0021\tnan\n",
      "A1008\t0.3906\t0.3854\tnan\t1.0118\tnan\n",
      "A1009\t0.1806\t0.1767\t0.1788\t0.9847\t0.3399\n",
      "A1010\t0.3419\t0.3355\tnan\t0.9949\tnan\n",
      "\n",
      "ğŸ“Š ë³€ë™ì„± ë“±ê¸‰ ê¸°ì¤€:\n",
      "  ì €ë³€ë™ì„±: < nan\n",
      "  ì¤‘ë³€ë™ì„±: nan ~ nan\n",
      "  ê³ ë³€ë™ì„±: > nan\n",
      "\n",
      "ë“±ê¸‰ë³„ ê³ ê° ë¶„ë¥˜:\n",
      "  A1001: 0.238 (ê³ ë³€ë™ì„±)\n",
      "  A1002: 0.630 (ê³ ë³€ë™ì„±)\n",
      "  A1003: 0.160 (ê³ ë³€ë™ì„±)\n",
      "  A1004: 0.333 (ê³ ë³€ë™ì„±)\n",
      "  A1005: 0.382 (ê³ ë³€ë™ì„±)\n",
      "  A1006: 0.296 (ê³ ë³€ë™ì„±)\n",
      "  A1007: nan (ê³ ë³€ë™ì„±)\n",
      "  A1008: nan (ê³ ë³€ë™ì„±)\n",
      "  A1009: 0.340 (ê³ ë³€ë™ì„±)\n",
      "  A1010: nan (ê³ ë³€ë™ì„±)\n",
      "\n",
      "ğŸ¯ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ì¸ì‚¬ì´íŠ¸:\n",
      "  1. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œì˜ í•„ìš”ì„± í™•ì¸\n",
      "  2. ì‹œê°„ ìœˆë„ìš°ë³„ ì°¨ë³„í™”ëœ ê°€ì¤‘ì¹˜ ì ìš©\n",
      "  3. ë°©í–¥ì„± ë³€ë™ì„±ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ë¹„ëŒ€ì¹­ì„± í¬ì°©\n",
      "  4. íŒ¨í„´ ì•ˆì •ì„±ìœ¼ë¡œ ì˜ˆì¸¡ê°€ëŠ¥ì„± í‰ê°€\n",
      "  5. ë³µí•© ìŠ¤ì½”ì–´ë¥¼ í†µí•œ ì¢…í•©ì  ë³€ë™ì„± í‰ê°€\n",
      "\n",
      "ğŸ’¡ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ë°©í–¥:\n",
      "  â€¢ Level-0 ëª¨ë¸: ê° ë³€ë™ì„± ì§€í‘œë¥¼ ê°œë³„ ëª¨ë¸ë¡œ êµ¬ì„±\n",
      "  â€¢ Level-1 ë©”íƒ€ëª¨ë¸: ê°€ì¤‘ ê²°í•©ìœ¼ë¡œ ìµœì¢… ë³€ë™ê³„ìˆ˜ ì‚°ì¶œ\n",
      "  â€¢ ê³¼ì í•© ë°©ì§€: êµì°¨ê²€ì¦ ë° ì •ê·œí™” ì ìš©\n",
      "\n",
      "ğŸ¯ 3ë‹¨ê³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ!\n",
      "ë‹¤ìŒ: 4ë‹¨ê³„ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.volatility_metrics = {}\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\"\"\"\n",
    "        print(\"=== ë³€ë™ì„± ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ===\")\n",
    "        \n",
    "        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” CSV íŒŒì¼ ì½ê¸°\n",
    "        # self.lp_data = pd.concat([\n",
    "        #     pd.read_csv('LPë°ì´í„°1.csv'),\n",
    "        #     pd.read_csv('LPë°ì´í„°2.csv')\n",
    "        # ], ignore_index=True)\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±\n",
    "        self.lp_data = self._create_volatility_test_data()\n",
    "        \n",
    "        # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì¶”ê°€\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(self.lp_data):,}ë ˆì½”ë“œ\")\n",
    "        return self.lp_data\n",
    "    \n",
    "    def _create_volatility_test_data(self):\n",
    "        \"\"\"ë³€ë™ì„± í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        # ë‹¤ì–‘í•œ ë³€ë™ì„± íŒ¨í„´ì„ ê°€ì§„ ê³ ê° ì •ì˜\n",
    "        volatility_profiles = {\n",
    "            'A1001': {'type': 'stable', 'base': 100, 'noise': 0.05},      # ì•ˆì •ì \n",
    "            'A1002': {'type': 'high_volatility', 'base': 80, 'noise': 0.3}, # ê³ ë³€ë™ì„±\n",
    "            'A1003': {'type': 'periodic', 'base': 120, 'noise': 0.1},     # ì£¼ê¸°ì \n",
    "            'A1004': {'type': 'trending', 'base': 90, 'noise': 0.15},     # íŠ¸ë Œë“œ\n",
    "            'A1005': {'type': 'seasonal', 'base': 110, 'noise': 0.2},     # ê³„ì ˆì„±\n",
    "            'A1006': {'type': 'jumpy', 'base': 85, 'noise': 0.1},         # ì í”„í˜•\n",
    "            'A1007': {'type': 'clustered', 'base': 95, 'noise': 0.25},    # í´ëŸ¬ìŠ¤í„°í˜•\n",
    "            'A1008': {'type': 'low_usage', 'base': 30, 'noise': 0.4},     # ì €ì‚¬ìš©ëŸ‰\n",
    "            'A1009': {'type': 'medium', 'base': 70, 'noise': 0.18},       # ì¤‘ê°„\n",
    "            'A1010': {'type': 'irregular', 'base': 105, 'noise': 0.35}    # ë¶ˆê·œì¹™\n",
    "        }\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = volatility_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # ê¸°ë³¸ íŒ¨í„´ ìƒì„±\n",
    "                        base_power = profile['base']\n",
    "                        \n",
    "                        # íƒ€ì…ë³„ íŒ¨í„´ ì ìš©\n",
    "                        if profile['type'] == 'stable':\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'high_volatility':\n",
    "                            # ë†’ì€ ë³€ë™ì„± - í° ë¬´ì‘ìœ„ ë³€í™”\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                            if np.random.random() < 0.1:  # 10% í™•ë¥ ë¡œ í° ì í”„\n",
    "                                power *= np.random.choice([0.3, 2.5])\n",
    "                        \n",
    "                        elif profile['type'] == 'periodic':\n",
    "                            # ì£¼ê¸°ì  íŒ¨í„´\n",
    "                            daily_cycle = np.sin(2 * np.pi * hour / 24)\n",
    "                            weekly_cycle = np.sin(2 * np.pi * day / 7)\n",
    "                            power = base_power * (1 + 0.3 * daily_cycle + 0.1 * weekly_cycle)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'trending':\n",
    "                            # íŠ¸ë Œë“œ íŒ¨í„´\n",
    "                            trend = 0.5 * day / days  # 30ì¼ê°„ 50% ì¦ê°€\n",
    "                            power = base_power * (1 + trend)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'seasonal':\n",
    "                            # ê³„ì ˆì„± íŒ¨í„´\n",
    "                            seasonal = 0.2 * np.sin(2 * np.pi * day / 30)\n",
    "                            power = base_power * (1 + seasonal)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'jumpy':\n",
    "                            # ê¸‰ê²©í•œ ë³€í™”ê°€ ìˆëŠ” íŒ¨í„´\n",
    "                            power = base_power\n",
    "                            if day % 7 == 0 and hour == 9:  # ì£¼ 1íšŒ í° ë³€í™”\n",
    "                                power *= 2.0\n",
    "                            elif day % 7 == 3 and hour == 15:\n",
    "                                power *= 0.4\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'clustered':\n",
    "                            # ë³€ë™ì„± í´ëŸ¬ìŠ¤í„°ë§ (ë³€ë™ì„±ì´ ë†’ì€ êµ¬ê°„ê³¼ ë‚®ì€ êµ¬ê°„)\n",
    "                            if day % 10 < 3:  # 30% ê¸°ê°„ ë™ì•ˆ ë†’ì€ ë³€ë™ì„±\n",
    "                                noise_factor = profile['noise'] * 2\n",
    "                            else:\n",
    "                                noise_factor = profile['noise'] * 0.5\n",
    "                            power = base_power + np.random.normal(0, base_power * noise_factor)\n",
    "                        \n",
    "                        else:  # irregular, low_usage, medium\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        power = max(0, power)  # ìŒìˆ˜ ë°©ì§€\n",
    "                        \n",
    "                        data.append({\n",
    "                            'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                            'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1),\n",
    "                            'ì§€ìƒë¬´íš¨': round(power * np.random.uniform(0.1, 0.3), 1),\n",
    "                            'ì§„ìƒë¬´íš¨': round(power * np.random.uniform(0.05, 0.15), 1),\n",
    "                            'í”¼ìƒì „ë ¥': round(power * np.random.uniform(1.0, 1.1), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_basic_volatility_metrics(self):\n",
    "        \"\"\"ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ===\")\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            # 1. ì „í†µì  ë³€ë™ê³„ìˆ˜ (CV)\n",
    "            cv = customer_data.std() / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 2. ë²”ìœ„ ê¸°ë°˜ ë³€ë™ì„±\n",
    "            range_volatility = (customer_data.max() - customer_data.min()) / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 3. ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë³€ë™ì„± (IQR/Median)\n",
    "            q75, q25 = np.percentile(customer_data, [75, 25])\n",
    "            iqr_volatility = (q75 - q25) / np.median(customer_data) if np.median(customer_data) > 0 else 0\n",
    "            \n",
    "            # 4. í‰ê· ì ˆëŒ€í¸ì°¨ (MAD)\n",
    "            mad = np.mean(np.abs(customer_data - customer_data.mean()))\n",
    "            mad_volatility = mad / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 5. ë³€í™”ìœ¨ ê¸°ë°˜ ë³€ë™ì„±\n",
    "            returns = customer_data.pct_change().dropna()\n",
    "            return_volatility = returns.std() if len(returns) > 0 else 0\n",
    "            \n",
    "            metrics[customer] = {\n",
    "                'cv': round(cv, 4),\n",
    "                'range_vol': round(range_volatility, 4),\n",
    "                'iqr_vol': round(iqr_volatility, 4),\n",
    "                'mad_vol': round(mad_volatility, 4),\n",
    "                'return_vol': round(return_volatility, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tCV\\të²”ìœ„ë³€ë™ì„±\\tIQRë³€ë™ì„±\\tMADë³€ë™ì„±\\tìˆ˜ìµë¥ ë³€ë™ì„±\")\n",
    "        for customer, metrics_dict in metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['cv']}\\t{metrics_dict['range_vol']}\\t{metrics_dict['iqr_vol']}\\t{metrics_dict['mad_vol']}\\t{metrics_dict['return_vol']}\")\n",
    "        \n",
    "        self.volatility_metrics['basic'] = metrics\n",
    "        return metrics\n",
    "    \n",
    "    def calculate_time_window_volatility(self):\n",
    "        \"\"\"ì‹œê°„ ìœˆë„ìš°ë³„ ë³€ë™ì„± ê³„ì‚°\"\"\"\n",
    "        print(\"\\n=== ì‹œê°„ ìœˆë„ìš°ë³„ ë³€ë™ì„± ë¶„ì„ ===\")\n",
    "        \n",
    "        window_metrics = {}\n",
    "        windows = {\n",
    "            'hourly': 4,    # 1ì‹œê°„ (4ê°œ 15ë¶„ êµ¬ê°„)\n",
    "            'daily': 96,    # 1ì¼ (96ê°œ 15ë¶„ êµ¬ê°„)\n",
    "            'weekly': 672   # 1ì£¼ (672ê°œ 15ë¶„ êµ¬ê°„)\n",
    "        }\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            window_metrics[customer] = {}\n",
    "            \n",
    "            for window_name, window_size in windows.items():\n",
    "                # ë¡¤ë§ ìœˆë„ìš°ë¡œ ë³€ë™ê³„ìˆ˜ ê³„ì‚°\n",
    "                rolling_std = power_series.rolling(window=window_size, min_periods=window_size//2).std()\n",
    "                rolling_mean = power_series.rolling(window=window_size, min_periods=window_size//2).mean()\n",
    "                rolling_cv = rolling_std / rolling_mean\n",
    "                \n",
    "                # ìœˆë„ìš°ë³„ ë³€ë™ì„± í†µê³„\n",
    "                window_metrics[customer][window_name] = {\n",
    "                    'mean_cv': round(rolling_cv.mean(), 4),\n",
    "                    'std_cv': round(rolling_cv.std(), 4),\n",
    "                    'max_cv': round(rolling_cv.max(), 4),\n",
    "                    'min_cv': round(rolling_cv.min(), 4)\n",
    "                }\n",
    "        \n",
    "        print(\"ìœˆë„ìš°ë³„ í‰ê·  ë³€ë™ê³„ìˆ˜:\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tì‹œê°„ë³„\\tì¼ë³„\\tì£¼ë³„\")\n",
    "        for customer in window_metrics.keys():\n",
    "            hourly_cv = window_metrics[customer]['hourly']['mean_cv']\n",
    "            daily_cv = window_metrics[customer]['daily']['mean_cv']\n",
    "            weekly_cv = window_metrics[customer]['weekly']['mean_cv']\n",
    "            print(f\"{customer}\\t{hourly_cv}\\t{daily_cv}\\t{weekly_cv}\")\n",
    "        \n",
    "        self.volatility_metrics['time_windows'] = window_metrics\n",
    "        return window_metrics\n",
    "    \n",
    "    def calculate_directional_volatility(self):\n",
    "        \"\"\"ë°©í–¥ì„± ë³€ë™ì„± ë¶„ì„ (ìƒìŠ¹/í•˜ë½ ë¹„ëŒ€ì¹­ì„±)\"\"\"\n",
    "        print(\"\\n=== ë°©í–¥ì„± ë³€ë™ì„± ë¶„ì„ ===\")\n",
    "        \n",
    "        directional_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            # ë³€í™”ìœ¨ ê³„ì‚°\n",
    "            returns = power_series.pct_change().dropna()\n",
    "            \n",
    "            # ìƒìŠ¹/í•˜ë½ ë¶„ë¦¬\n",
    "            upside_returns = returns[returns > 0]\n",
    "            downside_returns = returns[returns < 0]\n",
    "            \n",
    "            # ë°©í–¥ë³„ ë³€ë™ì„±\n",
    "            upside_volatility = upside_returns.std() if len(upside_returns) > 0 else 0\n",
    "            downside_volatility = abs(downside_returns.std()) if len(downside_returns) > 0 else 0\n",
    "            \n",
    "            # ë¹„ëŒ€ì¹­ì„± ì§€ìˆ˜\n",
    "            asymmetry_ratio = upside_volatility / downside_volatility if downside_volatility > 0 else 0\n",
    "            \n",
    "            # ê¸‰ê²©í•œ ë³€í™” íšŸìˆ˜\n",
    "            large_increases = len(returns[returns > 0.5])  # 50% ì´ìƒ ì¦ê°€\n",
    "            large_decreases = len(returns[returns < -0.5]) # 50% ì´ìƒ ê°ì†Œ\n",
    "            \n",
    "            directional_metrics[customer] = {\n",
    "                'upside_vol': round(upside_volatility, 4),\n",
    "                'downside_vol': round(downside_volatility, 4),\n",
    "                'asymmetry_ratio': round(asymmetry_ratio, 4),\n",
    "                'large_increases': large_increases,\n",
    "                'large_decreases': large_decreases\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tìƒìŠ¹ë³€ë™ì„±\\tí•˜ë½ë³€ë™ì„±\\të¹„ëŒ€ì¹­ë¹„ìœ¨\\tê¸‰ì¦íšŸìˆ˜\\tê¸‰ê°íšŸìˆ˜\")\n",
    "        for customer, metrics_dict in directional_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['upside_vol']}\\t{metrics_dict['downside_vol']}\\t{metrics_dict['asymmetry_ratio']}\\t{metrics_dict['large_increases']}\\t{metrics_dict['large_decreases']}\")\n",
    "        \n",
    "        self.volatility_metrics['directional'] = directional_metrics\n",
    "        return directional_metrics\n",
    "    \n",
    "    def calculate_pattern_stability(self):\n",
    "        \"\"\"íŒ¨í„´ ì•ˆì •ì„± ë¶„ì„\"\"\"\n",
    "        print(\"\\n=== íŒ¨í„´ ì•ˆì •ì„± ë¶„ì„ ===\")\n",
    "        \n",
    "        stability_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "            \n",
    "            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ì¼ê´€ì„±\n",
    "            hourly_patterns = []\n",
    "            for day in customer_data['date'].unique():\n",
    "                day_data = customer_data[customer_data['date'] == day]\n",
    "                hourly_avg = day_data.groupby('hour')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "                hourly_patterns.append(hourly_avg.values)\n",
    "            \n",
    "            # íŒ¨í„´ ê°„ ìƒê´€ê´€ê³„ (ì¼ê´€ì„± ì¸¡ì •)\n",
    "            if len(hourly_patterns) > 1:\n",
    "                correlations = []\n",
    "                for i in range(len(hourly_patterns)):\n",
    "                    for j in range(i+1, len(hourly_patterns)):\n",
    "                        if len(hourly_patterns[i]) == len(hourly_patterns[j]):\n",
    "                            corr = np.corrcoef(hourly_patterns[i], hourly_patterns[j])[0,1]\n",
    "                            if not np.isnan(corr):\n",
    "                                correlations.append(corr)\n",
    "                \n",
    "                pattern_consistency = np.mean(correlations) if correlations else 0\n",
    "            else:\n",
    "                pattern_consistency = 0\n",
    "            \n",
    "            # ì£¼ê¸°ì„± ê°•ë„ (FFT ê¸°ë°˜)\n",
    "            power_series = customer_data.sort_values('datetime')['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].values\n",
    "            if len(power_series) > 100:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ\n",
    "                fft = np.fft.fft(power_series)\n",
    "                fft_magnitude = np.abs(fft)\n",
    "                \n",
    "                # ì¼ì¼ ì£¼ê¸° (96í¬ì¸íŠ¸) ê°•ë„\n",
    "                daily_freq_idx = len(fft_magnitude) // 96 if len(fft_magnitude) >= 96 else 1\n",
    "                daily_periodicity = fft_magnitude[daily_freq_idx] / np.mean(fft_magnitude) if np.mean(fft_magnitude) > 0 else 0\n",
    "            else:\n",
    "                daily_periodicity = 0\n",
    "            \n",
    "            # ì˜ˆì¸¡ê°€ëŠ¥ì„± (ìê¸°ìƒê´€)\n",
    "            autocorr_1lag = power_series[1:].dot(power_series[:-1]) / (np.linalg.norm(power_series[1:]) * np.linalg.norm(power_series[:-1])) if len(power_series) > 1 else 0\n",
    "            \n",
    "            stability_metrics[customer] = {\n",
    "                'pattern_consistency': round(pattern_consistency, 4),\n",
    "                'daily_periodicity': round(daily_periodicity, 4),\n",
    "                'autocorrelation': round(autocorr_1lag, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tíŒ¨í„´ì¼ê´€ì„±\\tì¼ì¼ì£¼ê¸°ì„±\\tìê¸°ìƒê´€\")\n",
    "        for customer, metrics_dict in stability_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['pattern_consistency']}\\t{metrics_dict['daily_periodicity']}\\t{metrics_dict['autocorrelation']}\")\n",
    "        \n",
    "        self.volatility_metrics['stability'] = stability_metrics\n",
    "        return stability_metrics\n",
    "    \n",
    "    def create_composite_volatility_score(self):\n",
    "        \"\"\"ë³µí•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ìƒì„±\"\"\"\n",
    "        print(\"\\n=== ë³µí•© ë³€ë™ì„± ìŠ¤ì½”ì–´ ê³„ì‚° ===\")\n",
    "        \n",
    "        if not all(key in self.volatility_metrics for key in ['basic', 'time_windows', 'directional', 'stability']):\n",
    "            print(\"âŒ ëª¨ë“  ë³€ë™ì„± ì§€í‘œë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        composite_scores = {}\n",
    "        \n",
    "        for customer in self.lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° (0-1 ì •ê·œí™”)\n",
    "            basic_score = self.volatility_metrics['basic'][customer]['cv']\n",
    "            window_score = np.mean([\n",
    "                self.volatility_metrics['time_windows'][customer]['hourly']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['daily']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['weekly']['mean_cv']\n",
    "            ])\n",
    "            directional_score = (\n",
    "                self.volatility_metrics['directional'][customer]['upside_vol'] + \n",
    "                self.volatility_metrics['directional'][customer]['downside_vol']\n",
    "            ) / 2\n",
    "            \n",
    "            # ì•ˆì •ì„±ì€ ì—­ìˆ˜ë¡œ (ë‚®ì€ ì•ˆì •ì„± = ë†’ì€ ë³€ë™ì„±)\n",
    "            stability_score = 1 - self.volatility_metrics['stability'][customer]['pattern_consistency']\n",
    "            \n",
    "            # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "            weights = {\n",
    "                'basic': 0.3,       # ê¸°ë³¸ ë³€ë™ê³„ìˆ˜\n",
    "                'window': 0.3,      # ì‹œê°„ ìœˆë„ìš° ë³€ë™ì„±\n",
    "                'directional': 0.2, # ë°©í–¥ì„± ë³€ë™ì„±\n",
    "                'stability': 0.2    # íŒ¨í„´ ë¶ˆì•ˆì •ì„±\n",
    "            }\n",
    "            \n",
    "            composite_score = (\n",
    "                weights['basic'] * basic_score +\n",
    "                weights['window'] * window_score +\n",
    "                weights['directional'] * directional_score +\n",
    "                weights['stability'] * stability_score\n",
    "            )\n",
    "            \n",
    "            composite_scores[customer] = {\n",
    "                'basic_score': round(basic_score, 4),\n",
    "                'window_score': round(window_score, 4),\n",
    "                'directional_score': round(directional_score, 4),\n",
    "                'stability_score': round(stability_score, 4),\n",
    "                'composite_score': round(composite_score, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tê¸°ë³¸\\tìœˆë„ìš°\\të°©í–¥ì„±\\tì•ˆì •ì„±\\të³µí•©ì ìˆ˜\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            print(f\"{customer}\\t{scores['basic_score']}\\t{scores['window_score']}\\t{scores['directional_score']}\\t{scores['stability_score']}\\t{scores['composite_score']}\")\n",
    "        \n",
    "        # ë³€ë™ì„± ë“±ê¸‰ ë¶„ë¥˜\n",
    "        composite_values = [scores['composite_score'] for scores in composite_scores.values()]\n",
    "        percentiles = np.percentile(composite_values, [33, 67])\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ë³€ë™ì„± ë“±ê¸‰ ê¸°ì¤€:\")\n",
    "        print(f\"  ì €ë³€ë™ì„±: < {percentiles[0]:.3f}\")\n",
    "        print(f\"  ì¤‘ë³€ë™ì„±: {percentiles[0]:.3f} ~ {percentiles[1]:.3f}\")\n",
    "        print(f\"  ê³ ë³€ë™ì„±: > {percentiles[1]:.3f}\")\n",
    "        \n",
    "        print(\"\\në“±ê¸‰ë³„ ê³ ê° ë¶„ë¥˜:\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            score = scores['composite_score']\n",
    "            if score < percentiles[0]:\n",
    "                grade = \"ì €ë³€ë™ì„±\"\n",
    "            elif score < percentiles[1]:\n",
    "                grade = \"ì¤‘ë³€ë™ì„±\"\n",
    "            else:\n",
    "                grade = \"ê³ ë³€ë™ì„±\"\n",
    "            print(f\"  {customer}: {score:.3f} ({grade})\")\n",
    "        \n",
    "        self.volatility_metrics['composite'] = composite_scores\n",
    "        return composite_scores\n",
    "    \n",
    "    def generate_volatility_report(self):\n",
    "        \"\"\"ë³€ë™ì„± ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š ë³€ë™ì„± ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ëª¨ë“  ì§€í‘œ ê³„ì‚°\n",
    "        basic_metrics = self.calculate_basic_volatility_metrics()\n",
    "        window_metrics = self.calculate_time_window_volatility()\n",
    "        directional_metrics = self.calculate_directional_volatility()\n",
    "        stability_metrics = self.calculate_pattern_stability()\n",
    "        composite_scores = self.create_composite_volatility_score()\n",
    "        \n",
    "        print(\"\\nğŸ¯ ë³€ë™ê³„ìˆ˜ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ì¸ì‚¬ì´íŠ¸:\")\n",
    "        print(\"  1. ë‹¤ì°¨ì› ë³€ë™ì„± ì§€í‘œì˜ í•„ìš”ì„± í™•ì¸\")\n",
    "        print(\"  2. ì‹œê°„ ìœˆë„ìš°ë³„ ì°¨ë³„í™”ëœ ê°€ì¤‘ì¹˜ ì ìš©\")\n",
    "        print(\"  3. ë°©í–¥ì„± ë³€ë™ì„±ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ë¹„ëŒ€ì¹­ì„± í¬ì°©\")\n",
    "        print(\"  4. íŒ¨í„´ ì•ˆì •ì„±ìœ¼ë¡œ ì˜ˆì¸¡ê°€ëŠ¥ì„± í‰ê°€\")\n",
    "        print(\"  5. ë³µí•© ìŠ¤ì½”ì–´ë¥¼ í†µí•œ ì¢…í•©ì  ë³€ë™ì„± í‰ê°€\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ë°©í–¥:\")\n",
    "        print(\"  â€¢ Level-0 ëª¨ë¸: ê° ë³€ë™ì„± ì§€í‘œë¥¼ ê°œë³„ ëª¨ë¸ë¡œ êµ¬ì„±\")\n",
    "        print(\"  â€¢ Level-1 ë©”íƒ€ëª¨ë¸: ê°€ì¤‘ ê²°í•©ìœ¼ë¡œ ìµœì¢… ë³€ë™ê³„ìˆ˜ ì‚°ì¶œ\")\n",
    "        print(\"  â€¢ ê³¼ì í•© ë°©ì§€: êµì°¨ê²€ì¦ ë° ì •ê·œí™” ì ìš©\")\n",
    "        \n",
    "        return {\n",
    "            'basic': basic_metrics,\n",
    "            'time_windows': window_metrics,\n",
    "            'directional': directional_metrics,\n",
    "            'stability': stability_metrics,\n",
    "            'composite': composite_scores\n",
    "        }\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ\n",
    "if __name__ == \"__main__\":\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "    analyzer = KEPCOVolatilityAnalyzer()\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë”©\n",
    "    data = analyzer.load_and_prepare_data()\n",
    "    \n",
    "    # ì¢…í•© ë³€ë™ì„± ë¶„ì„\n",
    "    volatility_report = analyzer.generate_volatility_report()\n",
    "    \n",
    "    print(\"\\nğŸ¯ 3ë‹¨ê³„ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ!\")\n",
    "    print(\"ë‹¤ìŒ: 4ë‹¨ê³„ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ ê°œë°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3a0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸\n",
      "======================================================================\n",
      "1ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ìƒì„±...\n",
      "   ìƒì„± ì™„ë£Œ: 29,760ë ˆì½”ë“œ\n",
      "\n",
      "2ï¸âƒ£ ìŠ¤íƒœí‚¹ ëª¨ë¸ ì´ˆê¸°í™”...\n",
      "\n",
      "3ï¸âƒ£ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ...\n",
      "ğŸ”§ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ ì¤‘...\n",
      "âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: 10ëª… ê³ ê°, 13ê°œ íŠ¹ì„±\n",
      "\n",
      "4ï¸âƒ£ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„...\n",
      "ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: 10ê°œ ìƒ˜í”Œ, 12ê°œ íŠ¹ì„±\n",
      "\n",
      "5ï¸âƒ£ ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨...\n",
      "ğŸš€ ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
      "ğŸ” ë©”íƒ€ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n",
      "ëª¨ë¸ëª…\t\tMAE\t\tMSE\t\tRÂ²\n",
      "linear      \t0.0210\t\t0.0005\t\tnan\n",
      "ridge       \t0.0275\t\t0.0009\t\tnan\n",
      "lasso       \t0.1296\t\t0.0240\t\tnan\n",
      "elastic     \t0.0803\t\t0.0081\t\tnan\n",
      "rf          \t0.0958\t\t0.0119\t\tnan\n",
      "gbm         \t0.0795\t\t0.0069\t\tnan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "âŒ ì í•©í•œ ë©”íƒ€ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 545\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# 5. ëª¨ë¸ í›ˆë ¨\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5ï¸âƒ£ ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 545\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m    547\u001b[0m \u001b[38;5;66;03m# 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m6ï¸âƒ£ ì¢…í•© ë³€ë™ì„± ë¦¬í¬íŠ¸ ìƒì„±...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 292\u001b[0m, in \u001b[0;36mKEPCOVolatilityStackingModel.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ ì í•©í•œ ë©”íƒ€ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: âŒ ì í•©í•œ ë©”íƒ€ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityStackingModel:\n",
    "    \"\"\"\n",
    "    í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜\n",
    "    \n",
    "    Level-0: ë‹¤ì–‘í•œ ë³€ë™ì„± ì§€í‘œë“¤ì„ ê°œë³„ ëª¨ë¸ë¡œ êµ¬ì„±\n",
    "    Level-1: ë©”íƒ€ëª¨ë¸ë¡œ ìµœì¢… ë³€ë™ê³„ìˆ˜ ì‚°ì¶œ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.level0_models = {}\n",
    "        self.level1_model = None\n",
    "        self.scaler = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Level-0 ëª¨ë¸ ì •ì˜\n",
    "        self._initialize_level0_models()\n",
    "        \n",
    "        # ê³¼ì í•© ë°©ì§€ ì„¤ì •\n",
    "        self.cv_folds = 5\n",
    "        self.random_state = 42\n",
    "        \n",
    "    def _initialize_level0_models(self):\n",
    "        \"\"\"Level-0 ê¸°ë³¸ ëª¨ë¸ë“¤ ì´ˆê¸°í™”\"\"\"\n",
    "        self.level0_models = {\n",
    "            'traditional_cv': self._traditional_cv_model,\n",
    "            'range_volatility': self._range_volatility_model,\n",
    "            'iqr_volatility': self._iqr_volatility_model,\n",
    "            'mad_volatility': self._mad_volatility_model,\n",
    "            'return_volatility': self._return_volatility_model,\n",
    "            'window_volatility': self._window_volatility_model,\n",
    "            'percentile_volatility': self._percentile_volatility_model\n",
    "        }\n",
    "        \n",
    "    def _traditional_cv_model(self, data):\n",
    "        \"\"\"ì „í†µì  ë³€ë™ê³„ìˆ˜ ëª¨ë¸\"\"\"\n",
    "        return data.std() / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _range_volatility_model(self, data):\n",
    "        \"\"\"ë²”ìœ„ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        return (data.max() - data.min()) / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _iqr_volatility_model(self, data):\n",
    "        \"\"\"ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        q75, q25 = np.percentile(data, [75, 25])\n",
    "        median = np.median(data)\n",
    "        return (q75 - q25) / median if median > 0 else 0\n",
    "    \n",
    "    def _mad_volatility_model(self, data):\n",
    "        \"\"\"í‰ê· ì ˆëŒ€í¸ì°¨ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        mad = np.mean(np.abs(data - data.mean()))\n",
    "        return mad / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _return_volatility_model(self, data):\n",
    "        \"\"\"ìˆ˜ìµë¥  ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return 0\n",
    "        returns = data.pct_change().dropna()\n",
    "        return returns.std() if len(returns) > 0 else 0\n",
    "    \n",
    "    def _window_volatility_model(self, data, window_size=96):\n",
    "        \"\"\"ìœˆë„ìš° ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸ (ì¼ë³„)\"\"\"\n",
    "        if len(data) < window_size:\n",
    "            return self._traditional_cv_model(data)\n",
    "        \n",
    "        rolling_cv = []\n",
    "        for i in range(window_size, len(data) + 1):\n",
    "            window_data = data.iloc[i-window_size:i]\n",
    "            cv = self._traditional_cv_model(window_data)\n",
    "            rolling_cv.append(cv)\n",
    "        \n",
    "        return np.mean(rolling_cv) if rolling_cv else 0\n",
    "    \n",
    "    def _percentile_volatility_model(self, data):\n",
    "        \"\"\"ë°±ë¶„ìœ„ìˆ˜ ê¸°ë°˜ ë³€ë™ì„± ëª¨ë¸\"\"\"\n",
    "        p90 = np.percentile(data, 90)\n",
    "        p10 = np.percentile(data, 10)\n",
    "        p50 = np.percentile(data, 50)\n",
    "        return (p90 - p10) / p50 if p50 > 0 else 0\n",
    "    \n",
    "    def extract_features(self, lp_data):\n",
    "        \"\"\"\n",
    "        LP ë°ì´í„°ì—ì„œ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ\n",
    "        \n",
    "        Parameters:\n",
    "        lp_data: DataFrame with columns ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'LPìˆ˜ì‹ ì¼ì', 'ìˆœë°©í–¥ìœ íš¨ì „ë ¥', ...]\n",
    "        \n",
    "        Returns:\n",
    "        features_df: DataFrame with volatility features for each customer\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”§ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ ì¤‘...\")\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for customer in lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].unique():\n",
    "            customer_data = lp_data[lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'] == customer]\n",
    "            power_data = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥']\n",
    "            \n",
    "            # ì‹œê°„ ì •ë³´ ì¶”ê°€\n",
    "            if 'datetime' not in customer_data.columns:\n",
    "                customer_data = customer_data.copy()\n",
    "                customer_data['datetime'] = pd.to_datetime(customer_data['LPìˆ˜ì‹ ì¼ì'], format='%Y-%m-%d-%H:%M')\n",
    "                customer_data['hour'] = customer_data['datetime'].dt.hour\n",
    "                customer_data['weekday'] = customer_data['datetime'].dt.weekday\n",
    "            \n",
    "            # Level-0 ëª¨ë¸ë“¤ë¡œ íŠ¹ì„± ê³„ì‚°\n",
    "            features = {'customer_id': customer}\n",
    "            \n",
    "            for model_name, model_func in self.level0_models.items():\n",
    "                try:\n",
    "                    if model_name == 'window_volatility':\n",
    "                        features[model_name] = model_func(power_data, window_size=96)\n",
    "                    else:\n",
    "                        features[model_name] = model_func(power_data)\n",
    "                except:\n",
    "                    features[model_name] = 0\n",
    "            \n",
    "            # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ íŠ¹ì„±\n",
    "            features.update({\n",
    "                'mean_usage': power_data.mean(),\n",
    "                'total_usage': power_data.sum(),\n",
    "                'peak_ratio': power_data.max() / power_data.mean() if power_data.mean() > 0 else 0,\n",
    "                'zero_ratio': (power_data == 0).sum() / len(power_data),\n",
    "                'weekend_effect': self._calculate_weekend_effect(customer_data),\n",
    "                'peak_hour_concentration': self._calculate_peak_concentration(customer_data)\n",
    "            })\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        print(f\"âœ… íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(features_df)}ëª… ê³ ê°, {len(features_df.columns)-1}ê°œ íŠ¹ì„±\")\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def _calculate_weekend_effect(self, customer_data):\n",
    "        \"\"\"ì£¼ë§ íš¨ê³¼ ê³„ì‚°\"\"\"\n",
    "        try:\n",
    "            weekday_avg = customer_data[customer_data['weekday'] < 5]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            weekend_avg = customer_data[customer_data['weekday'] >= 5]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            \n",
    "            if weekday_avg > 0:\n",
    "                return abs(weekend_avg / weekday_avg - 1)\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _calculate_peak_concentration(self, customer_data):\n",
    "        \"\"\"í”¼í¬ ì‹œê°„ ì§‘ì¤‘ë„ ê³„ì‚°\"\"\"\n",
    "        try:\n",
    "            peak_hours = [9, 14, 18]  # ì¼ë°˜ì ì¸ í”¼í¬ ì‹œê°„\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            total_avg = customer_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()\n",
    "            \n",
    "            return peak_usage / total_avg if total_avg > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def prepare_training_data(self, features_df, target_column='traditional_cv'):\n",
    "        \"\"\"\n",
    "        í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„\n",
    "        target_column: íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ëª…\n",
    "        \n",
    "        Returns:\n",
    "        X, y: íŠ¹ì„± í–‰ë ¬ê³¼ íƒ€ê²Ÿ ë²¡í„°\n",
    "        \"\"\"\n",
    "        # íŠ¹ì„± ì„ íƒ (Level-0 ëª¨ë¸ ì¶œë ¥ë“¤)\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        y = features_df[target_column].copy()\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "        X = X.fillna(0)\n",
    "        y = y.fillna(0)\n",
    "        \n",
    "        # ì´ìƒì¹˜ ì²˜ë¦¬ (ìƒìœ„ 5%, í•˜ìœ„ 5% í´ë¦¬í•‘)\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                lower_bound = X[col].quantile(0.05)\n",
    "                upper_bound = X[col].quantile(0.95)\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {X.shape[0]}ê°œ ìƒ˜í”Œ, {X.shape[1]}ê°œ íŠ¹ì„±\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨\n",
    "        \n",
    "        Parameters:\n",
    "        X: íŠ¹ì„± í–‰ë ¬\n",
    "        y: íƒ€ê²Ÿ ë²¡í„° (ì‹¤ì œ ë³€ë™ê³„ìˆ˜)\n",
    "        \"\"\"\n",
    "        print(\"ğŸš€ ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
    "        \n",
    "        # ë°ì´í„° ì •ê·œí™”\n",
    "        self.scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ëŸ¬\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Level-1 ë©”íƒ€ëª¨ë¸ í›„ë³´ë“¤\n",
    "        meta_models = {\n",
    "            'linear': LinearRegression(),\n",
    "            'ridge': Ridge(alpha=1.0, random_state=self.random_state),\n",
    "            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state),\n",
    "            'rf': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=self.random_state),\n",
    "            'gbm': GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        # êµì°¨ê²€ì¦ìœ¼ë¡œ ìµœì  ë©”íƒ€ëª¨ë¸ ì„ íƒ\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "        best_model_name = None\n",
    "        \n",
    "        # ì‹œê³„ì—´ êµì°¨ê²€ì¦\n",
    "        tscv = TimeSeriesSplit(n_splits=self.cv_folds)\n",
    "        \n",
    "        print(\"ğŸ” ë©”íƒ€ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "        print(\"ëª¨ë¸ëª…\\t\\tMAE\\t\\tMSE\\t\\tRÂ²\")\n",
    "        \n",
    "        for model_name, model in meta_models.items():\n",
    "            try:\n",
    "                # êµì°¨ê²€ì¦ ì ìˆ˜ ê³„ì‚°\n",
    "                mae_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "                mse_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "                r2_scores = cross_val_score(model, X_scaled, y, cv=tscv, scoring='r2')\n",
    "                \n",
    "                avg_mae = np.mean(mae_scores)\n",
    "                avg_mse = np.mean(mse_scores)\n",
    "                avg_r2 = np.mean(r2_scores)\n",
    "                \n",
    "                print(f\"{model_name:<12}\\t{avg_mae:.4f}\\t\\t{avg_mse:.4f}\\t\\t{avg_r2:.4f}\")\n",
    "                \n",
    "                # RÂ² ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ\n",
    "                if avg_r2 > best_score:\n",
    "                    best_score = avg_r2\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{model_name:<12}\\tì—ëŸ¬: {str(e)[:30]}...\")\n",
    "        \n",
    "        # ìµœì  ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° í›ˆë ¨\n",
    "        if best_model is not None:\n",
    "            self.level1_model = best_model\n",
    "            self.level1_model.fit(X_scaled, y)\n",
    "            \n",
    "            print(f\"\\nğŸ† ìµœì  ë©”íƒ€ëª¨ë¸: {best_model_name} (RÂ² = {best_score:.4f})\")\n",
    "            \n",
    "            # íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "            if hasattr(self.level1_model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': self.level1_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\nğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ (ìƒìœ„ 5ê°œ):\")\n",
    "                for _, row in importance_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "            \n",
    "            elif hasattr(self.level1_model, 'coef_'):\n",
    "                coef_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'coefficient': self.level1_model.coef_\n",
    "                }).sort_values('coefficient', key=abs, ascending=False)\n",
    "                \n",
    "                print(\"\\nğŸ“Š ëª¨ë¸ ê³„ìˆ˜ (ì ˆëŒ“ê°’ ìƒìœ„ 5ê°œ):\")\n",
    "                for _, row in coef_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['coefficient']:.4f}\")\n",
    "            \n",
    "            self.is_fitted = True\n",
    "            print(\"âœ… ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"âŒ ì í•©í•œ ë©”íƒ€ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, features_df):\n",
    "        \"\"\"\n",
    "        ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„\n",
    "        \n",
    "        Returns:\n",
    "        predictions: ì˜ˆì¸¡ëœ ë³€ë™ê³„ìˆ˜\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.\")\n",
    "        \n",
    "        # í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ íŠ¹ì„± ì„ íƒ\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # ì •ê·œí™”\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # ì˜ˆì¸¡\n",
    "        predictions = self.level1_model.predict(X_scaled)\n",
    "        \n",
    "        # ìŒìˆ˜ ë°©ì§€\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ì˜ˆì¸¡\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        y_pred = self.level1_model.predict(X_test_scaled)\n",
    "        y_pred = np.maximum(y_pred, 0)  # ìŒìˆ˜ ë°©ì§€\n",
    "        \n",
    "        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # ìƒëŒ€ ì˜¤ì°¨ ê³„ì‚°\n",
    "        relative_errors = np.abs((y_test - y_pred) / (y_test + 1e-8))\n",
    "        mape = np.mean(relative_errors) * 100\n",
    "        \n",
    "        print(\"ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:\")\n",
    "        print(f\"  MAE (í‰ê· ì ˆëŒ€ì˜¤ì°¨): {mae:.4f}\")\n",
    "        print(f\"  MSE (í‰ê· ì œê³±ì˜¤ì°¨): {mse:.4f}\")\n",
    "        print(f\"  RMSE (ì œê³±ê·¼í‰ê· ì œê³±ì˜¤ì°¨): {rmse:.4f}\")\n",
    "        print(f\"  RÂ² (ê²°ì •ê³„ìˆ˜): {r2:.4f}\")\n",
    "        print(f\"  MAPE (í‰ê· ì ˆëŒ€ë°±ë¶„ìœ¨ì˜¤ì°¨): {mape:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape\n",
    "        }\n",
    "    \n",
    "    def detect_anomalous_volatility(self, features_df, threshold_percentile=95):\n",
    "        \"\"\"\n",
    "        ì´ìƒ ë³€ë™ì„± íƒì§€\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: íŠ¹ì„± ë°ì´í„°í”„ë ˆì„\n",
    "        threshold_percentile: ì´ìƒì¹˜ ê¸°ì¤€ ë°±ë¶„ìœ„ìˆ˜\n",
    "        \n",
    "        Returns:\n",
    "        anomaly_results: ì´ìƒì¹˜ íƒì§€ ê²°ê³¼\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(\"ğŸš¨ ì´ìƒ ë³€ë™ì„± íƒì§€ ì¤‘...\")\n",
    "        \n",
    "        # ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # ì´ìƒì¹˜ ê¸°ì¤€ ì„¤ì •\n",
    "        threshold = np.percentile(predicted_volatility, threshold_percentile)\n",
    "        \n",
    "        # ì´ìƒì¹˜ íƒì§€\n",
    "        anomaly_mask = predicted_volatility > threshold\n",
    "        \n",
    "        anomaly_results = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'is_anomaly': anomaly_mask,\n",
    "            'anomaly_score': (predicted_volatility - threshold) / threshold\n",
    "        })\n",
    "        \n",
    "        anomaly_customers = anomaly_results[anomaly_results['is_anomaly']]\n",
    "        \n",
    "        print(f\"ğŸ¯ íƒì§€ ê²°ê³¼:\")\n",
    "        print(f\"  ì „ì²´ ê³ ê°: {len(anomaly_results)}ëª…\")\n",
    "        print(f\"  ì´ìƒ ë³€ë™ì„± ê³ ê°: {len(anomaly_customers)}ëª…\")\n",
    "        print(f\"  ì´ìƒì¹˜ ë¹„ìœ¨: {len(anomaly_customers)/len(anomaly_results)*100:.1f}%\")\n",
    "        \n",
    "        if len(anomaly_customers) > 0:\n",
    "            print(f\"\\nâš ï¸ ì£¼ì˜ í•„ìš” ê³ ê°:\")\n",
    "            for _, row in anomaly_customers.sort_values('anomaly_score', ascending=False).iterrows():\n",
    "                print(f\"  {row['customer_id']}: ë³€ë™ê³„ìˆ˜ {row['predicted_volatility']:.3f} (ì ìˆ˜: {row['anomaly_score']:.2f})\")\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def generate_volatility_report(self, features_df):\n",
    "        \"\"\"ì¢…í•© ë³€ë™ì„± ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"âŒ ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ì¢…í•© ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ë³€ë™ê³„ìˆ˜ ì˜ˆì¸¡\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # í†µê³„ ìš”ì•½\n",
    "        print(\"\\nğŸ“Š ë³€ë™ê³„ìˆ˜ ë¶„í¬:\")\n",
    "        print(f\"  í‰ê· : {np.mean(predicted_volatility):.3f}\")\n",
    "        print(f\"  í‘œì¤€í¸ì°¨: {np.std(predicted_volatility):.3f}\")\n",
    "        print(f\"  ìµœì†Œê°’: {np.min(predicted_volatility):.3f}\")\n",
    "        print(f\"  ìµœëŒ€ê°’: {np.max(predicted_volatility):.3f}\")\n",
    "        print(f\"  ì¤‘ìœ„ìˆ˜: {np.median(predicted_volatility):.3f}\")\n",
    "        \n",
    "        # ë“±ê¸‰ë³„ ë¶„ë¥˜\n",
    "        p33 = np.percentile(predicted_volatility, 33)\n",
    "        p67 = np.percentile(predicted_volatility, 67)\n",
    "        \n",
    "        print(f\"\\nğŸ·ï¸ ë³€ë™ì„± ë“±ê¸‰:\")\n",
    "        print(f\"  ì•ˆì •í˜• (< {p33:.3f}): {np.sum(predicted_volatility < p33)}ëª…\")\n",
    "        print(f\"  ë³´í†µí˜• ({p33:.3f} ~ {p67:.3f}): {np.sum((predicted_volatility >= p33) & (predicted_volatility < p67))}ëª…\")\n",
    "        print(f\"  ë³€ë™í˜• (â‰¥ {p67:.3f}): {np.sum(predicted_volatility >= p67)}ëª…\")\n",
    "        \n",
    "        # ê³ ê°ë³„ ê²°ê³¼\n",
    "        results_df = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'actual_cv': features_df['traditional_cv'] if 'traditional_cv' in features_df.columns else np.nan\n",
    "        })\n",
    "        \n",
    "        results_df['grade'] = pd.cut(results_df['predicted_volatility'], \n",
    "                                   bins=[0, p33, p67, np.inf], \n",
    "                                   labels=['ì•ˆì •í˜•', 'ë³´í†µí˜•', 'ë³€ë™í˜•'])\n",
    "        \n",
    "        print(f\"\\nğŸ‘¥ ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜:\")\n",
    "        print(\"ê³ ê°ë²ˆí˜¸\\tì˜ˆì¸¡ë³€ë™ê³„ìˆ˜\\të“±ê¸‰\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['customer_id']}\\t{row['predicted_volatility']:.3f}\\t\\t{row['grade']}\")\n",
    "        \n",
    "        # ì´ìƒ ë³€ë™ì„± íƒì§€\n",
    "        anomaly_results = self.detect_anomalous_volatility(features_df)\n",
    "        \n",
    "        print(f\"\\nğŸ’¡ í™œìš© ë°©ì•ˆ:\")\n",
    "        print(\"  1. ë¹„ì •ìƒì  ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ì¡°ê¸° ê°ì§€\")\n",
    "        print(\"  2. ê³ ê°ë³„ ë§ì¶¤í˜• ì „ë ¥ ê´€ë¦¬ ì„œë¹„ìŠ¤\")\n",
    "        print(\"  3. ì˜ì—… ë¦¬ìŠ¤í¬ í‰ê°€ ë° ê´€ë¦¬\")\n",
    "        print(\"  4. ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ\")\n",
    "        \n",
    "        return {\n",
    "            'predictions': results_df,\n",
    "            'anomalies': anomaly_results,\n",
    "            'statistics': {\n",
    "                'mean': np.mean(predicted_volatility),\n",
    "                'std': np.std(predicted_volatility),\n",
    "                'percentiles': {'p33': p33, 'p67': p67}\n",
    "            }\n",
    "        }\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì œ ë° í…ŒìŠ¤íŠ¸\n",
    "def create_sample_lp_data():\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ LP ë°ì´í„° ìƒì„±\"\"\"\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    data = []\n",
    "    customers = [f'A{1001+i}' for i in range(10)]\n",
    "    start_date = datetime(2024, 3, 1)\n",
    "    \n",
    "    # ë‹¤ì–‘í•œ ë³€ë™ì„± íŒ¨í„´ì˜ ê³ ê°ë“¤\n",
    "    volatility_patterns = {\n",
    "        'A1001': 0.3,   # ì•ˆì •ì \n",
    "        'A1002': 1.2,   # ë†’ì€ ë³€ë™ì„±\n",
    "        'A1003': 0.25,  # ë§¤ìš° ì•ˆì •ì \n",
    "        'A1004': 0.8,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1005': 0.9,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1006': 0.2,   # ì•ˆì •ì \n",
    "        'A1007': 1.5,   # ë§¤ìš° ë†’ì€ ë³€ë™ì„±\n",
    "        'A1008': 0.7,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1009': 0.6,   # ì¤‘ê°„ ë³€ë™ì„±\n",
    "        'A1010': 0.4    # ì•ˆì •ì \n",
    "    }\n",
    "    \n",
    "    for customer in customers:\n",
    "        target_cv = volatility_patterns[customer]\n",
    "        base_power = random.uniform(50, 150)\n",
    "        \n",
    "        for day in range(31):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            for hour in range(24):\n",
    "                for minute in [0, 15, 30, 45]:\n",
    "                    timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                    \n",
    "                    # ë³€ë™ì„±ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ìƒì„±\n",
    "                    noise_std = base_power * target_cv\n",
    "                    power = base_power + random.gauss(0, noise_std)\n",
    "                    power = max(0, power)\n",
    "                    \n",
    "                    data.append({\n",
    "                        'ëŒ€ì²´ê³ ê°ë²ˆí˜¸': customer,\n",
    "                        'LPìˆ˜ì‹ ì¼ì': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                        'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': round(power, 1)\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ í•œêµ­ì „ë ¥ê³µì‚¬ ì „ë ¥ ì‚¬ìš©íŒ¨í„´ ë³€ë™ê³„ìˆ˜ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    print(\"1ï¸âƒ£ ìƒ˜í”Œ ë°ì´í„° ìƒì„±...\")\n",
    "    lp_data = create_sample_lp_data()\n",
    "    print(f\"   ìƒì„± ì™„ë£Œ: {len(lp_data):,}ë ˆì½”ë“œ\")\n",
    "    \n",
    "    # 2. ìŠ¤íƒœí‚¹ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    print(\"\\n2ï¸âƒ£ ìŠ¤íƒœí‚¹ ëª¨ë¸ ì´ˆê¸°í™”...\")\n",
    "    stacking_model = KEPCOVolatilityStackingModel()\n",
    "    \n",
    "    # 3. íŠ¹ì„± ì¶”ì¶œ\n",
    "    print(\"\\n3ï¸âƒ£ ë³€ë™ì„± íŠ¹ì„± ì¶”ì¶œ...\")\n",
    "    features_df = stacking_model.extract_features(lp_data)\n",
    "    \n",
    "    # 4. í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "    print(\"\\n4ï¸âƒ£ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„...\")\n",
    "    X, y = stacking_model.prepare_training_data(features_df)\n",
    "    \n",
    "    # 5. ëª¨ë¸ í›ˆë ¨\n",
    "    print(\"\\n5ï¸âƒ£ ìŠ¤íƒœí‚¹ ëª¨ë¸ í›ˆë ¨...\")\n",
    "    stacking_model.fit(X, y)\n",
    "    \n",
    "    # 6. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    print(\"\\n6ï¸âƒ£ ì¢…í•© ë³€ë™ì„± ë¦¬í¬íŠ¸ ìƒì„±...\")\n",
    "    report = stacking_model.generate_volatility_report(features_df)\n",
    "    \n",
    "    print(\"\\nğŸ¯ ìŠ¤íƒœí‚¹ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ì™„ë£Œ!\")\n",
    "    print(\"   â€¢ Level-0: 7ê°œ ê¸°ë³¸ ë³€ë™ì„± ëª¨ë¸\")\n",
    "    print(\"   â€¢ Level-1: ìµœì í™”ëœ ë©”íƒ€ëª¨ë¸\")\n",
    "    print(\"   â€¢ ê³¼ì í•© ë°©ì§€: êµì°¨ê²€ì¦ + ì •ê·œí™”\")\n",
    "    print(\"   â€¢ ì´ìƒ íƒì§€: ìë™ ì„ê³„ê°’ ì„¤ì •\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
