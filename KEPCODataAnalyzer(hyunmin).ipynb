{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7203f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 고객 기본정보 로딩 ===\n",
      "총 고객 수: 200명\n",
      "컬럼: ['순번', '고객번호', '계약전력', '계약종별', '사용용도', '주생산품', '산업분류']\n",
      "\n",
      "기본 정보:\n",
      "   순번   고객번호     계약전력            계약종별     사용용도  주생산품     산업분류\n",
      "0   1  A1001  100~199  226 일반용(을) 고압A   02 상업용   제조업  299기타업종\n",
      "1   2  A1002  500~599  222 일반용(갑)‖고압A   02 상업용    교회  769기타업종\n",
      "2   3  A1003  700~799  322 산업용(갑)‖고압A  09 광공업용  금속가공  858기타업종\n",
      "3   4  A1004  800~899  322 산업용(갑)‖고압A   02 상업용    상가  401기타업종\n",
      "4   5  A1005  800~899  226 일반용(을) 고압A   02 상업용   제조업  364기타업종\n",
      "\n",
      "=== 고객 분포 분석 ===\n",
      "\n",
      "📊 계약종별 분포:\n",
      "  322 산업용(갑)‖고압A: 56명 (28.0%)\n",
      "  222 일반용(갑)‖고압A: 52명 (26.0%)\n",
      "  226 일반용(을) 고압A: 46명 (23.0%)\n",
      "  726 산업용(을) 고압A: 46명 (23.0%)\n",
      "\n",
      "🏭 사용용도별 분포:\n",
      "  02 상업용: 101명 (50.5%)\n",
      "  09 광공업용: 99명 (49.5%)\n",
      "\n",
      "⚡ 계약전력 분포:\n",
      "  100~199kW: 28명 (14.0%)\n",
      "  200~299kW: 30명 (15.0%)\n",
      "  400~499kW: 31명 (15.5%)\n",
      "  500~599kW: 38명 (19.0%)\n",
      "  700~799kW: 27명 (13.5%)\n",
      "  800~899kW: 46명 (23.0%)\n",
      "\n",
      "=== LP 데이터 로딩 ===\n",
      "파일 1 로딩: LP데이터1.csv\n",
      "  레코드 수: 14,400\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-01-00:00 ~ 2024-03-15-23:45\n",
      "파일 2 로딩: LP데이터2.csv\n",
      "  레코드 수: 15,360\n",
      "  고객 수: 10\n",
      "  기간: 2024-03-16-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "✅ 전체 LP 데이터 결합 완료:\n",
      "  총 레코드: 29,760\n",
      "  총 고객: 10\n",
      "\n",
      "=== LP 데이터 품질 분석 ===\n",
      "📈 기본 통계:\n",
      "            순방향유효전력          지상무효          진상무효          피상전력\n",
      "count  29760.000000  29760.000000  29760.000000  29760.000000\n",
      "mean      79.859856     10.063058      5.113498     79.838306\n",
      "std       23.934716      4.852545      2.804499     23.595475\n",
      "min       12.600000      0.000000      0.000000     15.800000\n",
      "25%       60.000000      6.500000      3.000000     60.100000\n",
      "50%       80.100000     10.000000      5.000000     79.900000\n",
      "75%       99.600000     13.400000      7.000000     99.500000\n",
      "max      155.100000     31.200000     18.200000    143.200000\n",
      "\n",
      "⏰ 시간 간격 체크:\n",
      "  A1001: 평균 간격 15.0분, 표준편차 0.0분\n",
      "  A1002: 평균 간격 15.0분, 표준편차 0.0분\n",
      "  A1003: 평균 간격 15.0분, 표준편차 0.0분\n",
      "\n",
      "🔍 데이터 품질 체크:\n",
      "  순방향유효전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "    음수: 0건 (0.00%)\n",
      "  지상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 35건 (0.12%)\n",
      "    음수: 0건 (0.00%)\n",
      "  진상무효:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 104건 (0.35%)\n",
      "    음수: 0건 (0.00%)\n",
      "  피상전력:\n",
      "    결측치: 0건 (0.00%)\n",
      "    0값: 0건 (0.00%)\n",
      "    음수: 0건 (0.00%)\n",
      "\n",
      "👥 고객별 데이터 완정성:\n",
      "  예상 레코드 수: 2,976개/고객\n",
      "  실제 레코드 수: 2,976~2,976개/고객\n",
      "  ✅ 모든 고객 데이터 완정성 양호\n",
      "\n",
      "=== 이상치 탐지 (IQR 방법) ===\n",
      "\n",
      "📊 순방향유효전력:\n",
      "  이상치: 0건 (0.00%)\n",
      "\n",
      "📊 지상무효:\n",
      "  이상치: 86건 (0.29%)\n",
      "  범위: 23.8 ~ 31.2\n",
      "\n",
      "📊 진상무효:\n",
      "  이상치: 106건 (0.36%)\n",
      "  범위: 13.1 ~ 18.2\n",
      "\n",
      "📊 피상전력:\n",
      "  이상치: 0건 (0.00%)\n",
      "\n",
      "============================================================\n",
      "📋 데이터 품질 종합 리포트\n",
      "============================================================\n",
      "\n",
      "🔢 데이터 규모:\n",
      "  • 고객 기본정보: 200명\n",
      "  • LP 데이터: 29,760레코드\n",
      "  • 분석 대상 고객: 10명\n",
      "  • 분석 기간: 2024-03-01-00:00 ~ 2024-03-31-23:45\n",
      "\n",
      "✅ 데이터 품질 상태:\n",
      "  • 결측치 비율: 0.00%\n",
      "  • 0값 비율: 0.12%\n",
      "  • 음수값 비율: 0.00%\n",
      "\n",
      "💡 다음 단계 권장사항:\n",
      "  1. 시계열 패턴 분석 (일/주/월별 사용 패턴)\n",
      "  2. 고객별 사용량 프로파일링\n",
      "  3. 변동성 지표 계산 및 비교\n",
      "  4. 이상 패턴 탐지 알고리즘 개발\n",
      "\n",
      "🎯 1단계 데이터 품질 점검 완료!\n",
      "다음: 2단계 시계열 패턴 분석 준비 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class KEPCODataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.customer_data = None\n",
    "        self.lp_data = None\n",
    "        \n",
    "    def load_customer_data(self, file_path):\n",
    "        \"\"\"고객 기본정보 로딩 및 기본 분석\"\"\"\n",
    "        print(\"=== 고객 기본정보 로딩 ===\")\n",
    "        \n",
    "        # Excel 파일 읽기 (실제 환경에서는 이 부분 사용)\n",
    "        # self.customer_data = pd.read_excel(file_path)\n",
    "        \n",
    "        # 샘플 데이터 생성 (테스트용)\n",
    "        self.customer_data = self._create_sample_customer_data()\n",
    "        \n",
    "        print(f\"총 고객 수: {len(self.customer_data):,}명\")\n",
    "        print(f\"컬럼: {list(self.customer_data.columns)}\")\n",
    "        print(\"\\n기본 정보:\")\n",
    "        print(self.customer_data.head())\n",
    "        \n",
    "        return self._analyze_customer_distribution()\n",
    "    \n",
    "    def _create_sample_customer_data(self):\n",
    "        \"\"\"테스트용 샘플 고객 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        contract_types = ['222 일반용(갑)‖고압A', '226 일반용(을) 고압A', '322 산업용(갑)‖고압A', '726 산업용(을) 고압A']\n",
    "        usage_types = ['02 상업용', '09 광공업용']\n",
    "        power_ranges = ['100~199', '200~299', '400~499', '500~599', '700~799', '800~899']\n",
    "        industries = ['병원', '교회', '상가', 'CNG충전', '금속가공', '점포', '온천탕', '제조업', '마트']\n",
    "        \n",
    "        for i in range(1, 201):  # 200명\n",
    "            data.append({\n",
    "                '순번': i,\n",
    "                '고객번호': f'A{1000+i}',\n",
    "                '계약전력': np.random.choice(power_ranges),\n",
    "                '계약종별': np.random.choice(contract_types),\n",
    "                '사용용도': np.random.choice(usage_types),\n",
    "                '주생산품': np.random.choice(industries),\n",
    "                '산업분류': f'{np.random.randint(100,999)}기타업종'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_customer_distribution(self):\n",
    "        \"\"\"고객 분포 분석\"\"\"\n",
    "        print(\"\\n=== 고객 분포 분석 ===\")\n",
    "        \n",
    "        # 계약종별 분포\n",
    "        contract_dist = self.customer_data['계약종별'].value_counts()\n",
    "        print(\"\\n📊 계약종별 분포:\")\n",
    "        for contract, count in contract_dist.items():\n",
    "            print(f\"  {contract}: {count}명 ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # 사용용도별 분포\n",
    "        usage_dist = self.customer_data['사용용도'].value_counts()\n",
    "        print(\"\\n🏭 사용용도별 분포:\")\n",
    "        for usage, count in usage_dist.items():\n",
    "            print(f\"  {usage}: {count}명 ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        # 계약전력 분포\n",
    "        power_dist = self.customer_data['계약전력'].value_counts().sort_index()\n",
    "        print(\"\\n⚡ 계약전력 분포:\")\n",
    "        for power, count in power_dist.items():\n",
    "            print(f\"  {power}kW: {count}명 ({count/len(self.customer_data)*100:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'contract_distribution': contract_dist,\n",
    "            'usage_distribution': usage_dist,\n",
    "            'power_distribution': power_dist\n",
    "        }\n",
    "    \n",
    "    def load_lp_data(self, file_paths):\n",
    "        \"\"\"LP 데이터 로딩 및 결합\"\"\"\n",
    "        print(\"\\n=== LP 데이터 로딩 ===\")\n",
    "        \n",
    "        lp_dataframes = []\n",
    "        \n",
    "        for i, file_path in enumerate(file_paths, 1):\n",
    "            print(f\"파일 {i} 로딩: {file_path}\")\n",
    "            \n",
    "            # CSV 파일 읽기 (실제 환경에서는 이 부분 사용)\n",
    "            # df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            \n",
    "            # 샘플 데이터 생성 (테스트용)\n",
    "            df = self._create_sample_lp_data(i)\n",
    "            \n",
    "            print(f\"  레코드 수: {len(df):,}\")\n",
    "            print(f\"  고객 수: {df['대체고객번호'].nunique()}\")\n",
    "            print(f\"  기간: {df['LP수신일자'].min()} ~ {df['LP수신일자'].max()}\")\n",
    "            \n",
    "            lp_dataframes.append(df)\n",
    "        \n",
    "        # 데이터 결합\n",
    "        self.lp_data = pd.concat(lp_dataframes, ignore_index=True)\n",
    "        print(f\"\\n✅ 전체 LP 데이터 결합 완료:\")\n",
    "        print(f\"  총 레코드: {len(self.lp_data):,}\")\n",
    "        print(f\"  총 고객: {self.lp_data['대체고객번호'].nunique()}\")\n",
    "        \n",
    "        return self._analyze_lp_quality()\n",
    "    \n",
    "    def _create_sample_lp_data(self, file_num):\n",
    "        \"\"\"테스트용 샘플 LP 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        \n",
    "        # 파일별로 다른 기간 설정\n",
    "        if file_num == 1:\n",
    "            start_date = datetime(2024, 3, 1)\n",
    "            days = 15\n",
    "        else:\n",
    "            start_date = datetime(2024, 3, 16) \n",
    "            days = 16\n",
    "        \n",
    "        for customer in customers:\n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # 시간대별 패턴을 반영한 가상 데이터\n",
    "                        base_power = 80 + 30 * np.sin(2 * np.pi * hour / 24) + np.random.normal(0, 10)\n",
    "                        base_power = max(0, base_power)\n",
    "                        \n",
    "                        data.append({\n",
    "                            '대체고객번호': customer,\n",
    "                            'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            '순방향유효전력': round(base_power + np.random.normal(0, 5), 1),\n",
    "                            '지상무효': round(abs(np.random.normal(10, 5)), 1),\n",
    "                            '진상무효': round(abs(np.random.normal(5, 3)), 1),\n",
    "                            '피상전력': round(base_power + np.random.normal(0, 3), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def _analyze_lp_quality(self):\n",
    "        \"\"\"LP 데이터 품질 분석\"\"\"\n",
    "        print(\"\\n=== LP 데이터 품질 분석 ===\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        print(\"📈 기본 통계:\")\n",
    "        numeric_cols = ['순방향유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        print(self.lp_data[numeric_cols].describe())\n",
    "        \n",
    "        # 시간 간격 체크\n",
    "        print(\"\\n⏰ 시간 간격 체크:\")\n",
    "        self.lp_data['LP수신일자_dt'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique()[:3]:  # 처음 3명만 체크\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('LP수신일자_dt')\n",
    "            time_diffs = customer_data['LP수신일자_dt'].diff().dt.total_seconds() / 60  # 분 단위\n",
    "            \n",
    "            print(f\"  {customer}: 평균 간격 {time_diffs.mean():.1f}분, 표준편차 {time_diffs.std():.1f}분\")\n",
    "            \n",
    "            # 15분이 아닌 간격 찾기\n",
    "            non_15min = time_diffs[(time_diffs != 15.0) & (~time_diffs.isna())]\n",
    "            if len(non_15min) > 0:\n",
    "                print(f\"    ⚠️ 비정상 간격: {len(non_15min)}건\")\n",
    "        \n",
    "        # 결측치 및 이상치 체크\n",
    "        print(\"\\n🔍 데이터 품질 체크:\")\n",
    "        for col in numeric_cols:\n",
    "            null_count = self.lp_data[col].isnull().sum()\n",
    "            zero_count = (self.lp_data[col] == 0).sum()\n",
    "            negative_count = (self.lp_data[col] < 0).sum()\n",
    "            \n",
    "            print(f\"  {col}:\")\n",
    "            print(f\"    결측치: {null_count:,}건 ({null_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    0값: {zero_count:,}건 ({zero_count/len(self.lp_data)*100:.2f}%)\")\n",
    "            print(f\"    음수: {negative_count:,}건 ({negative_count/len(self.lp_data)*100:.2f}%)\")\n",
    "        \n",
    "        # 고객별 데이터 완정성\n",
    "        print(\"\\n👥 고객별 데이터 완정성:\")\n",
    "        customer_counts = self.lp_data['대체고객번호'].value_counts()\n",
    "        expected_records = 24 * 4 * 31  # 15분 간격 × 1개월\n",
    "        \n",
    "        print(f\"  예상 레코드 수: {expected_records:,}개/고객\")\n",
    "        print(f\"  실제 레코드 수: {customer_counts.min():,}~{customer_counts.max():,}개/고객\")\n",
    "        \n",
    "        incomplete_customers = customer_counts[customer_counts < expected_records * 0.95]  # 95% 미만\n",
    "        if len(incomplete_customers) > 0:\n",
    "            print(f\"  ⚠️ 불완전한 고객: {len(incomplete_customers)}명\")\n",
    "        else:\n",
    "            print(\"  ✅ 모든 고객 데이터 완정성 양호\")\n",
    "        \n",
    "        return {\n",
    "            'total_records': len(self.lp_data),\n",
    "            'customers': self.lp_data['대체고객번호'].nunique(),\n",
    "            'date_range': (self.lp_data['LP수신일자_dt'].min(), self.lp_data['LP수신일자_dt'].max()),\n",
    "            'data_quality': {col: {'null': self.lp_data[col].isnull().sum(), \n",
    "                                  'zero': (self.lp_data[col] == 0).sum(),\n",
    "                                  'negative': (self.lp_data[col] < 0).sum()} for col in numeric_cols}\n",
    "        }\n",
    "    \n",
    "    def detect_outliers(self, method='iqr'):\n",
    "        \"\"\"이상치 탐지\"\"\"\n",
    "        print(f\"\\n=== 이상치 탐지 ({method.upper()} 방법) ===\")\n",
    "        \n",
    "        numeric_cols = ['순방향유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        outliers_summary = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            data = self.lp_data[col].dropna()\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                \n",
    "                outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs((data - data.mean()) / data.std())\n",
    "                outliers = data[z_scores > 3]\n",
    "            \n",
    "            outliers_summary[col] = {\n",
    "                'count': len(outliers),\n",
    "                'percentage': len(outliers) / len(data) * 100,\n",
    "                'min_outlier': outliers.min() if len(outliers) > 0 else None,\n",
    "                'max_outlier': outliers.max() if len(outliers) > 0 else None\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n📊 {col}:\")\n",
    "            print(f\"  이상치: {len(outliers):,}건 ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"  범위: {outliers.min():.1f} ~ {outliers.max():.1f}\")\n",
    "        \n",
    "        return outliers_summary\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"데이터 품질 종합 리포트 생성\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 데이터 품질 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. 데이터 규모\n",
    "        print(\"\\n🔢 데이터 규모:\")\n",
    "        print(f\"  • 고객 기본정보: {len(self.customer_data):,}명\")\n",
    "        print(f\"  • LP 데이터: {len(self.lp_data):,}레코드\")\n",
    "        print(f\"  • 분석 대상 고객: {self.lp_data['대체고객번호'].nunique()}명\")\n",
    "        print(f\"  • 분석 기간: {self.lp_data['LP수신일자'].min()} ~ {self.lp_data['LP수신일자'].max()}\")\n",
    "        \n",
    "        # 2. 데이터 품질 요약\n",
    "        print(\"\\n✅ 데이터 품질 상태:\")\n",
    "        numeric_cols = ['순방향유효전력', '지상무효', '진상무효', '피상전력']\n",
    "        \n",
    "        total_records = len(self.lp_data)\n",
    "        total_nulls = sum(self.lp_data[col].isnull().sum() for col in numeric_cols)\n",
    "        total_zeros = sum((self.lp_data[col] == 0).sum() for col in numeric_cols)\n",
    "        total_negatives = sum((self.lp_data[col] < 0).sum() for col in numeric_cols)\n",
    "        \n",
    "        print(f\"  • 결측치 비율: {total_nulls/(total_records*4)*100:.2f}%\")\n",
    "        print(f\"  • 0값 비율: {total_zeros/(total_records*4)*100:.2f}%\") \n",
    "        print(f\"  • 음수값 비율: {total_negatives/(total_records*4)*100:.2f}%\")\n",
    "        \n",
    "        # 3. 권장사항\n",
    "        print(\"\\n💡 다음 단계 권장사항:\")\n",
    "        print(\"  1. 시계열 패턴 분석 (일/주/월별 사용 패턴)\")\n",
    "        print(\"  2. 고객별 사용량 프로파일링\")\n",
    "        print(\"  3. 변동성 지표 계산 및 비교\")\n",
    "        print(\"  4. 이상 패턴 탐지 알고리즘 개발\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCODataAnalyzer()\n",
    "    \n",
    "    # 1단계: 고객 기본정보 분석\n",
    "    customer_analysis = analyzer.load_customer_data('고객번호.xlsx')\n",
    "    \n",
    "    # 2단계: LP 데이터 분석  \n",
    "    lp_analysis = analyzer.load_lp_data(['LP데이터1.csv', 'LP데이터2.csv'])\n",
    "    \n",
    "    # 3단계: 이상치 탐지\n",
    "    outliers = analyzer.detect_outliers('iqr')\n",
    "    \n",
    "    # 4단계: 종합 리포트\n",
    "    analyzer.generate_quality_report()\n",
    "    \n",
    "    print(\"\\n🎯 1단계 데이터 품질 점검 완료!\")\n",
    "    print(\"다음: 2단계 시계열 패턴 분석 준비 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a4c697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LP 데이터 로딩 및 전처리 ===\n",
      "✅ 데이터 로딩 완료: 29,760레코드\n",
      "기간: 2024-03-01 00:00:00 ~ 2024-03-31 23:45:00\n",
      "고객 수: 10명\n",
      "\n",
      "============================================================\n",
      "📊 시계열 패턴 분석 종합 요약\n",
      "============================================================\n",
      "\n",
      "=== 시간대별 전력 사용 패턴 분석 ===\n",
      "📊 시간대별 평균 전력 사용량 (kW):\n",
      "시간\t평균\t표준편차\t최소\t최대\n",
      "00시\t57.9\t23.7\t7.4\t117.3\n",
      "01시\t65.9\t27.1\t9.0\t136.7\n",
      "02시\t73.3\t30.1\t10.8\t149.4\n",
      "03시\t80.0\t33.2\t11.5\t175.2\n",
      "04시\t84.8\t34.7\t12.1\t171.1\n",
      "05시\t87.9\t36.3\t12.0\t179.3\n",
      "06시\t92.8\t37.4\t12.9\t173.8\n",
      "07시\t103.9\t55.6\t16.9\t267.3\n",
      "08시\t110.7\t53.5\t15.3\t256.7\n",
      "09시\t107.8\t51.3\t16.4\t255.3\n",
      "10시\t94.3\t38.5\t10.8\t177.4\n",
      "11시\t79.3\t34.8\t8.6\t157.4\n",
      "12시\t67.8\t34.6\t11.1\t180.9\n",
      "13시\t62.6\t30.7\t10.0\t162.1\n",
      "14시\t55.0\t26.4\t8.4\t138.2\n",
      "15시\t45.6\t18.6\t5.1\t88.3\n",
      "16시\t35.0\t14.3\t3.9\t77.4\n",
      "17시\t32.8\t14.8\t4.8\t87.4\n",
      "18시\t32.5\t16.0\t4.8\t89.2\n",
      "19시\t37.5\t19.2\t5.5\t92.0\n",
      "20시\t38.2\t19.0\t4.0\t88.8\n",
      "21시\t42.4\t20.9\t4.6\t94.4\n",
      "22시\t44.3\t17.9\t5.2\t86.1\n",
      "23시\t50.0\t20.7\t6.4\t101.0\n",
      "\n",
      "⚡ 피크 시간대 (상위 20%): [6, 7, 8, 9, 10]시\n",
      "💤 비피크 시간대 (하위 30%): [16, 17, 18, 19, 20, 21, 22]시\n",
      "\n",
      "=== 일별/요일별 패턴 분석 ===\n",
      "📅 요일별 평균 일간 사용량 (kWh):\n",
      "월요일: 7,063.7 ± 1993.7\n",
      "화요일: 7,074.2 ± 1988.2\n",
      "수요일: 7,053.0 ± 1999.1\n",
      "목요일: 7,054.8 ± 2016.7\n",
      "금요일: 7,036.9 ± 1997.1\n",
      "토요일: 4,800.4 ± 2989.7\n",
      "일요일: 4,807.6 ± 2996.2\n",
      "\n",
      "📊 평일 vs 주말 비교:\n",
      "평일 평균: 7,055.6 kWh\n",
      "주말 평균: 4,804.0 kWh\n",
      "주말/평일 비율: 0.68\n",
      "\n",
      "=== 고객별 사용량 프로파일 분석 ===\n",
      "👥 고객별 기본 통계 (kW):\n",
      "고객번호\t평균\t표준편차\t변동계수\t최소\t최대\n",
      "A1001\t84.6\t36.1\t0.427\t22.1\t221.2\n",
      "A1002\t45.6\t29.6\t0.649\t5.3\t148.2\n",
      "A1003\t78.8\t28.7\t0.364\t23.0\t176.6\n",
      "A1004\t81.2\t63.6\t0.783\t3.9\t267.3\n",
      "A1005\t66.0\t24.3\t0.368\t22.9\t131.0\n",
      "A1006\t58.1\t26.8\t0.461\t16.9\t180.8\n",
      "A1007\t59.7\t41.5\t0.695\t4.8\t183.7\n",
      "A1008\t95.0\t44.0\t0.463\t27.6\t251.8\n",
      "A1009\t36.6\t19.8\t0.541\t5.6\t113.9\n",
      "A1010\t53.8\t26.0\t0.483\t11.7\t140.5\n",
      "\n",
      "📈 대용량 사용자 (상위 20%): ['A1001', 'A1008']\n",
      "📉 소용량 사용자 (하위 20%): ['A1002', 'A1009']\n",
      "\n",
      "🌊 고변동성 고객: ['A1004', 'A1007']\n",
      "📊 저변동성 고객: ['A1003', 'A1005']\n",
      "\n",
      "=== 부하율 및 효율성 지표 계산 ===\n",
      "⚡ 고객별 부하율 및 피크 집중도:\n",
      "고객번호\t부하율\t피크집중도\t평균부하\t최대부하\n",
      "A1001\t0.383\t1.045\t84.6\t221.2\n",
      "A1002\t0.307\t1.057\t45.6\t148.2\n",
      "A1003\t0.446\t0.926\t78.8\t176.6\n",
      "A1004\t0.304\t1.097\t81.2\t267.3\n",
      "A1005\t0.504\t0.861\t66.0\t131.0\n",
      "A1006\t0.321\t0.797\t58.1\t180.8\n",
      "A1007\t0.325\t1.09\t59.7\t183.7\n",
      "A1008\t0.377\t0.958\t95.0\t251.8\n",
      "A1009\t0.321\t0.968\t36.6\t113.9\n",
      "A1010\t0.383\t1.076\t53.8\t140.5\n",
      "\n",
      "📊 전체 부하율 분포:\n",
      "평균 부하율: 0.367\n",
      "부하율 범위: 0.304 ~ 0.504\n",
      "\n",
      "=== 사용량 이상 패턴 탐지 ===\n",
      "🚨 이상 패턴 탐지 결과:\n",
      "고객번호\t급격변화\t장기0값\t통계이상치\n",
      "A1001\t0\t0\t5\n",
      "A1002\t4\t0\t5\n",
      "A1003\t0\t0\t5\n",
      "A1004\t4\t0\t0\n",
      "A1006\t0\t0\t43\n",
      "A1007\t4\t0\t0\n",
      "A1008\t0\t0\t4\n",
      "A1009\t0\t0\t8\n",
      "A1010\t0\t0\t6\n",
      "\n",
      "🔍 주요 발견사항:\n",
      "  • 주요 피크 시간: [6, 7, 8, 9, 10]시\n",
      "  • 주말/평일 사용량 비율: 0.68\n",
      "  • 고객별 변동계수 범위: 0.364 ~ 0.783\n",
      "  • 평균 부하율: 0.367\n",
      "  • 이상 패턴 고객: 9명\n",
      "\n",
      "💡 변동계수 설계를 위한 인사이트:\n",
      "  1. 시간대별 가중치 필요 (피크/비피크 구분)\n",
      "  2. 요일별 보정 계수 고려\n",
      "  3. 고객별 기준 변동성 설정\n",
      "  4. 부하율과 변동성의 상관관계 분석\n",
      "  5. 다차원 변동성 지표 조합 검토\n",
      "\n",
      "🎯 2단계 시계열 패턴 분석 완료!\n",
      "다음: 3단계 변동성 지표 계산\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOTimeSeriesAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        \n",
    "    def load_sample_data(self):\n",
    "        \"\"\"샘플 LP 데이터 로딩 및 전처리\"\"\"\n",
    "        print(\"=== LP 데이터 로딩 및 전처리 ===\")\n",
    "        \n",
    "        # 실제 환경에서는 CSV 파일 읽기\n",
    "        # lp1 = pd.read_csv('LP데이터1.csv')\n",
    "        # lp2 = pd.read_csv('LP데이터2.csv') \n",
    "        # self.lp_data = pd.concat([lp1, lp2], ignore_index=True)\n",
    "        \n",
    "        # 테스트용 샘플 데이터 생성\n",
    "        self.lp_data = self._create_comprehensive_sample_data()\n",
    "        \n",
    "        # 날짜/시간 전처리\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['minute'] = self.lp_data['datetime'].dt.minute\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday  # 0=월요일\n",
    "        self.lp_data['is_weekend'] = self.lp_data['weekday'].isin([5, 6])\n",
    "        \n",
    "        print(f\"✅ 데이터 로딩 완료: {len(self.lp_data):,}레코드\")\n",
    "        print(f\"기간: {self.lp_data['datetime'].min()} ~ {self.lp_data['datetime'].max()}\")\n",
    "        print(f\"고객 수: {self.lp_data['대체고객번호'].nunique()}명\")\n",
    "        \n",
    "        return self.lp_data\n",
    "    \n",
    "    def _create_comprehensive_sample_data(self):\n",
    "        \"\"\"포괄적인 테스트용 샘플 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]  # A1001~A1010\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31  # 3월 전체\n",
    "        \n",
    "        # 고객별 특성 정의 (다양한 패턴)\n",
    "        customer_profiles = {\n",
    "            'A1001': {'type': 'hospital', 'base_power': 120, 'peak_hours': [9, 14, 20], 'weekend_factor': 0.8},\n",
    "            'A1002': {'type': 'office', 'base_power': 80, 'peak_hours': [9, 14], 'weekend_factor': 0.3},\n",
    "            'A1003': {'type': 'retail', 'base_power': 100, 'peak_hours': [11, 15, 19], 'weekend_factor': 1.2},\n",
    "            'A1004': {'type': 'factory', 'base_power': 150, 'peak_hours': [8, 13, 18], 'weekend_factor': 0.1},\n",
    "            'A1005': {'type': 'restaurant', 'base_power': 90, 'peak_hours': [12, 18], 'weekend_factor': 1.1},\n",
    "            'A1006': {'type': 'gym', 'base_power': 70, 'peak_hours': [7, 18, 21], 'weekend_factor': 1.3},\n",
    "            'A1007': {'type': 'school', 'base_power': 110, 'peak_hours': [10, 14], 'weekend_factor': 0.2},\n",
    "            'A1008': {'type': 'hotel', 'base_power': 130, 'peak_hours': [8, 20], 'weekend_factor': 1.0},\n",
    "            'A1009': {'type': 'warehouse', 'base_power': 60, 'peak_hours': [9, 16], 'weekend_factor': 0.5},\n",
    "            'A1010': {'type': 'clinic', 'base_power': 85, 'peak_hours': [10, 15], 'weekend_factor': 0.6}\n",
    "        }\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = customer_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                is_weekend = current_date.weekday() >= 5\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # 기본 전력 계산\n",
    "                        base_power = profile['base_power']\n",
    "                        \n",
    "                        # 시간대별 패턴 (사인파 기반)\n",
    "                        time_factor = 0.3 + 0.7 * (np.sin(2 * np.pi * hour / 24) + 1) / 2\n",
    "                        \n",
    "                        # 피크 시간 보정\n",
    "                        peak_factor = 1.0\n",
    "                        for peak_hour in profile['peak_hours']:\n",
    "                            if abs(hour - peak_hour) <= 1:\n",
    "                                peak_factor = 1.5\n",
    "                        \n",
    "                        # 주말 보정\n",
    "                        weekend_factor = profile['weekend_factor'] if is_weekend else 1.0\n",
    "                        \n",
    "                        # 최종 전력 계산\n",
    "                        power = base_power * time_factor * peak_factor * weekend_factor\n",
    "                        power += np.random.normal(0, power * 0.1)  # 10% 노이즈\n",
    "                        power = max(0, power)\n",
    "                        \n",
    "                        # 무효전력 계산\n",
    "                        reactive_lag = power * np.random.uniform(0.1, 0.3)\n",
    "                        reactive_lead = power * np.random.uniform(0.05, 0.15)\n",
    "                        apparent_power = np.sqrt(power**2 + (reactive_lag - reactive_lead)**2)\n",
    "                        \n",
    "                        data.append({\n",
    "                            '대체고객번호': customer,\n",
    "                            'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            '순방향유효전력': round(power, 1),\n",
    "                            '지상무효': round(reactive_lag, 1),\n",
    "                            '진상무효': round(reactive_lead, 1),\n",
    "                            '피상전력': round(apparent_power, 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def analyze_hourly_patterns(self):\n",
    "        \"\"\"시간대별 전력 사용 패턴 분석\"\"\"\n",
    "        print(\"\\n=== 시간대별 전력 사용 패턴 분석 ===\")\n",
    "        \n",
    "        # 시간대별 평균 사용량\n",
    "        hourly_avg = self.lp_data.groupby('hour')['순방향유효전력'].agg(['mean', 'std', 'min', 'max']).round(1)\n",
    "        \n",
    "        print(\"📊 시간대별 평균 전력 사용량 (kW):\")\n",
    "        print(\"시간\\t평균\\t표준편차\\t최소\\t최대\")\n",
    "        for hour in range(24):\n",
    "            stats = hourly_avg.loc[hour]\n",
    "            print(f\"{hour:02d}시\\t{stats['mean']}\\t{stats['std']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # 피크/비피크 시간대 식별\n",
    "        peak_threshold = hourly_avg['mean'].quantile(0.8)\n",
    "        peak_hours = hourly_avg[hourly_avg['mean'] >= peak_threshold].index.tolist()\n",
    "        off_peak_hours = hourly_avg[hourly_avg['mean'] < hourly_avg['mean'].quantile(0.3)].index.tolist()\n",
    "        \n",
    "        print(f\"\\n⚡ 피크 시간대 (상위 20%): {peak_hours}시\")\n",
    "        print(f\"💤 비피크 시간대 (하위 30%): {off_peak_hours}시\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_stats': hourly_avg,\n",
    "            'peak_hours': peak_hours,\n",
    "            'off_peak_hours': off_peak_hours\n",
    "        }\n",
    "    \n",
    "    def analyze_daily_patterns(self):\n",
    "        \"\"\"일별/요일별 패턴 분석\"\"\"\n",
    "        print(\"\\n=== 일별/요일별 패턴 분석 ===\")\n",
    "        \n",
    "        # 일별 총 사용량\n",
    "        daily_usage = self.lp_data.groupby(['대체고객번호', 'date'])['순방향유효전력'].sum().reset_index()\n",
    "        daily_usage['weekday'] = pd.to_datetime(daily_usage['date']).dt.weekday\n",
    "        daily_usage['is_weekend'] = daily_usage['weekday'].isin([5, 6])\n",
    "        \n",
    "        # 요일별 평균 사용량\n",
    "        weekday_avg = daily_usage.groupby('weekday')['순방향유효전력'].agg(['mean', 'std']).round(1)\n",
    "        weekday_names = ['월', '화', '수', '목', '금', '토', '일']\n",
    "        \n",
    "        print(\"📅 요일별 평균 일간 사용량 (kWh):\")\n",
    "        for i, day_name in enumerate(weekday_names):\n",
    "            stats = weekday_avg.loc[i]\n",
    "            print(f\"{day_name}요일: {stats['mean']:,.1f} ± {stats['std']:.1f}\")\n",
    "        \n",
    "        # 평일 vs 주말 비교\n",
    "        weekday_mean = daily_usage[~daily_usage['is_weekend']]['순방향유효전력'].mean()\n",
    "        weekend_mean = daily_usage[daily_usage['is_weekend']]['순방향유효전력'].mean()\n",
    "        weekend_ratio = weekend_mean / weekday_mean\n",
    "        \n",
    "        print(f\"\\n📊 평일 vs 주말 비교:\")\n",
    "        print(f\"평일 평균: {weekday_mean:,.1f} kWh\")\n",
    "        print(f\"주말 평균: {weekend_mean:,.1f} kWh\")\n",
    "        print(f\"주말/평일 비율: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'daily_usage': daily_usage,\n",
    "            'weekday_stats': weekday_avg,\n",
    "            'weekend_ratio': weekend_ratio\n",
    "        }\n",
    "    \n",
    "    def analyze_customer_profiles(self):\n",
    "        \"\"\"고객별 사용량 프로파일 분석\"\"\"\n",
    "        print(\"\\n=== 고객별 사용량 프로파일 분석 ===\")\n",
    "        \n",
    "        # 고객별 기본 통계\n",
    "        customer_stats = self.lp_data.groupby('대체고객번호')['순방향유효전력'].agg([\n",
    "            'count', 'mean', 'std', 'min', 'max'\n",
    "        ]).round(1)\n",
    "        customer_stats['cv'] = (customer_stats['std'] / customer_stats['mean']).round(3)  # 변동계수\n",
    "        \n",
    "        print(\"👥 고객별 기본 통계 (kW):\")\n",
    "        print(\"고객번호\\t평균\\t표준편차\\t변동계수\\t최소\\t최대\")\n",
    "        for customer in customer_stats.index:\n",
    "            stats = customer_stats.loc[customer]\n",
    "            print(f\"{customer}\\t{stats['mean']}\\t{stats['std']}\\t{stats['cv']}\\t{stats['min']}\\t{stats['max']}\")\n",
    "        \n",
    "        # 사용량 규모별 분류\n",
    "        mean_usage = customer_stats['mean']\n",
    "        high_users = mean_usage[mean_usage >= mean_usage.quantile(0.8)].index.tolist()\n",
    "        low_users = mean_usage[mean_usage <= mean_usage.quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\n📈 대용량 사용자 (상위 20%): {high_users}\")\n",
    "        print(f\"📉 소용량 사용자 (하위 20%): {low_users}\")\n",
    "        \n",
    "        # 변동성별 분류\n",
    "        high_volatility = customer_stats[customer_stats['cv'] >= customer_stats['cv'].quantile(0.8)].index.tolist()\n",
    "        low_volatility = customer_stats[customer_stats['cv'] <= customer_stats['cv'].quantile(0.2)].index.tolist()\n",
    "        \n",
    "        print(f\"\\n🌊 고변동성 고객: {high_volatility}\")\n",
    "        print(f\"📊 저변동성 고객: {low_volatility}\")\n",
    "        \n",
    "        return {\n",
    "            'customer_stats': customer_stats,\n",
    "            'usage_segments': {\n",
    "                'high_users': high_users,\n",
    "                'low_users': low_users,\n",
    "                'high_volatility': high_volatility,\n",
    "                'low_volatility': low_volatility\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def calculate_load_factors(self):\n",
    "        \"\"\"부하율 및 효율성 지표 계산\"\"\"\n",
    "        print(\"\\n=== 부하율 및 효율성 지표 계산 ===\")\n",
    "        \n",
    "        # 고객별 부하율 계산\n",
    "        customer_load_factors = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "            \n",
    "            avg_load = customer_data['순방향유효전력'].mean()\n",
    "            max_load = customer_data['순방향유효전력'].max()\n",
    "            load_factor = avg_load / max_load if max_load > 0 else 0\n",
    "            \n",
    "            # 피크 집중도 (피크 시간대 사용량 비중)\n",
    "            peak_hours = [9, 14, 18]  # 대표 피크 시간\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['순방향유효전력'].mean()\n",
    "            total_avg = customer_data['순방향유효전력'].mean()\n",
    "            peak_concentration = peak_usage / total_avg if total_avg > 0 else 0\n",
    "            \n",
    "            customer_load_factors[customer] = {\n",
    "                'load_factor': round(load_factor, 3),\n",
    "                'peak_concentration': round(peak_concentration, 3),\n",
    "                'avg_load': round(avg_load, 1),\n",
    "                'max_load': round(max_load, 1)\n",
    "            }\n",
    "        \n",
    "        print(\"⚡ 고객별 부하율 및 피크 집중도:\")\n",
    "        print(\"고객번호\\t부하율\\t피크집중도\\t평균부하\\t최대부하\")\n",
    "        for customer, metrics in customer_load_factors.items():\n",
    "            print(f\"{customer}\\t{metrics['load_factor']}\\t{metrics['peak_concentration']}\\t{metrics['avg_load']}\\t{metrics['max_load']}\")\n",
    "        \n",
    "        # 전체 부하율 분포\n",
    "        load_factors = [metrics['load_factor'] for metrics in customer_load_factors.values()]\n",
    "        avg_load_factor = np.mean(load_factors)\n",
    "        \n",
    "        print(f\"\\n📊 전체 부하율 분포:\")\n",
    "        print(f\"평균 부하율: {avg_load_factor:.3f}\")\n",
    "        print(f\"부하율 범위: {min(load_factors):.3f} ~ {max(load_factors):.3f}\")\n",
    "        \n",
    "        return customer_load_factors\n",
    "    \n",
    "    def detect_usage_anomalies(self):\n",
    "        \"\"\"사용량 이상 패턴 탐지\"\"\"\n",
    "        print(\"\\n=== 사용량 이상 패턴 탐지 ===\")\n",
    "        \n",
    "        anomalies = []\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].copy()\n",
    "            customer_data = customer_data.sort_values('datetime')\n",
    "            \n",
    "            # 1. 급격한 변화 탐지 (전시점 대비 200% 이상 변화)\n",
    "            customer_data['power_change'] = customer_data['순방향유효전력'].pct_change()\n",
    "            sudden_changes = customer_data[abs(customer_data['power_change']) > 2.0]\n",
    "            \n",
    "            # 2. 연속적인 0값 탐지 (2시간 이상)\n",
    "            customer_data['is_zero'] = customer_data['순방향유효전력'] == 0\n",
    "            customer_data['zero_group'] = (customer_data['is_zero'] != customer_data['is_zero'].shift()).cumsum()\n",
    "            zero_periods = customer_data[customer_data['is_zero']].groupby('zero_group').size()\n",
    "            long_zero_periods = zero_periods[zero_periods >= 8]  # 2시간 = 8개 15분 구간\n",
    "            \n",
    "            # 3. 통계적 이상치 (Z-score > 3)\n",
    "            mean_power = customer_data['순방향유효전력'].mean()\n",
    "            std_power = customer_data['순방향유효전력'].std()\n",
    "            if std_power > 0:\n",
    "                customer_data['z_score'] = abs(customer_data['순방향유효전력'] - mean_power) / std_power\n",
    "                statistical_outliers = customer_data[customer_data['z_score'] > 3]\n",
    "            else:\n",
    "                statistical_outliers = pd.DataFrame()\n",
    "            \n",
    "            # 이상치 정보 저장\n",
    "            if len(sudden_changes) > 0 or len(long_zero_periods) > 0 or len(statistical_outliers) > 0:\n",
    "                anomalies.append({\n",
    "                    'customer': customer,\n",
    "                    'sudden_changes': len(sudden_changes),\n",
    "                    'long_zero_periods': len(long_zero_periods),\n",
    "                    'statistical_outliers': len(statistical_outliers)\n",
    "                })\n",
    "        \n",
    "        print(\"🚨 이상 패턴 탐지 결과:\")\n",
    "        if anomalies:\n",
    "            print(\"고객번호\\t급격변화\\t장기0값\\t통계이상치\")\n",
    "            for anomaly in anomalies:\n",
    "                print(f\"{anomaly['customer']}\\t{anomaly['sudden_changes']}\\t{anomaly['long_zero_periods']}\\t{anomaly['statistical_outliers']}\")\n",
    "        else:\n",
    "            print(\"✅ 심각한 이상 패턴 없음\")\n",
    "        \n",
    "        return anomalies\n",
    "    \n",
    "    def generate_pattern_summary(self):\n",
    "        \"\"\"패턴 분석 종합 요약\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 시계열 패턴 분석 종합 요약\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 주요 패턴 특성\n",
    "        hourly_stats = self.analyze_hourly_patterns()\n",
    "        daily_stats = self.analyze_daily_patterns()\n",
    "        customer_stats = self.analyze_customer_profiles()\n",
    "        load_factors = self.calculate_load_factors()\n",
    "        anomalies = self.detect_usage_anomalies()\n",
    "        \n",
    "        print(\"\\n🔍 주요 발견사항:\")\n",
    "        \n",
    "        # 1. 시간 패턴\n",
    "        peak_hours = hourly_stats['peak_hours']\n",
    "        print(f\"  • 주요 피크 시간: {peak_hours}시\")\n",
    "        \n",
    "        # 2. 요일 패턴  \n",
    "        weekend_ratio = daily_stats['weekend_ratio']\n",
    "        print(f\"  • 주말/평일 사용량 비율: {weekend_ratio:.2f}\")\n",
    "        \n",
    "        # 3. 고객 다양성\n",
    "        cv_range = customer_stats['customer_stats']['cv']\n",
    "        print(f\"  • 고객별 변동계수 범위: {cv_range.min():.3f} ~ {cv_range.max():.3f}\")\n",
    "        \n",
    "        # 4. 부하율\n",
    "        load_factor_avg = np.mean([lf['load_factor'] for lf in load_factors.values()])\n",
    "        print(f\"  • 평균 부하율: {load_factor_avg:.3f}\")\n",
    "        \n",
    "        # 5. 이상 패턴\n",
    "        anomaly_customers = len(anomalies)\n",
    "        print(f\"  • 이상 패턴 고객: {anomaly_customers}명\")\n",
    "        \n",
    "        print(\"\\n💡 변동계수 설계를 위한 인사이트:\")\n",
    "        print(\"  1. 시간대별 가중치 필요 (피크/비피크 구분)\")\n",
    "        print(\"  2. 요일별 보정 계수 고려\") \n",
    "        print(\"  3. 고객별 기준 변동성 설정\")\n",
    "        print(\"  4. 부하율과 변동성의 상관관계 분석\")\n",
    "        print(\"  5. 다차원 변동성 지표 조합 검토\")\n",
    "        \n",
    "        return {\n",
    "            'hourly_patterns': hourly_stats,\n",
    "            'daily_patterns': daily_stats,\n",
    "            'customer_profiles': customer_stats,\n",
    "            'load_factors': load_factors,\n",
    "            'anomalies': anomalies\n",
    "        }\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCOTimeSeriesAnalyzer()\n",
    "    \n",
    "    # 데이터 로딩\n",
    "    lp_data = analyzer.load_sample_data()\n",
    "    \n",
    "    # 패턴 분석 실행\n",
    "    pattern_summary = analyzer.generate_pattern_summary()\n",
    "    \n",
    "    print(\"\\n🎯 2단계 시계열 패턴 분석 완료!\")\n",
    "    print(\"다음: 3단계 변동성 지표 계산\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77981da6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 변동성 분석을 위한 데이터 준비 ===\n",
      "✅ 데이터 준비 완료: 29,760레코드\n",
      "\n",
      "============================================================\n",
      "📊 변동성 분석 종합 리포트\n",
      "============================================================\n",
      "\n",
      "=== 기본 변동성 지표 계산 ===\n",
      "고객번호\tCV\t범위변동성\tIQR변동성\tMAD변동성\t수익률변동성\n",
      "A1001\t0.0506\t0.3199\t0.069\t0.0404\t0.0712\n",
      "A1002\t0.4685\t3.819\t0.4496\t0.3145\t1.132\n",
      "A1003\t0.246\t1.2983\t0.397\t0.2091\t0.151\n",
      "A1004\t0.1925\t1.4063\t0.2638\t0.1541\t0.2313\n",
      "A1005\t0.2487\t1.6808\t0.3404\t0.1992\t0.3534\n",
      "A1006\t0.1393\t2.1185\t0.1366\t0.0893\t0.1682\n",
      "A1007\t0.2909\t2.5063\t0.2315\t0.1917\tnan\n",
      "A1008\t0.3906\t2.33\t0.5331\t0.313\tnan\n",
      "A1009\t0.1806\t1.309\t0.2369\t0.1441\t0.2859\n",
      "A1010\t0.3419\t2.1765\t0.4417\t0.2714\tnan\n",
      "\n",
      "=== 시간 윈도우별 변동성 분석 ===\n",
      "윈도우별 평균 변동계수:\n",
      "고객번호\t시간별\t일별\t주별\n",
      "A1001\t0.0469\t0.0507\t0.051\n",
      "A1002\t0.3781\t0.4642\t0.4682\n",
      "A1003\t0.0964\t0.2381\t0.2449\n",
      "A1004\t0.1401\t0.1515\t0.1545\n",
      "A1005\t0.1923\t0.2052\t0.2138\n",
      "A1006\t0.0987\t0.1282\t0.1374\n",
      "A1007\t0.2277\t0.2414\t0.28\n",
      "A1008\t0.3766\t0.3906\t0.3891\n",
      "A1009\t0.1682\t0.1808\t0.181\n",
      "A1010\t0.3256\t0.3406\t0.3404\n",
      "\n",
      "=== 방향성 변동성 분석 ===\n",
      "고객번호\t상승변동성\t하락변동성\t비대칭비율\t급증횟수\t급감횟수\n",
      "A1001\t0.0454\t0.0392\t1.1583\t0\t0\n",
      "A1002\t1.3262\t0.2305\t5.7542\t667\t379\n",
      "A1003\t0.1081\t0.0758\t1.4276\t11\t0\n",
      "A1004\t0.1816\t0.1085\t1.6741\t101\t3\n",
      "A1005\t0.3157\t0.1414\t2.2324\t272\t51\n",
      "A1006\t0.1446\t0.0839\t1.7235\t20\t5\n",
      "A1007\tnan\t0.2168\tnan\t283\t160\n",
      "A1008\tnan\t0.2293\tnan\t706\t373\n",
      "A1009\t0.2321\t0.1255\t1.8492\t183\t28\n",
      "A1010\tnan\t0.2107\tnan\t610\t295\n",
      "\n",
      "=== 패턴 안정성 분석 ===\n",
      "고객번호\t패턴일관성\t일일주기성\t자기상관\n",
      "A1001\t0.0044\t0.6002\t0.9975\n",
      "A1002\t-0.0153\t1.5469\t0.824\n",
      "A1003\t0.9524\t68.3914\t0.9896\n",
      "A1004\t-0.0102\t0.9968\t0.9778\n",
      "A1005\t-0.0033\t0.7561\t0.9581\n",
      "A1006\t0.0267\t3.4987\t0.9874\n",
      "A1007\t-0.0021\t0.4741\t0.9234\n",
      "A1008\t-0.0118\t0.5864\t0.8678\n",
      "A1009\t0.0153\t1.4262\t0.9683\n",
      "A1010\t0.0051\t0.2849\t0.8935\n",
      "\n",
      "=== 복합 변동성 스코어 계산 ===\n",
      "고객번호\t기본\t윈도우\t방향성\t안정성\t복합점수\n",
      "A1001\t0.0506\t0.0495\t0.0423\t0.9956\t0.2376\n",
      "A1002\t0.4685\t0.4368\t0.7783\t1.0153\t0.6303\n",
      "A1003\t0.246\t0.1931\t0.092\t0.0476\t0.1596\n",
      "A1004\t0.1925\t0.1487\t0.1451\t1.0102\t0.3334\n",
      "A1005\t0.2487\t0.2038\t0.2285\t1.0033\t0.3821\n",
      "A1006\t0.1393\t0.1214\t0.1143\t0.9733\t0.2957\n",
      "A1007\t0.2909\t0.2497\tnan\t1.0021\tnan\n",
      "A1008\t0.3906\t0.3854\tnan\t1.0118\tnan\n",
      "A1009\t0.1806\t0.1767\t0.1788\t0.9847\t0.3399\n",
      "A1010\t0.3419\t0.3355\tnan\t0.9949\tnan\n",
      "\n",
      "📊 변동성 등급 기준:\n",
      "  저변동성: < nan\n",
      "  중변동성: nan ~ nan\n",
      "  고변동성: > nan\n",
      "\n",
      "등급별 고객 분류:\n",
      "  A1001: 0.238 (고변동성)\n",
      "  A1002: 0.630 (고변동성)\n",
      "  A1003: 0.160 (고변동성)\n",
      "  A1004: 0.333 (고변동성)\n",
      "  A1005: 0.382 (고변동성)\n",
      "  A1006: 0.296 (고변동성)\n",
      "  A1007: nan (고변동성)\n",
      "  A1008: nan (고변동성)\n",
      "  A1009: 0.340 (고변동성)\n",
      "  A1010: nan (고변동성)\n",
      "\n",
      "🎯 변동계수 알고리즘 설계 인사이트:\n",
      "  1. 다차원 변동성 지표의 필요성 확인\n",
      "  2. 시간 윈도우별 차별화된 가중치 적용\n",
      "  3. 방향성 변동성으로 리스크 비대칭성 포착\n",
      "  4. 패턴 안정성으로 예측가능성 평가\n",
      "  5. 복합 스코어를 통한 종합적 변동성 평가\n",
      "\n",
      "💡 스태킹 알고리즘 설계 방향:\n",
      "  • Level-0 모델: 각 변동성 지표를 개별 모델로 구성\n",
      "  • Level-1 메타모델: 가중 결합으로 최종 변동계수 산출\n",
      "  • 과적합 방지: 교차검증 및 정규화 적용\n",
      "\n",
      "🎯 3단계 변동성 지표 계산 완료!\n",
      "다음: 4단계 스태킹 알고리즘 개발\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.lp_data = None\n",
    "        self.volatility_metrics = {}\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"데이터 로딩 및 전처리\"\"\"\n",
    "        print(\"=== 변동성 분석을 위한 데이터 준비 ===\")\n",
    "        \n",
    "        # 실제 환경에서는 CSV 파일 읽기\n",
    "        # self.lp_data = pd.concat([\n",
    "        #     pd.read_csv('LP데이터1.csv'),\n",
    "        #     pd.read_csv('LP데이터2.csv')\n",
    "        # ], ignore_index=True)\n",
    "        \n",
    "        # 테스트용 데이터 생성\n",
    "        self.lp_data = self._create_volatility_test_data()\n",
    "        \n",
    "        # 시간 관련 컬럼 추가\n",
    "        self.lp_data['datetime'] = pd.to_datetime(self.lp_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "        self.lp_data['date'] = self.lp_data['datetime'].dt.date\n",
    "        self.lp_data['hour'] = self.lp_data['datetime'].dt.hour\n",
    "        self.lp_data['weekday'] = self.lp_data['datetime'].dt.weekday\n",
    "        \n",
    "        print(f\"✅ 데이터 준비 완료: {len(self.lp_data):,}레코드\")\n",
    "        return self.lp_data\n",
    "    \n",
    "    def _create_volatility_test_data(self):\n",
    "        \"\"\"변동성 테스트용 데이터 생성\"\"\"\n",
    "        data = []\n",
    "        customers = [f'A{1001+i}' for i in range(10)]\n",
    "        start_date = datetime(2024, 3, 1)\n",
    "        days = 31\n",
    "        \n",
    "        # 다양한 변동성 패턴을 가진 고객 정의\n",
    "        volatility_profiles = {\n",
    "            'A1001': {'type': 'stable', 'base': 100, 'noise': 0.05},      # 안정적\n",
    "            'A1002': {'type': 'high_volatility', 'base': 80, 'noise': 0.3}, # 고변동성\n",
    "            'A1003': {'type': 'periodic', 'base': 120, 'noise': 0.1},     # 주기적\n",
    "            'A1004': {'type': 'trending', 'base': 90, 'noise': 0.15},     # 트렌드\n",
    "            'A1005': {'type': 'seasonal', 'base': 110, 'noise': 0.2},     # 계절성\n",
    "            'A1006': {'type': 'jumpy', 'base': 85, 'noise': 0.1},         # 점프형\n",
    "            'A1007': {'type': 'clustered', 'base': 95, 'noise': 0.25},    # 클러스터형\n",
    "            'A1008': {'type': 'low_usage', 'base': 30, 'noise': 0.4},     # 저사용량\n",
    "            'A1009': {'type': 'medium', 'base': 70, 'noise': 0.18},       # 중간\n",
    "            'A1010': {'type': 'irregular', 'base': 105, 'noise': 0.35}    # 불규칙\n",
    "        }\n",
    "        \n",
    "        for customer in customers:\n",
    "            profile = volatility_profiles[customer]\n",
    "            \n",
    "            for day in range(days):\n",
    "                current_date = start_date + timedelta(days=day)\n",
    "                \n",
    "                for hour in range(24):\n",
    "                    for minute in [0, 15, 30, 45]:\n",
    "                        timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                        \n",
    "                        # 기본 패턴 생성\n",
    "                        base_power = profile['base']\n",
    "                        \n",
    "                        # 타입별 패턴 적용\n",
    "                        if profile['type'] == 'stable':\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'high_volatility':\n",
    "                            # 높은 변동성 - 큰 무작위 변화\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                            if np.random.random() < 0.1:  # 10% 확률로 큰 점프\n",
    "                                power *= np.random.choice([0.3, 2.5])\n",
    "                        \n",
    "                        elif profile['type'] == 'periodic':\n",
    "                            # 주기적 패턴\n",
    "                            daily_cycle = np.sin(2 * np.pi * hour / 24)\n",
    "                            weekly_cycle = np.sin(2 * np.pi * day / 7)\n",
    "                            power = base_power * (1 + 0.3 * daily_cycle + 0.1 * weekly_cycle)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'trending':\n",
    "                            # 트렌드 패턴\n",
    "                            trend = 0.5 * day / days  # 30일간 50% 증가\n",
    "                            power = base_power * (1 + trend)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'seasonal':\n",
    "                            # 계절성 패턴\n",
    "                            seasonal = 0.2 * np.sin(2 * np.pi * day / 30)\n",
    "                            power = base_power * (1 + seasonal)\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'jumpy':\n",
    "                            # 급격한 변화가 있는 패턴\n",
    "                            power = base_power\n",
    "                            if day % 7 == 0 and hour == 9:  # 주 1회 큰 변화\n",
    "                                power *= 2.0\n",
    "                            elif day % 7 == 3 and hour == 15:\n",
    "                                power *= 0.4\n",
    "                            power += np.random.normal(0, power * profile['noise'])\n",
    "                        \n",
    "                        elif profile['type'] == 'clustered':\n",
    "                            # 변동성 클러스터링 (변동성이 높은 구간과 낮은 구간)\n",
    "                            if day % 10 < 3:  # 30% 기간 동안 높은 변동성\n",
    "                                noise_factor = profile['noise'] * 2\n",
    "                            else:\n",
    "                                noise_factor = profile['noise'] * 0.5\n",
    "                            power = base_power + np.random.normal(0, base_power * noise_factor)\n",
    "                        \n",
    "                        else:  # irregular, low_usage, medium\n",
    "                            power = base_power + np.random.normal(0, base_power * profile['noise'])\n",
    "                        \n",
    "                        power = max(0, power)  # 음수 방지\n",
    "                        \n",
    "                        data.append({\n",
    "                            '대체고객번호': customer,\n",
    "                            'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                            '순방향유효전력': round(power, 1),\n",
    "                            '지상무효': round(power * np.random.uniform(0.1, 0.3), 1),\n",
    "                            '진상무효': round(power * np.random.uniform(0.05, 0.15), 1),\n",
    "                            '피상전력': round(power * np.random.uniform(1.0, 1.1), 1)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_basic_volatility_metrics(self):\n",
    "        \"\"\"기본 변동성 지표 계산\"\"\"\n",
    "        print(\"\\n=== 기본 변동성 지표 계산 ===\")\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]['순방향유효전력']\n",
    "            \n",
    "            # 1. 전통적 변동계수 (CV)\n",
    "            cv = customer_data.std() / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 2. 범위 기반 변동성\n",
    "            range_volatility = (customer_data.max() - customer_data.min()) / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 3. 분위수 기반 변동성 (IQR/Median)\n",
    "            q75, q25 = np.percentile(customer_data, [75, 25])\n",
    "            iqr_volatility = (q75 - q25) / np.median(customer_data) if np.median(customer_data) > 0 else 0\n",
    "            \n",
    "            # 4. 평균절대편차 (MAD)\n",
    "            mad = np.mean(np.abs(customer_data - customer_data.mean()))\n",
    "            mad_volatility = mad / customer_data.mean() if customer_data.mean() > 0 else 0\n",
    "            \n",
    "            # 5. 변화율 기반 변동성\n",
    "            returns = customer_data.pct_change().dropna()\n",
    "            return_volatility = returns.std() if len(returns) > 0 else 0\n",
    "            \n",
    "            metrics[customer] = {\n",
    "                'cv': round(cv, 4),\n",
    "                'range_vol': round(range_volatility, 4),\n",
    "                'iqr_vol': round(iqr_volatility, 4),\n",
    "                'mad_vol': round(mad_volatility, 4),\n",
    "                'return_vol': round(return_volatility, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\tCV\\t범위변동성\\tIQR변동성\\tMAD변동성\\t수익률변동성\")\n",
    "        for customer, metrics_dict in metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['cv']}\\t{metrics_dict['range_vol']}\\t{metrics_dict['iqr_vol']}\\t{metrics_dict['mad_vol']}\\t{metrics_dict['return_vol']}\")\n",
    "        \n",
    "        self.volatility_metrics['basic'] = metrics\n",
    "        return metrics\n",
    "    \n",
    "    def calculate_time_window_volatility(self):\n",
    "        \"\"\"시간 윈도우별 변동성 계산\"\"\"\n",
    "        print(\"\\n=== 시간 윈도우별 변동성 분석 ===\")\n",
    "        \n",
    "        window_metrics = {}\n",
    "        windows = {\n",
    "            'hourly': 4,    # 1시간 (4개 15분 구간)\n",
    "            'daily': 96,    # 1일 (96개 15분 구간)\n",
    "            'weekly': 672   # 1주 (672개 15분 구간)\n",
    "        }\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['순방향유효전력']\n",
    "            \n",
    "            window_metrics[customer] = {}\n",
    "            \n",
    "            for window_name, window_size in windows.items():\n",
    "                # 롤링 윈도우로 변동계수 계산\n",
    "                rolling_std = power_series.rolling(window=window_size, min_periods=window_size//2).std()\n",
    "                rolling_mean = power_series.rolling(window=window_size, min_periods=window_size//2).mean()\n",
    "                rolling_cv = rolling_std / rolling_mean\n",
    "                \n",
    "                # 윈도우별 변동성 통계\n",
    "                window_metrics[customer][window_name] = {\n",
    "                    'mean_cv': round(rolling_cv.mean(), 4),\n",
    "                    'std_cv': round(rolling_cv.std(), 4),\n",
    "                    'max_cv': round(rolling_cv.max(), 4),\n",
    "                    'min_cv': round(rolling_cv.min(), 4)\n",
    "                }\n",
    "        \n",
    "        print(\"윈도우별 평균 변동계수:\")\n",
    "        print(\"고객번호\\t시간별\\t일별\\t주별\")\n",
    "        for customer in window_metrics.keys():\n",
    "            hourly_cv = window_metrics[customer]['hourly']['mean_cv']\n",
    "            daily_cv = window_metrics[customer]['daily']['mean_cv']\n",
    "            weekly_cv = window_metrics[customer]['weekly']['mean_cv']\n",
    "            print(f\"{customer}\\t{hourly_cv}\\t{daily_cv}\\t{weekly_cv}\")\n",
    "        \n",
    "        self.volatility_metrics['time_windows'] = window_metrics\n",
    "        return window_metrics\n",
    "    \n",
    "    def calculate_directional_volatility(self):\n",
    "        \"\"\"방향성 변동성 분석 (상승/하락 비대칭성)\"\"\"\n",
    "        print(\"\\n=== 방향성 변동성 분석 ===\")\n",
    "        \n",
    "        directional_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer].sort_values('datetime')\n",
    "            power_series = customer_data['순방향유효전력']\n",
    "            \n",
    "            # 변화율 계산\n",
    "            returns = power_series.pct_change().dropna()\n",
    "            \n",
    "            # 상승/하락 분리\n",
    "            upside_returns = returns[returns > 0]\n",
    "            downside_returns = returns[returns < 0]\n",
    "            \n",
    "            # 방향별 변동성\n",
    "            upside_volatility = upside_returns.std() if len(upside_returns) > 0 else 0\n",
    "            downside_volatility = abs(downside_returns.std()) if len(downside_returns) > 0 else 0\n",
    "            \n",
    "            # 비대칭성 지수\n",
    "            asymmetry_ratio = upside_volatility / downside_volatility if downside_volatility > 0 else 0\n",
    "            \n",
    "            # 급격한 변화 횟수\n",
    "            large_increases = len(returns[returns > 0.5])  # 50% 이상 증가\n",
    "            large_decreases = len(returns[returns < -0.5]) # 50% 이상 감소\n",
    "            \n",
    "            directional_metrics[customer] = {\n",
    "                'upside_vol': round(upside_volatility, 4),\n",
    "                'downside_vol': round(downside_volatility, 4),\n",
    "                'asymmetry_ratio': round(asymmetry_ratio, 4),\n",
    "                'large_increases': large_increases,\n",
    "                'large_decreases': large_decreases\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\t상승변동성\\t하락변동성\\t비대칭비율\\t급증횟수\\t급감횟수\")\n",
    "        for customer, metrics_dict in directional_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['upside_vol']}\\t{metrics_dict['downside_vol']}\\t{metrics_dict['asymmetry_ratio']}\\t{metrics_dict['large_increases']}\\t{metrics_dict['large_decreases']}\")\n",
    "        \n",
    "        self.volatility_metrics['directional'] = directional_metrics\n",
    "        return directional_metrics\n",
    "    \n",
    "    def calculate_pattern_stability(self):\n",
    "        \"\"\"패턴 안정성 분석\"\"\"\n",
    "        print(\"\\n=== 패턴 안정성 분석 ===\")\n",
    "        \n",
    "        stability_metrics = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            customer_data = self.lp_data[self.lp_data['대체고객번호'] == customer]\n",
    "            \n",
    "            # 시간대별 패턴 일관성\n",
    "            hourly_patterns = []\n",
    "            for day in customer_data['date'].unique():\n",
    "                day_data = customer_data[customer_data['date'] == day]\n",
    "                hourly_avg = day_data.groupby('hour')['순방향유효전력'].mean()\n",
    "                hourly_patterns.append(hourly_avg.values)\n",
    "            \n",
    "            # 패턴 간 상관관계 (일관성 측정)\n",
    "            if len(hourly_patterns) > 1:\n",
    "                correlations = []\n",
    "                for i in range(len(hourly_patterns)):\n",
    "                    for j in range(i+1, len(hourly_patterns)):\n",
    "                        if len(hourly_patterns[i]) == len(hourly_patterns[j]):\n",
    "                            corr = np.corrcoef(hourly_patterns[i], hourly_patterns[j])[0,1]\n",
    "                            if not np.isnan(corr):\n",
    "                                correlations.append(corr)\n",
    "                \n",
    "                pattern_consistency = np.mean(correlations) if correlations else 0\n",
    "            else:\n",
    "                pattern_consistency = 0\n",
    "            \n",
    "            # 주기성 강도 (FFT 기반)\n",
    "            power_series = customer_data.sort_values('datetime')['순방향유효전력'].values\n",
    "            if len(power_series) > 100:  # 충분한 데이터가 있을 때만\n",
    "                fft = np.fft.fft(power_series)\n",
    "                fft_magnitude = np.abs(fft)\n",
    "                \n",
    "                # 일일 주기 (96포인트) 강도\n",
    "                daily_freq_idx = len(fft_magnitude) // 96 if len(fft_magnitude) >= 96 else 1\n",
    "                daily_periodicity = fft_magnitude[daily_freq_idx] / np.mean(fft_magnitude) if np.mean(fft_magnitude) > 0 else 0\n",
    "            else:\n",
    "                daily_periodicity = 0\n",
    "            \n",
    "            # 예측가능성 (자기상관)\n",
    "            autocorr_1lag = power_series[1:].dot(power_series[:-1]) / (np.linalg.norm(power_series[1:]) * np.linalg.norm(power_series[:-1])) if len(power_series) > 1 else 0\n",
    "            \n",
    "            stability_metrics[customer] = {\n",
    "                'pattern_consistency': round(pattern_consistency, 4),\n",
    "                'daily_periodicity': round(daily_periodicity, 4),\n",
    "                'autocorrelation': round(autocorr_1lag, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\t패턴일관성\\t일일주기성\\t자기상관\")\n",
    "        for customer, metrics_dict in stability_metrics.items():\n",
    "            print(f\"{customer}\\t{metrics_dict['pattern_consistency']}\\t{metrics_dict['daily_periodicity']}\\t{metrics_dict['autocorrelation']}\")\n",
    "        \n",
    "        self.volatility_metrics['stability'] = stability_metrics\n",
    "        return stability_metrics\n",
    "    \n",
    "    def create_composite_volatility_score(self):\n",
    "        \"\"\"복합 변동성 스코어 생성\"\"\"\n",
    "        print(\"\\n=== 복합 변동성 스코어 계산 ===\")\n",
    "        \n",
    "        if not all(key in self.volatility_metrics for key in ['basic', 'time_windows', 'directional', 'stability']):\n",
    "            print(\"❌ 모든 변동성 지표를 먼저 계산해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        composite_scores = {}\n",
    "        \n",
    "        for customer in self.lp_data['대체고객번호'].unique():\n",
    "            # 각 카테고리별 점수 계산 (0-1 정규화)\n",
    "            basic_score = self.volatility_metrics['basic'][customer]['cv']\n",
    "            window_score = np.mean([\n",
    "                self.volatility_metrics['time_windows'][customer]['hourly']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['daily']['mean_cv'],\n",
    "                self.volatility_metrics['time_windows'][customer]['weekly']['mean_cv']\n",
    "            ])\n",
    "            directional_score = (\n",
    "                self.volatility_metrics['directional'][customer]['upside_vol'] + \n",
    "                self.volatility_metrics['directional'][customer]['downside_vol']\n",
    "            ) / 2\n",
    "            \n",
    "            # 안정성은 역수로 (낮은 안정성 = 높은 변동성)\n",
    "            stability_score = 1 - self.volatility_metrics['stability'][customer]['pattern_consistency']\n",
    "            \n",
    "            # 가중 평균으로 최종 스코어 계산\n",
    "            weights = {\n",
    "                'basic': 0.3,       # 기본 변동계수\n",
    "                'window': 0.3,      # 시간 윈도우 변동성\n",
    "                'directional': 0.2, # 방향성 변동성\n",
    "                'stability': 0.2    # 패턴 불안정성\n",
    "            }\n",
    "            \n",
    "            composite_score = (\n",
    "                weights['basic'] * basic_score +\n",
    "                weights['window'] * window_score +\n",
    "                weights['directional'] * directional_score +\n",
    "                weights['stability'] * stability_score\n",
    "            )\n",
    "            \n",
    "            composite_scores[customer] = {\n",
    "                'basic_score': round(basic_score, 4),\n",
    "                'window_score': round(window_score, 4),\n",
    "                'directional_score': round(directional_score, 4),\n",
    "                'stability_score': round(stability_score, 4),\n",
    "                'composite_score': round(composite_score, 4)\n",
    "            }\n",
    "        \n",
    "        print(\"고객번호\\t기본\\t윈도우\\t방향성\\t안정성\\t복합점수\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            print(f\"{customer}\\t{scores['basic_score']}\\t{scores['window_score']}\\t{scores['directional_score']}\\t{scores['stability_score']}\\t{scores['composite_score']}\")\n",
    "        \n",
    "        # 변동성 등급 분류\n",
    "        composite_values = [scores['composite_score'] for scores in composite_scores.values()]\n",
    "        percentiles = np.percentile(composite_values, [33, 67])\n",
    "        \n",
    "        print(f\"\\n📊 변동성 등급 기준:\")\n",
    "        print(f\"  저변동성: < {percentiles[0]:.3f}\")\n",
    "        print(f\"  중변동성: {percentiles[0]:.3f} ~ {percentiles[1]:.3f}\")\n",
    "        print(f\"  고변동성: > {percentiles[1]:.3f}\")\n",
    "        \n",
    "        print(\"\\n등급별 고객 분류:\")\n",
    "        for customer, scores in composite_scores.items():\n",
    "            score = scores['composite_score']\n",
    "            if score < percentiles[0]:\n",
    "                grade = \"저변동성\"\n",
    "            elif score < percentiles[1]:\n",
    "                grade = \"중변동성\"\n",
    "            else:\n",
    "                grade = \"고변동성\"\n",
    "            print(f\"  {customer}: {score:.3f} ({grade})\")\n",
    "        \n",
    "        self.volatility_metrics['composite'] = composite_scores\n",
    "        return composite_scores\n",
    "    \n",
    "    def generate_volatility_report(self):\n",
    "        \"\"\"변동성 분석 종합 리포트\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 변동성 분석 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 모든 지표 계산\n",
    "        basic_metrics = self.calculate_basic_volatility_metrics()\n",
    "        window_metrics = self.calculate_time_window_volatility()\n",
    "        directional_metrics = self.calculate_directional_volatility()\n",
    "        stability_metrics = self.calculate_pattern_stability()\n",
    "        composite_scores = self.create_composite_volatility_score()\n",
    "        \n",
    "        print(\"\\n🎯 변동계수 알고리즘 설계 인사이트:\")\n",
    "        print(\"  1. 다차원 변동성 지표의 필요성 확인\")\n",
    "        print(\"  2. 시간 윈도우별 차별화된 가중치 적용\")\n",
    "        print(\"  3. 방향성 변동성으로 리스크 비대칭성 포착\")\n",
    "        print(\"  4. 패턴 안정성으로 예측가능성 평가\")\n",
    "        print(\"  5. 복합 스코어를 통한 종합적 변동성 평가\")\n",
    "        \n",
    "        print(\"\\n💡 스태킹 알고리즘 설계 방향:\")\n",
    "        print(\"  • Level-0 모델: 각 변동성 지표를 개별 모델로 구성\")\n",
    "        print(\"  • Level-1 메타모델: 가중 결합으로 최종 변동계수 산출\")\n",
    "        print(\"  • 과적합 방지: 교차검증 및 정규화 적용\")\n",
    "        \n",
    "        return {\n",
    "            'basic': basic_metrics,\n",
    "            'time_windows': window_metrics,\n",
    "            'directional': directional_metrics,\n",
    "            'stability': stability_metrics,\n",
    "            'composite': composite_scores\n",
    "        }\n",
    "\n",
    "# 사용 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 분석기 초기화\n",
    "    analyzer = KEPCOVolatilityAnalyzer()\n",
    "    \n",
    "    # 데이터 로딩\n",
    "    data = analyzer.load_and_prepare_data()\n",
    "    \n",
    "    # 종합 변동성 분석\n",
    "    volatility_report = analyzer.generate_volatility_report()\n",
    "    \n",
    "    print(\"\\n🎯 3단계 변동성 지표 계산 완료!\")\n",
    "    print(\"다음: 4단계 스태킹 알고리즘 개발\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df3a0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 한국전력공사 전력 사용패턴 변동계수 스태킹 알고리즘 테스트\n",
      "======================================================================\n",
      "1️⃣ 샘플 데이터 생성...\n",
      "   생성 완료: 29,760레코드\n",
      "\n",
      "2️⃣ 스태킹 모델 초기화...\n",
      "\n",
      "3️⃣ 변동성 특성 추출...\n",
      "🔧 변동성 특성 추출 중...\n",
      "✅ 특성 추출 완료: 10명 고객, 13개 특성\n",
      "\n",
      "4️⃣ 훈련 데이터 준비...\n",
      "📊 훈련 데이터 준비 완료: 10개 샘플, 12개 특성\n",
      "\n",
      "5️⃣ 스태킹 모델 훈련...\n",
      "🚀 스태킹 모델 훈련 시작...\n",
      "🔍 메타모델 성능 비교:\n",
      "모델명\t\tMAE\t\tMSE\t\tR²\n",
      "linear      \t0.0210\t\t0.0005\t\tnan\n",
      "ridge       \t0.0275\t\t0.0009\t\tnan\n",
      "lasso       \t0.1296\t\t0.0240\t\tnan\n",
      "elastic     \t0.0803\t\t0.0081\t\tnan\n",
      "rf          \t0.0958\t\t0.0119\t\tnan\n",
      "gbm         \t0.0795\t\t0.0069\t\tnan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "❌ 적합한 메타모델을 찾을 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 545\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# 5. 모델 훈련\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m5️⃣ 스태킹 모델 훈련...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 545\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m    547\u001b[0m \u001b[38;5;66;03m# 6. 종합 리포트 생성\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m6️⃣ 종합 변동성 리포트 생성...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 292\u001b[0m, in \u001b[0;36mKEPCOVolatilityStackingModel.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 스태킹 모델 훈련 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 적합한 메타모델을 찾을 수 없습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: ❌ 적합한 메타모델을 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KEPCOVolatilityStackingModel:\n",
    "    \"\"\"\n",
    "    한국전력공사 전력 사용패턴 변동계수 스태킹 알고리즘\n",
    "    \n",
    "    Level-0: 다양한 변동성 지표들을 개별 모델로 구성\n",
    "    Level-1: 메타모델로 최종 변동계수 산출\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.level0_models = {}\n",
    "        self.level1_model = None\n",
    "        self.scaler = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "        # Level-0 모델 정의\n",
    "        self._initialize_level0_models()\n",
    "        \n",
    "        # 과적합 방지 설정\n",
    "        self.cv_folds = 5\n",
    "        self.random_state = 42\n",
    "        \n",
    "    def _initialize_level0_models(self):\n",
    "        \"\"\"Level-0 기본 모델들 초기화\"\"\"\n",
    "        self.level0_models = {\n",
    "            'traditional_cv': self._traditional_cv_model,\n",
    "            'range_volatility': self._range_volatility_model,\n",
    "            'iqr_volatility': self._iqr_volatility_model,\n",
    "            'mad_volatility': self._mad_volatility_model,\n",
    "            'return_volatility': self._return_volatility_model,\n",
    "            'window_volatility': self._window_volatility_model,\n",
    "            'percentile_volatility': self._percentile_volatility_model\n",
    "        }\n",
    "        \n",
    "    def _traditional_cv_model(self, data):\n",
    "        \"\"\"전통적 변동계수 모델\"\"\"\n",
    "        return data.std() / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _range_volatility_model(self, data):\n",
    "        \"\"\"범위 기반 변동성 모델\"\"\"\n",
    "        return (data.max() - data.min()) / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _iqr_volatility_model(self, data):\n",
    "        \"\"\"분위수 기반 변동성 모델\"\"\"\n",
    "        q75, q25 = np.percentile(data, [75, 25])\n",
    "        median = np.median(data)\n",
    "        return (q75 - q25) / median if median > 0 else 0\n",
    "    \n",
    "    def _mad_volatility_model(self, data):\n",
    "        \"\"\"평균절대편차 기반 변동성 모델\"\"\"\n",
    "        mad = np.mean(np.abs(data - data.mean()))\n",
    "        return mad / data.mean() if data.mean() > 0 else 0\n",
    "    \n",
    "    def _return_volatility_model(self, data):\n",
    "        \"\"\"수익률 변동성 모델\"\"\"\n",
    "        if len(data) < 2:\n",
    "            return 0\n",
    "        returns = data.pct_change().dropna()\n",
    "        return returns.std() if len(returns) > 0 else 0\n",
    "    \n",
    "    def _window_volatility_model(self, data, window_size=96):\n",
    "        \"\"\"윈도우 기반 변동성 모델 (일별)\"\"\"\n",
    "        if len(data) < window_size:\n",
    "            return self._traditional_cv_model(data)\n",
    "        \n",
    "        rolling_cv = []\n",
    "        for i in range(window_size, len(data) + 1):\n",
    "            window_data = data.iloc[i-window_size:i]\n",
    "            cv = self._traditional_cv_model(window_data)\n",
    "            rolling_cv.append(cv)\n",
    "        \n",
    "        return np.mean(rolling_cv) if rolling_cv else 0\n",
    "    \n",
    "    def _percentile_volatility_model(self, data):\n",
    "        \"\"\"백분위수 기반 변동성 모델\"\"\"\n",
    "        p90 = np.percentile(data, 90)\n",
    "        p10 = np.percentile(data, 10)\n",
    "        p50 = np.percentile(data, 50)\n",
    "        return (p90 - p10) / p50 if p50 > 0 else 0\n",
    "    \n",
    "    def extract_features(self, lp_data):\n",
    "        \"\"\"\n",
    "        LP 데이터에서 변동성 특성 추출\n",
    "        \n",
    "        Parameters:\n",
    "        lp_data: DataFrame with columns ['대체고객번호', 'LP수신일자', '순방향유효전력', ...]\n",
    "        \n",
    "        Returns:\n",
    "        features_df: DataFrame with volatility features for each customer\n",
    "        \"\"\"\n",
    "        print(\"🔧 변동성 특성 추출 중...\")\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for customer in lp_data['대체고객번호'].unique():\n",
    "            customer_data = lp_data[lp_data['대체고객번호'] == customer]\n",
    "            power_data = customer_data['순방향유효전력']\n",
    "            \n",
    "            # 시간 정보 추가\n",
    "            if 'datetime' not in customer_data.columns:\n",
    "                customer_data = customer_data.copy()\n",
    "                customer_data['datetime'] = pd.to_datetime(customer_data['LP수신일자'], format='%Y-%m-%d-%H:%M')\n",
    "                customer_data['hour'] = customer_data['datetime'].dt.hour\n",
    "                customer_data['weekday'] = customer_data['datetime'].dt.weekday\n",
    "            \n",
    "            # Level-0 모델들로 특성 계산\n",
    "            features = {'customer_id': customer}\n",
    "            \n",
    "            for model_name, model_func in self.level0_models.items():\n",
    "                try:\n",
    "                    if model_name == 'window_volatility':\n",
    "                        features[model_name] = model_func(power_data, window_size=96)\n",
    "                    else:\n",
    "                        features[model_name] = model_func(power_data)\n",
    "                except:\n",
    "                    features[model_name] = 0\n",
    "            \n",
    "            # 추가 컨텍스트 특성\n",
    "            features.update({\n",
    "                'mean_usage': power_data.mean(),\n",
    "                'total_usage': power_data.sum(),\n",
    "                'peak_ratio': power_data.max() / power_data.mean() if power_data.mean() > 0 else 0,\n",
    "                'zero_ratio': (power_data == 0).sum() / len(power_data),\n",
    "                'weekend_effect': self._calculate_weekend_effect(customer_data),\n",
    "                'peak_hour_concentration': self._calculate_peak_concentration(customer_data)\n",
    "            })\n",
    "            \n",
    "            features_list.append(features)\n",
    "        \n",
    "        features_df = pd.DataFrame(features_list)\n",
    "        print(f\"✅ 특성 추출 완료: {len(features_df)}명 고객, {len(features_df.columns)-1}개 특성\")\n",
    "        \n",
    "        return features_df\n",
    "    \n",
    "    def _calculate_weekend_effect(self, customer_data):\n",
    "        \"\"\"주말 효과 계산\"\"\"\n",
    "        try:\n",
    "            weekday_avg = customer_data[customer_data['weekday'] < 5]['순방향유효전력'].mean()\n",
    "            weekend_avg = customer_data[customer_data['weekday'] >= 5]['순방향유효전력'].mean()\n",
    "            \n",
    "            if weekday_avg > 0:\n",
    "                return abs(weekend_avg / weekday_avg - 1)\n",
    "            else:\n",
    "                return 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _calculate_peak_concentration(self, customer_data):\n",
    "        \"\"\"피크 시간 집중도 계산\"\"\"\n",
    "        try:\n",
    "            peak_hours = [9, 14, 18]  # 일반적인 피크 시간\n",
    "            peak_usage = customer_data[customer_data['hour'].isin(peak_hours)]['순방향유효전력'].mean()\n",
    "            total_avg = customer_data['순방향유효전력'].mean()\n",
    "            \n",
    "            return peak_usage / total_avg if total_avg > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def prepare_training_data(self, features_df, target_column='traditional_cv'):\n",
    "        \"\"\"\n",
    "        훈련 데이터 준비\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: 특성 데이터프레임\n",
    "        target_column: 타겟 변수로 사용할 컬럼명\n",
    "        \n",
    "        Returns:\n",
    "        X, y: 특성 행렬과 타겟 벡터\n",
    "        \"\"\"\n",
    "        # 특성 선택 (Level-0 모델 출력들)\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        y = features_df[target_column].copy()\n",
    "        \n",
    "        # 결측치 처리\n",
    "        X = X.fillna(0)\n",
    "        y = y.fillna(0)\n",
    "        \n",
    "        # 이상치 처리 (상위 5%, 하위 5% 클리핑)\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                lower_bound = X[col].quantile(0.05)\n",
    "                upper_bound = X[col].quantile(0.95)\n",
    "                X[col] = X[col].clip(lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"📊 훈련 데이터 준비 완료: {X.shape[0]}개 샘플, {X.shape[1]}개 특성\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        스태킹 모델 훈련\n",
    "        \n",
    "        Parameters:\n",
    "        X: 특성 행렬\n",
    "        y: 타겟 벡터 (실제 변동계수)\n",
    "        \"\"\"\n",
    "        print(\"🚀 스태킹 모델 훈련 시작...\")\n",
    "        \n",
    "        # 데이터 정규화\n",
    "        self.scaler = RobustScaler()  # 이상치에 강건한 스케일러\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Level-1 메타모델 후보들\n",
    "        meta_models = {\n",
    "            'linear': LinearRegression(),\n",
    "            'ridge': Ridge(alpha=1.0, random_state=self.random_state),\n",
    "            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state),\n",
    "            'rf': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=self.random_state),\n",
    "            'gbm': GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        # 교차검증으로 최적 메타모델 선택\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "        best_model_name = None\n",
    "        \n",
    "        # 시계열 교차검증\n",
    "        tscv = TimeSeriesSplit(n_splits=self.cv_folds)\n",
    "        \n",
    "        print(\"🔍 메타모델 성능 비교:\")\n",
    "        print(\"모델명\\t\\tMAE\\t\\tMSE\\t\\tR²\")\n",
    "        \n",
    "        for model_name, model in meta_models.items():\n",
    "            try:\n",
    "                # 교차검증 점수 계산\n",
    "                mae_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "                mse_scores = -cross_val_score(model, X_scaled, y, cv=tscv, scoring='neg_mean_squared_error')\n",
    "                r2_scores = cross_val_score(model, X_scaled, y, cv=tscv, scoring='r2')\n",
    "                \n",
    "                avg_mae = np.mean(mae_scores)\n",
    "                avg_mse = np.mean(mse_scores)\n",
    "                avg_r2 = np.mean(r2_scores)\n",
    "                \n",
    "                print(f\"{model_name:<12}\\t{avg_mae:.4f}\\t\\t{avg_mse:.4f}\\t\\t{avg_r2:.4f}\")\n",
    "                \n",
    "                # R² 점수 기준으로 최적 모델 선택\n",
    "                if avg_r2 > best_score:\n",
    "                    best_score = avg_r2\n",
    "                    best_model = model\n",
    "                    best_model_name = model_name\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{model_name:<12}\\t에러: {str(e)[:30]}...\")\n",
    "        \n",
    "        # 최적 모델로 전체 데이터 훈련\n",
    "        if best_model is not None:\n",
    "            self.level1_model = best_model\n",
    "            self.level1_model.fit(X_scaled, y)\n",
    "            \n",
    "            print(f\"\\n🏆 최적 메타모델: {best_model_name} (R² = {best_score:.4f})\")\n",
    "            \n",
    "            # 특성 중요도 출력 (가능한 경우)\n",
    "            if hasattr(self.level1_model, 'feature_importances_'):\n",
    "                importance_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'importance': self.level1_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\n📊 특성 중요도 (상위 5개):\")\n",
    "                for _, row in importance_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "            \n",
    "            elif hasattr(self.level1_model, 'coef_'):\n",
    "                coef_df = pd.DataFrame({\n",
    "                    'feature': X.columns,\n",
    "                    'coefficient': self.level1_model.coef_\n",
    "                }).sort_values('coefficient', key=abs, ascending=False)\n",
    "                \n",
    "                print(\"\\n📊 모델 계수 (절댓값 상위 5개):\")\n",
    "                for _, row in coef_df.head().iterrows():\n",
    "                    print(f\"  {row['feature']}: {row['coefficient']:.4f}\")\n",
    "            \n",
    "            self.is_fitted = True\n",
    "            print(\"✅ 스태킹 모델 훈련 완료!\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"❌ 적합한 메타모델을 찾을 수 없습니다.\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, features_df):\n",
    "        \"\"\"\n",
    "        변동계수 예측\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: 특성 데이터프레임\n",
    "        \n",
    "        Returns:\n",
    "        predictions: 예측된 변동계수\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다. fit() 메서드를 먼저 호출하세요.\")\n",
    "        \n",
    "        # 훈련 시와 동일한 특성 선택\n",
    "        feature_columns = list(self.level0_models.keys()) + [\n",
    "            'mean_usage', 'peak_ratio', 'zero_ratio', \n",
    "            'weekend_effect', 'peak_hour_concentration'\n",
    "        ]\n",
    "        \n",
    "        X = features_df[feature_columns].copy()\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        # 정규화\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # 예측\n",
    "        predictions = self.level1_model.predict(X_scaled)\n",
    "        \n",
    "        # 음수 방지\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"모델 성능 평가\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다.\")\n",
    "        \n",
    "        # 예측\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        y_pred = self.level1_model.predict(X_test_scaled)\n",
    "        y_pred = np.maximum(y_pred, 0)  # 음수 방지\n",
    "        \n",
    "        # 성능 지표 계산\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # 상대 오차 계산\n",
    "        relative_errors = np.abs((y_test - y_pred) / (y_test + 1e-8))\n",
    "        mape = np.mean(relative_errors) * 100\n",
    "        \n",
    "        print(\"📊 모델 성능 평가:\")\n",
    "        print(f\"  MAE (평균절대오차): {mae:.4f}\")\n",
    "        print(f\"  MSE (평균제곱오차): {mse:.4f}\")\n",
    "        print(f\"  RMSE (제곱근평균제곱오차): {rmse:.4f}\")\n",
    "        print(f\"  R² (결정계수): {r2:.4f}\")\n",
    "        print(f\"  MAPE (평균절대백분율오차): {mape:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'mape': mape\n",
    "        }\n",
    "    \n",
    "    def detect_anomalous_volatility(self, features_df, threshold_percentile=95):\n",
    "        \"\"\"\n",
    "        이상 변동성 탐지\n",
    "        \n",
    "        Parameters:\n",
    "        features_df: 특성 데이터프레임\n",
    "        threshold_percentile: 이상치 기준 백분위수\n",
    "        \n",
    "        Returns:\n",
    "        anomaly_results: 이상치 탐지 결과\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다.\")\n",
    "        \n",
    "        print(\"🚨 이상 변동성 탐지 중...\")\n",
    "        \n",
    "        # 변동계수 예측\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # 이상치 기준 설정\n",
    "        threshold = np.percentile(predicted_volatility, threshold_percentile)\n",
    "        \n",
    "        # 이상치 탐지\n",
    "        anomaly_mask = predicted_volatility > threshold\n",
    "        \n",
    "        anomaly_results = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'is_anomaly': anomaly_mask,\n",
    "            'anomaly_score': (predicted_volatility - threshold) / threshold\n",
    "        })\n",
    "        \n",
    "        anomaly_customers = anomaly_results[anomaly_results['is_anomaly']]\n",
    "        \n",
    "        print(f\"🎯 탐지 결과:\")\n",
    "        print(f\"  전체 고객: {len(anomaly_results)}명\")\n",
    "        print(f\"  이상 변동성 고객: {len(anomaly_customers)}명\")\n",
    "        print(f\"  이상치 비율: {len(anomaly_customers)/len(anomaly_results)*100:.1f}%\")\n",
    "        \n",
    "        if len(anomaly_customers) > 0:\n",
    "            print(f\"\\n⚠️ 주의 필요 고객:\")\n",
    "            for _, row in anomaly_customers.sort_values('anomaly_score', ascending=False).iterrows():\n",
    "                print(f\"  {row['customer_id']}: 변동계수 {row['predicted_volatility']:.3f} (점수: {row['anomaly_score']:.2f})\")\n",
    "        \n",
    "        return anomaly_results\n",
    "    \n",
    "    def generate_volatility_report(self, features_df):\n",
    "        \"\"\"종합 변동성 리포트 생성\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"❌ 모델이 훈련되지 않았습니다.\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📋 전력 사용패턴 변동계수 종합 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 변동계수 예측\n",
    "        predicted_volatility = self.predict(features_df)\n",
    "        \n",
    "        # 통계 요약\n",
    "        print(\"\\n📊 변동계수 분포:\")\n",
    "        print(f\"  평균: {np.mean(predicted_volatility):.3f}\")\n",
    "        print(f\"  표준편차: {np.std(predicted_volatility):.3f}\")\n",
    "        print(f\"  최소값: {np.min(predicted_volatility):.3f}\")\n",
    "        print(f\"  최대값: {np.max(predicted_volatility):.3f}\")\n",
    "        print(f\"  중위수: {np.median(predicted_volatility):.3f}\")\n",
    "        \n",
    "        # 등급별 분류\n",
    "        p33 = np.percentile(predicted_volatility, 33)\n",
    "        p67 = np.percentile(predicted_volatility, 67)\n",
    "        \n",
    "        print(f\"\\n🏷️ 변동성 등급:\")\n",
    "        print(f\"  안정형 (< {p33:.3f}): {np.sum(predicted_volatility < p33)}명\")\n",
    "        print(f\"  보통형 ({p33:.3f} ~ {p67:.3f}): {np.sum((predicted_volatility >= p33) & (predicted_volatility < p67))}명\")\n",
    "        print(f\"  변동형 (≥ {p67:.3f}): {np.sum(predicted_volatility >= p67)}명\")\n",
    "        \n",
    "        # 고객별 결과\n",
    "        results_df = pd.DataFrame({\n",
    "            'customer_id': features_df['customer_id'],\n",
    "            'predicted_volatility': predicted_volatility,\n",
    "            'actual_cv': features_df['traditional_cv'] if 'traditional_cv' in features_df.columns else np.nan\n",
    "        })\n",
    "        \n",
    "        results_df['grade'] = pd.cut(results_df['predicted_volatility'], \n",
    "                                   bins=[0, p33, p67, np.inf], \n",
    "                                   labels=['안정형', '보통형', '변동형'])\n",
    "        \n",
    "        print(f\"\\n👥 고객별 변동계수:\")\n",
    "        print(\"고객번호\\t예측변동계수\\t등급\")\n",
    "        for _, row in results_df.iterrows():\n",
    "            print(f\"{row['customer_id']}\\t{row['predicted_volatility']:.3f}\\t\\t{row['grade']}\")\n",
    "        \n",
    "        # 이상 변동성 탐지\n",
    "        anomaly_results = self.detect_anomalous_volatility(features_df)\n",
    "        \n",
    "        print(f\"\\n💡 활용 방안:\")\n",
    "        print(\"  1. 비정상적 전력 사용 패턴 조기 감지\")\n",
    "        print(\"  2. 고객별 맞춤형 전력 관리 서비스\")\n",
    "        print(\"  3. 영업 리스크 평가 및 관리\")\n",
    "        print(\"  4. 전력 수요 예측 정확도 향상\")\n",
    "        \n",
    "        return {\n",
    "            'predictions': results_df,\n",
    "            'anomalies': anomaly_results,\n",
    "            'statistics': {\n",
    "                'mean': np.mean(predicted_volatility),\n",
    "                'std': np.std(predicted_volatility),\n",
    "                'percentiles': {'p33': p33, 'p67': p67}\n",
    "            }\n",
    "        }\n",
    "\n",
    "# 사용 예제 및 테스트\n",
    "def create_sample_lp_data():\n",
    "    \"\"\"테스트용 샘플 LP 데이터 생성\"\"\"\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    data = []\n",
    "    customers = [f'A{1001+i}' for i in range(10)]\n",
    "    start_date = datetime(2024, 3, 1)\n",
    "    \n",
    "    # 다양한 변동성 패턴의 고객들\n",
    "    volatility_patterns = {\n",
    "        'A1001': 0.3,   # 안정적\n",
    "        'A1002': 1.2,   # 높은 변동성\n",
    "        'A1003': 0.25,  # 매우 안정적\n",
    "        'A1004': 0.8,   # 중간 변동성\n",
    "        'A1005': 0.9,   # 중간 변동성\n",
    "        'A1006': 0.2,   # 안정적\n",
    "        'A1007': 1.5,   # 매우 높은 변동성\n",
    "        'A1008': 0.7,   # 중간 변동성\n",
    "        'A1009': 0.6,   # 중간 변동성\n",
    "        'A1010': 0.4    # 안정적\n",
    "    }\n",
    "    \n",
    "    for customer in customers:\n",
    "        target_cv = volatility_patterns[customer]\n",
    "        base_power = random.uniform(50, 150)\n",
    "        \n",
    "        for day in range(31):\n",
    "            current_date = start_date + timedelta(days=day)\n",
    "            \n",
    "            for hour in range(24):\n",
    "                for minute in [0, 15, 30, 45]:\n",
    "                    timestamp = current_date.replace(hour=hour, minute=minute)\n",
    "                    \n",
    "                    # 변동성에 따른 노이즈 생성\n",
    "                    noise_std = base_power * target_cv\n",
    "                    power = base_power + random.gauss(0, noise_std)\n",
    "                    power = max(0, power)\n",
    "                    \n",
    "                    data.append({\n",
    "                        '대체고객번호': customer,\n",
    "                        'LP수신일자': timestamp.strftime('%Y-%m-%d-%H:%M'),\n",
    "                        '순방향유효전력': round(power, 1)\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 한국전력공사 전력 사용패턴 변동계수 스태킹 알고리즘 테스트\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. 샘플 데이터 생성\n",
    "    print(\"1️⃣ 샘플 데이터 생성...\")\n",
    "    lp_data = create_sample_lp_data()\n",
    "    print(f\"   생성 완료: {len(lp_data):,}레코드\")\n",
    "    \n",
    "    # 2. 스태킹 모델 초기화\n",
    "    print(\"\\n2️⃣ 스태킹 모델 초기화...\")\n",
    "    stacking_model = KEPCOVolatilityStackingModel()\n",
    "    \n",
    "    # 3. 특성 추출\n",
    "    print(\"\\n3️⃣ 변동성 특성 추출...\")\n",
    "    features_df = stacking_model.extract_features(lp_data)\n",
    "    \n",
    "    # 4. 훈련 데이터 준비\n",
    "    print(\"\\n4️⃣ 훈련 데이터 준비...\")\n",
    "    X, y = stacking_model.prepare_training_data(features_df)\n",
    "    \n",
    "    # 5. 모델 훈련\n",
    "    print(\"\\n5️⃣ 스태킹 모델 훈련...\")\n",
    "    stacking_model.fit(X, y)\n",
    "    \n",
    "    # 6. 종합 리포트 생성\n",
    "    print(\"\\n6️⃣ 종합 변동성 리포트 생성...\")\n",
    "    report = stacking_model.generate_volatility_report(features_df)\n",
    "    \n",
    "    print(\"\\n🎯 스태킹 알고리즘 구현 완료!\")\n",
    "    print(\"   • Level-0: 7개 기본 변동성 모델\")\n",
    "    print(\"   • Level-1: 최적화된 메타모델\")\n",
    "    print(\"   • 과적합 방지: 교차검증 + 정규화\")\n",
    "    print(\"   • 이상 탐지: 자동 임계값 설정\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
