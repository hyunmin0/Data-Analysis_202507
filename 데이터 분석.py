import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# í°íŠ¸ ì„¤ì • (ì—ëŸ¬ ë°©ì§€)
plt.rcParams['axes.unicode_minus'] = False

class KepcoDataPreprocessor:
    """
    í•œêµ­ì „ë ¥ê³µì‚¬ LP ë°ì´í„° ì „ì²˜ë¦¬ ë° íƒìƒ‰ì  ë¶„ì„
    
    ë‹¨ê³„ë³„ ë¶„ì„ ê³„íš:
    1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„)
    2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) 
    3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„)
    4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„)
    5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„)
    """
    
    def __init__(self):
        self.data_quality_report = {}
        self.pattern_analysis = {}
        self.variability_analysis = {}
    
    # ============ ë°ì´í„° ë¡œë”© ë° ê²°í•© ============
    
    def load_and_combine_lp_data(self, lp_files):
        """
        LP ë°ì´í„° íŒŒì¼ë“¤ì„ ì½ì–´ì„œ ê²°í•©
        íŒŒì¼ë“¤ì€ í•œ ë‹¬ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì œê³µë¨ (ì˜ˆ: LPë°ì´í„°1.csv + LPë°ì´í„°2.csv)
        """
        print("ğŸ“‚ LP ë°ì´í„° íŒŒì¼ë“¤ ë¡œë”© ë° ê²°í•© ì¤‘...")
        
        combined_data = []
        
        for i, file_path in enumerate(lp_files):
            try:
                df = pd.read_csv(file_path)
                print(f"  âœ… {file_path}: {len(df):,}ê±´ ë¡œë”©")
                
                # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
                if 'LPìˆ˜ì‹ ì¼ì' in df.columns:
                    dates = pd.to_datetime(df['LPìˆ˜ì‹ ì¼ì'])
                    print(f"     ê¸°ê°„: {dates.min()} ~ {dates.max()}")
                    print(f"     ê³ ê°ìˆ˜: {df['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…")
                
                combined_data.append(df)
                
            except Exception as e:
                print(f"  âŒ {file_path} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        if not combined_data:
            raise ValueError("ë¡œë”©ëœ LP ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„° ê²°í•©
        final_data = pd.concat(combined_data, ignore_index=True)
        
        print(f"  ğŸ”— ê²°í•© ì™„ë£Œ: ì´ {len(final_data):,}ê±´")
        print(f"     ì „ì²´ ê¸°ê°„: {pd.to_datetime(final_data['LPìˆ˜ì‹ ì¼ì']).min()} ~ {pd.to_datetime(final_data['LPìˆ˜ì‹ ì¼ì']).max()}")
        print(f"     ì´ ê³ ê°ìˆ˜: {final_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()}ëª…")
        
        return final_data
    
    # ============ 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„) ============
    
    def check_data_quality(self, lp_data, customer_data):
        """
        ë°ì´í„° í’ˆì§ˆ ì ê²€ ë° ê¸°ë³¸ ì •ë³´ ë¶„ì„
        """
        print("ğŸ” 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ ì‹œì‘...")
        
        # ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„
        customer_info = self._analyze_customer_info(customer_data)
        
        # LP ë°ì´í„° í’ˆì§ˆ ì ê²€
        lp_quality = self._check_lp_data_quality(lp_data)
        
        self.data_quality_report = {
            'customer_info': customer_info,
            'lp_quality': lp_quality,
            'data_completeness': self._calculate_completeness(lp_data),
            'anomaly_detection': self._detect_data_anomalies(lp_data)
        }
        
        self._print_quality_summary()
        return self.data_quality_report
    
    def _analyze_customer_info(self, customer_data):
        """ê³ ê° ê¸°ë³¸ì •ë³´ ë¶„ì„"""
        if customer_data is None:
            return {"message": "ê³ ê° ë°ì´í„° ì—†ìŒ"}
        
        print(f"  ğŸ“‹ ê³ ê° ë°ì´í„° ì»¬ëŸ¼: {list(customer_data.columns)}")
        
        # ê°€ëŠ¥í•œ ì»¬ëŸ¼ëª…ë“¤ ë§¤í•‘
        column_mapping = {
            'ê³„ì•½ì¢…ë³„': ['ê³„ì•½ì¢…ë³„', 'contract_type', 'Contract_Type'],
            'ì‚¬ìš©ìš©ë„': ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose'], 
            'ì‚°ì—…ë¶„ë¥˜': ['ì‚°ì—…ë¶„ë¥˜', 'industry', 'Industry']
        }
        
        info = {'total_customers': len(customer_data)}
        
        for key, possible_cols in column_mapping.items():
            found_col = None
            for col in possible_cols:
                if col in customer_data.columns:
                    found_col = col
                    break
            
            if found_col:
                info[f'{key}_dist'] = customer_data[found_col].value_counts().to_dict()
                print(f"  âœ… {key} ë¶„í¬: {dict(list(info[f'{key}_dist'].items())[:3])}...")  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
            else:
                info[f'{key}_dist'] = {}
                print(f"  âš ï¸ {key} ì»¬ëŸ¼ ì—†ìŒ")
        
        print(f"âœ… ê³ ê°ìˆ˜: {info['total_customers']:,}ëª…")
        return info
    
    def _check_lp_data_quality(self, lp_data):
        """LP ë°ì´í„° í’ˆì§ˆ ì ê²€"""
        # ë°ì´í„° íƒ€ì… ë³€í™˜
        lp_data['LPìˆ˜ì‹ ì¼ì'] = pd.to_datetime(lp_data['LPìˆ˜ì‹ ì¼ì'])
        
        quality = {
            'total_records': len(lp_data),
            'date_range': {
                'start': lp_data['LPìˆ˜ì‹ ì¼ì'].min(),
                'end': lp_data['LPìˆ˜ì‹ ì¼ì'].max()
            },
            'unique_customers': lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),
            'missing_values': lp_data.isnull().sum().to_dict(),
            'negative_values': (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] < 0).sum(),
            'zero_values': (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] == 0).sum(),
        }
        
        print(f"âœ… ì´ ë ˆì½”ë“œ: {quality['total_records']:,}ê±´")
        print(f"âœ… ê¸°ê°„: {quality['date_range']['start']} ~ {quality['date_range']['end']}")
        print(f"âœ… ê³ ê°ìˆ˜: {quality['unique_customers']:,}ëª…")
        print(f"âœ… ìŒìˆ˜ê°’: {quality['negative_values']:,}ê±´")
        print(f"âœ… 0ê°’: {quality['zero_values']:,}ê±´")
        
        return quality
    
    def _calculate_completeness(self, lp_data):
        """ë°ì´í„° ì™„ì •ì„± ê³„ì‚°"""
        lp_data['date'] = lp_data['LPìˆ˜ì‹ ì¼ì'].dt.date
        lp_data['hour'] = lp_data['LPìˆ˜ì‹ ì¼ì'].dt.hour
        lp_data['quarter_hour'] = (lp_data['LPìˆ˜ì‹ ì¼ì'].dt.minute // 15) * 15
        
        # 15ë¶„ ê°„ê²© ì •í™•ì„± ì²´í¬
        expected_intervals = pd.date_range(
            start=lp_data['LPìˆ˜ì‹ ì¼ì'].min(),
            end=lp_data['LPìˆ˜ì‹ ì¼ì'].max(),
            freq='15min'
        )
        
        completeness = {
            'expected_records': len(expected_intervals) * lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique(),
            'actual_records': len(lp_data),
            'completeness_rate': len(lp_data) / (len(expected_intervals) * lp_data['ëŒ€ì²´ê³ ê°ë²ˆí˜¸'].nunique()) * 100
        }
        
        # ê³ ê°ë³„ ì™„ì •ì„±
        customer_completeness = lp_data.groupby('ëŒ€ì²´ê³ ê°ë²ˆí˜¸').size() / len(expected_intervals) * 100
        completeness['customer_completeness'] = {
            'mean': customer_completeness.mean(),
            'min': customer_completeness.min(),
            'max': customer_completeness.max(),
            'std': customer_completeness.std()
        }
        
        print(f"âœ… ë°ì´í„° ì™„ì •ì„±: {completeness['completeness_rate']:.2f}%")
        
        return completeness
    
    def _detect_data_anomalies(self, lp_data):
        """ë°ì´í„° ì´ìƒì¹˜ íƒì§€"""
        # í†µê³„ì  ì´ìƒì¹˜ íƒì§€
        Q1 = lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].quantile(0.25)
        Q3 = lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].quantile(0.75)
        IQR = Q3 - Q1
        
        outliers_iqr = lp_data[
            (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] < Q1 - 1.5 * IQR) | 
            (lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] > Q3 + 1.5 * IQR)
        ]
        
        # Z-score ì´ìƒì¹˜
        z_scores = np.abs((lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'] - lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].mean()) / lp_data['ìˆœë°©í–¥ìœ íš¨ì „ë ¥'].std())
        outliers_zscore = lp_data[z_scores > 3]
        
        anomalies = {
            'iqr_outliers': len(outliers_iqr),
            'zscore_outliers': len(outliers_zscore),
            'outlier_rate_iqr': len(outliers_iqr) / len(lp_data) * 100,
            'outlier_rate_zscore': len(outliers_zscore) / len(lp_data) * 100
        }
        
        print(f"âœ… IQR ì´ìƒì¹˜: {anomalies['outlier_rate_iqr']:.3f}%")
        print(f"âœ… Z-score ì´ìƒì¹˜: {anomalies['outlier_rate_zscore']:.3f}%")
        
        return anomalies
    
    def _print_quality_summary(self):
        """í’ˆì§ˆ ì ê²€ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ ì ê²€ ì™„ë£Œ")
        print("="*50)
    
    # ============ 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) ============
    
    def analyze_basic_patterns(self, lp_data, customer_data=None, calendar_data=None):
        """
        ê¸°ë³¸ ì „ë ¥ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
        """
        print("\nğŸ“Š 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ ì‹œì‘...")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        processed_data = self._preprocess_for_pattern_analysis(lp_data, customer_data, calendar_data)
        
        # ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„
        time_patterns = self._analyze_time_patterns(processed_data)
        
        # ê³ ê° ì„¸ë¶„í™” ê¸°ì´ˆ ë¶„ì„
        customer_segmentation = self._analyze_customer_segmentation(processed_data)
        
        self.pattern_analysis = {
            'time_patterns': time_patterns,
            'customer_segmentation': customer_segmentation,
            'processed_data': processed_data
        }
        
        return self.pattern_analysis
    
    def _preprocess_for_pattern_analysis(self, lp_data, customer_data, calendar_data):
        """íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬"""
        print("  ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        # ì‹œê°„ ë³€ìˆ˜ ìƒì„±
        lp_data['datetime'] = pd.to_datetime(lp_data['LPìˆ˜ì‹ ì¼ì'])
        lp_data['date'] = lp_data['datetime'].dt.date
        lp_data['hour'] = lp_data['datetime'].dt.hour
        lp_data['weekday'] = lp_data['datetime'].dt.weekday  # 0=ì›”ìš”ì¼
        lp_data['month'] = lp_data['datetime'].dt.month
        lp_data['quarter'] = lp_data['datetime'].dt.quarter
        lp_data['is_weekend'] = lp_data['weekday'].isin([5, 6])  # í† , ì¼
        
        # ì¼ê°„ ì§‘ê³„ ë°ì´í„° ìƒì„±
        daily_agg = lp_data.groupby(['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'date']).agg({
            'ìˆœë°©í–¥ìœ íš¨ì „ë ¥': ['sum', 'mean', 'max', 'min', 'std']
        }).reset_index()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        daily_agg.columns = ['customer_id', 'date', 'daily_sum', 'daily_mean', 'daily_max', 'daily_min', 'daily_std']
        
        # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜ ë‹¤ì‹œ ìƒì„± (ì¼ê°„ ì§‘ê³„ í›„)
        daily_agg['date_dt'] = pd.to_datetime(daily_agg['date'])
        daily_agg['weekday'] = daily_agg['date_dt'].dt.weekday  # 0=ì›”ìš”ì¼
        daily_agg['month'] = daily_agg['date_dt'].dt.month
        daily_agg['quarter'] = daily_agg['date_dt'].dt.quarter
        daily_agg['is_weekend'] = daily_agg['weekday'].isin([5, 6])  # í† , ì¼
        
        print(f"  âœ… ì‹œê°„ í”¼ì²˜ ìƒì„± ì™„ë£Œ: weekday, month, quarter, is_weekend")
        
        # ê³ ê° ì •ë³´ ë³‘í•©
        if customer_data is not None:
            # ê³ ê° ë°ì´í„°ì˜ ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸
            customer_key_col = None
            possible_keys = ['ëŒ€ì²´ê³ ê°ë²ˆí˜¸', 'ê³ ê°ë²ˆí˜¸', 'customer_id', 'Customer_ID']
            
            for col in possible_keys:
                if col in customer_data.columns:
                    customer_key_col = col
                    break
            
            if customer_key_col:
                daily_agg = daily_agg.merge(customer_data, left_on='customer_id', right_on=customer_key_col, how='left')
                print(f"  âœ… ê³ ê° ì •ë³´ ë³‘í•© ì™„ë£Œ (í‚¤: {customer_key_col})")
            else:
                print(f"  âš ï¸ ê³ ê° ë°ì´í„° ë³‘í•© ì‹¤íŒ¨ - í‚¤ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(customer_data.columns)}")
                print(f"  â„¹ï¸ ê³ ê° ì •ë³´ ì—†ì´ ë¶„ì„ ê³„ì†...")
        
        # ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•©
        if calendar_data is not None:
            # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ ë° ë³€í™˜
            date_col = None
            possible_date_cols = ['date', 'ë‚ ì§œ', 'Date', 'DATE']
            
            for col in possible_date_cols:
                if col in calendar_data.columns:
                    date_col = col
                    break
            
            if date_col:
                # ê¸°ìƒ ë°ì´í„°ê°€ weather_daily_processed.csvì¸ ê²½ìš° ë‚ ì§œ í˜•ì‹ í™•ì¸
                if 'year' in calendar_data.columns and 'month' in calendar_data.columns and 'day' in calendar_data.columns:
                    # year, month, day ì»¬ëŸ¼ìœ¼ë¡œ ë‚ ì§œ ìƒì„±
                    calendar_data['date_parsed'] = pd.to_datetime(calendar_data[['year', 'month', 'day']])
                    calendar_data['date_for_merge'] = calendar_data['date_parsed'].dt.date
                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on='date_for_merge', how='left')
                    print(f"  âœ… ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•© ì™„ë£Œ (year-month-day ê¸°ì¤€)")
                else:
                    # ì¼ë°˜ì ì¸ date ì»¬ëŸ¼ ì‚¬ìš©
                    calendar_data[date_col] = pd.to_datetime(calendar_data[date_col]).dt.date
                    daily_agg = daily_agg.merge(calendar_data, left_on='date', right_on=date_col, how='left')
                    print(f"  âœ… ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ë³‘í•© ì™„ë£Œ (í‚¤: {date_col})")
            else:
                print(f"  âš ï¸ ê¸°ìƒ/ë‹¬ë ¥ ë°ì´í„° ë³‘í•© ì‹¤íŒ¨ - ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(calendar_data.columns)}")
                print(f"  â„¹ï¸ ê¸°ìƒ/ë‹¬ë ¥ ì •ë³´ ì—†ì´ ë¶„ì„ ê³„ì†...")
        
        print(f"  âœ… ì¼ê°„ ì§‘ê³„ ë°ì´í„°: {len(daily_agg):,}ê±´")
        return daily_agg
    
    def _analyze_time_patterns(self, data):
        """ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„"""
        print("  ğŸ“ˆ ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        patterns = {}
        
        # 1. ì‹œê°„ëŒ€ë³„ íŒ¨í„´ (ì¼ê°„ ì§‘ê³„ ë°ì´í„°ì—ì„œëŠ” ì˜ë¯¸ ì—†ìœ¼ë¯€ë¡œ ê±´ë„ˆë›°ê¸°)
        # hourly_pattern = data.groupby('hour')['daily_mean'].agg(['mean', 'std', 'count'])
        # patterns['hourly'] = hourly_pattern
        
        # 2. ìš”ì¼ë³„ íŒ¨í„´
        if 'weekday' in data.columns:
            weekday_pattern = data.groupby('weekday')['daily_mean'].agg(['mean', 'std', 'count'])
            patterns['weekday'] = weekday_pattern
        
        # 3. ì›”ë³„ íŒ¨í„´ (ê³„ì ˆì„±)
        if 'month' in data.columns:
            monthly_pattern = data.groupby('month')['daily_mean'].agg(['mean', 'std', 'count'])
            patterns['monthly'] = monthly_pattern
        
        # 4. ì£¼ì¤‘/ì£¼ë§ íŒ¨í„´
        if 'is_weekend' in data.columns:
            weekend_pattern = data.groupby('is_weekend')['daily_mean'].agg(['mean', 'std', 'count'])
            patterns['weekend'] = weekend_pattern
        
        # 5. ì—…ì¢…ë³„ íŒ¨í„´ (ê³ ê° ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        usage_purpose_col = None
        possible_usage_cols = ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose']
        
        for col in possible_usage_cols:
            if col in data.columns:
                usage_purpose_col = col
                break
        
        if usage_purpose_col:
            industry_pattern = data.groupby(usage_purpose_col)['daily_mean'].agg(['mean', 'std', 'count'])
            patterns['industry'] = industry_pattern
            print(f"  âœ… {usage_purpose_col} ê¸°ì¤€ ì—…ì¢…ë³„ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
        
        print(f"  âœ… ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({len(patterns)}ê°œ íŒ¨í„´)")
        return patterns
    
    def _analyze_customer_segmentation(self, data):
        """ê³ ê° ì„¸ë¶„í™” ê¸°ì´ˆ ë¶„ì„"""
        print("  ğŸ‘¥ ê³ ê° ì„¸ë¶„í™” ë¶„ì„ ì¤‘...")
        
        # ê³ ê°ë³„ í‰ê·  ì‚¬ìš©ëŸ‰ ê³„ì‚°
        customer_avg = data.groupby('customer_id')['daily_mean'].mean()
        
        # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë¶„ë¥˜
        segmentation = {
            'large_users': customer_avg.quantile(0.9),  # ìƒìœ„ 10%
            'medium_users': customer_avg.quantile(0.5),  # ì¤‘ê°„ 50%
            'small_users': customer_avg.quantile(0.1),   # í•˜ìœ„ 10%
        }
        
        # ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ ë¶„í¬
        customer_stats = {
            'customer_count': len(customer_avg),
            'usage_distribution': {
                'mean': customer_avg.mean(),
                'std': customer_avg.std(),
                'min': customer_avg.min(),
                'max': customer_avg.max(),
                'q25': customer_avg.quantile(0.25),
                'q50': customer_avg.quantile(0.50),
                'q75': customer_avg.quantile(0.75)
            },
            'segmentation_thresholds': segmentation
        }
        
        print(f"  âœ… {customer_stats['customer_count']:,}ëª… ê³ ê° ì„¸ë¶„í™” ì™„ë£Œ")
        return customer_stats
    
    # ============ 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„) ============
    
    def analyze_variability(self, processed_data):
        """
        ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ - ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ê¸°ì´ˆ ì‘ì—…
        """
        print("\nğŸ“ˆ 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ ì‹œì‘...")
        
        # ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°
        basic_variability = self._calculate_basic_variability(processed_data)
        
        # ë³€ë™ì„± íŒ¨í„´ ë¶„ì„
        variability_patterns = self._analyze_variability_patterns(processed_data)
        
        self.variability_analysis = {
            'basic_variability': basic_variability,
            'variability_patterns': variability_patterns
        }
        
        return self.variability_analysis
    
    def _calculate_basic_variability(self, data):
        """ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚°"""
        print("  ğŸ“Š ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì¤‘...")
        
        variability_metrics = {}
        
        # ê³ ê°ë³„ ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        customer_cv = data.groupby('customer_id').apply(
            lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan
        )
        
        # 1. ì¼ê°„ ë³€ë™ê³„ìˆ˜
        variability_metrics['daily_cv'] = {
            'mean': customer_cv.mean(),
            'std': customer_cv.std(),
            'distribution': customer_cv.describe()
        }
        
        # 2. ì£¼ê°„ ë³€ë™ê³„ìˆ˜ (ì£¼ë³„ íŒ¨í„´ì˜ ì¼ê´€ì„±)
        try:
            # ì£¼ ë²ˆí˜¸ ìƒì„±
            data_with_week = data.copy()
            data_with_week['week'] = pd.to_datetime(data_with_week['date']).dt.isocalendar().week
            
            weekly_cv = data_with_week.groupby(['customer_id', 'week']).agg({
                'daily_mean': ['mean', 'std']
            }).reset_index()
            weekly_cv.columns = ['customer_id', 'week', 'weekly_mean', 'weekly_std']
            weekly_cv['weekly_cv'] = weekly_cv['weekly_std'] / weekly_cv['weekly_mean']
            
            customer_weekly_cv = weekly_cv.groupby('customer_id')['weekly_cv'].mean()
            variability_metrics['weekly_cv'] = {
                'mean': customer_weekly_cv.mean(),
                'std': customer_weekly_cv.std(),
                'distribution': customer_weekly_cv.describe()
            }
        except Exception as e:
            print(f"    âš ï¸ ì£¼ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            variability_metrics['weekly_cv'] = {'mean': np.nan, 'std': np.nan}
        
        # 3. ì›”ê°„ ë³€ë™ê³„ìˆ˜
        try:
            if 'month' in data.columns:
                monthly_cv = data.groupby(['customer_id', 'month']).agg({
                    'daily_mean': ['mean', 'std']
                }).reset_index()
                monthly_cv.columns = ['customer_id', 'month', 'monthly_mean', 'monthly_std']
                monthly_cv['monthly_cv'] = monthly_cv['monthly_std'] / monthly_cv['monthly_mean']
                
                customer_monthly_cv = monthly_cv.groupby('customer_id')['monthly_cv'].mean()
                variability_metrics['monthly_cv'] = {
                    'mean': customer_monthly_cv.mean(),
                    'std': customer_monthly_cv.std(),
                    'distribution': customer_monthly_cv.describe()
                }
            else:
                print("    âš ï¸ month ì»¬ëŸ¼ì´ ì—†ì–´ ì›”ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ê±´ë„ˆë›°ê¸°")
                variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}
        except Exception as e:
            print(f"    âš ï¸ ì›”ê°„ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            variability_metrics['monthly_cv'] = {'mean': np.nan, 'std': np.nan}
        
        # 4. ì¶”ê°€ ë³€ë™ì„± ì§€í‘œ
        # ë²”ìœ„ ê¸°ë°˜ ë³€ë™ì„±
        try:
            customer_range_cv = data.groupby('customer_id').apply(
                lambda x: (x['daily_mean'].max() - x['daily_mean'].min()) / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan
            )
            
            variability_metrics['range_based_cv'] = {
                'mean': customer_range_cv.mean(),
                'std': customer_range_cv.std(),
                'distribution': customer_range_cv.describe()
            }
        except Exception as e:
            print(f"    âš ï¸ ë²”ìœ„ ê¸°ë°˜ ë³€ë™ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            variability_metrics['range_based_cv'] = {'mean': np.nan, 'std': np.nan}
        
        print(f"  âœ… ê¸°ë³¸ ë³€ë™ì„± ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        return variability_metrics
    
    def _analyze_variability_patterns(self, data):
        """ë³€ë™ì„± íŒ¨í„´ ë¶„ì„"""
        print("  ğŸ” ë³€ë™ì„± íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        patterns = {}
        
        # 1. ì—…ì¢…ë³„ ë³€ë™ì„± ë¹„êµ
        usage_purpose_col = None
        possible_usage_cols = ['ì‚¬ìš©ìš©ë„', 'usage_purpose', 'Usage_Purpose']
        
        for col in possible_usage_cols:
            if col in data.columns:
                usage_purpose_col = col
                break
        
        if usage_purpose_col:
            try:
                industry_variability = data.groupby([usage_purpose_col, 'customer_id']).apply(
                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan
                ).reset_index()
                industry_variability.columns = [usage_purpose_col, 'customer_id', 'cv']
                
                industry_cv_summary = industry_variability.groupby(usage_purpose_col)['cv'].agg(['mean', 'std', 'count'])
                patterns['industry_variability'] = industry_cv_summary
                print(f"  âœ… {usage_purpose_col} ê¸°ì¤€ ì—…ì¢…ë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ")
            except Exception as e:
                print(f"  âš ï¸ ì—…ì¢…ë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # 2. ê³„ì ˆë³„ ë³€ë™ì„± ì°¨ì´
        try:
            if 'month' in data.columns:
                seasonal_variability = data.groupby(['customer_id', 'month']).apply(
                    lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan
                ).reset_index()
                seasonal_variability.columns = ['customer_id', 'month', 'cv']
                
                seasonal_cv_summary = seasonal_variability.groupby('month')['cv'].agg(['mean', 'std', 'count'])
                patterns['seasonal_variability'] = seasonal_cv_summary
                print(f"  âœ… ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ")
            else:
                print("  âš ï¸ month ì»¬ëŸ¼ì´ ì—†ì–´ ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ê±´ë„ˆë›°ê¸°")
        except Exception as e:
            print(f"  âš ï¸ ê³„ì ˆë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # 3. ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„±
        try:
            customer_avg_usage = data.groupby('customer_id')['daily_mean'].mean()
            customer_cv = data.groupby('customer_id').apply(
                lambda x: x['daily_mean'].std() / x['daily_mean'].mean() if x['daily_mean'].mean() > 0 else np.nan
            )
            
            # ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ê·¸ë£¹í•‘
            usage_quantiles = customer_avg_usage.quantile([0.33, 0.67])
            def categorize_usage(usage):
                if usage <= usage_quantiles.iloc[0]:
                    return 'Low'
                elif usage <= usage_quantiles.iloc[1]:
                    return 'Medium'
                else:
                    return 'High'
            
            customer_usage_category = customer_avg_usage.apply(categorize_usage)
            usage_cv_df = pd.DataFrame({
                'usage_category': customer_usage_category,
                'cv': customer_cv
            })
            
            usage_cv_summary = usage_cv_df.groupby('usage_category')['cv'].agg(['mean', 'std', 'count'])
            patterns['usage_level_variability'] = usage_cv_summary
            print(f"  âœ… ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„± ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš ï¸ ì‚¬ìš©ëŸ‰ ê·œëª¨ë³„ ë³€ë™ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        print(f"  âœ… ë³€ë™ì„± íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({len(patterns)}ê°œ íŒ¨í„´)")
        return patterns
    
    # ============ 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„) ============
    
    def detect_anomalous_patterns(self, processed_data):
        """
        ì´ìƒ íŒ¨í„´ íƒì§€
        """
        print("\nğŸ¯ 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ ì‹œì‘...")
        
        # í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„
        statistical_outliers = self._identify_statistical_outliers(processed_data)
        
        # ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€
        temporal_anomalies = self._detect_temporal_anomalies(processed_data)
        
        # ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜
        abnormal_patterns = self._define_abnormal_patterns(processed_data)
        
        anomaly_results = {
            'statistical_outliers': statistical_outliers,
            'temporal_anomalies': temporal_anomalies,
            'abnormal_patterns': abnormal_patterns
        }
        
        return anomaly_results
    
    def _identify_statistical_outliers(self, data):
        """í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„"""
        print("  ğŸ” í†µê³„ì  ì´ìƒì¹˜ ì‹ë³„ ì¤‘...")
        
        outliers = {}
        
        # IQR ë°©ë²•
        Q1 = data['daily_mean'].quantile(0.25)
        Q3 = data['daily_mean'].quantile(0.75)
        IQR = Q3 - Q1
        
        iqr_outliers = data[
            (data['daily_mean'] < Q1 - 1.5 * IQR) | 
            (data['daily_mean'] > Q3 + 1.5 * IQR)
        ]
        
        outliers['iqr_outliers'] = {
            'count': len(iqr_outliers),
            'rate': len(iqr_outliers) / len(data) * 100,
            'customer_count': iqr_outliers['customer_id'].nunique()
        }
        
        # Z-score ë°©ë²•
        z_scores = np.abs((data['daily_mean'] - data['daily_mean'].mean()) / data['daily_mean'].std())
        zscore_outliers = data[z_scores > 3]
        
        outliers['zscore_outliers'] = {
            'count': len(zscore_outliers),
            'rate': len(zscore_outliers) / len(data) * 100,
            'customer_count': zscore_outliers['customer_id'].nunique()
        }
        
        print(f"  âœ… IQR ì´ìƒì¹˜: {outliers['iqr_outliers']['count']:,}ê±´ ({outliers['iqr_outliers']['rate']:.2f}%)")
        print(f"  âœ… Z-score ì´ìƒì¹˜: {outliers['zscore_outliers']['count']:,}ê±´ ({outliers['zscore_outliers']['rate']:.2f}%)")
        
        return outliers
    
    def _detect_temporal_anomalies(self, data):
        """ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€"""
        print("  â° ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€ ì¤‘...")
        
        temporal_anomalies = {}
        
        # ê³ ê°ë³„ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€
        for customer_id in data['customer_id'].unique()[:100]:  # ìƒ˜í”Œë¡œ 100ëª…ë§Œ
            customer_data = data[data['customer_id'] == customer_id].sort_values('date')
            
            if len(customer_data) < 30:  # ìµœì†Œ 30ì¼ ë°ì´í„° í•„ìš”
                continue
            
            # ê¸‰ê²©í•œ ì¦ê°€/ê°ì†Œ íƒì§€ (>200% ë³€í™”)
            customer_data['pct_change'] = customer_data['daily_mean'].pct_change()
            sudden_changes = customer_data[abs(customer_data['pct_change']) > 2.0]  # 200% ë³€í™”
            
            # ì—°ì†ì ì¸ 0ê°’ íƒì§€
            zero_streaks = customer_data[customer_data['daily_mean'] == 0]
            
            if len(sudden_changes) > 0 or len(zero_streaks) > 5:  # 5ì¼ ì´ìƒ ì—°ì† 0ê°’
                temporal_anomalies[customer_id] = {
                    'sudden_changes': len(sudden_changes),
                    'zero_streaks': len(zero_streaks)
                }
        
        print(f"  âœ… {len(temporal_anomalies):,}ëª… ê³ ê°ì—ì„œ ì‹œê³„ì—´ ì´ìƒ íƒì§€")
        
        return temporal_anomalies
    
    def _define_abnormal_patterns(self, data):
        """ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜"""
        print("  ğŸ“‹ ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜ ì¤‘...")
        
        abnormal_patterns = {
            'pattern_definitions': {
                1: 'ì „ë ¥ ì‚¬ìš© ê¸‰ì¦/ê¸‰ê° (ì‚¬ì—… í™•ì¥/ì¶•ì†Œ)',
                2: 'ì‚¬ìš© íŒ¨í„´ ë³€í™” (ìš´ì˜ì‹œê°„ ë³€ê²½)', 
                3: 'íš¨ìœ¨ì„± ê¸‰ë³€ (ì„¤ë¹„ êµì²´/ê³ ì¥)',
                4: 'ê³„ì ˆì„± ì´íƒˆ (ì‚¬ì—… ëª¨ë¸ ë³€í™”)'
            },
            'detection_criteria': {
                'usage_spike': 'daily_mean > mean + 3*std',
                'usage_drop': 'daily_mean < mean - 3*std',
                'pattern_shift': 'monthly pattern change > 50%',
                'efficiency_change': 'weekly efficiency variance > threshold'
            }
        }
        
        print(f"  âœ… {len(abnormal_patterns['pattern_definitions'])}ê°€ì§€ ë¹„ì •ìƒ íŒ¨í„´ ì •ì˜ ì™„ë£Œ")
        
        return abnormal_patterns
    
    # ============ 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„) ============
    
    def decide_preprocessing_strategy(self, data_quality_report, pattern_analysis, variability_analysis):
        """
        ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì •
        """
        print("\nğŸ”§ 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì •...")
        
        preprocessing_strategy = {
            'missing_data_handling': self._decide_missing_data_strategy(data_quality_report),
            'outlier_handling': self._decide_outlier_strategy(data_quality_report),
            'normalization_method': self._decide_normalization_strategy(pattern_analysis),
            'feature_engineering': self._decide_feature_engineering(pattern_analysis, variability_analysis)
        }
        
        self._print_preprocessing_summary(preprocessing_strategy)
        
        return preprocessing_strategy
    
    def _decide_missing_data_strategy(self, quality_report):
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ ê²°ì •"""
        completeness_rate = quality_report['data_completeness']['completeness_rate']
        
        if completeness_rate > 95:
            strategy = "ì„ í˜•ë³´ê°„ ë˜ëŠ” forward fill"
        elif completeness_rate > 80:
            strategy = "ê³„ì ˆì„± ê³ ë ¤ ë³´ê°„"
        else:
            strategy = "ì¥ê¸° ê²°ì¸¡ ê¸°ê°„ ë¶„ì„ ì œì™¸"
        
        return {
            'completeness_rate': completeness_rate,
            'recommended_strategy': strategy
        }
    
    def _decide_outlier_strategy(self, quality_report):
        """ì´ìƒì¹˜ ì²˜ë¦¬ ì „ëµ ê²°ì •"""
        outlier_rate = quality_report['anomaly_detection']['outlier_rate_iqr']
        
        if outlier_rate < 1:
            strategy = "ì´ìƒì¹˜ ìœ ì§€ (ì •ìƒ ë²”ìœ„)"
        elif outlier_rate < 5:
            strategy = "extreme outlierë§Œ ì œê±°"
        else:
            strategy = "robust í†µê³„ëŸ‰ ì‚¬ìš©"
        
        return {
            'outlier_rate': outlier_rate,
            'recommended_strategy': strategy
        }
    
    def _decide_normalization_strategy(self, pattern_analysis):
        """ì •ê·œí™” ë°©ë²• ê²°ì •"""
        customer_stats = pattern_analysis['customer_segmentation']
        usage_std = customer_stats['usage_distribution']['std']
        usage_mean = customer_stats['usage_distribution']['mean']
        cv = usage_std / usage_mean if usage_mean > 0 else 0
        
        if cv > 1.0:
            strategy = "ê³ ê°ë³„ í‘œì¤€í™” + ë¡œê·¸ ë³€í™˜"
        elif cv > 0.5:
            strategy = "ê³ ê°ë³„ í‘œì¤€í™”"
        else:
            strategy = "ì „ì²´ Min-Max ì •ê·œí™”"
        
        return {
            'coefficient_of_variation': cv,
            'recommended_strategy': strategy
        }
    
    def _decide_feature_engineering(self, pattern_analysis, variability_analysis):
        """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì „ëµ ê²°ì •"""
        features_to_create = []
        
        # ì‹œê°„ ê¸°ë°˜ í”¼ì²˜
        time_patterns = pattern_analysis['time_patterns']
        if 'monthly' in time_patterns:
            features_to_create.extend([
                'month_sin', 'month_cos',  # ê³„ì ˆ ìˆœí™˜ í”¼ì²˜
                'is_summer', 'is_winter',  # ê³„ì ˆ ë”ë¯¸ ë³€ìˆ˜
            ])
        
        # ìš”ì¼ ê¸°ë°˜ í”¼ì²˜
        if 'weekday' in time_patterns:
            features_to_create.extend([
                'weekday_sin', 'weekday_cos',  # ìš”ì¼ ìˆœí™˜ í”¼ì²˜
                'is_weekend'                   # ì£¼ë§ ì—¬ë¶€
            ])
        
        # ë³€ë™ì„± ê¸°ë°˜ í”¼ì²˜
        if variability_analysis:
            features_to_create.extend([
                'rolling_mean_7d',       # 7ì¼ ì´ë™í‰ê· 
                'rolling_std_7d',        # 7ì¼ ì´ë™í‘œì¤€í¸ì°¨
                'usage_volatility',      # ë³€ë™ì„± ì§€ìˆ˜
            ])
        
        # ê³ ê° ê¸°ë°˜ í”¼ì²˜
        features_to_create.extend([
            'customer_avg_usage',      # ê³ ê° í‰ê·  ì‚¬ìš©ëŸ‰
            'customer_usage_rank',     # ê³ ê° ì‚¬ìš©ëŸ‰ ìˆœìœ„
            'deviation_from_avg'       # í‰ê·  ëŒ€ë¹„ í¸ì°¨
        ])
        
        return {
            'features_to_create': features_to_create,
            'total_features': len(features_to_create)
        }
    
    def _print_preprocessing_summary(self, strategy):
        """ì „ì²˜ë¦¬ ì „ëµ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ”§ ì „ì²˜ë¦¬ ì „ëµ ê²°ì • ì™„ë£Œ")
        print("="*60)
        
        print(f"ğŸ“‹ ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {strategy['missing_data_handling']['recommended_strategy']}")
        print(f"ğŸ¯ ì´ìƒì¹˜ ì²˜ë¦¬: {strategy['outlier_handling']['recommended_strategy']}")
        print(f"ğŸ“Š ì •ê·œí™” ë°©ë²•: {strategy['normalization_method']['recommended_strategy']}")
        print(f"ğŸ› ï¸ ìƒì„±í•  í”¼ì²˜: {strategy['feature_engineering']['total_features']}ê°œ")
        
        print("\nğŸ’¡ ë³€ë™ê³„ìˆ˜ ì„¤ê³„ë¥¼ ìœ„í•œ ì¸ì‚¬ì´íŠ¸:")
        print("- ì–´ë–¤ ë³€ë™ì„± ì§€í‘œê°€ ì‹¤ì œ ì‚¬ì—… ë³€í™”ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ”ê°€?")
        print("- ì—…ì¢…ë³„ë¡œ ë‹¤ë¥¸ ì„ê³„ê°’ì´ í•„ìš”í•œê°€?")
        print("- ì‹œê°„ ìœˆë„ìš°ëŠ” ì–¼ë§ˆë‚˜ ì„¤ì •í•´ì•¼ í•˜ëŠ”ê°€?")
        print("- ê³„ì ˆì„± ë³´ì •ì´ í•„ìš”í•œê°€?")
    
    # ============ ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„± ============
    
    def create_eda_visualizations(self, processed_data):
        """íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì‹œê°í™”"""
        print("\nğŸ“ˆ EDA ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # 1. ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´ (ì‹œê°„ëŒ€ë³„ ëŒ€ì‹ )
        self._plot_weekday_patterns_daily(processed_data)
        
        # 2. ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´ (ë°” ì°¨íŠ¸)
        self._plot_weekday_patterns(processed_data)
        
        # 3. ì›”ë³„ ì‚¬ìš©ëŸ‰ ë°•ìŠ¤í”Œë¡¯
        self._plot_monthly_boxplot(processed_data)
        
        # 4. ê³ ê°ë³„ ì‚¬ìš©ëŸ‰ ë¶„í¬
        self._plot_customer_distribution(processed_data)
        
        print("âœ… ì‹œê°í™” ìƒì„± ì™„ë£Œ")
    
    def _plot_weekday_patterns_daily(self, data):
        """ìš”ì¼ë³„ í‰ê·  ì‚¬ìš© íŒ¨í„´ (ë¼ì¸ ì°¨íŠ¸)"""
        if 'weekday' in data.columns:
            weekday_avg = data.groupby('weekday')['daily_mean'].mean()
            
            plt.figure(figsize=(10, 6))
            plt.plot(range(7), weekday_avg.values, marker='o', linewidth=2, markersize=8)
            plt.title('Daily Power Usage by Weekday', fontsize=14, fontweight='bold')
            plt.xlabel('Weekday')
            plt.ylabel('Average Usage (kWh)')
            plt.xticks(range(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.show()
        else:
            print("  âš ï¸ ìš”ì¼ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - weekday ì»¬ëŸ¼ ì—†ìŒ")
    
    def _plot_weekday_patterns(self, data):
        """ìš”ì¼ë³„ ì‚¬ìš© íŒ¨í„´"""
        if 'weekday' not in data.columns:
            print("  âš ï¸ ìš”ì¼ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - weekday ì»¬ëŸ¼ ì—†ìŒ")
            return
            
        weekday_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        weekday_avg = data.groupby('weekday')['daily_mean'].mean()
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(range(7), weekday_avg.values, color=['skyblue' if i < 5 else 'lightcoral' for i in range(7)])
        plt.title('Average Power Usage by Weekday', fontsize=14, fontweight='bold')
        plt.xlabel('Weekday')
        plt.ylabel('Average Usage (kWh)')
        plt.xticks(range(7), weekday_names)
        plt.grid(True, alpha=0.3, axis='y')
        
        # ì£¼ì¤‘/ì£¼ë§ êµ¬ë¶„ í‘œì‹œ
        for i, bar in enumerate(bars):
            if i >= 5:  # ì£¼ë§
                bar.set_label('Weekend' if i == 5 else '')
            else:  # ì£¼ì¤‘
                bar.set_label('Weekday' if i == 0 else '')
        
        plt.legend()
        plt.tight_layout()
        plt.show()
    
    def _plot_monthly_boxplot(self, data):
        """ì›”ë³„ ì‚¬ìš©ëŸ‰ ë°•ìŠ¤í”Œë¡¯"""
        if 'month' not in data.columns:
            print("  âš ï¸ ì›”ë³„ íŒ¨í„´ ì‹œê°í™” ë¶ˆê°€ - month ì»¬ëŸ¼ ì—†ìŒ")
            return
            
        plt.figure(figsize=(14, 8))
        
        # ì›”ë³„ ë°ì´í„° ì¤€ë¹„
        monthly_data = [data[data['month'] == m]['daily_mean'].values for m in range(1, 13)]
        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                      'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        
        # ë°ì´í„°ê°€ ìˆëŠ” ì›”ë§Œ í‘œì‹œ
        valid_months = []
        valid_data = []
        valid_names = []
        
        for i, month_data in enumerate(monthly_data):
            if len(month_data) > 0:
                valid_months.append(i + 1)
                valid_data.append(month_data)
                valid_names.append(month_names[i])
        
        if not valid_data:
            print("  âš ï¸ ì›”ë³„ ë°ì´í„° ì—†ìŒ")
            return
        
        box_plot = plt.boxplot(valid_data, labels=valid_names, patch_artist=True)
        
        # ê³„ì ˆë³„ ìƒ‰ìƒ êµ¬ë¶„
        colors = []
        for month in valid_months:
            if month in [12, 1, 2]:  # ê²¨ìš¸
                colors.append('lightblue')
            elif month in [3, 4, 5]:  # ë´„
                colors.append('lightgreen')
            elif month in [6, 7, 8]:  # ì—¬ë¦„
                colors.append('lightcoral')
            else:  # ê°€ì„
                colors.append('orange')
        
        for patch, color in zip(box_plot['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        plt.title('Monthly Power Usage Distribution', fontsize=14, fontweight='bold')
        plt.xlabel('Month')
        plt.ylabel('Daily Average Usage (kWh)')
        plt.grid(True, alpha=0.3, axis='y')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()
    
    def _plot_customer_distribution(self, data):
        """ê³ ê°ë³„ í‰ê·  ì‚¬ìš©ëŸ‰ ë¶„í¬"""
        customer_avg = data.groupby('customer_id')['daily_mean'].mean()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # íˆìŠ¤í† ê·¸ë¨
        ax1.hist(customer_avg.values, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.set_title('Customer Average Usage Distribution', fontweight='bold')
        ax1.set_xlabel('Average Usage (kWh)')
        ax1.set_ylabel('Number of Customers')
        ax1.grid(True, alpha=0.3)
        
        # ë°•ìŠ¤í”Œë¡¯
        ax2.boxplot(customer_avg.values, vert=True)
        ax2.set_title('Customer Average Usage Boxplot', fontweight='bold')
        ax2.set_ylabel('Average Usage (kWh)')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def generate_eda_report(self):
        """EDA ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*70)
        print("KEPCO LP Data Preprocessing and EDA Report")
        print("="*70)
        
        if hasattr(self, 'data_quality_report'):
            print("\nğŸ” Step 1: Data Quality Check Results")
            print("-" * 40)
            quality = self.data_quality_report
            print(f"Total Records: {quality['lp_quality']['total_records']:,}")
            print(f"Customers: {quality['lp_quality']['unique_customers']:,}")
            print(f"Data Completeness: {quality['data_completeness']['completeness_rate']:.2f}%")
            print(f"Outlier Rate: {quality['anomaly_detection']['outlier_rate_iqr']:.3f}%")
        
        if hasattr(self, 'pattern_analysis'):
            print("\nğŸ“Š Step 2: Basic Pattern Analysis Results")
            print("-" * 40)
            pattern = self.pattern_analysis
            if 'customer_segmentation' in pattern:
                seg = pattern['customer_segmentation']
                print(f"Analyzed Customers: {seg['customer_count']:,}")
                print(f"Average Usage: {seg['usage_distribution']['mean']:.2f} kWh")
                print(f"Usage Std Dev: {seg['usage_distribution']['std']:.2f} kWh")
        
        if hasattr(self, 'variability_analysis'):
            print("\nğŸ“ˆ Step 3: Variability Analysis Results")
            print("-" * 40)
            var = self.variability_analysis
            if 'basic_variability' in var:
                basic = var['basic_variability']
                print(f"Average Daily CV: {basic['daily_cv']['mean']:.4f}")
                if not pd.isna(basic['weekly_cv']['mean']):
                    print(f"Average Weekly CV: {basic['weekly_cv']['mean']:.4f}")
                if not pd.isna(basic['monthly_cv']['mean']):
                    print(f"Average Monthly CV: {basic['monthly_cv']['mean']:.4f}")
        
        print("\nğŸ’¡ Next Steps Recommendations:")
        print("- Define and design variability coefficient")
        print("- Implement stacking ensemble model") 
        print("- Apply overfitting prevention techniques")
        print("- Develop business activity change prediction algorithm")
        
        print("\n" + "="*70)

# ì‚¬ìš© ì˜ˆì‹œ - LP ë°ì´í„° íŒŒì¼ë“¤ì„ ê²°í•©í•˜ì—¬ ë¶„ì„

# ë°ì´í„° ë¡œë”© (ì—¬ëŸ¬ LP íŒŒì¼ë“¤ ê²°í•©)
lp_files = ['LPë°ì´í„°1.csv', 'LPë°ì´í„°2.csv']  # í•œ ë‹¬ì„ ë°˜ìœ¼ë¡œ ë‚˜ëˆˆ íŒŒì¼ë“¤
customer_data = pd.read_excel('ê³ ê°ë²ˆí˜¸.xlsx')
weather_data = pd.read_csv('weather_daily_processed.csv') 
calendar_data = pd.read_csv('power_analysis_calendar_2022_2025.csv')

# ì „ì²˜ë¦¬ ë° EDA ì‹¤í–‰
preprocessor = KepcoDataPreprocessor()

# LP ë°ì´í„° ê²°í•©
combined_lp_data = preprocessor.load_and_combine_lp_data(lp_files)

# 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ì ê²€ (30ë¶„)
quality_report = preprocessor.check_data_quality(combined_lp_data, customer_data)

# 2ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ íƒìƒ‰ (60ë¶„) - ê¸°ìƒ ë°ì´í„°ë„ í•¨ê»˜ ë³‘í•©!
pattern_analysis = preprocessor.analyze_basic_patterns(combined_lp_data, customer_data, weather_data)

# 3ë‹¨ê³„: ë³€ë™ì„± ê¸°ì´ˆ ë¶„ì„ (90ë¶„)
variability_analysis = preprocessor.analyze_variability(pattern_analysis['processed_data'])

# 4ë‹¨ê³„: ì´ìƒ íŒ¨í„´ íƒì§€ (60ë¶„)
anomaly_results = preprocessor.detect_anomalous_patterns(pattern_analysis['processed_data'])

# 5ë‹¨ê³„: ì „ì²˜ë¦¬ ë°©í–¥ ê²°ì • (30ë¶„)
preprocessing_strategy = preprocessor.decide_preprocessing_strategy(
    quality_report, pattern_analysis, variability_analysis
)

# ì‹œê°í™” ìƒì„± (í°íŠ¸ ì˜¤ë¥˜ ì—†ì´)
preprocessor.create_eda_visualizations(pattern_analysis['processed_data'])

# ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
preprocessor.generate_eda_report()

print("âœ… Complete LP data preprocessing and EDA finished!")
print("ğŸ“¤ Next: Define variability coefficient and implement stacking model")
